{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/76","id":708534779,"node_id":"MDU6SXNzdWU3MDg1MzQ3Nzk=","number":76,"title":"ResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]","user":{"login":"zrajabi","id":11507560,"node_id":"MDQ6VXNlcjExNTA3NTYw","avatar_url":"https://avatars.githubusercontent.com/u/11507560?v=4","gravatar_id":"","url":"https://api.github.com/users/zrajabi","html_url":"https://github.com/zrajabi","followers_url":"https://api.github.com/users/zrajabi/followers","following_url":"https://api.github.com/users/zrajabi/following{/other_user}","gists_url":"https://api.github.com/users/zrajabi/gists{/gist_id}","starred_url":"https://api.github.com/users/zrajabi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zrajabi/subscriptions","organizations_url":"https://api.github.com/users/zrajabi/orgs","repos_url":"https://api.github.com/users/zrajabi/repos","events_url":"https://api.github.com/users/zrajabi/events{/privacy}","received_events_url":"https://api.github.com/users/zrajabi/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-09-24T23:29:37Z","updated_at":"2020-09-24T23:32:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I am training a model using bert and TF2.3. I have loaded both language-agnostic Bert Sentence Encoder model (laBSE) and another multilingual model from TF hub or bert_multi_cased_L-12_H-768_A-12, and I get the following OOM error.\r\n\r\n**ResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]**\r\n\r\nI have tried to reduce batch size as low as 8 then even 1, also lowered max_seq_length to 20, I have 4 GPUs of 16 GB, but I still get this error. What is the solution? Please help, thank you!\r\n---------------------------------------------------------------------------\r\nResourceExhaustedError                    Traceback (most recent call last)\r\n<ipython-input-33-425973c654ad> in <module>\r\n     33             batch_sample = (query_batch, pos_batch, neg_batch, position_bias_batch)\r\n     34 \r\n---> 35             batch_loss, loss_history, logits = train_step(query_batch, pos_batch, neg_batch, position_bias_batch, triplet_model, optimizer, loss_history, margin)\r\n     36             total_loss += batch_loss\r\n     37 \r\n\r\n<ipython-input-33-425973c654ad> in train_step(query_batch, pos_batch, neg_batch, position_bias_batch, triplet_model, optimizer, loss_history, margin)\r\n     18         loss_history.append(loss_value.numpy().mean())\r\n     19         grads = tape.gradient(loss_value, triplet_model.trainable_variables)\r\n---> 20         optimizer.apply_gradients(zip(grads, triplet_model.trainable_variables))\r\n     21 \r\n     22         return loss_value.numpy().mean(), loss_history, logits  # is it batch loss\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\r\n    547           args=(grads_and_vars,),\r\n    548           kwargs={\r\n--> 549               \"name\": name,\r\n    550           })\r\n    551 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in merge_call(self, merge_fn, args, kwargs)\r\n   2713     merge_fn = autograph.tf_convert(\r\n   2714         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\r\n-> 2715     return self._merge_call(merge_fn, args, kwargs)\r\n   2716 \r\n   2717   def _merge_call(self, merge_fn, args, kwargs):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in _merge_call(self, merge_fn, args, kwargs)\r\n   2720         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\r\n   2721     try:\r\n-> 2722       return merge_fn(self._strategy, *args, **kwargs)\r\n   2723     finally:\r\n   2724       _pop_per_thread_mode()\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    273   def wrapper(*args, **kwargs):\r\n    274     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\r\n--> 275       return func(*args, **kwargs)\r\n    276 \r\n    277   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _distributed_apply(self, distribution, grads_and_vars, name, apply_state)\r\n    631                               \"update_\" + var.op.name, skip_on_eager=True):\r\n    632             update_ops.extend(distribution.extended.update(\r\n--> 633                 var, apply_grad_to_update_var, args=(grad,), group=False))\r\n    634 \r\n    635       any_symbolic = any(isinstance(i, ops.Operation) or\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in update(self, var, fn, args, kwargs, group)\r\n   2298         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\r\n   2299     with self._container_strategy().scope():\r\n-> 2300       return self._update(var, fn, args, kwargs, group)\r\n   2301 \r\n   2302   def _update(self, var, fn, args, kwargs, group):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in _update(self, var, fn, args, kwargs, group)\r\n   2953     # The implementations of _update() and _update_non_slot() are identical\r\n   2954     # except _update() passes `var` as the first argument to `fn()`.\r\n-> 2955     return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)\r\n   2956 \r\n   2957   def _update_non_slot(self, colocate_with, fn, args, kwargs, should_group):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py in _update_non_slot(self, colocate_with, fn, args, kwargs, should_group)\r\n   2959     # once that value is used for something.\r\n   2960     with UpdateContext(colocate_with):\r\n-> 2961       result = fn(*args, **kwargs)\r\n   2962       if should_group:\r\n   2963         return result\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    273   def wrapper(*args, **kwargs):\r\n    274     with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.UNSPECIFIED):\r\n--> 275       return func(*args, **kwargs)\r\n    276 \r\n    277   if inspect.isfunction(func) or inspect.ismethod(func):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_grad_to_update_var(var, grad)\r\n    602           apply_kwargs[\"apply_state\"] = apply_state\r\n    603         return self._resource_apply_sparse_duplicate_indices(\r\n--> 604             grad.values, var, grad.indices, **apply_kwargs)\r\n    605 \r\n    606       if \"apply_state\" in self._dense_apply_args:\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _resource_apply_sparse_duplicate_indices(self, grad, handle, indices, **kwargs)\r\n   1124         values=grad, indices=indices)\r\n   1125     return self._resource_apply_sparse(summed_grad, handle, unique_indices,\r\n-> 1126                                        **kwargs)\r\n   1127 \r\n   1128   def _resource_apply_sparse(self, grad, handle, indices, apply_state):\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/adam.py in _resource_apply_sparse(self, grad, var, indices, apply_state)\r\n    224       v_sqrt = math_ops.sqrt(v_t)\r\n    225       var_update = state_ops.assign_sub(\r\n--> 226           var, coefficients['lr'] * m_t / (v_sqrt + coefficients['epsilon']),\r\n    227           use_locking=self._use_locking)\r\n    228       return control_flow_ops.group(*[var_update, m_t, v_t])\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\r\n   1123     with ops.name_scope(None, op_name, [x, y]) as name:\r\n   1124       try:\r\n-> 1125         return func(x, y, name=name)\r\n   1126       except (TypeError, ValueError) as e:\r\n   1127         # Even if dispatching the op failed, the RHS may be a tensor aware\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in _mul_dispatch(x, y, name)\r\n   1455     return sparse_tensor.SparseTensor(y.indices, new_vals, y.dense_shape)\r\n   1456   else:\r\n-> 1457     return multiply(x, y, name=name)\r\n   1458 \r\n   1459 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)\r\n    199     \"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\r\n    200     try:\r\n--> 201       return target(*args, **kwargs)\r\n    202     except (TypeError, ValueError):\r\n    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in multiply(x, y, name)\r\n    507   \"\"\"\r\n    508 \r\n--> 509   return gen_math_ops.mul(x, y, name)\r\n    510 \r\n    511 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py in mul(x, y, name)\r\n   6164       return _result\r\n   6165     except _core._NotOkStatusException as e:\r\n-> 6166       _ops.raise_from_not_ok_status(e, name)\r\n   6167     except _core._FallbackException:\r\n   6168       pass\r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)\r\n   6841   message = e.message + (\" name: \" + name if name is not None else \"\")\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n   6845 \r\n\r\n~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/six.py in raise_from(value, from_value)\r\n\r\nResourceExhaustedError: OOM when allocating tensor with shape[501153,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Mul]\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/76/timeline","performed_via_github_app":null,"state_reason":null}