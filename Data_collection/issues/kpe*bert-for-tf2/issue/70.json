{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/70","id":638210666,"node_id":"MDU6SXNzdWU2MzgyMTA2NjY=","number":70,"title":"Problems of fine-tuning LayerNormalization layer and exploring all layers of the model","user":{"login":"BaoshengHeTR","id":60898384,"node_id":"MDQ6VXNlcjYwODk4Mzg0","avatar_url":"https://avatars.githubusercontent.com/u/60898384?v=4","gravatar_id":"","url":"https://api.github.com/users/BaoshengHeTR","html_url":"https://github.com/BaoshengHeTR","followers_url":"https://api.github.com/users/BaoshengHeTR/followers","following_url":"https://api.github.com/users/BaoshengHeTR/following{/other_user}","gists_url":"https://api.github.com/users/BaoshengHeTR/gists{/gist_id}","starred_url":"https://api.github.com/users/BaoshengHeTR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BaoshengHeTR/subscriptions","organizations_url":"https://api.github.com/users/BaoshengHeTR/orgs","repos_url":"https://api.github.com/users/BaoshengHeTR/repos","events_url":"https://api.github.com/users/BaoshengHeTR/events{/privacy}","received_events_url":"https://api.github.com/users/BaoshengHeTR/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-06-13T18:40:11Z","updated_at":"2020-07-29T07:09:03Z","closed_at":"2020-07-29T07:09:03Z","author_association":"NONE","active_lock_reason":null,"body":"I have a question. When I call:\r\n`l_bert.apply_adapter_freeze()`\r\nwill that freeze the original LN layer as well?\r\nFrom your example, the function:\r\n```\r\ndef freeze_bert_layers(l_bert):\r\n    \"\"\"\r\n    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\r\n    \"\"\"\r\n```\r\nis used to freeze layers but LN and adaptive layers. However, the function `flatten_layers` called in `freeze_bert_layers` doesn't work properly and it could not locate the layers supposed to be trainable.  It would be great if you give the way that we can check the status of each layer.\r\n\r\nFinally, I want to confirm that the pretrained bert model used in `bert-for-tf2` does not have the pooling layer with `tanh` activation layer, right?\r\n\r\nThanks.","closed_by":{"login":"kpe","id":2535923,"node_id":"MDQ6VXNlcjI1MzU5MjM=","avatar_url":"https://avatars.githubusercontent.com/u/2535923?v=4","gravatar_id":"","url":"https://api.github.com/users/kpe","html_url":"https://github.com/kpe","followers_url":"https://api.github.com/users/kpe/followers","following_url":"https://api.github.com/users/kpe/following{/other_user}","gists_url":"https://api.github.com/users/kpe/gists{/gist_id}","starred_url":"https://api.github.com/users/kpe/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpe/subscriptions","organizations_url":"https://api.github.com/users/kpe/orgs","repos_url":"https://api.github.com/users/kpe/repos","events_url":"https://api.github.com/users/kpe/events{/privacy}","received_events_url":"https://api.github.com/users/kpe/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/70/timeline","performed_via_github_app":null,"state_reason":"completed"}