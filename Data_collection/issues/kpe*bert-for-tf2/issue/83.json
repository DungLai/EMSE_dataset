{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/83","id":745610309,"node_id":"MDU6SXNzdWU3NDU2MTAzMDk=","number":83,"title":"albert classification error(Failed copying input tensor from GPU in order to run Identity: GPU sync failed [Op:Identity])","user":{"login":"DrinkingMilktea","id":38062461,"node_id":"MDQ6VXNlcjM4MDYyNDYx","avatar_url":"https://avatars.githubusercontent.com/u/38062461?v=4","gravatar_id":"","url":"https://api.github.com/users/DrinkingMilktea","html_url":"https://github.com/DrinkingMilktea","followers_url":"https://api.github.com/users/DrinkingMilktea/followers","following_url":"https://api.github.com/users/DrinkingMilktea/following{/other_user}","gists_url":"https://api.github.com/users/DrinkingMilktea/gists{/gist_id}","starred_url":"https://api.github.com/users/DrinkingMilktea/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DrinkingMilktea/subscriptions","organizations_url":"https://api.github.com/users/DrinkingMilktea/orgs","repos_url":"https://api.github.com/users/DrinkingMilktea/repos","events_url":"https://api.github.com/users/DrinkingMilktea/events{/privacy}","received_events_url":"https://api.github.com/users/DrinkingMilktea/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-11-18T12:17:42Z","updated_at":"2020-11-18T12:42:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> #tokenization define\r\nfrom bert.tokenization.albert_tokenization import FullTokenizer\r\ndef createTokenizer():\r\n    return FullTokenizer(\"../albert_base/assets/30k-clean.vocab\", spm_model_file=\"../albert_base/assets/30k-clean.model\", do_lower_case=True)\r\n\r\n> def get_masks(tokens, max_seq_length):\r\n    \"\"\"Mask for padding\"\"\"\r\n    if len(tokens)>max_seq_length:\r\n        #Cutting down the excess length\r\n        tokens = tokens[0:max_seq_length]\r\n        return [1]*len(tokens)\r\n    else :\r\n      return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\r\n\r\n> def get_segments(tokens, max_seq_length):\r\n    if len(tokens)>max_seq_length:\r\n      #Cutting down the excess length\r\n      tokens = tokens[:max_seq_length]\r\n      segments = []\r\n      current_segment_id = 0\r\n      for token in tokens:\r\n        segments.append(current_segment_id)\r\n        if token == \"[SEP]\":\r\n          current_segment_id = 1\r\n      return segments\r\n    else:\r\n      segments = []\r\n      current_segment_id = 0\r\n      for token in tokens:\r\n        segments.append(current_segment_id)\r\n        if token == \"[SEP]\":\r\n          current_segment_id = 1\r\n      return segments + [0] * (max_seq_length - len(tokens))\r\n\r\n> def get_ids(tokens, tokenizer, max_seq_length):\r\n    if len(tokens)>max_seq_length:\r\n      tokens = tokens[:max_seq_length]\r\n      token_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n      return token_ids\r\n    else:\r\n      token_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n      input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\r\n      return input_ids\r\n\r\n> tokenizer = createTokenizer()\r\nmax_seq_length = 64  #This number will determine the number of tokens\r\n\r\n> def prep(s, get = 'id'):\r\n    stokens = tokenizer.tokenize(s)\r\n    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\r\n    if get == 'id':\r\n        input_ids = get_ids(stokens, tokenizer, max_seq_length)\r\n        return input_ids\r\n    elif get == 'mask':\r\n        input_masks = get_masks(stokens, max_seq_length)\r\n        return input_masks\r\n    else:\r\n        input_segments = get_segments(stokens, max_seq_length)\r\n        return input_segments\r\n\r\n> #train and test data load\r\nimport pandas as pd\r\ntrain_set = pd.read_csv(\"../goemotion/train_set.csv\")\r\ntest_set = pd.read_csv(\"../goemotion/test_set.csv\")\r\ntrain_X = [prep(sentence) for sentence in train_set[\"text\"]]\r\ntrain_Y = list(map(int, train_set[\"emotion\"].tolist()))\r\ntest_X = [prep(sentence) for sentence in test_set[\"text\"]]\r\ntest_Y = list(map(int, test_set[\"emotion\"].tolist()))\r\nprint(\"data preprocess finished\")\r\n\r\n> #albert model calling\r\nimport os\r\nimport bert\r\nimport tensorflow as tf\r\n\r\n> #GPU config\r\ntf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices(\"GPU\")[0], True)\r\n\r\n> #parameters\r\nmodel_name = \"albert_base_v2\"\r\nmodel_ckpt = os.path.join(\"../albert_base\", \"model.ckpt-best\")\r\nmodel_params = bert.albert_params(\"../albert_base/\")\r\n\r\n> #call and define model layers\r\nalbert_layer = bert.BertModelLayer.from_params(model_params, name=\"albert\")\r\nmodel_layer = tf.keras.Sequential([\r\n    tf.keras.layers.Input(shape=(max_seq_length,), dtype=\"int32\", name=\"input_ids\"),\r\n    albert_layer,\r\n    tf.keras.layers.Dense(112, activation=tf.nn.relu),\r\n    tf.keras.layers.Dense(27, activation=tf.nn.softmax),#0~27\r\n    tf.keras.layers.Dense(1, activation=tf.nn.softmax)\r\n])\r\nmodel_layer.build(input_shape=(None, max_seq_length))\r\nbert.load_albert_weights(albert_layer, model_ckpt)\r\n\r\n> #compile\r\nmodel_layer.compile(loss=\"sparse_categorical_crossentropy\", optimizer=tf.optimizers.Adam(lr=0.00001), metrics=[\"sparse_categorical_accuracy\"])\r\nprint(model_layer.summary())\r\n\r\n> #train start\r\ncheckpointName = os.path.join(\"../albert_base/models/\", \"albert_faq.ckpt\")\r\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointName,\r\n                                                save_weights_only=True,\r\n                                                verbose=1)\r\n\r\n> #train_start\r\nhistory = model_layer.fit(\r\n            test_X,\r\n            test_Y,\r\n            epochs=300,\r\n            validation_data=(train_X, train_Y),\r\n            verbose=1,\r\n            callbacks=[cp_callback],\r\n            batch_size=2)\r\n\r\nabove is my code and\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 455.23.04    Driver Version: 455.23.04    CUDA Version: 11.1     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 3090    On   | 00000000:09:00.0  On |                  N/A |\r\n| 33%   53C    P2   111W / 350W |   1016MiB / 24265MiB |      1%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\nthis is nvidia-smi\r\n\r\nI use tensorflow-gpu 2.2 and cuda toolkit 10.1 and cudnn 7.6\r\nMy computer is 3900X 128GB(RAM) RTX3090 500GB(SSD)\r\n\r\nand if run above code error message is below.\r\n\r\n\r\n  File \"/home/sentiment/anaconda3/envs/mybert/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 6606, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: GPU sync failed [Op:Identity]\r\n\r\nI want to train albert finetuning.\r\nif i use tensorflow for cpu. it work fine but 1 epoch per 6 hour for training.\r\nso I hope to use gpu\r\n\r\nI really hard to find out solution for fixing but failed.\r\n\r\nis there anyone know how to fix this error?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/83/timeline","performed_via_github_app":null,"state_reason":null}