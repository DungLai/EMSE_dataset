{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/75","id":705416530,"node_id":"MDU6SXNzdWU3MDU0MTY1MzA=","number":75,"title":"Custom tokenizer layer","user":{"login":"ptamas88","id":36891268,"node_id":"MDQ6VXNlcjM2ODkxMjY4","avatar_url":"https://avatars.githubusercontent.com/u/36891268?v=4","gravatar_id":"","url":"https://api.github.com/users/ptamas88","html_url":"https://github.com/ptamas88","followers_url":"https://api.github.com/users/ptamas88/followers","following_url":"https://api.github.com/users/ptamas88/following{/other_user}","gists_url":"https://api.github.com/users/ptamas88/gists{/gist_id}","starred_url":"https://api.github.com/users/ptamas88/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ptamas88/subscriptions","organizations_url":"https://api.github.com/users/ptamas88/orgs","repos_url":"https://api.github.com/users/ptamas88/repos","events_url":"https://api.github.com/users/ptamas88/events{/privacy}","received_events_url":"https://api.github.com/users/ptamas88/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-09-21T09:01:55Z","updated_at":"2021-01-26T10:34:13Z","closed_at":"2020-10-21T14:46:20Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\nI would like to incorporate the tokenization process into a model which is using bert layer.\r\nHere is my custom layer:\r\n```\r\nclass TokenizationLayer(tf.keras.layers.Layer):\r\n    def __init__(self, vocab_path, max_length, **kwargs):\r\n        self.vocab_path = vocab_path\r\n        self.length = max_length\r\n        self.tokenizer = bert.bert_tokenization.FullTokenizer(vocab_path, do_lower_case=False)\r\n        super(TokenizationLayer, self).__init__(**kwargs)\r\n\r\n    def call(self,inputs):\r\n        tokens = self.tokenizer.tokenize(inputs)\r\n        ids = self.tokenizer.convert_tokens_to_ids(tokens)\r\n        ids += [self.tokenizer.vocab['[PAD]']] * (self.length-len(ids))\r\n        return ids\r\n```\r\nAnd here is my code to test the custom layer within a dummy model:\r\n```\r\ninputs = tf.keras.layers.Input(shape=(), dtype='string')\r\ntokenization_layer = TokenizationLayer(vocab_path, 10, dtype=tf.string)\r\noutputs = tokenization_layer(inputs)\r\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n```\r\nI get the following traceback:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-68-8df4885e5c7a> in <module>\r\n      1 inputs = tf.keras.layers.Input(shape=(), dtype='string')\r\n      2 tokenization_layer = TokenizationLayer(vocab_path, 10, dtype=tf.string)\r\n----> 3 outputs = tokenization_layer(inputs)\r\n      4 model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\r\n\r\n/s/Demo/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)\r\n    924     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):\r\n    925       return self._functional_construction_call(inputs, args, kwargs,\r\n--> 926                                                 input_list)\r\n    927 \r\n    928     # Maintains info about the `Layer.call` stack.\r\n\r\n/s/Demo/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)\r\n   1115           try:\r\n   1116             with ops.enable_auto_cast_variables(self._compute_dtype_object):\r\n-> 1117               outputs = call_fn(cast_inputs, *args, **kwargs)\r\n   1118 \r\n   1119           except errors.OperatorNotAllowedInGraphError as e:\r\n\r\n/s/Demo/venv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)\r\n    256       except Exception as e:  # pylint:disable=broad-except\r\n    257         if hasattr(e, 'ag_error_metadata'):\r\n--> 258           raise e.ag_error_metadata.to_exception(e)\r\n    259         else:\r\n    260           raise\r\n\r\nValueError: in user code:\r\n\r\n    <ipython-input-60-d6c12f7d1b14>:17 call  *\r\n        tokens = self.tokenizer.tokenize(inputs)\r\n    /s/Demo/venv/lib/python3.7/site-packages/bert/tokenization/bert_tokenization.py:172 tokenize  *\r\n        for token in self.basic_tokenizer.tokenize(text):\r\n    /s/Demo/venv/lib/python3.7/site-packages/bert/tokenization/bert_tokenization.py:198 tokenize  *\r\n        text = convert_to_unicode(text)\r\n    /s/Demo/venv/lib/python3.7/site-packages/bert/tokenization/bert_tokenization.py:86 convert_to_unicode  *\r\n        raise ValueError(\"Unsupported string type: %s\" % (type(text)))\r\n\r\n    ValueError: Unsupported string type: <class 'tensorflow.python.framework.ops.Tensor'>\r\n```\r\nCan you lease help how to solve this issue?\r\nI think the problem is that the tokenizer gets tensors not string and that is why it can't tokenize it.\r\nBut if that is the case how should I mkae this work?\r\nThanks","closed_by":{"login":"kpe","id":2535923,"node_id":"MDQ6VXNlcjI1MzU5MjM=","avatar_url":"https://avatars.githubusercontent.com/u/2535923?v=4","gravatar_id":"","url":"https://api.github.com/users/kpe","html_url":"https://github.com/kpe","followers_url":"https://api.github.com/users/kpe/followers","following_url":"https://api.github.com/users/kpe/following{/other_user}","gists_url":"https://api.github.com/users/kpe/gists{/gist_id}","starred_url":"https://api.github.com/users/kpe/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpe/subscriptions","organizations_url":"https://api.github.com/users/kpe/orgs","repos_url":"https://api.github.com/users/kpe/repos","events_url":"https://api.github.com/users/kpe/events{/privacy}","received_events_url":"https://api.github.com/users/kpe/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/75/timeline","performed_via_github_app":null,"state_reason":"completed"}