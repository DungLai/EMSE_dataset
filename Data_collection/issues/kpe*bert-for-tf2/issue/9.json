{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/9","id":505852076,"node_id":"MDU6SXNzdWU1MDU4NTIwNzY=","number":9,"title":"Inconsistency in number of parameters for the loaded original pre-trained BERT models","user":{"login":"govind-govind","id":12267689,"node_id":"MDQ6VXNlcjEyMjY3Njg5","avatar_url":"https://avatars.githubusercontent.com/u/12267689?v=4","gravatar_id":"","url":"https://api.github.com/users/govind-govind","html_url":"https://github.com/govind-govind","followers_url":"https://api.github.com/users/govind-govind/followers","following_url":"https://api.github.com/users/govind-govind/following{/other_user}","gists_url":"https://api.github.com/users/govind-govind/gists{/gist_id}","starred_url":"https://api.github.com/users/govind-govind/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/govind-govind/subscriptions","organizations_url":"https://api.github.com/users/govind-govind/orgs","repos_url":"https://api.github.com/users/govind-govind/repos","events_url":"https://api.github.com/users/govind-govind/events{/privacy}","received_events_url":"https://api.github.com/users/govind-govind/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-10-11T13:25:52Z","updated_at":"2019-10-11T14:31:14Z","closed_at":"2019-10-11T14:31:14Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, I tried creating the `BertModelLayer` using the parameters file of original pre-trained BERT models. As per the official repo of BERT, both `uncased_L-12_H-768_A-12`   and `multi_cased_L-12_H-768_A-12`  should have 110M parameters. But when I create `BertModelLayer` using `bert-for-tf2` for these models, they have 108M and 177M parameters respectively. Is this the expected behavior? What can be the reasons?\r\n\r\nCode:\r\n```\r\nmodel_dir = 'uncased_L-12_H-768_A-12'   # 'multi_cased_L-12_H-768_A-12'  \r\n\r\nbert_params = params_from_pretrained_ckpt(model_dir)\r\nl_bert = BertModelLayer.from_params(bert_params, name=\"bert\")\r\n\r\nmax_seq_len = 256\r\nl_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\nl_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\r\n\r\noutput = l_bert([l_input_ids, l_token_type_ids])          \r\nmodel = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\r\nmodel.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\r\nmodel.summary()\r\n```\r\n\r\nOutput for uncased_L-12_H-768_A-12\r\n```\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\nbert (BertModelLayer)           (None, 256, 768)     108891648   input_1[0][0]                    \r\n                                                                 input_2[0][0]                    \r\n==================================================================================================\r\nTotal params: 108,891,648\r\nTrainable params: 108,891,648\r\nNon-trainable params: 0\r\n```\r\n\r\nOutput for multi_cased_L-12_H-768_A-12\r\n```\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_1 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\nbert (BertModelLayer)           (None, 256, 768)     177262848   input_1[0][0]                    \r\n                                                                 input_2[0][0]                    \r\n==================================================================================================\r\nTotal params: 177,262,848\r\nTrainable params: 177,262,848\r\nNon-trainable params: 0\r\n\r\n```","closed_by":{"login":"kpe","id":2535923,"node_id":"MDQ6VXNlcjI1MzU5MjM=","avatar_url":"https://avatars.githubusercontent.com/u/2535923?v=4","gravatar_id":"","url":"https://api.github.com/users/kpe","html_url":"https://github.com/kpe","followers_url":"https://api.github.com/users/kpe/followers","following_url":"https://api.github.com/users/kpe/following{/other_user}","gists_url":"https://api.github.com/users/kpe/gists{/gist_id}","starred_url":"https://api.github.com/users/kpe/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpe/subscriptions","organizations_url":"https://api.github.com/users/kpe/orgs","repos_url":"https://api.github.com/users/kpe/repos","events_url":"https://api.github.com/users/kpe/events{/privacy}","received_events_url":"https://api.github.com/users/kpe/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/9/timeline","performed_via_github_app":null,"state_reason":"completed"}