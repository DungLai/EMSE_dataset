{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/89","id":944133077,"node_id":"MDU6SXNzdWU5NDQxMzMwNzc=","number":89,"title":"may be there is  some problem work with tf hub","user":{"login":"Kiris-tingna","id":10670294,"node_id":"MDQ6VXNlcjEwNjcwMjk0","avatar_url":"https://avatars.githubusercontent.com/u/10670294?v=4","gravatar_id":"","url":"https://api.github.com/users/Kiris-tingna","html_url":"https://github.com/Kiris-tingna","followers_url":"https://api.github.com/users/Kiris-tingna/followers","following_url":"https://api.github.com/users/Kiris-tingna/following{/other_user}","gists_url":"https://api.github.com/users/Kiris-tingna/gists{/gist_id}","starred_url":"https://api.github.com/users/Kiris-tingna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Kiris-tingna/subscriptions","organizations_url":"https://api.github.com/users/Kiris-tingna/orgs","repos_url":"https://api.github.com/users/Kiris-tingna/repos","events_url":"https://api.github.com/users/Kiris-tingna/events{/privacy}","received_events_url":"https://api.github.com/users/Kiris-tingna/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-07-14T07:28:16Z","updated_at":"2021-07-14T07:28:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"hi, I am using this script to generate albert saved model which is capativble with `tf serving` \r\n\r\nsince i genrated model , the input is \r\n{\r\n    \"instances\":[\r\n        {\"inputs\": [\"你好么\"]}\r\n    ]\r\n}\r\noutput result seem not right, actually i want the albert out embedding vector.\r\n\r\n{\r\n\t\"predictions\": [\r\n\t\t[\r\n\t\t\t101,\r\n\t\t\t872,\r\n\t\t\t1962,\r\n\t\t\t720,\r\n\t\t\t102,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0,\r\n\t\t\t0\r\n\t\t]\r\n\t]\r\n}\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\nimport tensorflow_text as text\r\nimport bert, os\r\n\r\nfrom tensorflow.python.framework.ops import disable_eager_execution\r\ndisable_eager_execution()\r\n\r\nmodel_name = \"albert_base\"\r\nmodel_dir = bert.fetch_brightmart_albert_model(model_name, \".models\")\r\nmodel_ckpt = os.path.join(model_dir, \"albert_model.ckpt\")\r\n\r\nbert_params = bert.params_from_pretrained_ckpt(model_dir)\r\nl_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\r\n\r\n\r\nclass BertTokenizerLayer(tf.keras.layers.Layer):\r\n    def __init__(self, vocab_file_path, sequence_length=128, lower_case=True):\r\n        super(BertTokenizerLayer, self).__init__()\r\n        self.CLS_ID = tf.constant(101, dtype=tf.int64)\r\n        self.SEP_ID = tf.constant(102, dtype=tf.int64)\r\n        self.PAD_ID = tf.constant(0, dtype=tf.int64)\r\n        self.sequence_length = tf.constant(sequence_length)\r\n        vocab = self.load_vocab(vocab_file_path)\r\n        # These two lines are basically what makes it work\r\n        # assigning the vocab to a tf.Module and then later assigning the\r\n        # intantiated Module to e.g. a Keras Model\r\n        self.create_vocab_table(vocab)\r\n        self.bert_tokenizer = text.BertTokenizer(\r\n            vocab_lookup_table=self.vocab_table,\r\n            token_out_type=tf.int64,\r\n            lower_case=lower_case,\r\n        )\r\n\r\n    def load_vocab(self, vocab_file):\r\n        \"\"\"Loads a vocabulary file into a list.\"\"\"\r\n        vocab = []\r\n        with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\r\n            while True:\r\n                token = reader.readline()\r\n                if not token:\r\n                    break\r\n                token = token.strip()\r\n                vocab.append(token)\r\n        return vocab\r\n\r\n    def create_vocab_table(self, vocab, num_oov=1):\r\n        vocab_values = tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64)\r\n        self.init = tf.lookup.KeyValueTensorInitializer(\r\n            keys=vocab, values=vocab_values, key_dtype=tf.string, value_dtype=tf.int64\r\n        )\r\n        self.vocab_table = tf.lookup.StaticVocabularyTable(\r\n            self.init, num_oov, lookup_key_dtype=tf.string\r\n        )\r\n\r\n    @tf.function\r\n    def call(self, inputs: tf.Tensor) -> tf.Tensor:\r\n        \"\"\"\r\n        Perform the BERT preprocessing from text -> input token id\r\n        \"\"\"\r\n        # Convert text into token ids\r\n        tokens = self.bert_tokenizer.tokenize(inputs)\r\n\r\n        # Flatten the ragged tensors\r\n        tokens = tokens.merge_dims(1, 2)\r\n\r\n        # Add start and end token ids to the id sequence\r\n        start_tokens = tf.fill([tf.shape(inputs)[0], 1], self.CLS_ID)\r\n        end_tokens = tf.fill([tf.shape(inputs)[0], 1], self.SEP_ID)\r\n        tokens = tf.concat([start_tokens, tokens, end_tokens], axis=1)\r\n\r\n        # Truncate to sequence length\r\n        tokens = tokens[:, : self.sequence_length]\r\n\r\n        # Convert ragged tensor to tensor and pad with PAD_ID\r\n        tokens = tokens.to_tensor(default_value=self.PAD_ID)\r\n\r\n        # Pad to sequence length\r\n        pad = self.sequence_length - tf.shape(tokens)[1]\r\n        tokens = tf.pad(tokens, [[0, 0], [0, pad]], constant_values=self.PAD_ID)\r\n\r\n        return tf.reshape(tokens, [-1, self.sequence_length])\r\n\r\n# text_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\r\n# tokenizerd = BertTokenizerLayer(vocab_file_path=os.path.join(model_dir, \"vocab.txt\"))\r\n# input_tokens = tokenizerd(text_input)\r\n# embed_output = l_bert(input_tokens)    # output: [batch_size, max_seq_len, hidden_size]\r\n# model = tf.keras.Model(inputs=text_input, outputs=embed_output)\r\n# model.save(\"./models/albert-zh/1\", signatures=tokenizerd.call.get_concrete_function(tf.TensorSpec([], tf.string)))\r\n\r\n\r\nmodel = tf.keras.Sequential([\r\n    tf.keras.Input(shape=(1,), dtype=tf.string),\r\n    l_bert\r\n])\r\nmodel.tokenizer = BertTokenizerLayer(vocab_file_path=os.path.join(model_dir, \"vocab.txt\"))\r\nmodel.save(\"./models/albert-zh/1\", signatures=model.tokenizer.call.get_concrete_function(tf.TensorSpec(None, tf.string)))\r\n\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/89/timeline","performed_via_github_app":null,"state_reason":null}