{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30","repository_url":"https://api.github.com/repos/kpe/bert-for-tf2","labels_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30/labels{/name}","comments_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30/comments","events_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30/events","html_url":"https://github.com/kpe/bert-for-tf2/issues/30","id":540885904,"node_id":"MDU6SXNzdWU1NDA4ODU5MDQ=","number":30,"title":"What would be a good way to pad input texts?","user":{"login":"hygkim95","id":42794837,"node_id":"MDQ6VXNlcjQyNzk0ODM3","avatar_url":"https://avatars.githubusercontent.com/u/42794837?v=4","gravatar_id":"","url":"https://api.github.com/users/hygkim95","html_url":"https://github.com/hygkim95","followers_url":"https://api.github.com/users/hygkim95/followers","following_url":"https://api.github.com/users/hygkim95/following{/other_user}","gists_url":"https://api.github.com/users/hygkim95/gists{/gist_id}","starred_url":"https://api.github.com/users/hygkim95/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hygkim95/subscriptions","organizations_url":"https://api.github.com/users/hygkim95/orgs","repos_url":"https://api.github.com/users/hygkim95/repos","events_url":"https://api.github.com/users/hygkim95/events{/privacy}","received_events_url":"https://api.github.com/users/hygkim95/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-12-20T09:25:57Z","updated_at":"2020-01-02T14:55:44Z","closed_at":"2020-01-02T14:55:20Z","author_association":"NONE","active_lock_reason":null,"body":"Currently, I am just adding 0s to token_ids to match max_seq_len.\r\n`def tokenize_text(texts):\r\n    model_name = \"albert_base\"\r\n    max_seq_len = 64\r\n    model_dir = bert.fetch_tfhub_albert_model(model_name, \".models\")\r\n    spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\") \r\n    sp = spm.SentencePieceProcessor() \r\n    sp.load(spm_model) \r\n    do_lower_case = True\r\n\r\n    tokenized = []\r\n    for text in texts:\r\n        processed_text = bert.albert_tokenization.preprocess_text(text, lower=do_lower_case)\r\n        token_ids = bert.albert_tokenization.encode_ids(sp, processed_text)\r\n        token_ids = np.append(token_ids, np.zeros(max_seq_len-len(token_ids)))\r\n        tokenized.append(token_ids)\r\n    return np.array(tokenized)`\r\n\r\nHowever I found out even the zero tokens were embedded to non-zero vectors. Is this something I have to worry about? If it is, what is the proper way of padding input texts? ","closed_by":{"login":"kpe","id":2535923,"node_id":"MDQ6VXNlcjI1MzU5MjM=","avatar_url":"https://avatars.githubusercontent.com/u/2535923?v=4","gravatar_id":"","url":"https://api.github.com/users/kpe","html_url":"https://github.com/kpe","followers_url":"https://api.github.com/users/kpe/followers","following_url":"https://api.github.com/users/kpe/following{/other_user}","gists_url":"https://api.github.com/users/kpe/gists{/gist_id}","starred_url":"https://api.github.com/users/kpe/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kpe/subscriptions","organizations_url":"https://api.github.com/users/kpe/orgs","repos_url":"https://api.github.com/users/kpe/repos","events_url":"https://api.github.com/users/kpe/events{/privacy}","received_events_url":"https://api.github.com/users/kpe/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kpe/bert-for-tf2/issues/30/timeline","performed_via_github_app":null,"state_reason":"completed"}