{"url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73","repository_url":"https://api.github.com/repos/nanoporetech/taiyaki","labels_url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73/labels{/name}","comments_url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73/comments","events_url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73/events","html_url":"https://github.com/nanoporetech/taiyaki/issues/73","id":595716487,"node_id":"MDU6SXNzdWU1OTU3MTY0ODc=","number":73,"title":"Speeding up prepare_mapped_reads","user":{"login":"mbhall88","id":20403931,"node_id":"MDQ6VXNlcjIwNDAzOTMx","avatar_url":"https://avatars.githubusercontent.com/u/20403931?v=4","gravatar_id":"","url":"https://api.github.com/users/mbhall88","html_url":"https://github.com/mbhall88","followers_url":"https://api.github.com/users/mbhall88/followers","following_url":"https://api.github.com/users/mbhall88/following{/other_user}","gists_url":"https://api.github.com/users/mbhall88/gists{/gist_id}","starred_url":"https://api.github.com/users/mbhall88/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mbhall88/subscriptions","organizations_url":"https://api.github.com/users/mbhall88/orgs","repos_url":"https://api.github.com/users/mbhall88/repos","events_url":"https://api.github.com/users/mbhall88/events{/privacy}","received_events_url":"https://api.github.com/users/mbhall88/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":true,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2020-04-07T09:12:54Z","updated_at":"2020-10-08T09:40:54Z","closed_at":"2020-04-08T08:38:12Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi, \r\n\r\nI have been having some problems trying to get `prepare_mapped_reads.py` to run in a \"reasonable\" amount of time. What I mean by reasonable is before my cluster kills the job after running for 1 week.  \r\nThrowing more CPUs at it doesn't seem to be having much of an effect either.  \r\n\r\nAre you able to suggest a way I could speed this up? I have ~1.3 million reads. Should I be more hardcore with my filtering? Or is there a way I can slit this up and merge the resulting HDF5 files at the end?","closed_by":{"login":"mbhall88","id":20403931,"node_id":"MDQ6VXNlcjIwNDAzOTMx","avatar_url":"https://avatars.githubusercontent.com/u/20403931?v=4","gravatar_id":"","url":"https://api.github.com/users/mbhall88","html_url":"https://github.com/mbhall88","followers_url":"https://api.github.com/users/mbhall88/followers","following_url":"https://api.github.com/users/mbhall88/following{/other_user}","gists_url":"https://api.github.com/users/mbhall88/gists{/gist_id}","starred_url":"https://api.github.com/users/mbhall88/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mbhall88/subscriptions","organizations_url":"https://api.github.com/users/mbhall88/orgs","repos_url":"https://api.github.com/users/mbhall88/repos","events_url":"https://api.github.com/users/mbhall88/events{/privacy}","received_events_url":"https://api.github.com/users/mbhall88/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nanoporetech/taiyaki/issues/73/timeline","performed_via_github_app":null,"state_reason":"completed"}