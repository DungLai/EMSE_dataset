{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/1030","id":752758430,"node_id":"MDU6SXNzdWU3NTI3NTg0MzA=","number":1030,"title":"Hyperopt metric_score uses last epoch instead of best epoch ","user":{"login":"michaelbzhu","id":17104161,"node_id":"MDQ6VXNlcjE3MTA0MTYx","avatar_url":"https://avatars.githubusercontent.com/u/17104161?v=4","gravatar_id":"","url":"https://api.github.com/users/michaelbzhu","html_url":"https://github.com/michaelbzhu","followers_url":"https://api.github.com/users/michaelbzhu/followers","following_url":"https://api.github.com/users/michaelbzhu/following{/other_user}","gists_url":"https://api.github.com/users/michaelbzhu/gists{/gist_id}","starred_url":"https://api.github.com/users/michaelbzhu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/michaelbzhu/subscriptions","organizations_url":"https://api.github.com/users/michaelbzhu/orgs","repos_url":"https://api.github.com/users/michaelbzhu/repos","events_url":"https://api.github.com/users/michaelbzhu/events{/privacy}","received_events_url":"https://api.github.com/users/michaelbzhu/received_events","type":"User","site_admin":false},"labels":[{"id":1174068769,"node_id":"MDU6TGFiZWwxMTc0MDY4NzY5","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":1434966350,"node_id":"MDU6TGFiZWwxNDM0OTY2MzUw","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/looking%20into%20it","name":"looking into it","color":"ffa54f","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-11-28T20:09:51Z","updated_at":"2020-12-24T02:11:42Z","closed_at":"2020-12-24T02:11:42Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"`hyperopt()` generates a `hyperopt_statistics.json` file which contains `training_stats` and the `metric_score` from each sample\r\n\r\nI'm optimizing for validation accuracy, but I noticed that the final `metric_score` that gets reported in the json file uses the validation accuracy from the last epoch during training rather than the best epoch during training\r\n\r\nBelow is the output from my `hyperopt_statistics.json` file\r\n```\r\n\"hyperopt_results\": [\r\n        {\r\n            \"eval_stats\": {\r\n                \"combined\": {\r\n                    \"loss\": 1.4781785011291504\r\n                },\r\n                \"label\": {\r\n                    \"accuracy\": 0.47138965129852295,  // this is the final reported accuracy\r\n...\r\n            },\r\n            \"metric_score\": 0.47138965129852295,   // this value comes from HyperoptExecutor.get_metric_score(self, eval_stats) which just copies the value above\r\n            \"parameters\": {\r\n                \"training.learning_rate\": 0.0006019209790229743,\r\n                \"utterance.cell_type\": \"gru\",\r\n                \"utterance.num_layers\": 1,\r\n                \"utterance.state_size\": 495\r\n            },\r\n            \"training_stats\": {\r\n                \"validation\": {\r\n                    \"combined\": {\r\n                        \"loss\": [\r\n                            1.3932547569274902,\r\n                            1.2642898559570312,\r\n                            1.3837428092956543,\r\n                            1.2704368829727173,\r\n                            1.3504513502120972,\r\n                            1.3695340156555176,\r\n                            1.6437498331069946,\r\n                            1.589107632637024,\r\n                            1.4781785011291504\r\n                        ]\r\n                    },\r\n                    \"label\": {\r\n                        \"accuracy\": [\r\n                            0.40962761640548706,\r\n                            0.440508633852005,\r\n                            0.4423251450061798,\r\n                            0.47320616245269775,  // this is the best validation accuracy from training\r\n                            0.47320616245269775,\r\n                            0.440508633852005,\r\n                            0.4523160755634308,\r\n                            0.40690281987190247,\r\n                            0.47138965129852295   // this value from the last epoch is what's actually reported above\r\n                        ],\r\n...\r\n```\r\n\r\nIs this intended because the logger output from LudwigModel.train() uses the best validation accuracy so I thought hyperopt would have similar behavior?\r\n\r\nIf this is a bug I can try to help fix it as I have a general idea of where this behavior comes from in the codebase\r\n\r\n**Environment:**\r\n - Run in Google Colab\r\n- Python version 3.6.9\r\n- Ludwig version 0.3.1\r\n","closed_by":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1030/timeline","performed_via_github_app":null,"state_reason":"completed"}