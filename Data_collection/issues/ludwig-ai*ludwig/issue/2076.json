{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/2076","id":1252689978,"node_id":"I_kwDOCbx2hs5Kqog6","number":2076,"title":"Training a multi-column sequence classification model (transformer based)","user":{"login":"msakthiganesh","id":47854670,"node_id":"MDQ6VXNlcjQ3ODU0Njcw","avatar_url":"https://avatars.githubusercontent.com/u/47854670?v=4","gravatar_id":"","url":"https://api.github.com/users/msakthiganesh","html_url":"https://github.com/msakthiganesh","followers_url":"https://api.github.com/users/msakthiganesh/followers","following_url":"https://api.github.com/users/msakthiganesh/following{/other_user}","gists_url":"https://api.github.com/users/msakthiganesh/gists{/gist_id}","starred_url":"https://api.github.com/users/msakthiganesh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msakthiganesh/subscriptions","organizations_url":"https://api.github.com/users/msakthiganesh/orgs","repos_url":"https://api.github.com/users/msakthiganesh/repos","events_url":"https://api.github.com/users/msakthiganesh/events{/privacy}","received_events_url":"https://api.github.com/users/msakthiganesh/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-05-30T12:44:22Z","updated_at":"2022-06-01T14:02:53Z","closed_at":"2022-06-01T14:02:53Z","author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\n- Trying to train a transformer model on Colab using multiple text columns and input features and a binary target column.\r\n- Tried with the BoolQ dataset from SuperGLUE Benchmark. Can be obtained from [here](https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip).\r\n\r\nInput Feature Columns: `question` (type: text), `passage` (type: text)\r\nTarget Column: : `label` (type: binary)\r\n\r\n**Trial 1 Ludwig Configuration:**\r\n```\r\nconfig = {\r\n  \"preprocessing\": {\r\n    \"text\": {\r\n      \"tokenizer\": \"hf_tokenizer\",\r\n      \"pretrained_model_name_or_path\": \"roberta\",\r\n      \"max_sequence_length\": 512\r\n    }\r\n  },\r\n  \"input_features\": [\r\n    {\r\n      \"name\": \"question\",\r\n      \"type\": \"text\",\r\n      \"encoder\": \"roberta\",\r\n      \"trainable\": True,\r\n      \"dropout\": 0.2\r\n    },\r\n    {\r\n      \"name\": \"passage\",\r\n      \"type\": \"text\",\r\n      \"encoder\": \"roberta\",\r\n      \"trainable\": True,\r\n      \"dropout\": 0.2\r\n    }\r\n  ],\r\n  \"output_features\": [\r\n    {\r\n      \"name\": \"label\",\r\n      \"type\": \"binary\"\r\n    }\r\n  ],\r\n  \"combiner\": {\r\n    \"type\": \"transformer\",\r\n    \"output_size\": 512\r\n  },\r\n  \"trainer\":{\r\n      \"epochs\": 100,\r\n      \"early_stop\": 15\r\n  }\r\n}\r\n```\r\n**Error Stack for Trial 1:** \r\n```\r\nRuntimeError: CUDA error: device-side assert triggered\r\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[/usr/local/lib/python3.7/dist-packages/torch/serialization.py](https://localhost:8080/#) in __init__(self, name, mode)\r\n    210 class _open_file(_opener):\r\n    211     def __init__(self, name, mode):\r\n--> 212         super(_open_file, self).__init__(open(name, mode))\r\n    213 \r\n    214     def __exit__(self, *args):\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '/content/results/api_experiment_run_5/model/model_weights'\r\n```\r\n\r\nSince the previous trial failed, I tried to remove the preprocessing config and try it again.\r\n\r\n**Trial 2 Ludwig Configuration: **\r\n\r\n```\r\nconfig = {\r\n  \"input_features\": [\r\n    {\r\n      \"name\": \"question\",\r\n      \"type\": \"text\",\r\n      \"encoder\": \"roberta\",\r\n      \"trainable\": True,\r\n      \"dropout\": 0.2\r\n    },\r\n    {\r\n      \"name\": \"passage\",\r\n      \"type\": \"text\",\r\n      \"encoder\": \"roberta\",\r\n      \"trainable\": True,\r\n      \"dropout\": 0.2\r\n    }\r\n  ],\r\n  \"output_features\": [\r\n    {\r\n      \"name\": \"label\",\r\n      \"type\": \"binary\"\r\n    }\r\n  ],\r\n  \"combiner\": {\r\n    \"type\": \"transformer\",\r\n    \"output_size\": 512\r\n  },\r\n  \"trainer\":{\r\n      \"epochs\": 100,\r\n      \"early_stop\": 15\r\n  }\r\n}\r\n```\r\nThis time, the training begins, but there is no improvement in training or validation accuracy even after 10 epochs. \r\n\r\nSo, I created another trial with default encoders (Trial 3).\r\n\r\n**Trial 3 Ludwig Configuration:**\r\n\r\n```\r\nconfig = {\r\n  \"input_features\": [\r\n    {\r\n      \"name\": \"question\",\r\n      \"type\": \"text\"\r\n    },\r\n    {\r\n      \"name\": \"passage\",\r\n      \"type\": \"text\"\r\n    }\r\n  ],\r\n  \"output_features\": [\r\n    {\r\n      \"name\": \"label\",\r\n      \"type\": \"binary\"\r\n    }\r\n  ],\r\n  \"trainer\":{\r\n      \"epochs\": 100,\r\n      \"early_stop\": 15\r\n  }\r\n}\r\n```\r\n\r\n This time, the training and validation accuracy increased. Training started at approx 60% and reached 98% by 15 epochs while validation reached around 64%, indicating that the default parallel_cnn model is training while the roberta model in Trials 1 and 2 was not being trained.\r\n\r\n1. Any possible reason why the roberta model is not being trained?\r\n2. How can I use a `roberta-large` model to train for sequence classification tasks? My input features would be `question` and `passage` columns in the BoolQ dataset. The columns would have to be tokenized and encoded for roberta model.\r\n3. On a more generic note, does Ludwig use two separate roberta (or any encoder model) models for two separate input features or a single model takes the concatenated/processed/tokenized/encoded input of multiple input sequence columns? If Ludwig uses two separate encoder models for each input and then uses a combiner to concatenate the embeddings, I believe the model would not train as the `question` and `passage` columns have to be fed to a single model to derive the target `label`, as each column individually would not be enough to make the prediction.\r\n4. If multiple encoders are used for multiple input sequence columns, how can I feed multiple input sequence columns to a single encoder model?\r\n\r\n\r\n**To Reproduce**\r\n```\r\n!python -m pip install git+https://github.com/ludwig-ai/ludwig.git --quiet\r\n!pip install ray\r\n```\r\n```\r\n!wget https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip\r\n!unzip BoolQ.zip\r\n```\r\n```\r\nimport pandas as pd\r\n\r\ntrain_df = pd.read_json(\"BoolQ/train.jsonl\", lines=True)\r\nval_df = pd.read_json(\"BoolQ/val.jsonl\", lines=True)\r\ntest_df = pd.read_json(\"BoolQ/test.jsonl\", lines=True)\r\n```\r\n```\r\nINSERT CONFIG FROM TRIAL 1 OR 2 HERE\r\n```\r\n```\r\nimport logging\r\nfrom ludwig.api import LudwigModel\r\nmodel = LudwigModel(config, logging_level=logging.INFO)\r\n```\r\n```\r\ntrain_stats, preprocessed_data, output_directory = model.train(dataset=train_df)\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n\r\n- Google Colab (Tesla T4 GPU)\r\n- Python version: 3.7.13\r\n- Ludwig version: 0.5.1\r\nInstalled Ludwig using: `!python -m pip install git+https://github.com/ludwig-ai/ludwig.git --quiet`\r\n\r\nThanks! :D","closed_by":{"login":"msakthiganesh","id":47854670,"node_id":"MDQ6VXNlcjQ3ODU0Njcw","avatar_url":"https://avatars.githubusercontent.com/u/47854670?v=4","gravatar_id":"","url":"https://api.github.com/users/msakthiganesh","html_url":"https://github.com/msakthiganesh","followers_url":"https://api.github.com/users/msakthiganesh/followers","following_url":"https://api.github.com/users/msakthiganesh/following{/other_user}","gists_url":"https://api.github.com/users/msakthiganesh/gists{/gist_id}","starred_url":"https://api.github.com/users/msakthiganesh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/msakthiganesh/subscriptions","organizations_url":"https://api.github.com/users/msakthiganesh/orgs","repos_url":"https://api.github.com/users/msakthiganesh/repos","events_url":"https://api.github.com/users/msakthiganesh/events{/privacy}","received_events_url":"https://api.github.com/users/msakthiganesh/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2076/timeline","performed_via_github_app":null,"state_reason":"completed"}