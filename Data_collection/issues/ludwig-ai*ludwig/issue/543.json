{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/543","id":510391154,"node_id":"MDU6SXNzdWU1MTAzOTExNTQ=","number":543,"title":"date feature question / michelangelo","user":{"login":"tboneai","id":56497528,"node_id":"MDQ6VXNlcjU2NDk3NTI4","avatar_url":"https://avatars.githubusercontent.com/u/56497528?v=4","gravatar_id":"","url":"https://api.github.com/users/tboneai","html_url":"https://github.com/tboneai","followers_url":"https://api.github.com/users/tboneai/followers","following_url":"https://api.github.com/users/tboneai/following{/other_user}","gists_url":"https://api.github.com/users/tboneai/gists{/gist_id}","starred_url":"https://api.github.com/users/tboneai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tboneai/subscriptions","organizations_url":"https://api.github.com/users/tboneai/orgs","repos_url":"https://api.github.com/users/tboneai/repos","events_url":"https://api.github.com/users/tboneai/events{/privacy}","received_events_url":"https://api.github.com/users/tboneai/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-10-22T02:10:47Z","updated_at":"2019-10-25T01:30:21Z","closed_at":"2019-10-23T05:49:38Z","author_association":"NONE","active_lock_reason":null,"body":"I am having an issue with the date that I cannot figure out and a general question. I am a driver, not a programmer, so please bear with me.   :)\r\n\r\nMy csv file passes the date as yyyymmdd hhmmss. I have tried the default, different yaml nesting,  and multiple combinations of '%Y%m%d %H%M%S' but I still cannot get it to work. Does the file need changed to include a / and : in the date field? Any help would be appreciated. (leaving the date preprocessing blank has gotten me the farthest, or clearest error) \r\n\r\nMy other question revolves around predictions and the Uber Summit video regarding Michelangelo and Uber Eats where an update is passed to better determine when the food will be ready and when a driver is called. I am trying to predict the forex market 5 minutes in advance and would like to pass new data along (lets say at 2 and 4 minutes) to better determine the closing price at 5 minutes. Is that possible with Ludwig without re-running the entire job? If I just add data and then re-run it, without Ludwig knowing the previous prediction, I am afraid it may not recalculate correctly. \r\nLastly, I would like to say Thank You to the crew at Uber Engineering for making Ludwig available. I know there may be better, or easier, cut and paste options for me from the internet but that's no way to learn. I apologize for the cut and paste job\r\n------------------------------------------------------------------------------------------------------------\r\nDatestamp,open,high,low,close,volume\r\n20191001 000000,1.088760,1.088790,1.088650,1.088650,0\r\n\r\n-------------------------------------------------------------------------------------------------------------\r\ninput_features:\r\n    -\r\n        name: datestamp\r\n        type: date\r\n    -\r\n        name: open\r\n        type: numerical\r\n    -\r\n        name: high\r\n        type: numerical\r\n    -\r\n        name: low\r\n        type: numerical\r\n    -\r\n        name: close\r\n        type: numerical\r\noutput_features:\r\n    -\r\n        name: open\r\n        type: numerical\r\n    -\r\n        name: high\r\n        type: numerical\r\n    -\r\n        name: low\r\n        type: numerical\r\n    -\r\n        name: close\r\n        type: numerical\r\npreprocessing:\r\n        force_split: false\r\n        split_probabilities: [0.7,0.1,0.2]\r\n        stratify: null\r\n        missing_value_strategy: backfill\r\n        encoder: wave\r\n        cell_type: lstm_cudnn\r\n        gpus: 0\r\n\r\n-------------------------------------------------------------------------------------------------------------\r\ncommand: ('C:\\\\Users\\\\tmine\\\\env\\\\Scripts\\\\ludwig experiment --data_csv '\r\n 'env\\\\lib\\\\eurusd_data.csv -mdf env\\\\lib\\\\model_definition_file.yaml')\r\nrandom_seed: 42\r\ninput_data: 'env\\\\lib\\\\eurusd_data.csv'\r\nmodel_definition: {   'combiner': {'type': 'concat'},\r\n    'input_features': [   {   'name': 'datestamp',\r\n                              'tied_weights': None,\r\n                              'type': 'date'},\r\n                          {   'name': 'open',\r\n                              'tied_weights': None,\r\n                              'type': 'numerical'},\r\n                          {   'name': 'high',\r\n                              'tied_weights': None,\r\n                              'type': 'numerical'},\r\n                          {   'name': 'low',\r\n                              'tied_weights': None,\r\n                              'type': 'numerical'},\r\n                          {   'name': 'close',\r\n                              'tied_weights': None,\r\n                              'type': 'numerical'}],\r\n    'output_features': [   {   'clip': None,\r\n                               'dependencies': [],\r\n                               'loss': {   'type': 'mean_squared_error',\r\n                                           'weight': 1},\r\n                               'name': 'open',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'numerical'},\r\n                           {   'clip': None,\r\n                               'dependencies': [],\r\n                               'loss': {   'type': 'mean_squared_error',\r\n                                           'weight': 1},\r\n                               'name': 'high',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'numerical'},\r\n                           {   'clip': None,\r\n                               'dependencies': [],\r\n                               'loss': {   'type': 'mean_squared_error',\r\n                                           'weight': 1},\r\n                               'name': 'low',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'numerical'},\r\n                           {   'clip': None,\r\n                               'dependencies': [],\r\n                               'loss': {   'type': 'mean_squared_error',\r\n                                           'weight': 1},\r\n                               'name': 'close',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'numerical'}],\r\n    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\r\n                                      'audio_file_length_limit_in_s': 7.5,\r\n                                      'in_memory': True,\r\n                                      'missing_value_strategy': 'backfill',\r\n                                      'norm': None,\r\n                                      'padding_value': 0},\r\n                         'bag': {   'fill_value': '',\r\n                                    'lowercase': False,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': 10000,\r\n                                    'tokenizer': 'space'},\r\n                         'binary': {   'fill_value': 0,\r\n                                       'missing_value_strategy': 'fill_with_const'},\r\n                         'category': {   'fill_value': '<UNK>',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 10000},\r\n                         'cell_type': 'lstm_cudnn',\r\n                         'date': {   'datetime_format': None,\r\n                                     'fill_value': '',\r\n                                     'missing_value_strategy': 'fill_with_const'},\r\n                         'encoder': 'wave',\r\n                         'force_split': False,\r\n                         'gpus': 0,\r\n                         'h3': {   'fill_value': 576495936675512319,\r\n                                   'missing_value_strategy': 'fill_with_const'},\r\n                         'image': {   'in_memory': True,\r\n                                      'missing_value_strategy': 'backfill',\r\n                                      'num_processes': 1,\r\n                                      'resize_method': 'interpolate',\r\n                                      'scaling': 'pixel_normalization'},\r\n                         'missing_value_strategy': 'backfill',\r\n                         'numerical': {   'fill_value': 0,\r\n                                          'missing_value_strategy': 'fill_with_const',\r\n                                          'normalization': None},\r\n                         'sequence': {   'fill_value': '',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 20000,\r\n                                         'padding': 'right',\r\n                                         'padding_symbol': '<PAD>',\r\n                                         'sequence_length_limit': 256,\r\n                                         'tokenizer': 'space',\r\n                                         'unknown_symbol': '<UNK>',\r\n                                         'vocab_file': None},\r\n                         'set': {   'fill_value': '',\r\n                                    'lowercase': False,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': 10000,\r\n                                    'tokenizer': 'space'},\r\n                         'split_probabilities': [0.7, 0.1, 0.2],\r\n                         'stratify': None,\r\n                         'text': {   'char_most_common': 70,\r\n                                     'char_sequence_length_limit': 1024,\r\n                                     'char_tokenizer': 'characters',\r\n                                     'char_vocab_file': None,\r\n                                     'fill_value': '',\r\n                                     'lowercase': True,\r\n                                     'missing_value_strategy': 'fill_with_const',\r\n                                     'padding': 'right',\r\n                                     'padding_symbol': '<PAD>',\r\n                                     'unknown_symbol': '<UNK>',\r\n                                     'word_most_common': 20000,\r\n                                     'word_sequence_length_limit': 256,\r\n                                     'word_tokenizer': 'space_punct',\r\n                                     'word_vocab_file': None},\r\n                         'timeseries': {   'fill_value': '',\r\n                                           'missing_value_strategy': 'fill_with_const',\r\n                                           'padding': 'right',\r\n                                           'padding_value': 0,\r\n                                           'timeseries_length_limit': 256,\r\n                                           'tokenizer': 'space'},\r\n                         'vector': {   'fill_value': '',\r\n                                       'missing_value_strategy': 'fill_with_const'}},\r\n    'training': {   'batch_size': 128,\r\n                    'bucketing_field': None,\r\n                    'decay': False,\r\n                    'decay_rate': 0.96,\r\n                    'decay_steps': 10000,\r\n                    'dropout_rate': 0.0,\r\n                    'early_stop': 5,\r\n                    'epochs': 100,\r\n                    'eval_batch_size': 0,\r\n                    'gradient_clipping': None,\r\n                    'increase_batch_size_on_plateau': 0,\r\n                    'increase_batch_size_on_plateau_max': 512,\r\n                    'increase_batch_size_on_plateau_patience': 5,\r\n                    'increase_batch_size_on_plateau_rate': 2,\r\n                    'learning_rate': 0.001,\r\n                    'learning_rate_warmup_epochs': 1,\r\n                    'optimizer': {   'beta1': 0.9,\r\n                                     'beta2': 0.999,\r\n                                     'epsilon': 1e-08,\r\n                                     'type': 'adam'},\r\n                    'reduce_learning_rate_on_plateau': 0,\r\n                    'reduce_learning_rate_on_plateau_patience': 5,\r\n                    'reduce_learning_rate_on_plateau_rate': 0.5,\r\n                    'regularization_lambda': 0,\r\n                    'regularizer': 'l2',\r\n                    'staircase': False,\r\n                    'validation_field': 'combined',\r\n                    'validation_measure': 'loss'}}\r\n\r\n\r\nUsing full raw csv, no hdf5 and json file with the same name have been found\r\nBuilding dataset (it may take a while)\r\nTraceback (most recent call last):\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 2897, in get_loc\r\n    return self._engine.get_loc(key)\r\n  File \"pandas\\_libs\\index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas\\_libs\\index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 'datestamp'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Users\\tmine\\env\\Scripts\\ludwig.exe\\__main__.py\", line 9, in <module>\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\cli.py\", line 108, in main\r\n    CLI()\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\cli.py\", line 64, in __init__\r\n    getattr(self, args.command)()\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\cli.py\", line 69, in experiment\r\n    experiment.cli(sys.argv[2:])\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\experiment.py\", line 529, in cli\r\n    experiment(**vars(args))\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\experiment.py\", line 219, in experiment\r\n    **kwargs\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\train.py\", line 287, in full_train\r\n    random_seed=random_seed\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 339, in preprocess_for_training\r\n    random_seed=random_seed\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 485, in preprocess_for_training_by_type\r\n    random_seed=random_seed\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 614, in _preprocess_csv_for_training\r\n    random_seed=random_seed\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 68, in build_dataset\r\n    **kwargs\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 89, in build_dataset_df\r\n    global_preprocessing_parameters\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\ludwig\\data\\preprocessing.py\", line 129, in build_metadata\r\n    dataset_df[feature['name']].astype(str),\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\pandas\\core\\frame.py\", line 2980, in __getitem__\r\n    indexer = self.columns.get_loc(key)\r\n  File \"c:\\users\\tmine\\env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 2899, in get_loc\r\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\r\n  File \"pandas\\_libs\\index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas\\_libs\\index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\r\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\n  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\r\nKeyError: 'datestamp'\r\n\r\n","closed_by":{"login":"tboneai","id":56497528,"node_id":"MDQ6VXNlcjU2NDk3NTI4","avatar_url":"https://avatars.githubusercontent.com/u/56497528?v=4","gravatar_id":"","url":"https://api.github.com/users/tboneai","html_url":"https://github.com/tboneai","followers_url":"https://api.github.com/users/tboneai/followers","following_url":"https://api.github.com/users/tboneai/following{/other_user}","gists_url":"https://api.github.com/users/tboneai/gists{/gist_id}","starred_url":"https://api.github.com/users/tboneai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tboneai/subscriptions","organizations_url":"https://api.github.com/users/tboneai/orgs","repos_url":"https://api.github.com/users/tboneai/repos","events_url":"https://api.github.com/users/tboneai/events{/privacy}","received_events_url":"https://api.github.com/users/tboneai/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/543/timeline","performed_via_github_app":null,"state_reason":"completed"}