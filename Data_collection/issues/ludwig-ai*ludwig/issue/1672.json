{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/1672","id":1100713552,"node_id":"I_kwDOCbx2hs5Bm45Q","number":1672,"title":"SavedModel Exporting doesn't export ludwig layers with the model","user":{"login":"smiraldr","id":42693530,"node_id":"MDQ6VXNlcjQyNjkzNTMw","avatar_url":"https://avatars.githubusercontent.com/u/42693530?v=4","gravatar_id":"","url":"https://api.github.com/users/smiraldr","html_url":"https://github.com/smiraldr","followers_url":"https://api.github.com/users/smiraldr/followers","following_url":"https://api.github.com/users/smiraldr/following{/other_user}","gists_url":"https://api.github.com/users/smiraldr/gists{/gist_id}","starred_url":"https://api.github.com/users/smiraldr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smiraldr/subscriptions","organizations_url":"https://api.github.com/users/smiraldr/orgs","repos_url":"https://api.github.com/users/smiraldr/repos","events_url":"https://api.github.com/users/smiraldr/events{/privacy}","received_events_url":"https://api.github.com/users/smiraldr/received_events","type":"User","site_admin":false},"labels":[{"id":1434966350,"node_id":"MDU6TGFiZWwxNDM0OTY2MzUw","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/looking%20into%20it","name":"looking into it","color":"ffa54f","default":false,"description":""}],"state":"closed","locked":false,"assignee":{"login":"justinxzhao","id":3459541,"node_id":"MDQ6VXNlcjM0NTk1NDE=","avatar_url":"https://avatars.githubusercontent.com/u/3459541?v=4","gravatar_id":"","url":"https://api.github.com/users/justinxzhao","html_url":"https://github.com/justinxzhao","followers_url":"https://api.github.com/users/justinxzhao/followers","following_url":"https://api.github.com/users/justinxzhao/following{/other_user}","gists_url":"https://api.github.com/users/justinxzhao/gists{/gist_id}","starred_url":"https://api.github.com/users/justinxzhao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/justinxzhao/subscriptions","organizations_url":"https://api.github.com/users/justinxzhao/orgs","repos_url":"https://api.github.com/users/justinxzhao/repos","events_url":"https://api.github.com/users/justinxzhao/events{/privacy}","received_events_url":"https://api.github.com/users/justinxzhao/received_events","type":"User","site_admin":false},"assignees":[{"login":"justinxzhao","id":3459541,"node_id":"MDQ6VXNlcjM0NTk1NDE=","avatar_url":"https://avatars.githubusercontent.com/u/3459541?v=4","gravatar_id":"","url":"https://api.github.com/users/justinxzhao","html_url":"https://github.com/justinxzhao","followers_url":"https://api.github.com/users/justinxzhao/followers","following_url":"https://api.github.com/users/justinxzhao/following{/other_user}","gists_url":"https://api.github.com/users/justinxzhao/gists{/gist_id}","starred_url":"https://api.github.com/users/justinxzhao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/justinxzhao/subscriptions","organizations_url":"https://api.github.com/users/justinxzhao/orgs","repos_url":"https://api.github.com/users/justinxzhao/repos","events_url":"https://api.github.com/users/justinxzhao/events{/privacy}","received_events_url":"https://api.github.com/users/justinxzhao/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2022-01-12T19:29:56Z","updated_at":"2022-01-20T10:05:00Z","closed_at":"2022-01-20T10:05:00Z","author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\nWhile using export_savedmodel some ludwig layers are skipped which may result in faulty output when used for inference.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Train any bert based model on data\r\n2. Use export_savedmodel function to export \r\n\r\nLogs:\r\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\r\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\r\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\r\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\r\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <ludwig.modules.reduction_modules.SequenceReducer object at 0x7ef9fdda6160>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <ludwig.modules.reduction_modules.SequenceReducer object at 0x7ef9fdda6160>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <ludwig.modules.reduction_modules.ReduceSum object at 0x7ef9fdda6880>, because it is not built.\r\nWARNING:tensorflow:Skipping full serialization of Keras layer <ludwig.modules.reduction_modules.ReduceSum object at 0x7ef9fdda6880>, because it is not built.\r\nWARNING:absl:Found untraced functions such as dense_8_layer_call_fn, dense_8_layer_call_and_return_conditional_losses, dense_8_layer_call_fn, dense_8_layer_call_and_return_conditional_losses, dense_8_layer_call_and_return_conditional_losses while saving (showing 5 of 1055). These functions will not be directly callable after loading.\r\nINFO:tensorflow:Assets written to: models/results/BERT_128_712_bert_128_0/export/assets\r\nINFO:tensorflow:Assets written to: models/results/BERT_128_712_bert_128_0/export/assets\r\n\r\n### After loading model to tf with following\r\n\r\n`model = tf.saved_model.load(\"models/results/BERT_128_712_bert_128_0/export\")`\r\nlogs:\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).output_features.category.decoder_obj.dense.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).output_features.category.decoder_obj.dense.kernel\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).output_features.category.decoder_obj.dense.bias\r\nWARNING:tensorflow:Unresolved object in checkpoint: (root).output_features.category.decoder_obj.dense.bias\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\nWARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\r\n\r\n`print(list(model.signatures.keys()))`\r\n['serving_default']\r\n\r\n`print(model.signatures[\"serving_default\"])`\r\nConcreteFunction signature_wrapper(*, text_input_feature_8_input)\r\n  Args:\r\n    text_input_feature_8_input: int32 Tensor, shape=(None, None)\r\n  Returns:\r\n    {'category_output_feature_8': <1>, 'category_output_feature_8_1': <2>, 'category_output_feature_8_2': <3>}\r\n      <1>: float32 Tensor, shape=(None, 768)\r\n      <2>: float32 Tensor, shape=(None, 28)\r\n      <3>: float32 Tensor, shape=(None, 768)\r\n\r\nWhile my original ludwig input and output signatures were:\r\n\"config\": {\r\n        \"combiner\": {\r\n            \"type\": \"concat\"\r\n        },\r\n        \"input_features\": [\r\n            {\r\n                \"column\": \"clean_title\",\r\n                \"encoder\": \"bert\",\r\n                \"level\": \"word\",\r\n                \"name\": \"clean_title\",\r\n                \"pretrained_model_name_or_path\": \"bert-base-uncased\",\r\n                \"proc_column\": \"clean_title_mZFLky\",\r\n                \"tied\": null,\r\n                \"type\": \"text\"\r\n            }\r\n        ],\r\n        \"output_features\": [\r\n            {\r\n                \"column\": \"category\",\r\n                \"dependencies\": [],\r\n                \"loss\": {\r\n                    \"class_similarities_temperature\": 0,\r\n                    \"class_weights\": 1,\r\n                    \"confidence_penalty\": 0,\r\n                    \"labels_smoothing\": 0,\r\n                    \"robust_lambda\": 0,\r\n                    \"type\": \"softmax_cross_entropy\",\r\n                    \"weight\": 1\r\n                },\r\n                \"name\": \"category\",\r\n                \"proc_column\": \"category_mZFLky\",\r\n                \"reduce_dependencies\": \"sum\",\r\n                \"reduce_input\": \"sum\",\r\n                \"top_k\": 3,\r\n                \"type\": \"category\"\r\n            }\r\n        ]\r\n\r\n\r\nP.S. Everytime I try to export the model to savedmodel format and check the output signature - category_output_feature_8 changes to category_output_feature_{anydifferentnumber}\r\n\r\n\r\n\r\nEnvironment (please complete the following information):\r\nOS: Ubuntu 18.04\r\nVersion\r\nPython version - 3.8.10\r\nLudwig version - 0.4 tf inference (installed using pip install ludwig)\r\n","closed_by":{"login":"justinxzhao","id":3459541,"node_id":"MDQ6VXNlcjM0NTk1NDE=","avatar_url":"https://avatars.githubusercontent.com/u/3459541?v=4","gravatar_id":"","url":"https://api.github.com/users/justinxzhao","html_url":"https://github.com/justinxzhao","followers_url":"https://api.github.com/users/justinxzhao/followers","following_url":"https://api.github.com/users/justinxzhao/following{/other_user}","gists_url":"https://api.github.com/users/justinxzhao/gists{/gist_id}","starred_url":"https://api.github.com/users/justinxzhao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/justinxzhao/subscriptions","organizations_url":"https://api.github.com/users/justinxzhao/orgs","repos_url":"https://api.github.com/users/justinxzhao/repos","events_url":"https://api.github.com/users/justinxzhao/events{/privacy}","received_events_url":"https://api.github.com/users/justinxzhao/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1672/timeline","performed_via_github_app":null,"state_reason":"completed"}