{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/507","id":486238635,"node_id":"MDU6SXNzdWU0ODYyMzg2MzU=","number":507,"title":"Forcing distributed Evaluation","user":{"login":"tboo","id":19666785,"node_id":"MDQ6VXNlcjE5NjY2Nzg1","avatar_url":"https://avatars.githubusercontent.com/u/19666785?v=4","gravatar_id":"","url":"https://api.github.com/users/tboo","html_url":"https://github.com/tboo","followers_url":"https://api.github.com/users/tboo/followers","following_url":"https://api.github.com/users/tboo/following{/other_user}","gists_url":"https://api.github.com/users/tboo/gists{/gist_id}","starred_url":"https://api.github.com/users/tboo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tboo/subscriptions","organizations_url":"https://api.github.com/users/tboo/orgs","repos_url":"https://api.github.com/users/tboo/repos","events_url":"https://api.github.com/users/tboo/events{/privacy}","received_events_url":"https://api.github.com/users/tboo/received_events","type":"User","site_admin":false},"labels":[{"id":1434966350,"node_id":"MDU6TGFiZWwxNDM0OTY2MzUw","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/looking%20into%20it","name":"looking into it","color":"ffa54f","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-08-28T08:42:11Z","updated_at":"2022-06-22T23:57:55Z","closed_at":"2022-06-22T23:57:54Z","author_association":"NONE","active_lock_reason":null,"body":"Hi there -\r\n\r\nI occasionally still run into this [issue](https://github.com/horovod/horovod/issues/403) which seems to be Horovod related and specific to GPU distributed training:\r\n\r\n\r\n```\r\n[2019-08-28 08:31:55.292536: W horovod/common/operations.cc:764] One or more tensors were submitted to be reduced, gathered or broadcasted by subset of ranks and are waiting for remainder of ranks for more than 60 seconds. This may indicate that different ranks are trying to submit different tensors or that only subset of ranks is submitting tensors, which will cause deadlock. \r\nStalled ranks:\r\n0: [optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_2_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_4_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_6_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_class_predictions_class_add_grad_tuple_control_dependency_1_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_image_path_conv_0_add_grad_tuple_control_dependency_1_0 ...]\r\n1: [optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_2_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_4_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_6_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_class_predictions_class_add_grad_tuple_control_dependency_1_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_image_path_conv_0_add_grad_tuple_control_dependency_1_0 ...]\r\n2: [optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_2_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_4_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_AddN_6_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_class_predictions_class_add_grad_tuple_control_dependency_1_0, optimizer/DistributedAdamOptimizer_Allreduce/HorovodAllreduce_optimizer_gradients_image_path_conv_0_add_grad_tuple_control_dependency_1_0 ...] \r\n```\r\n\r\nI am observing that stall especially with (very) small batch sizes. The stall occurs during evaluation of the training set. Horovod/Ludwig seems to wait for evaluation results from all GPUs previously used for training but the evaluation of small batch sizes is often carried out on one (or some) GPUs but not all. If I increase the `eval_batch_size` independent of `batch_size` I get more cards busy to perform evaluation (but again, not all). With very large images the batch size is fairly limited, however.\r\n\r\nWhen all cards are being utilized for evaluation (that happens in my case only when batch sizes are large) I do not run into this issue. So I am wondering if there is a way to force the distribution of the evaluation to the same number of GPUs that is used for training. Because the number of cards used for evaluations seems rather arbitrary to me.\r\n\r\nYou should be able to reproduce that problem if you are choosing a batch size of 1 or 2 and train on more than 1 GPU.\r\n\r\nLudwig 0.2.1 (image classification tasks)\r\nOpenMPI 4.0.1\r\nHorovod 0.16\r\n4xTesla V100 PCIe (but I also observed similar behavior on an 8x V100 SMX2 infrastructure)","closed_by":{"login":"justinxzhao","id":3459541,"node_id":"MDQ6VXNlcjM0NTk1NDE=","avatar_url":"https://avatars.githubusercontent.com/u/3459541?v=4","gravatar_id":"","url":"https://api.github.com/users/justinxzhao","html_url":"https://github.com/justinxzhao","followers_url":"https://api.github.com/users/justinxzhao/followers","following_url":"https://api.github.com/users/justinxzhao/following{/other_user}","gists_url":"https://api.github.com/users/justinxzhao/gists{/gist_id}","starred_url":"https://api.github.com/users/justinxzhao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/justinxzhao/subscriptions","organizations_url":"https://api.github.com/users/justinxzhao/orgs","repos_url":"https://api.github.com/users/justinxzhao/repos","events_url":"https://api.github.com/users/justinxzhao/events{/privacy}","received_events_url":"https://api.github.com/users/justinxzhao/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/507/timeline","performed_via_github_app":null,"state_reason":"completed"}