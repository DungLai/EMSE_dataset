{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/201","id":419696737,"node_id":"MDU6SXNzdWU0MTk2OTY3Mzc=","number":201,"title":"pandas.errors.ParserError: Error tokenizing data. C error: Expected 83 fields in line 40, saw 92","user":{"login":"IzzyHibbert","id":45388237,"node_id":"MDQ6VXNlcjQ1Mzg4MjM3","avatar_url":"https://avatars.githubusercontent.com/u/45388237?v=4","gravatar_id":"","url":"https://api.github.com/users/IzzyHibbert","html_url":"https://github.com/IzzyHibbert","followers_url":"https://api.github.com/users/IzzyHibbert/followers","following_url":"https://api.github.com/users/IzzyHibbert/following{/other_user}","gists_url":"https://api.github.com/users/IzzyHibbert/gists{/gist_id}","starred_url":"https://api.github.com/users/IzzyHibbert/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/IzzyHibbert/subscriptions","organizations_url":"https://api.github.com/users/IzzyHibbert/orgs","repos_url":"https://api.github.com/users/IzzyHibbert/repos","events_url":"https://api.github.com/users/IzzyHibbert/events{/privacy}","received_events_url":"https://api.github.com/users/IzzyHibbert/received_events","type":"User","site_admin":false},"labels":[{"id":1174068771,"node_id":"MDU6TGFiZWwxMTc0MDY4Nzcx","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/feature","name":"feature","color":"0377d6","default":false,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":29,"created_at":"2019-03-11T21:40:49Z","updated_at":"2021-06-18T11:18:54Z","closed_at":"2020-02-19T08:44:38Z","author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\nHi,\r\nI am getting this error message:\r\n**pandas.errors.ParserError: Error tokenizing data. C error: Expected 83 fields in line 40, saw 92**\r\n\r\nI made all the possible in order to use a good CSV, but I guess something is not fine.\r\nAs a suggestion I believe it could be good to find a way to **handle bad lines** (**skip**) and present a value (total) of ignored lines.\r\n\r\nBack to my settings.\r\nI have UTF-8 for the saved csv file. My columns contain mostly text and some got ',' which I turned into '\\\\t' as founded in the documentation.\r\nAlso, I made the following basic test: I have re-imported the csv in Excel and made sure all the lines and columns (24 in total in my case) are in the right place. That was the case: all fine inside of MS Excel view.\r\n\r\nThe YAML has been validated correctly. I exclude it is that one my case.\r\n\r\n_Here below the lines from my terminal._\r\n\r\nludwig_version: '0.1.0'\r\ncommand: ('/Library/Frameworks/Python.framework/Versions/3.6/bin/ludwig train '\r\n '--data_csv /Users/my_mac/Projects/ML/LUDWIG/Wine/red_dataset.csv '\r\n '--model_definition_file '\r\n '/Users/my_mac/Projects/ML/LUDWIG/Wine/dataset.yaml')\r\ndataset_type: '/Users/my_mac/Projects/ML/LUDWIG/Wine/red_dataset.csv'\r\nmodel_definition: {   'combiner': {'type': 'concat'},\r\n    'input_features': [   {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Order',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Produttore',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Tipo',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Descrizione',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Vitigni',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Vigneti',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Vinificazione',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Affinamento',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Filosofia',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Temperatura',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Quando_aprire',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Ideale',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Quando_bere',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Descrizione_long',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Colore',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Profumo',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'encoder': 'parallel_cnn',\r\n                              'level': 'word',\r\n                              'name': 'Gusto',\r\n                              'tied_weights': None,\r\n                              'type': 'text'},\r\n                          {   'in_memory': True,\r\n                              'name': 'Immagine-src',\r\n                              'should_resize': False,\r\n                              'tied_weights': None,\r\n                              'type': 'image'}],\r\n    'output_features': [   {   'dependencies': [],\r\n                               'loss': {   'type': 'mean_squared_error',\r\n                                           'weight': 1},\r\n                               'name': 'Prezzo',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'numerical',\r\n                               'weight': 1},\r\n                           {   'decoder': 'generator',\r\n                               'dependencies': [],\r\n                               'level': 'char',\r\n                               'loss': {   'class_distance_temperature': 0,\r\n                                           'class_weights': 1,\r\n                                           'type': 'softmax_cross_entropy',\r\n                                           'weight': 1},\r\n                               'name': 'Denominazione',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'text',\r\n                               'weight': 1},\r\n                           {   'decoder': 'generator',\r\n                               'dependencies': [],\r\n                               'level': 'char',\r\n                               'loss': {   'class_distance_temperature': 0,\r\n                                           'class_weights': 1,\r\n                                           'type': 'softmax_cross_entropy',\r\n                                           'weight': 1},\r\n                               'name': 'Regione',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'text',\r\n                               'weight': 1},\r\n                           {   'decoder': 'generator',\r\n                               'dependencies': [],\r\n                               'level': 'char',\r\n                               'loss': {   'class_distance_temperature': 0,\r\n                                           'class_weights': 1,\r\n                                           'type': 'softmax_cross_entropy',\r\n                                           'weight': 1},\r\n                               'name': 'Gradazione',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'text',\r\n                               'weight': 1}],\r\n    'preprocessing': {   'bag': {   'fill_value': '',\r\n                                    'format': 'space',\r\n                                    'lowercase': 10000,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': False},\r\n                         'binary': {   'fill_value': 0,\r\n                                       'missing_value_strategy': 'fill_with_const'},\r\n                         'category': {   'fill_value': '<UNK>',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 10000},\r\n                         'force_split': False,\r\n                         'image': {'missing_value_strategy': 'backfill'},\r\n                         'numerical': {   'fill_value': 0,\r\n                                          'missing_value_strategy': 'fill_with_const'},\r\n                         'sequence': {   'fill_value': '',\r\n                                         'format': 'space',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 20000,\r\n                                         'padding': 'right',\r\n                                         'padding_symbol': '<PAD>',\r\n                                         'sequence_length_limit': 256,\r\n                                         'unknown_symbol': '<UNK>'},\r\n                         'set': {   'fill_value': '',\r\n                                    'format': 'space',\r\n                                    'lowercase': False,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': 10000},\r\n                         'split_probabilities': (0.7, 0.1, 0.2),\r\n                         'stratify': None,\r\n                         'text': {   'char_format': 'characters',\r\n                                     'char_most_common': 70,\r\n                                     'char_sequence_length_limit': 1024,\r\n                                     'fill_value': '',\r\n                                     'lowercase': True,\r\n                                     'missing_value_strategy': 'fill_with_const',\r\n                                     'padding': 'right',\r\n                                     'padding_symbol': '<PAD>',\r\n                                     'unknown_symbol': '<UNK>',\r\n                                     'word_format': 'space_punct',\r\n                                     'word_most_common': 20000,\r\n                                     'word_sequence_length_limit': 256},\r\n                         'timeseries': {   'fill_value': '',\r\n                                           'format': 'space',\r\n                                           'missing_value_strategy': 'fill_with_const',\r\n                                           'padding': 'right',\r\n                                           'padding_value': 0,\r\n                                           'timeseries_length_limit': 256}},\r\n    'training': {   'batch_size': 128,\r\n                    'bucketing_field': None,\r\n                    'decay': False,\r\n                    'decay_rate': 0.96,\r\n                    'decay_steps': 10000,\r\n                    'dropout_rate': 0.0,\r\n                    'early_stop': 3,\r\n                    'epochs': 10,\r\n                    'gradient_clipping': None,\r\n                    'increase_batch_size_on_plateau': 0,\r\n                    'increase_batch_size_on_plateau_max': 512,\r\n                    'increase_batch_size_on_plateau_patience': 5,\r\n                    'increase_batch_size_on_plateau_rate': 2,\r\n                    'learning_rate': 0.001,\r\n                    'learning_rate_warmup_epochs': 5,\r\n                    'optimizer': {   'beta1': 0.9,\r\n                                     'beta2': 0.999,\r\n                                     'epsilon': 1e-08,\r\n                                     'type': 'adam'},\r\n                    'reduce_learning_rate_on_plateau': 0,\r\n                    'reduce_learning_rate_on_plateau_patience': 5,\r\n                    'reduce_learning_rate_on_plateau_rate': 0.5,\r\n                    'regularization_lambda': 0,\r\n                    'regularizer': 'l2',\r\n                    'staircase': False,\r\n                    'validation_field': 'combined',\r\n                    'validation_measure': 'loss'}}\r\n\r\n\r\nUsing full raw csv, no hdf5 and json file with the same name have been found\r\nBuilding dataset (it may take a while)\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/utils/data_utils.py\", line 46, in read_csv\r\n    df = pd.read_csv(data_fp, header=header)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\", line 678, in parser_f\r\n    return _read(filepath_or_buffer, kwds)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\", line 446, in _read\r\n    data = parser.read(nrows)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1036, in read\r\n    ret = self._engine.read(nrows)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/parsers.py\", line 1848, in read\r\n    data = self._reader.read(nrows)\r\n  File \"pandas/_libs/parsers.pyx\", line 876, in pandas._libs.parsers.TextReader.read\r\n  File \"pandas/_libs/parsers.pyx\", line 891, in pandas._libs.parsers.TextReader._read_low_memory\r\n  File \"pandas/_libs/parsers.pyx\", line 945, in pandas._libs.parsers.TextReader._read_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 932, in pandas._libs.parsers.TextReader._tokenize_rows\r\n  File \"pandas/_libs/parsers.pyx\", line 2112, in pandas._libs.parsers.raise_parser_error\r\npandas.errors.ParserError: Error tokenizing data. C error: Expected 83 fields in line 40, saw 92\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/bin/ludwig\", line 11, in <module>\r\n    sys.exit(main())\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/cli.py\", line 86, in main\r\n    CLI()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/cli.py\", line 64, in __init__\r\n    getattr(self, args.command)()\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/cli.py\", line 70, in train\r\n    train.cli(sys.argv[2:])\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/train.py\", line 663, in cli\r\n    full_train(**vars(args))\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/train.py\", line 224, in full_train\r\n    random_seed=random_seed\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/data/preprocessing.py\", line 457, in preprocess_for_training\r\n    random_seed=random_seed\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/data/preprocessing.py\", line 54, in build_dataset\r\n    dataset_df = read_csv(dataset_csv)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ludwig/utils/data_utils.py\", line 48, in read_csv\r\n    logging.WARNING('Failed to parse the CSV with pandas default way,'\r\nTypeError: 'int' object is not callable","closed_by":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/201/timeline","performed_via_github_app":null,"state_reason":"completed"}