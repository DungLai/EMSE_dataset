{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/1401","id":1031507582,"node_id":"I_kwDOCbx2hs49e45-","number":1401,"title":"[ray] Implement automatic scale-up of workers over time","user":{"login":"tgaddair","id":1742912,"node_id":"MDQ6VXNlcjE3NDI5MTI=","avatar_url":"https://avatars.githubusercontent.com/u/1742912?v=4","gravatar_id":"","url":"https://api.github.com/users/tgaddair","html_url":"https://github.com/tgaddair","followers_url":"https://api.github.com/users/tgaddair/followers","following_url":"https://api.github.com/users/tgaddair/following{/other_user}","gists_url":"https://api.github.com/users/tgaddair/gists{/gist_id}","starred_url":"https://api.github.com/users/tgaddair/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tgaddair/subscriptions","organizations_url":"https://api.github.com/users/tgaddair/orgs","repos_url":"https://api.github.com/users/tgaddair/repos","events_url":"https://api.github.com/users/tgaddair/events{/privacy}","received_events_url":"https://api.github.com/users/tgaddair/received_events","type":"User","site_admin":false},"labels":[{"id":1174068771,"node_id":"MDU6TGFiZWwxMTc0MDY4Nzcx","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/feature","name":"feature","color":"0377d6","default":false,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-20T14:52:05Z","updated_at":"2021-10-20T14:52:44Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"One of the key takeaways from the [Elastic Horovod](https://eng.uber.com/horovod-ray/) work was that there are cost, throughput, and convergence benefits to incrementally scaling up over time, similar to the `increase_batch_size_on_plateau` on param.\r\n\r\nFor distributed training, we can adjust this behavior to, instead of increasing the per-worker batch size (which we can assume is already set to maximize GPU utilization), we can double the batch size by adding additional training workers to the process (we can enforce `increase_batch_size_on_plateau_rate = 2` when using Ray for now to achieve this).\r\n\r\nThis can be done naively at first, by simply raise an exception during training when the batch size increase is triggered. This can then be handled on the Ray Train side, which can catch the exception and re-run training with the new workers, resuming from the last checkpoint.\r\n\r\nIn the future, once Ray Train and RayDatasets supports elastic repartitioning, we can do this with Elastic Horovod.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1401/timeline","performed_via_github_app":null,"state_reason":null}