{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/1093","id":802991060,"node_id":"MDU6SXNzdWU4MDI5OTEwNjA=","number":1093,"title":"Reported epoch loss value for single output feature model does not match reported epoch combined loss","user":{"login":"jimthompson5802","id":1425269,"node_id":"MDQ6VXNlcjE0MjUyNjk=","avatar_url":"https://avatars.githubusercontent.com/u/1425269?v=4","gravatar_id":"","url":"https://api.github.com/users/jimthompson5802","html_url":"https://github.com/jimthompson5802","followers_url":"https://api.github.com/users/jimthompson5802/followers","following_url":"https://api.github.com/users/jimthompson5802/following{/other_user}","gists_url":"https://api.github.com/users/jimthompson5802/gists{/gist_id}","starred_url":"https://api.github.com/users/jimthompson5802/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimthompson5802/subscriptions","organizations_url":"https://api.github.com/users/jimthompson5802/orgs","repos_url":"https://api.github.com/users/jimthompson5802/repos","events_url":"https://api.github.com/users/jimthompson5802/events{/privacy}","received_events_url":"https://api.github.com/users/jimthompson5802/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-07T16:22:41Z","updated_at":"2021-06-29T00:47:51Z","closed_at":"2021-06-29T00:47:33Z","author_association":"COLLABORATOR","active_lock_reason":null,"body":"**Describe the bug**\r\nFor a single output feature model, the per epoch training/validation/test output feature loss value does not equal the `combined` training/validation/test loss values\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Run `examples/mnist/simple_model_training.py` with `logging_level=logging.INFO`\r\n2. See following extract of log file.  The output feature `label`'s loss values are not equal to the `combined` loss value for every epoch.  There are 'relatively' close but not equal.\r\n```\r\n╒══════════╕\r\n│ TRAINING │\r\n╘══════════╛\r\n\r\n\r\nEpoch 1\r\nTraining: 100%|██████████| 469/469 [00:23<00:00, 19.83it/s]\r\nEvaluation train: 100%|██████████| 469/469 [00:07<00:00, 62.35it/s]\r\nEvaluation test : 100%|██████████| 79/79 [00:01<00:00, 77.71it/s]\r\nTook 32.2061s\r\n╒═════════╤════════╤════════════╤═════════════╕\r\n│ label   │   loss │   accuracy │   hits_at_k │\r\n╞═════════╪════════╪════════════╪═════════════╡\r\n│ train   │ 0.1175 │     0.9683 │      0.9966 │\r\n├─────────┼────────┼────────────┼─────────────┤\r\n│ test    │ 0.1067 │     0.9708 │      0.9970 │\r\n╘═════════╧════════╧════════════╧═════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.0960 │\r\n├────────────┼────────┤\r\n│ test       │ 0.0831 │\r\n╘════════════╧════════╛\r\n\r\n\r\nEpoch 2\r\nTraining: 100%|██████████| 469/469 [00:20<00:00, 22.43it/s]\r\nEvaluation train: 100%|██████████| 469/469 [00:05<00:00, 86.99it/s]\r\nEvaluation test : 100%|██████████| 79/79 [00:01<00:00, 42.91it/s]\r\nTook 28.1646s\r\n╒═════════╤════════╤════════════╤═════════════╕\r\n│ label   │   loss │   accuracy │   hits_at_k │\r\n╞═════════╪════════╪════════════╪═════════════╡\r\n│ train   │ 0.0675 │     0.9838 │      0.9989 │\r\n├─────────┼────────┼────────────┼─────────────┤\r\n│ test    │ 0.0662 │     0.9830 │      0.9983 │\r\n╘═════════╧════════╧════════════╧═════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.0527 │\r\n├────────────┼────────┤\r\n│ test       │ 0.0476 │\r\n╘════════════╧════════╛\r\n\r\n\r\nEpoch 3\r\nTraining: 100%|██████████| 469/469 [00:22<00:00, 21.27it/s]\r\nEvaluation train: 100%|██████████| 469/469 [00:05<00:00, 81.35it/s]\r\nEvaluation test : 100%|██████████| 79/79 [00:00<00:00, 95.46it/s]\r\nTook 28.6584s\r\n╒═════════╤════════╤════════════╤═════════════╕\r\n│ label   │   loss │   accuracy │   hits_at_k │\r\n╞═════════╪════════╪════════════╪═════════════╡\r\n│ train   │ 0.0472 │     0.9875 │      0.9991 │\r\n├─────────┼────────┼────────────┼─────────────┤\r\n│ test    │ 0.0472 │     0.9863 │      0.9993 │\r\n╘═════════╧════════╧════════════╧═════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.0428 │\r\n├────────────┼────────┤\r\n│ test       │ 0.0391 │\r\n╘════════════╧════════╛\r\n\r\n\r\nEpoch 4\r\nTraining: 100%|██████████| 469/469 [00:21<00:00, 22.08it/s]\r\nEvaluation train: 100%|██████████| 469/469 [00:04<00:00, 98.50it/s] \r\nEvaluation test : 100%|██████████| 79/79 [00:00<00:00, 88.37it/s]\r\nTook 26.9187s\r\n╒═════════╤════════╤════════════╤═════════════╕\r\n│ label   │   loss │   accuracy │   hits_at_k │\r\n╞═════════╪════════╪════════════╪═════════════╡\r\n│ train   │ 0.0400 │     0.9894 │      0.9996 │\r\n├─────────┼────────┼────────────┼─────────────┤\r\n│ test    │ 0.0422 │     0.9867 │      0.9993 │\r\n╘═════════╧════════╧════════════╧═════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.0381 │\r\n├────────────┼────────┤\r\n│ test       │ 0.0375 │\r\n╘════════════╧════════╛\r\n\r\n\r\nEpoch 5\r\nTraining: 100%|██████████| 469/469 [00:19<00:00, 24.25it/s]\r\nEvaluation train: 100%|██████████| 469/469 [00:04<00:00, 95.41it/s]\r\nEvaluation test : 100%|██████████| 79/79 [00:00<00:00, 96.54it/s]\r\nTook 25.9466s\r\n╒═════════╤════════╤════════════╤═════════════╕\r\n│ label   │   loss │   accuracy │   hits_at_k │\r\n╞═════════╪════════╪════════════╪═════════════╡\r\n│ train   │ 0.0291 │     0.9931 │      0.9997 │\r\n├─────────┼────────┼────────────┼─────────────┤\r\n│ test    │ 0.0324 │     0.9907 │      0.9997 │\r\n╘═════════╧════════╧════════════╧═════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.0219 │\r\n├────────────┼────────┤\r\n│ test       │ 0.0224 │\r\n╘════════════╧════════╛\r\n\r\n```\r\n\r\n\r\n**Expected behavior**\r\nWith only a single output feature, I was expecting the `combined` loss value to equal the single output feature's loss value per epoch\r\n\r\n**Screenshots**\r\nSee log extract above\r\n\r\n**Environment (please complete the following information):**\r\n - OS: MacOS 10.15.7, running Docker for Mac 3.04\r\n- Python version 3.6.9\r\n- Ludwig version 0.3.2. 0.3.3, 0.4-dev0\r\n\r\n**Additional context**\r\nFrom what I can tell this is solely reporting issue and does not affect a model's ability for convergence. \r\n\r\nI have possible root cause for the reporting issue.  In the context of the categorical feature, the `CategoryOutputFeature._setup_loss()` and `CategoryOutputFeature._setup_metrics()` methods may have an unintended interaction.  In `CategoryOutputFeature._setup_loss()` there is this code fragment:\r\n```\r\n        self.eval_loss_function = SoftmaxCrossEntropyMetric(\r\n            num_classes=self.num_classes,\r\n            feature_loss=self.loss,\r\n            name='eval_loss'\r\n        )\r\n```\r\nand in `CategoryOutputFeature._setup_loss()`\r\n```\r\n        self.metric_functions[LOSS] = self.eval_loss_function\r\n```\r\nAt conclusion of output feature construction, both `self.eval_loss_function` and `self.metric_functions[LOSS]` are pointing to the same instance of a Keras Metric object.\r\n\r\nSo during the training loop, this metric object is called twice, once in the context of calculating the loss for the output feature and a second time to capture the `combined` loss.  During training the `combined` loss is accumulated in `ECD.eval_loss_metric` for reporting purposes.  This results in the epoch loss calculations using different values for the output feature and `combined`.\r\n\r\nBased on testing I've done, the fix is to change `CategoryOutputFeature.eval_loss_function` to be a Keras Loss function and not a metric and revise metric specification to `CategoryOutputFeture.metric_functions[LOSS]` to not reuse `CategoryOutputFeature.eval_loss_function`.\r\n\r\nFurther, this reporting issue appears to be present in other output features.  Similar changes are needed in the other output features' `_setup_loss()` and `_setup_metrics()` methods.\r\n\r\nI'll start a PR to address this issue.","closed_by":{"login":"jimthompson5802","id":1425269,"node_id":"MDQ6VXNlcjE0MjUyNjk=","avatar_url":"https://avatars.githubusercontent.com/u/1425269?v=4","gravatar_id":"","url":"https://api.github.com/users/jimthompson5802","html_url":"https://github.com/jimthompson5802","followers_url":"https://api.github.com/users/jimthompson5802/followers","following_url":"https://api.github.com/users/jimthompson5802/following{/other_user}","gists_url":"https://api.github.com/users/jimthompson5802/gists{/gist_id}","starred_url":"https://api.github.com/users/jimthompson5802/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jimthompson5802/subscriptions","organizations_url":"https://api.github.com/users/jimthompson5802/orgs","repos_url":"https://api.github.com/users/jimthompson5802/repos","events_url":"https://api.github.com/users/jimthompson5802/events{/privacy}","received_events_url":"https://api.github.com/users/jimthompson5802/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1093/timeline","performed_via_github_app":null,"state_reason":"completed"}