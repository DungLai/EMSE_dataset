{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/408","id":463248637,"node_id":"MDU6SXNzdWU0NjMyNDg2Mzc=","number":408,"title":"Adding tokenization and stop words removal for Chinese Language","user":{"login":"prakass1","id":6094960,"node_id":"MDQ6VXNlcjYwOTQ5NjA=","avatar_url":"https://avatars.githubusercontent.com/u/6094960?v=4","gravatar_id":"","url":"https://api.github.com/users/prakass1","html_url":"https://github.com/prakass1","followers_url":"https://api.github.com/users/prakass1/followers","following_url":"https://api.github.com/users/prakass1/following{/other_user}","gists_url":"https://api.github.com/users/prakass1/gists{/gist_id}","starred_url":"https://api.github.com/users/prakass1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prakass1/subscriptions","organizations_url":"https://api.github.com/users/prakass1/orgs","repos_url":"https://api.github.com/users/prakass1/repos","events_url":"https://api.github.com/users/prakass1/events{/privacy}","received_events_url":"https://api.github.com/users/prakass1/received_events","type":"User","site_admin":false},"labels":[{"id":1174068771,"node_id":"MDU6TGFiZWwxMTc0MDY4Nzcx","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/feature","name":"feature","color":"0377d6","default":false,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-07-02T14:03:45Z","updated_at":"2020-06-18T00:22:19Z","closed_at":"2020-06-18T00:19:26Z","author_association":"NONE","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\r\nThere is issue already mentioned at:\r\nhttps://github.com/uber/ludwig/issues/122\r\n\r\n**Describe the use case**\r\nA clear and concise description of what the use case for this feature is.\r\n\r\nThe feature is to add function similar to english_tokenize and english_remove_stopwords() for chinese text.\r\n\r\n\r\n**Describe the solution you'd like**\r\nA clear and concise description of what you want to happen.\r\n\r\nI have tried this already by adding couple of functions as below  \r\n\r\nstring_utils.py\r\n`def chinese_tokenize(text):\r\n    return process_text(text, load_nlp_pipeline('zh'))`\r\n\r\n`def chinese_tokenize_remove_stopwords(text):\r\n    return process_text(text, load_nlp_pipeline('zh'),\r\n                        filter_stopwords=True)`\r\n\r\nAnd have added the functions into format_registry.\r\n\r\nIn nlp_utils.py:\r\n\r\nfrom spacy.lang.zh import Chinese\r\nnlp_pipelines = {\r\n    'en': None,\r\n    'it': None,\r\n    'es': None,\r\n    'de': None,\r\n    'fr': None,\r\n    'pt': None,\r\n    'nl': None,\r\n    'el': None,\r\n    'zh': None, # This is added\r\n    'xx': None\r\n}\r\nlanguage_module_registry = {\r\n    'en': 'en_core_web_sm',\r\n    'it': 'it_core_news_sm',\r\n    'es': 'es_core_news_sm',\r\n    'de': 'de_core_news_sm',\r\n    'fr': 'fr_core_news_sm',\r\n    'pt': 'pt_core_news_sm',\r\n    'nl': 'nl_core_news_sm',\r\n    'el': 'el_core_news_sm',\r\n    'zh': 'zh_core', # This is a dummy name that i have choosen and is checked below\r\n    'xx': 'xx_ent_wiki_sm'\r\n}\r\ndefault_characters = [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j',\r\n                      'k', 'l', 'm', 'n', 'o', 'p',\r\n                      'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0',\r\n                      '1', '2', '3', '4', '5', '6',\r\n                      '8', '9', '-', ',', ';', '.', '!', '?', ':', '\\'', '\\'',\r\n                      '/', '\\\\', '|', '_', '@', '#',\r\n                      '$', '%', '^', '&', '*', '~', '`', '+', '-', '=', '<',\r\n                      '>', '(', ')', '[', ']', '{',\r\n                      '}']\r\npunctuation = {'.', ',', '@', '$', '%', '/', ':', ';', '+', '='}\r\n\r\ndef load_nlp_pipeline(language='xx'):\r\n    if language not in language_module_registry:\r\n        logging.error(\r\n            'Language {} is not supported.'\r\n            'Suported languages are: {}'.format(\r\n                language,\r\n                language_module_registry.keys()\r\n            ))\r\n        raise ValueError\r\n    else:\r\n        spacy_module_name = language_module_registry[language]\r\n    global nlp_pipelines\r\n    if nlp_pipelines[language] is None:\r\n        logging.info('Loading NLP pipeline')\r\n        import spacy\r\n        try:\r\n            # For Chinese\r\n            if spacy_module_name is 'zh_core':\r\n                nlp_pipelines[language] = Chinese() # Adding Chinese class for tokenizing ...\r\n            else:\r\n                nlp_pipelines[language] = spacy.load(\r\n                    spacy_module_name,\r\n                    disable=['parser', 'tagger', 'ner']\r\n            )\r\n        except OSError:\r\n            logging.error(\r\n                ' Unable to load spacy model {}. '\r\n                'Make sure to download it with: '\r\n                'python -m spacy download {}'.format(\r\n                    spacy_module_name,\r\n                    spacy_module_name\r\n                ))\r\n            sys.exit(-1)\r\n    return nlp_pipelines[language]\r\n\r\nSome output of the test is:\r\n\r\nprint(process_text('客户说，当钥匙关闭时，它会变硬并且气缸需要强制关闭。',\r\n                   load_nlp_pipeline(language='zh')))\r\n['客户说，当钥匙关闭时，它会变硬并且气缸需要强制关闭', `'。']\r\n\r\nprint(process_text('客户说，当钥匙关闭时，它会变硬并且气缸需要强制关闭。',`\r\n                   load_nlp_pipeline(language='zh'), filter_stopwords=True))\r\n['客户说，当钥匙关闭时，它会变硬并且气缸需要强制关闭']\r\n\r\nProviding word_format as word_format: chinese_tokenize in yaml works.\r\n\r\nIf it is suitable, I think it can be added.","closed_by":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/408/timeline","performed_via_github_app":null,"state_reason":"completed"}