{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/2633","id":1406768396,"node_id":"I_kwDOCbx2hs5T2ZUM","number":2633,"title":"Presence of hyperopt in config causes issue with model.train","user":{"login":"arnavgarg1","id":106701836,"node_id":"U_kgDOBlwkDA","avatar_url":"https://avatars.githubusercontent.com/u/106701836?v=4","gravatar_id":"","url":"https://api.github.com/users/arnavgarg1","html_url":"https://github.com/arnavgarg1","followers_url":"https://api.github.com/users/arnavgarg1/followers","following_url":"https://api.github.com/users/arnavgarg1/following{/other_user}","gists_url":"https://api.github.com/users/arnavgarg1/gists{/gist_id}","starred_url":"https://api.github.com/users/arnavgarg1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arnavgarg1/subscriptions","organizations_url":"https://api.github.com/users/arnavgarg1/orgs","repos_url":"https://api.github.com/users/arnavgarg1/repos","events_url":"https://api.github.com/users/arnavgarg1/events{/privacy}","received_events_url":"https://api.github.com/users/arnavgarg1/received_events","type":"User","site_admin":false},"labels":[{"id":1174068769,"node_id":"MDU6TGFiZWwxMTc0MDY4NzY5","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-10-12T20:27:29Z","updated_at":"2022-10-13T22:22:37Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\nWhen the hyperopt section exists in a Ludwig config but one just wants to run regular model training, it results in really large step sizes. For example, this was seen with the rotten tomatoes dataset.\r\n\r\n```\r\n╒══════════╕\r\n│ TRAINING │\r\n╘══════════╛\r\n\r\nTraining for 2425746845692806037241 step(s), approximately 9223372036854775808 epoch(s).\r\nEarly stopping policy: -1 round(s) of evaluation, or -263 step(s), approximately -1 epoch(s).\r\n\r\nStarting with step 0, epoch: 0\r\nTraining:   0%|                                           | 243/2425746845692806037241 [00:00<2085587686282003:37:36, 323.08it/s]\r\nRunning evaluation for step: 263, epoch: 0\r\nEvaluation train: 100%|██████████████████████████████████████████████████████████████████████| 263/263 [00:00<00:00, 1043.65it/s]\r\nEvaluation valid: 100%|████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 1131.75it/s]\r\nEvaluation test : 100%|████████████████████████████████████████████████████████████████████████| 75/75 [00:00<00:00, 1119.07it/s]\r\n╒═══════════════╤════════╤═══════════╤════════════╕\r\n│ recommended   │   loss │   roc_auc │   accuracy │\r\n╞═══════════════╪════════╪═══════════╪════════════╡\r\n│ train         │ 0.6531 │    0.5854 │     0.6279 │\r\n├───────────────┼────────┼───────────┼────────────┤\r\n│ validation    │ 0.6640 │    0.5612 │     0.6202 │\r\n├───────────────┼────────┼───────────┼────────────┤\r\n│ test          │ 0.6575 │    0.5719 │     0.6291 │\r\n╘═══════════════╧════════╧═══════════╧════════════╛\r\n╒════════════╤════════╕\r\n│ combined   │   loss │\r\n╞════════════╪════════╡\r\n│ train      │ 0.6531 │\r\n├────────────┼────────┤\r\n│ validation │ 0.6640 │\r\n├────────────┼────────┤\r\n│ test       │ 0.6575 │\r\n╘════════════╧════════╛\r\nValidation roc_auc on recommended improved, model saved.\r\n\r\nTraining:   0%|                                           | 409/2425746845692806037241 [00:01<2634330423846122:57:04, 255.78it/s]^C\r\nReceived SIGINT, will finish this training step and then conclude training.\r\nSend another SIGINT to immediately interrupt the process.\r\nTraining:   0%|                                           | 423/2425746845692806037241 [00:01<2804679651448325:41:20, 240.25it/s]\r\n```\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Download the rotten tomatoes dataset from here: [here](https://ludwig.ai/latest/data/rotten_tomatoes_test.csv)\r\n2. Run ```ludwig init_config --dataset /data/rotten_tomatoes.csv --target=recommended --hyperopt=true --time_limit_s=300 --output /data/rotten_tomatoes.yaml```\r\n3. Run ```ludwig train --config rotten_tomatoes.yaml --dataset /data/rotten_tomatoes.csv```\r\n\r\n**Expected behavior**\r\nThis should produce a reasonable number of steps per epoch and epochs.\r\n\r\n**Environment (please complete the following information):**\r\n\r\n- OS: \\[e.g. iOS\\]: MacOS 12.6\r\n- Python version: 3.8\r\n- Ludwig version: 0.6.1\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2633/timeline","performed_via_github_app":null,"state_reason":null}