{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/2361","id":1332264800,"node_id":"I_kwDOCbx2hs5PaL9g","number":2361,"title":" [Question] Best practice to use remote storage","user":{"login":"Jeffwan","id":4739316,"node_id":"MDQ6VXNlcjQ3MzkzMTY=","avatar_url":"https://avatars.githubusercontent.com/u/4739316?v=4","gravatar_id":"","url":"https://api.github.com/users/Jeffwan","html_url":"https://github.com/Jeffwan","followers_url":"https://api.github.com/users/Jeffwan/followers","following_url":"https://api.github.com/users/Jeffwan/following{/other_user}","gists_url":"https://api.github.com/users/Jeffwan/gists{/gist_id}","starred_url":"https://api.github.com/users/Jeffwan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Jeffwan/subscriptions","organizations_url":"https://api.github.com/users/Jeffwan/orgs","repos_url":"https://api.github.com/users/Jeffwan/repos","events_url":"https://api.github.com/users/Jeffwan/events{/privacy}","received_events_url":"https://api.github.com/users/Jeffwan/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-08-08T18:55:21Z","updated_at":"2022-08-09T20:32:54Z","closed_at":"2022-08-09T20:32:53Z","author_association":"NONE","active_lock_reason":null,"body":"I am using input file in S3 or HDFS. I am trying to understand any tricks using dataset in remote store. \r\n\r\nI notice ludwig CLi like `init_config` etc read data file, the preprocessing and training part, the ray cluster reads file as well. \r\n\r\n1. does ludwig operation like `init_config` underneath use ray dataset as well? I didn't see s3 or hdfs protocol support in ludwig code base, I assume Ray plays the role here?\r\n\r\n2. If I use remote Ray cluster as executor and ludwig program as the driver. Using S3 as an example, I think I should set AWS env in both side? \r\n\r\n3. Maybe it's a good idea to support dumping config file directly in S3 so we do not need to copy file to remote any more.\r\n\r\n```\r\nludwig init_config --dataset s3://ludwig-automl/rotten_tomatoes.csv --target=recommended --hyperopt=true --time_limit_s=300 --output s3://ludwig-automl/rotten_tomatoes.yaml\r\nNumExpr defaulting to 8 threads.\r\n/usr/local/lib/python3.8/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\r\n  warnings.warn(\r\n███████████████████████\r\n█ █ █ █  ▜█ █ █ █ █   █\r\n█ █ █ █ █ █ █ █ █ █ ███\r\n█ █   █ █ █ █ █ █ █ ▌ █\r\n█ █████ █ █ █ █ █ █ █ █\r\n█     █  ▟█     █ █   █\r\n███████████████████████\r\nludwig v0.6.dev - Init Config\r\n\r\nInitializing new Ray cluster...\r\n2022-08-08 18:54:36,720\tINFO services.py:1470 -- View the Ray dashboard at http://127.0.0.1:8265\r\n2022-08-08 18:54:36,769\tWARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.81gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\r\nFound credentials in environment variables.\r\n.......\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/ludwig\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.8/site-packages/ludwig/cli.py\", line 166, in main\r\n    CLI()\r\n  File \"/usr/local/lib/python3.8/site-packages/ludwig/cli.py\", line 66, in __init__\r\n    getattr(self, args.command)()\r\n  File \"/usr/local/lib/python3.8/site-packages/ludwig/cli.py\", line 151, in init_config\r\n    automl.cli_init_config(sys.argv[2:])\r\n  File \"/usr/local/lib/python3.8/site-packages/ludwig/automl/automl.py\", line 414, in cli_init_config\r\n    init_config(**vars(args))\r\n  File \"/usr/local/lib/python3.8/site-packages/ludwig/automl/automl.py\", line 341, in init_config\r\n    with open(output, \"w\") as f:\r\nFileNotFoundError: [Errno 2] No such file or directory: 's3://ludwig-automl/rotten_tomatoes.yaml'\r\n```\r\n","closed_by":{"login":"tgaddair","id":1742912,"node_id":"MDQ6VXNlcjE3NDI5MTI=","avatar_url":"https://avatars.githubusercontent.com/u/1742912?v=4","gravatar_id":"","url":"https://api.github.com/users/tgaddair","html_url":"https://github.com/tgaddair","followers_url":"https://api.github.com/users/tgaddair/followers","following_url":"https://api.github.com/users/tgaddair/following{/other_user}","gists_url":"https://api.github.com/users/tgaddair/gists{/gist_id}","starred_url":"https://api.github.com/users/tgaddair/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tgaddair/subscriptions","organizations_url":"https://api.github.com/users/tgaddair/orgs","repos_url":"https://api.github.com/users/tgaddair/repos","events_url":"https://api.github.com/users/tgaddair/events{/privacy}","received_events_url":"https://api.github.com/users/tgaddair/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2361/timeline","performed_via_github_app":null,"state_reason":"completed"}