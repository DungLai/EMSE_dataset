{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/1145","id":852839648,"node_id":"MDU6SXNzdWU4NTI4Mzk2NDg=","number":1145,"title":"Maybe a conflict between pakages of auto-sklearn and ludwig","user":{"login":"zuliani99","id":50734202,"node_id":"MDQ6VXNlcjUwNzM0MjAy","avatar_url":"https://avatars.githubusercontent.com/u/50734202?v=4","gravatar_id":"","url":"https://api.github.com/users/zuliani99","html_url":"https://github.com/zuliani99","followers_url":"https://api.github.com/users/zuliani99/followers","following_url":"https://api.github.com/users/zuliani99/following{/other_user}","gists_url":"https://api.github.com/users/zuliani99/gists{/gist_id}","starred_url":"https://api.github.com/users/zuliani99/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zuliani99/subscriptions","organizations_url":"https://api.github.com/users/zuliani99/orgs","repos_url":"https://api.github.com/users/zuliani99/repos","events_url":"https://api.github.com/users/zuliani99/events{/privacy}","received_events_url":"https://api.github.com/users/zuliani99/received_events","type":"User","site_admin":false},"labels":[{"id":1174068775,"node_id":"MDU6TGFiZWwxMTc0MDY4Nzc1","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/waiting%20for%20answer","name":"waiting for answer","color":"fff36b","default":false,"description":"Further information is requested"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-04-07T21:37:36Z","updated_at":"2021-04-24T18:54:45Z","closed_at":"2021-04-24T18:54:45Z","author_association":"NONE","active_lock_reason":null,"body":"I'm trying to do a benchmark of autosklearn and ludwig like this:\r\n\r\n```\r\nfrom algorithms.ludwig import ludwig_class \r\nfrom algorithms.auto_sklearn import autoSklearn_class\r\nimport openml\r\n\r\nX, y = fetch_openml(data_id=727, as_frame=True, return_X_y=True, cache=True)\r\ny = y.to_frame()\r\nX[y.columns[0]] = y\r\ndf = X\r\n\r\nprint(\"--------------------------------AUTOSKLEARN--------------------------------\")\r\nprint(autoSklearn_class(df))\r\nprint(\"--------------------------------AUTOSKLEARN--------------------------------\\n\\n\")\r\n\r\nprint(\"--------------------------------LUDWIG--------------------------------\")\r\nprint(ludwig_class(df))\r\nprint(\"--------------------------------LUDWIG--------------------------------\\n\\n\")\r\n```\r\n\r\n\r\n When I lunch the single algoritm stand alone with a test dataset it works, but when I try to lunch first autosklearn and then ludwig, the first one finish but with lots of worning regarding tensorflow, I don't know why tensorflow because autosklearn does not use it, and then ludwig finish with all these stuff:\r\n\r\n\r\n```\r\nludwig_version: '0.3.3'\r\ncommand: 'start.py'\r\ncommit_hash: '3a6ea0c18073'\r\nrandom_seed: 42\r\ndata_format: \"<class 'pandas.core.frame.DataFrame'>\"\r\nconfig: {   'combiner': {'type': 'concat'},\r\n    'input_features': [   {   'column': 'x1',\r\n                              'name': 'x1',\r\n                              'proc_column': 'x1_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x2',\r\n                              'name': 'x2',\r\n                              'proc_column': 'x2_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x3',\r\n                              'name': 'x3',\r\n                              'proc_column': 'x3_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x4',\r\n                              'name': 'x4',\r\n                              'proc_column': 'x4_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x5',\r\n                              'name': 'x5',\r\n                              'proc_column': 'x5_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x6',\r\n                              'name': 'x6',\r\n                              'proc_column': 'x6_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x7',\r\n                              'name': 'x7',\r\n                              'proc_column': 'x7_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x8',\r\n                              'name': 'x8',\r\n                              'proc_column': 'x8_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x9',\r\n                              'name': 'x9',\r\n                              'proc_column': 'x9_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'},\r\n                          {   'column': 'x10',\r\n                              'name': 'x10',\r\n                              'proc_column': 'x10_mZFLky',\r\n                              'tied': None,\r\n                              'type': 'numerical'}],\r\n    'output_features': [   {   'column': 'binaryClass',\r\n                               'decoder': 'generator',\r\n                               'dependencies': [],\r\n                               'level': 'word',\r\n                               'loss': {   'class_similarities_temperature': 0,\r\n                                           'class_weights': 1,\r\n                                           'confidence_penalty': 0,\r\n                                           'distortion': 1,\r\n                                           'labels_smoothing': 0,\r\n                                           'negative_samples': 0,\r\n                                           'robust_lambda': 0,\r\n                                           'sampler': None,\r\n                                           'type': 'softmax_cross_entropy',\r\n                                           'unique': False,\r\n                                           'weight': 1},\r\n                               'name': 'binaryClass',\r\n                               'proc_column': 'binaryClass_mZFLky',\r\n                               'reduce_dependencies': 'sum',\r\n                               'reduce_input': 'sum',\r\n                               'type': 'text'}],\r\n    'preprocessing': {   'audio': {   'audio_feature': {'type': 'raw'},\r\n                                      'audio_file_length_limit_in_s': 7.5,\r\n                                      'in_memory': True,\r\n                                      'missing_value_strategy': 'backfill',\r\n                                      'norm': None,\r\n                                      'padding_value': 0},\r\n                         'bag': {   'fill_value': '<UNK>',\r\n                                    'lowercase': False,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': 10000,\r\n                                    'tokenizer': 'space'},\r\n                         'binary': {   'fill_value': 0,\r\n                                       'missing_value_strategy': 'fill_with_const'},\r\n                         'category': {   'fill_value': '<UNK>',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 10000},\r\n                         'date': {   'datetime_format': None,\r\n                                     'fill_value': '',\r\n                                     'missing_value_strategy': 'fill_with_const'},\r\n                         'force_split': False,\r\n                         'h3': {   'fill_value': 576495936675512319,\r\n                                   'missing_value_strategy': 'fill_with_const'},\r\n                         'image': {   'in_memory': True,\r\n                                      'missing_value_strategy': 'backfill',\r\n                                      'num_processes': 1,\r\n                                      'resize_method': 'interpolate',\r\n                                      'scaling': 'pixel_normalization'},\r\n                         'numerical': {   'fill_value': 0,\r\n                                          'missing_value_strategy': 'fill_with_const',\r\n                                          'normalization': None},\r\n                         'sequence': {   'fill_value': '<UNK>',\r\n                                         'lowercase': False,\r\n                                         'missing_value_strategy': 'fill_with_const',\r\n                                         'most_common': 20000,\r\n                                         'padding': 'right',\r\n                                         'padding_symbol': '<PAD>',\r\n                                         'sequence_length_limit': 256,\r\n                                         'tokenizer': 'space',\r\n                                         'unknown_symbol': '<UNK>',\r\n                                         'vocab_file': None},\r\n                         'set': {   'fill_value': '<UNK>',\r\n                                    'lowercase': False,\r\n                                    'missing_value_strategy': 'fill_with_const',\r\n                                    'most_common': 10000,\r\n                                    'tokenizer': 'space'},\r\n                         'split_probabilities': (0.7, 0.1, 0.2),\r\n                         'stratify': None,\r\n                         'text': {   'char_most_common': 70,\r\n                                     'char_sequence_length_limit': 1024,\r\n                                     'char_tokenizer': 'characters',\r\n                                     'char_vocab_file': None,\r\n                                     'fill_value': '<UNK>',\r\n                                     'lowercase': True,\r\n                                     'missing_value_strategy': 'fill_with_const',\r\n                                     'padding': 'right',\r\n                                     'padding_symbol': '<PAD>',\r\n                                     'pretrained_model_name_or_path': None,\r\n                                     'unknown_symbol': '<UNK>',\r\n                                     'word_most_common': 20000,\r\n                                     'word_sequence_length_limit': 256,\r\n                                     'word_tokenizer': 'space_punct',\r\n                                     'word_vocab_file': None},\r\n                         'timeseries': {   'fill_value': '',\r\n                                           'missing_value_strategy': 'fill_with_const',\r\n                                           'padding': 'right',\r\n                                           'padding_value': 0,\r\n                                           'timeseries_length_limit': 256,\r\n                                           'tokenizer': 'space'},\r\n                         'vector': {   'fill_value': '',\r\n                                       'missing_value_strategy': 'fill_with_const'}},\r\n    'training': {   'batch_size': 128,\r\n                    'bucketing_field': None,\r\n                    'decay': False,\r\n                    'decay_rate': 0.96,\r\n                    'decay_steps': 10000,\r\n                    'early_stop': 5,\r\n                    'epochs': 5,\r\n                    'eval_batch_size': 0,\r\n                    'gradient_clipping': None,\r\n                    'increase_batch_size_on_plateau': 0,\r\n                    'increase_batch_size_on_plateau_max': 512,\r\n                    'increase_batch_size_on_plateau_patience': 5,\r\n                    'increase_batch_size_on_plateau_rate': 2,\r\n                    'learning_rate': 0.001,\r\n                    'learning_rate_warmup_epochs': 1,\r\n                    'optimizer': {   'beta_1': 0.9,\r\n                                     'beta_2': 0.999,\r\n                                     'epsilon': 1e-08,\r\n                                     'type': 'adam'},\r\n                    'reduce_learning_rate_on_plateau': 0,\r\n                    'reduce_learning_rate_on_plateau_patience': 5,\r\n                    'reduce_learning_rate_on_plateau_rate': 0.5,\r\n                    'regularization_lambda': 0,\r\n                    'regularizer': 'l2',\r\n                    'staircase': False,\r\n                    'validation_field': 'binaryClass',\r\n                    'validation_metric': 'last_accuracy'}}\r\ntf_version: '2.4.1'\r\n\r\n\r\nUsing full dataframe\r\nBuilding dataset (it may take a while)\r\n/home/riccardo/.local/lib/python3.8/site-packages/ludwig/features/numerical_feature.py:53: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n  dataset_df[feature[COLUMN]] = backend.df_engine.df_lib.to_numeric(\r\n/home/riccardo/.local/lib/python3.8/site-packages/ludwig/data/preprocessing.py:1189: SettingWithCopyWarning: \r\nA value is trying to be set on a copy of a slice from a DataFrame.\r\nTry using .loc[row_indexer,col_indexer] = value instead\r\n\r\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n  dataset_df[feature[COLUMN]] = dataset_df[feature[COLUMN]].fillna(\r\nTraining set: 22900\r\nValidation set: 3209\r\nTest set: 6505\r\n2021-04-07 23:19:19.376229: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-04-07 23:19:19.380039: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n\r\n╒══════════╕\r\n│ TRAINING │\r\n╘══════════╛\r\n\r\n\r\nEpoch 1\r\nTraining:   0%|                                                                                                                                                                                                                                           | 0/179 [00:00<?, ?it/s]WARNING:tensorflow:AutoGraph could not transform <bound method ECD.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f709d6e5370>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,860:tensorflow] AutoGraph could not transform <bound method ECD.train_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f709d6e5370>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method ECD.call of <ludwig.models.ecd.ECD object at 0x7f70a00d5730>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,866:tensorflow] AutoGraph could not transform <bound method ECD.call of <ludwig.models.ecd.ECD object at 0x7f70a00d5730>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method NumericalInputFeature.call of <ludwig.features.numerical_feature.NumericalInputFeature object at 0x7f709dd67ee0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,872:tensorflow] AutoGraph could not transform <bound method NumericalInputFeature.call of <ludwig.features.numerical_feature.NumericalInputFeature object at 0x7f709dd67ee0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method PassthroughEncoder.call of <ludwig.encoders.generic_encoders.PassthroughEncoder object at 0x7f70a00d5ac0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,881:tensorflow] AutoGraph could not transform <bound method PassthroughEncoder.call of <ludwig.encoders.generic_encoders.PassthroughEncoder object at 0x7f70a00d5ac0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method ConcatCombiner.call of <ludwig.combiners.combiners.ConcatCombiner object at 0x7f709dd67160>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,921:tensorflow] AutoGraph could not transform <bound method ConcatCombiner.call of <ludwig.combiners.combiners.ConcatCombiner object at 0x7f709dd67160>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method OutputFeature.call of <ludwig.features.text_feature.TextOutputFeature object at 0x7f709fa291f0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,931:tensorflow] AutoGraph could not transform <bound method OutputFeature.call of <ludwig.features.text_feature.TextOutputFeature object at 0x7f709fa291f0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method FCStack.call of <ludwig.modules.fully_connected_modules.FCStack object at 0x7f709fa29400>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:19,937:tensorflow] AutoGraph could not transform <bound method FCStack.call of <ludwig.modules.fully_connected_modules.FCStack object at 0x7f709fa29400>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method BaseDecoder.call of <tensorflow_addons.seq2seq.basic_decoder.BasicDecoder object at 0x7f70a01c7c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:20,004:tensorflow] AutoGraph could not transform <bound method BaseDecoder.call of <tensorflow_addons.seq2seq.basic_decoder.BasicDecoder object at 0x7f70a01c7c10>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nWARNING:tensorflow:AutoGraph could not transform <bound method SequenceLoss.call of <ludwig.modules.loss_modules.SequenceLoss object at 0x7f709d68ce50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:20,289:tensorflow] AutoGraph could not transform <bound method SequenceLoss.call of <ludwig.modules.loss_modules.SequenceLoss object at 0x7f709d68ce50>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2021-04-07 23:19:21.689818: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\n2021-04-07 23:19:21.740893: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2294445000 Hz\r\nTraining: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:05<00:00, 32.54it/s]\r\nEvaluation train:   0%|                                                                                                                                                                                                                                   | 0/179 [00:00<?, ?it/s]WARNING:tensorflow:AutoGraph could not transform <bound method ECD.evaluation_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f709dcf22e0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n[WARNING] [2021-04-07 23:19:25,351:tensorflow] AutoGraph could not transform <bound method ECD.evaluation_step of <tensorflow.python.eager.function.TfMethodTarget object at 0x7f709dcf22e0>> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nTraceback (most recent call last):\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 447, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 284, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 286, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 470, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/transpiler.py\", line 346, in transform_function\r\n    node, source = parser.parse_entity(fn, future_features=future_features)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/parser.py\", line 150, in parse_entity\r\n    original_source = inspect_utils.getimmediatesource(entity)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 146, in getimmediatesource\r\n    _fix_linecache_record(obj)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/inspect_utils.py\", line 138, in _fix_linecache_record\r\n    if hasattr(m, '__file__') and m.__file__ == obj_file:\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/lazy_import/__init__.py\", line 156, in __getattribute__\r\n    _load_module(self)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/lazy_import/__init__.py\", line 536, in _load_module\r\n    raise_from(ImportError(\r\n  File \"<string>\", line 3, in raise_from\r\nImportError: smac attempted to use a functionality that requires module emcee, but it couldn't be loaded. Please install emcee and retry.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"start.py\", line 113, in <module>\r\n    print(ludwig_class(df))\r\n  File \"/home/riccardo/Desktop/AutoML-Benchmark/algorithms/ludwig.py\", line 100, in ludwig_class\r\n    train_stats = model.train(dataset=X_train, logging_level=logging.INFO)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/api.py\", line 482, in train\r\n    train_stats = trainer.train(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/models/trainer.py\", line 604, in train\r\n    self.evaluation(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/models/trainer.py\", line 802, in evaluation\r\n    metrics, predictions = predictor.batch_evaluation(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/models/predictor.py\", line 133, in batch_evaluation\r\n    preds = model.evaluation_step(inputs, targets)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 871, in _call\r\n    self._initialize(args, kwds, add_initializers_to=initializers)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 725, in _initialize\r\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 2969, in _get_concrete_function_internal_garbage_collected\r\n    graph_function, _ = self._maybe_define_function(args, kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3361, in _maybe_define_function\r\n    graph_function = self._create_graph_function(args, kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3196, in _create_graph_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 990, in func_graph_from_py_func\r\n    func_outputs = python_func(*func_args, **func_kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\", line 634, in wrapped_fn\r\n    out = weak_wrapped_fn().__wrapped__(*args, **kwds)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3887, in bound_method_wrapper\r\n    return wrapped_fn(*args, **kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\", line 966, in wrapper\r\n    return autograph.converted_call(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 454, in converted_call\r\n    return _fall_back_unconverted(f, args, kwargs, options, e)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 501, in _fall_back_unconverted\r\n    return _call_unconverted(f, args, kwargs, options)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 475, in _call_unconverted\r\n    return f.__self__.call(args, kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\", line 3849, in call\r\n    return wrapped_fn(self.weakrefself_target__(), *args, **kwargs)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/models/ecd.py\", line 189, in evaluation_step\r\n    predictions = self.predictions(inputs, output_features=None)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/models/ecd.py\", line 165, in predictions\r\n    predictions[of_name] = self.output_features[of_name].predictions(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/features/sequence_feature.py\", line 252, in predictions\r\n    return self.decoder_obj._predictions_eval(inputs, training=training)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/decoders/sequence_decoders.py\", line 599, in _predictions_eval\r\n    decoder_outputs = self.call(inputs, training=training)\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/decoders/sequence_decoders.py\", line 584, in call\r\n    decoder_outputs = self.decoder_greedy(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/ludwig/decoders/sequence_decoders.py\", line 524, in decoder_greedy\r\n    if seq_len_diff > 0:\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 885, in __bool__\r\n    self._disallow_bool_casting()\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 488, in _disallow_bool_casting\r\n    self._disallow_when_autograph_enabled(\r\n  File \"/home/riccardo/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 474, in _disallow_when_autograph_enabled\r\n    raise errors.OperatorNotAllowedInGraphError(\r\ntensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\r\nEvaluation train:   0%|     \r\n```\r\n\r\nIn addition to that I've try also to switch the import of them but nothing\r\n\r\n","closed_by":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1145/timeline","performed_via_github_app":null,"state_reason":"completed"}