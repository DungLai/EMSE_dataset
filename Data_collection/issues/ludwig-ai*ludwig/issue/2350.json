{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350","repository_url":"https://api.github.com/repos/ludwig-ai/ludwig","labels_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350/labels{/name}","comments_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350/comments","events_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350/events","html_url":"https://github.com/ludwig-ai/ludwig/issues/2350","id":1329277190,"node_id":"I_kwDOCbx2hs5POykG","number":2350,"title":"Hyperopt Bug: Large number of steps with negative values for rounds of evaluation, steps and epochs using Async Hyperband","user":{"login":"arnavgarg1","id":106701836,"node_id":"U_kgDOBlwkDA","avatar_url":"https://avatars.githubusercontent.com/u/106701836?v=4","gravatar_id":"","url":"https://api.github.com/users/arnavgarg1","html_url":"https://github.com/arnavgarg1","followers_url":"https://api.github.com/users/arnavgarg1/followers","following_url":"https://api.github.com/users/arnavgarg1/following{/other_user}","gists_url":"https://api.github.com/users/arnavgarg1/gists{/gist_id}","starred_url":"https://api.github.com/users/arnavgarg1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arnavgarg1/subscriptions","organizations_url":"https://api.github.com/users/arnavgarg1/orgs","repos_url":"https://api.github.com/users/arnavgarg1/repos","events_url":"https://api.github.com/users/arnavgarg1/events{/privacy}","received_events_url":"https://api.github.com/users/arnavgarg1/received_events","type":"User","site_admin":false},"labels":[{"id":1174068769,"node_id":"MDU6TGFiZWwxMTc0MDY4NzY5","url":"https://api.github.com/repos/ludwig-ai/ludwig/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":{"login":"arnavgarg1","id":106701836,"node_id":"U_kgDOBlwkDA","avatar_url":"https://avatars.githubusercontent.com/u/106701836?v=4","gravatar_id":"","url":"https://api.github.com/users/arnavgarg1","html_url":"https://github.com/arnavgarg1","followers_url":"https://api.github.com/users/arnavgarg1/followers","following_url":"https://api.github.com/users/arnavgarg1/following{/other_user}","gists_url":"https://api.github.com/users/arnavgarg1/gists{/gist_id}","starred_url":"https://api.github.com/users/arnavgarg1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arnavgarg1/subscriptions","organizations_url":"https://api.github.com/users/arnavgarg1/orgs","repos_url":"https://api.github.com/users/arnavgarg1/repos","events_url":"https://api.github.com/users/arnavgarg1/events{/privacy}","received_events_url":"https://api.github.com/users/arnavgarg1/received_events","type":"User","site_admin":false},"assignees":[{"login":"arnavgarg1","id":106701836,"node_id":"U_kgDOBlwkDA","avatar_url":"https://avatars.githubusercontent.com/u/106701836?v=4","gravatar_id":"","url":"https://api.github.com/users/arnavgarg1","html_url":"https://github.com/arnavgarg1","followers_url":"https://api.github.com/users/arnavgarg1/followers","following_url":"https://api.github.com/users/arnavgarg1/following{/other_user}","gists_url":"https://api.github.com/users/arnavgarg1/gists{/gist_id}","starred_url":"https://api.github.com/users/arnavgarg1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arnavgarg1/subscriptions","organizations_url":"https://api.github.com/users/arnavgarg1/orgs","repos_url":"https://api.github.com/users/arnavgarg1/repos","events_url":"https://api.github.com/users/arnavgarg1/events{/privacy}","received_events_url":"https://api.github.com/users/arnavgarg1/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2022-08-04T23:33:52Z","updated_at":"2022-09-20T14:57:17Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\nWhen trying to run Hyperopt with AsyncHyperband, I see the following on my console:\r\n\r\n```\r\nStage 0: : 4it [00:10,  2.27s/it] pid=5557, ip=192.168.44.226) \r\nStage 0: : 4it [00:10,  2.39s/it] pid=5556, ip=192.168.44.226) \r\nTraining:   0%|          | 1/101457092405402533877 [00:00<22007761214225922:16:32,  1.28it/s]\r\nTraining:   0%|          | 0/101457092405402533877 [00:00<?, ?it/s]\r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) Note: steps_per_checkpoint (was 2000) is now set to the number of steps per epoch: 11.\r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) \r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) Training for 101457092405402533877 step(s), approximately 9223372036854775808 epoch(s).\r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) Early stopping policy: -1 round(s) of evaluation, or -11 step(s), approximately -1 epoch(s).\r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) \r\n(BaseWorkerMixin pid=581, ip=192.168.35.224) Starting with step 0, epoch: 0\r\nStage 0: : 4it [00:10,  3.15s/it] pid=5555, ip=192.168.44.226) \r\nStage 0: : 5it [00:11,  2.21s/it] pid=5555, ip=192.168.44.226) \r\nTraining:   0%|          | 2/101457092405402533877 [00:01<18155989808141202:46:24,  1.55it/s]\r\nTraining:   0%|          | 1/101457092405402533877 [00:00<21823230788576069:24:16,  1.29it/s]\r\nTraining:   0%|          | 3/101457092405402533877 [00:01<16904276494085920:59:44,  1.67it/s]\r\nTraining:   0%|          | 2/101457092405402533877 [00:01<18189075729949848:27:44,  1.55it/s]\r\nTraining:   0%|          | 0/101457092405402533877 [00:00<?, ?it/s]\r\nTraining:   0%|          | 4/101457092405402533877 [00:02<16324377596747498:22:56,  1.73it/s]\r\n```\r\n\r\nLudwig prints out a massive number of steps and epoch number, along with negative values for the early stopping policy. This feels like a big bug where some internals have gone terribly wrong. In particular, this seems to happen when `time_budget_s` is reached before training even starts for a given hyperopt experiment.\r\n\r\n```\r\nStage 0: : 3it [00:03,  1.28it/s] pid=12614) \r\nStage 0: : 2it [00:03,  1.48s/it]                     \r\nStage 0: : 3it [00:03,  1.15it/s] pid=12615) 3.51s/it]\r\n2022-08-04 16:26:11,494 INFO stopper.py:350 -- Reached timeout of 20 seconds. Stopping all trials.\r\n== Status ==\r\nCurrent time: 2022-08-04 16:26:11 (running for 00:00:24.99)\r\nMemory usage on this node: 9.1/246.4 GiB\r\nUsing AsyncHyperBand: num_stopped=0\r\nBracket: Iter 50.000: None\r\nResources requested: 0/150 CPUs, 0/12 GPUs, 0.0/500.0 GiB heap, 0.0/500.0 GiB objects (0.0/3.0 accelerator_type:T4)\r\nResult logdir: /home/ray/src/results/criteo_hyperopt\r\nNumber of trials: 4/4 (4 TERMINATED)\r\n+-------------------+------------+---------------------+-----------------------+-------------------------+\r\n| Trial name        | status     | loc                 |   trainer.decay_steps |   trainer.learning_rate |\r\n|-------------------+------------+---------------------+-----------------------+-------------------------|\r\n| trial_c467a_00000 | TERMINATED | 192.168.44.226:5383 |                 10000 |                   0.001 |\r\n| trial_c467a_00001 | TERMINATED | 192.168.86.8:12066  |                  2000 |                   0.005 |\r\n| trial_c467a_00002 | TERMINATED | 192.168.72.27:697   |                 10000 |                   0.005 |\r\n| trial_c467a_00003 | TERMINATED | 192.168.44.226:5414 |                  8000 |                   0.001 |\r\n+-------------------+------------+---------------------+-----------------------+-------------------------+\r\n```\r\n\r\nRight after this, the model seems to start training with the values for number of steps and number of epochs shown above, but in reality, the trials have already been canceled and should not actually train. There seems to be some issue with coordination between Tune failing/stopping and trials/models initializing or something along those lines.\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n\r\n1. Take any dataset (somewhat large so it takes sometime to start the first training epoch)\r\n2. Set the hyperopt executor to the following:\r\n\r\n```yaml\r\n  executor:\r\n    type: ray\r\n    scheduler:\r\n      type: async_hyperband\r\n      max_t: 50\r\n      time_attr: time_total_s\r\n      grace_period: 50\r\n      reduction_factor: 5\r\n    num_samples: 4\r\n    time_budget_s: 20\r\n    cpu_resources_per_trial: 1\r\n    gpu_resources_per_trial: 1\r\n```\r\n\r\nThe main thing to make sure here is that your `time_budget_s` is smaller than the time before the model actually starts training. The main goal is to get the trials to terminate before they even start training.\r\n\r\nPlease provide code, yaml config file and a sample of data in order to entirely reproduce the issue.\r\nIssues that are not reproducible will be ignored.\r\n\r\n**Expected behavior**\r\nTraining should never start, and an empty hyperopt_results object should be returned. Ideally, with a very clear warning that things have failed. \r\n\r\n**Environment (please complete the following information):**\r\n\r\n- OS: \\[e.g. iOS\\] MacOS 12.4\r\n- Python version: 3.8.13\r\n- Ludwig version: 0.6.dev","closed_by":null,"reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/2350/timeline","performed_via_github_app":null,"state_reason":null}