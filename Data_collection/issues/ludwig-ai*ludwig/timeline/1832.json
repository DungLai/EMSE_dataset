[{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/comments/1079511069","html_url":"https://github.com/ludwig-ai/ludwig/issues/1832#issuecomment-1079511069","issue_url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/1832","id":1079511069,"node_id":"IC_kwDOCbx2hs5AWAgd","user":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"created_at":"2022-03-25T23:21:40Z","updated_at":"2022-03-25T23:21:40Z","author_association":"COLLABORATOR","body":"Pasting here the answer from the Ludwig Slack:\r\n\r\nThere’s a misunderstanding of what word / character / sequence / text / embedding mean in Ludwig. Let me try to give you context :wink:\r\nText and Sequence are identical in Ludwig, the only difference is the encoders and tokenizers available. What happens in both text and sequence is that the input string is tokenized into tokens. In the case of sequence, whitespace is the default tokenizer, for text you have several more options. Glove embeddings can be provided to both text and sequence features to define how to encode each token that is the output of the tokenizer. So the GloVe embeddings (or any set of embeddings obtained by any algorithm and saved in GloVe format) are used as first embedding layer in all encoder models (parallel_cnn, rnn/lstm/gru, transformer) for both text and sequence. The difference is that text also allows you to use pretrained transformers. The way to use them is to just specify the encoder, for instance gpt2. The preprocessing parameters (including the tokenizer, the vocabulary and others) associated with gpt2 will be automatically used, you don’t have to specify anything else in preprocessing. The output of the text encoder loaded from these pretrained models will be a vector (“sentence embedding” if you want, although i don’t like using the word embedding for the output of encoders because it has a very different meaning from the meaning of “embedding layer” where each embedding is a set of weights in an embedding matrix). That vector will be used down the line to perform a task.\r\nSo Ludwig provides already a bunch of text encoders obtained from these pretrained models, like bert, gpt2, roberta, t5, etc. The Autotransformer encoder that I was suggesting allow you to specify the bath of the transformer model on disk or it’s name like you would provide to huggingface to obtain it, and use it directly 8again the associated tokenizer will automatically be used).\r\nDoes this help?","reactions":{"url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/comments/1079511069/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false}},{"id":6310504100,"node_id":"CE_lADOCbx2hs5GCC8YzwAAAAF4Iqak","url":"https://api.github.com/repos/ludwig-ai/ludwig/issues/events/6310504100","actor":{"login":"w4nderlust","id":349256,"node_id":"MDQ6VXNlcjM0OTI1Ng==","avatar_url":"https://avatars.githubusercontent.com/u/349256?v=4","gravatar_id":"","url":"https://api.github.com/users/w4nderlust","html_url":"https://github.com/w4nderlust","followers_url":"https://api.github.com/users/w4nderlust/followers","following_url":"https://api.github.com/users/w4nderlust/following{/other_user}","gists_url":"https://api.github.com/users/w4nderlust/gists{/gist_id}","starred_url":"https://api.github.com/users/w4nderlust/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/w4nderlust/subscriptions","organizations_url":"https://api.github.com/users/w4nderlust/orgs","repos_url":"https://api.github.com/users/w4nderlust/repos","events_url":"https://api.github.com/users/w4nderlust/events{/privacy}","received_events_url":"https://api.github.com/users/w4nderlust/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2022-03-25T23:21:40Z","state_reason":null,"performed_via_github_app":null}]