{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3303","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/3303/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/3303/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/3303/events","html_url":"https://github.com/mlflow/mlflow/issues/3303","id":683225058,"node_id":"MDU6SXNzdWU2ODMyMjUwNTg=","number":3303,"title":"[FR] Support serving Keras/TensorFlow models with TensorFlow Serving","user":{"login":"amesar","id":1721873,"node_id":"MDQ6VXNlcjE3MjE4NzM=","avatar_url":"https://avatars.githubusercontent.com/u/1721873?v=4","gravatar_id":"","url":"https://api.github.com/users/amesar","html_url":"https://github.com/amesar","followers_url":"https://api.github.com/users/amesar/followers","following_url":"https://api.github.com/users/amesar/following{/other_user}","gists_url":"https://api.github.com/users/amesar/gists{/gist_id}","starred_url":"https://api.github.com/users/amesar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amesar/subscriptions","organizations_url":"https://api.github.com/users/amesar/orgs","repos_url":"https://api.github.com/users/amesar/repos","events_url":"https://api.github.com/users/amesar/events{/privacy}","received_events_url":"https://api.github.com/users/amesar/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-08-21T03:07:23Z","updated_at":"2020-08-25T05:12:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Thank you for submitting a feature request. **Before proceeding, please review MLflow's [Issue Policy for feature requests](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md#feature-requests) and the [MLflow Contributing Guide](https://github.com/mlflow/mlflow/blob/master/CONTRIBUTING.rst)**.\r\n\r\n**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nTensorFlow Serving (TFS) provides a convenient REST API for deploying Keras/TensorFlow models and managing deployments. We should make it easy to deploy Keras/TensorFlow models logged using MLflow to a TFS server. We should only focus on TensorFlow 2.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n- Why is this use case valuable to support for MLflow users in general?\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\nWhile it's currently possible to locally deploy Keras models using mlflow models serve, TorchServe contains a number of additional useful features when deploying Keras models, e.g. instrumentation on request success/error rates, configurable access logs, inference endpoints for different versions of a model, etc.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ x] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n### Links\r\n* https://www.tensorflow.org/tfx/guide/serving\r\n* https://www.tensorflow.org/tfx/serving/serving_basic\r\n* https://github.com/tensorflow/serving\r\n\r\n### Proposed Solutions\r\n\r\nSince this ticket is analogous to issue [3065](https://github.com/mlflow/mlflow/issues/3065) (Support serving Pytorch models via TorchServe), see that issue for general implementation approaches. Below we discuss the TFS-specific issues.\r\n\r\n### TensorFlow Serving Specific Observations\r\n\r\nObservations:\r\n* TFS expects the model to be in [SavedModel](https://www.tensorflow.org/guide/saved_model) format which is the default for saving Keras models in TensorFlow 2.\r\n* In MLflow the current default is to log Keras models in legacy HD5 format. Due to a bug, you can't log a KerasModel as SaveModel. \r\n* We need change the MLflow TF 2.0 default to SavedModel. Without this, you can't deploy a model to TFS from the model registry.\r\n\r\nRelated issues:\r\n* [3224](https://github.com/mlflow/mlflow/issues/3224) - Cannot save Keras/TF_2.x model as SavedModel format using mlflow.keras.log_model with kwargs { \"save_format\": \"tf\" }\r\n* [3246]( https://github.com/mlflow/mlflow/issues/3246) - [FR] Serialization format for TensorFlow 2.x (Keras) model should be Save Model per TF 2.0 doc\r\n\r\n### Implementation\r\n\r\nThe TFS server is similar to the MLflow scoring server. It expects and returns a JSON format which is slightly different. It should be straightforward to create an MLflow TFS deployment API. \r\n\r\nRequest format:\r\n```\r\n{\"instances\": [\r\n  [ 7,   0.27, 0.36]\r\n  ]}\r\n```\r\nResponse format:\r\n```\r\n{\r\n  \"predictions\": [\r\n    [\r\n      0.998985946\r\n   ]}\r\n```\r\n\r\nTFS has several options to create docker container. See [TensorFlow Serving with Docker](https://www.tensorflow.org/tfx/serving/docker) We should implement the variant where the model is baked into the container. See [Creating your own serving image](https://www.tensorflow.org/tfx/serving/docker#creating_your_own_serving_image).\r\n\r\n### Example 1\r\n\r\nFrom https://github.com/amesar/mlflow-tools, see [Serve MLflow Keras model with TensorFlow Serving](https://github.com/amesar/mlflow-tools/tree/master/mlflow_tools/tensorflow_serving).\r\n\r\n```\r\npython launch_tensorflow_serving.py \\\r\n  --model-uri runs:/774f1d5e4573499a8eb2043c397cd98a/keras-model \\\r\n  --tfs-model-name keras_mnist\r\n  --container tfs_serving_keras_mnist\r\n```\r\n\r\nTo score:\r\n```\r\ncurl -X POST \\\r\n  http://localhost:8502/v1/models/keras_mnist:predict  \\\r\n  -d @data/mnist.json \r\n```\r\n\r\n### Example 2\r\n\r\nI have already proofed this with an MNIST model at https://github.com/amesar/mlflow-examples/blob/master/python/keras_tf_mnist/README.md#tensorflow-serving-real-time-scoring.\r\n\r\nSince I couldn't use mlflow.keras.log_model, I converted the model to SavedModel format and logged it as an artifact. See [train.py](https://github.com/amesar/mlflow-examples/blob/master/python/keras_tf_mnist/train.py#L52).\r\n\r\nSample code to deploy a TFS server.\r\n```\r\nHOST_PORT=8502\r\nMODEL=keras_mnist\r\nCONTAINER=tfs_serving_$MODEL\r\nDOCKER_MODEL_PATH=/models/$MODEL/01\r\n\r\nRUN_ID=7e674524514846799310c41f10d6b99d\r\nHOST_MODEL_PATH=`mlflow artifacts download --run-id $RUN_ID --artifact-path tensorflow-model`\r\nBASE_CONTAINER=tfs_serving_base\r\ndocker run -d --name $BASE_CONTAINER tensorflow/serving\r\ndocker cp $HOST_MODEL_PATH/ $BASE_CONTAINER:/tmp\r\ndocker exec -d $BASE_CONTAINER mkdir -p /models/$MODEL\r\ndocker exec -d $BASE_CONTAINER mv /tmp/tensorflow-model /models/$MODEL/01\r\ndocker commit --change \"ENV MODEL_NAME $MODEL\" $BASE_CONTAINER $CONTAINER\r\ndocker rm -f $BASE_CONTAINER\r\ndocker run -d --name $CONTAINER -p $HOST_PORT:8501 $CONTAINER\r\n```\r\nTo score:\r\n```\r\ncurl -X POST \\\r\n  http://localhost:8502/v1/models/keras_mnist:predict  \\\r\n  -d @../../data/score/mnist/mnist-tf-serving.json \r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3303/reactions","total_count":4,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":4,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/3303/timeline","performed_via_github_app":null,"state_reason":null}