{"url":"https://api.github.com/repos/mlflow/mlflow/issues/598","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/598/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/598/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/598/events","html_url":"https://github.com/mlflow/mlflow/issues/598","id":366942817,"node_id":"MDU6SXNzdWUzNjY5NDI4MTc=","number":598,"title":"[Proposal/RFC] Support for metric series and metric groups","user":{"login":"aadamson","id":3607110,"node_id":"MDQ6VXNlcjM2MDcxMTA=","avatar_url":"https://avatars.githubusercontent.com/u/3607110?v=4","gravatar_id":"","url":"https://api.github.com/users/aadamson","html_url":"https://github.com/aadamson","followers_url":"https://api.github.com/users/aadamson/followers","following_url":"https://api.github.com/users/aadamson/following{/other_user}","gists_url":"https://api.github.com/users/aadamson/gists{/gist_id}","starred_url":"https://api.github.com/users/aadamson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aadamson/subscriptions","organizations_url":"https://api.github.com/users/aadamson/orgs","repos_url":"https://api.github.com/users/aadamson/repos","events_url":"https://api.github.com/users/aadamson/events{/privacy}","received_events_url":"https://api.github.com/users/aadamson/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2018-10-04T19:47:26Z","updated_at":"2021-10-05T21:07:42Z","closed_at":"2019-07-26T21:53:42Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"# Motivation\r\nWhen we do machine learning experiments, we are often interested not only in understanding how to optimize the value of a single metric, but also to understand how that metric evolves over time (e.g. in a learning curve) and what if any fundamental tradeoffs may exist between the value we can achieve on that metric vs. other metrics.\r\n\r\nFor example, suppose we are building a recommendation system that needs to predict which products a user may be interested in basedon a query. The user enters a query and the system, having been trained on a set of past queries and subsequent user actions such as clicks and purchases, recommends what it considers the most relevant products.  Since ultimately you can present the recommender’s results to the user as a list, and you have flexibility around how many results to show the user, you’re interested in evaluating precision at k, i.e. on average how many of the top k results for a query are relevant, and recall at k, i.e. on average what proportion of documents relevant to the query appear in the top k documents. We may want to consider many different possible values of k and ultimately select the k for which there’s little marginal improvement to be had by recommending more documents. We can think of recall and precision as a metric which itself has a parameter k. Precision and recall could have other parameters if we’re interested in doing things such as measuring precision and recall over types of queries that we’ve decided are distinct and important. We could also be using an iterative model training process for our recommender system such as stochastic gradient descent and be interested in comparing the learning curves of many recommender systems under consideration, possibly for fixed values of the other parameters, or aggregating over the other parameters. The scheme described above leaves us with a metrics chart that looks like this:\r\n\r\nEpoch | K | Category | Precision | Recall\r\n-- | -- | -- | -- | --\r\n1 | 1 | person | 0.9 | 0.5\r\n1 | 3 | place | 0.7 | 0.7\r\n1 | 5 | event | 0.5 | 0.9\r\n2 | 1 | person | 0.95 | 0.55\r\n2 | 3 | place | 0.75 | 0.75\r\n2 | 5 | event | 0.55 | 0.95\r\n\r\n\r\nThe need to represent in a structured way motivates the introduction of metric groups with common, named parameters and indices. \r\n\r\nWith metric groups, we can answer two important classes of questions that ML practitioners often encounter. First, we can construct learning curves to see how a metric evolves over time (e.g. from epoch to epoch). Second, we can look at how a metric is different across different subsets of the training data, either at a point in time or over a curve in time.\r\n\r\n# Design\r\n\r\nMetric groups will be introduced as a separate entity in MLflow belonging to runs. They will have the same basic API as metrics: mlflow.tracking.MlflowClient will have a new method, log_metric_group, which will take a key (an identifier for the metric group), a list of tuples (metric name, metric value), a list of tuples (index/parameter name, index/parameter value), and the timestamp. If no metric group already exists with that key, a new metric group will be created. Otherwise, the arguments will be validated (i.e. the index and metric names match what they were on creation) and appended to the file as a line of space separated values (alternative: type constraints on index values and all metrics are floats so we can use a binary format). On disk, the representation will be similar to that of metrics currently so that appending remains easy (other options include maintaining a Parquet file on disk or one file per metric and one per index). A tag will indicate the names and order of metrics in the metric group, and the name and the order of indices/parameters in the metric group. On the experiment view UI, there will be a new section for metric groups that will look similar to the metrics section but with delineation between metrics of different groups. Similar to metrics, by default, only the most recent value of the metric group’s metrics will be displayed (regardless of index values). The user can click an expander to see a table or plot of the metric group’s values for each metric over timestamp or index (or indices) plotted over each other. This will be facilitated by a GetMetricGroupHistory service call similar to the one that exists for metrics. Additionally, on the run comparison view, the user can compare metric groups’ metric values over time or index value.\r\n\r\n# Alternatives\r\nRight now, to get something similar to a metric group, the user could simply create a metric per member of the metric group (in the example above, we’d have something like precision_at_1, precision_at_3, precision_at_10, … as separate metrics). The user could also abuse the timestamp parameter when logging metrics to track epoch (or some other index that they’re interested in) instead. However, I still think that this would fall short of the flexibility of the metric group approach. You can imagine a case where you want to track the timestamp, an index column on epoch, and an index column on examples seen so far. With metrics alone, you cannot track that much metadata about metric values. Additionally, the constraint that metrics in a metric group need to have the same index makes it easier for charting built into MLflow and analysis tools witten on top of MLflow data to treat the metrics series as intentionally ordered and therefore comparable across indexes rather than coincidentally ordered (i.e. since all metric values were explicitly entered by the user as having the same index, we can plot them against the same index, confident that the comparison will makes sense). Finally, I believe the ability to unambiguously specify that a set of metric values correspond to the same state of a model/the same point of model training is essential to an experiment tracking being able to represent an iterative training process.\r\n","closed_by":{"login":"dbczumar","id":39497902,"node_id":"MDQ6VXNlcjM5NDk3OTAy","avatar_url":"https://avatars.githubusercontent.com/u/39497902?v=4","gravatar_id":"","url":"https://api.github.com/users/dbczumar","html_url":"https://github.com/dbczumar","followers_url":"https://api.github.com/users/dbczumar/followers","following_url":"https://api.github.com/users/dbczumar/following{/other_user}","gists_url":"https://api.github.com/users/dbczumar/gists{/gist_id}","starred_url":"https://api.github.com/users/dbczumar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbczumar/subscriptions","organizations_url":"https://api.github.com/users/dbczumar/orgs","repos_url":"https://api.github.com/users/dbczumar/repos","events_url":"https://api.github.com/users/dbczumar/events{/privacy}","received_events_url":"https://api.github.com/users/dbczumar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/598/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/598/timeline","performed_via_github_app":null,"state_reason":"completed"}