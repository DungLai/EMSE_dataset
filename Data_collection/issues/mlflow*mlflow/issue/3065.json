{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3065","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/3065/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/3065/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/3065/events","html_url":"https://github.com/mlflow/mlflow/issues/3065","id":652787140,"node_id":"MDU6SXNzdWU2NTI3ODcxNDA=","number":3065,"title":"[FR] Support serving Pytorch models via TorchServe","user":{"login":"smurching","id":2358483,"node_id":"MDQ6VXNlcjIzNTg0ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2358483?v=4","gravatar_id":"","url":"https://api.github.com/users/smurching","html_url":"https://github.com/smurching","followers_url":"https://api.github.com/users/smurching/followers","following_url":"https://api.github.com/users/smurching/following{/other_user}","gists_url":"https://api.github.com/users/smurching/gists{/gist_id}","starred_url":"https://api.github.com/users/smurching/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smurching/subscriptions","organizations_url":"https://api.github.com/users/smurching/orgs","repos_url":"https://api.github.com/users/smurching/repos","events_url":"https://api.github.com/users/smurching/events{/privacy}","received_events_url":"https://api.github.com/users/smurching/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"},{"id":2022863969,"node_id":"MDU6TGFiZWwyMDIyODYzOTY5","url":"https://api.github.com/repos/mlflow/mlflow/labels/priority/important-soon","name":"priority/important-soon","color":"534cb5","default":false,"description":"The issue is worked on by the community currently or will be very soon, ideally in time for the"},{"id":2317074151,"node_id":"MDU6TGFiZWwyMzE3MDc0MTUx","url":"https://api.github.com/repos/mlflow/mlflow/labels/integrations/pytorch","name":"integrations/pytorch","color":"ffbce5","default":false,"description":"MLflow x Pytorch integrations"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2020-07-08T00:49:23Z","updated_at":"2021-05-20T09:47:05Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"**Please fill in this feature request template to ensure a timely and thorough response.**\r\n\r\n## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community <-- willing to help review contributions here!\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\n[TorchServe](https://pytorch.org/serve/) provides a convenient REST API for deploying Pytorch models and managing deployments. We should make it easy to deploy Pytorch models logged using MLflow to a TorchServe server.\r\n\r\n## Motivation\r\n\r\nWhile it's currently possible to locally deploy Pytorch models using `mlflow models serve`, TorchServe contains a number of additional useful features when deploying Pytorch models, e.g. [instrumentation](https://pytorch.org/serve/metrics.html) on request success/error rates, configurable access logs, inference endpoints for different versions of a model, etc.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for\r\nModel Registry\r\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [x] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n\r\n## Details\r\n\r\n### Proposed Solutions\r\nThere are at least two high-level approaches.\r\n\r\n1. Use MLflow's pluggable [`mlflow.deployments` interface](https://www.mlflow.org/docs/latest/python_api/mlflow.deployments.html#module-mlflow.deployments) for defining model-deployment logic to targets like Sagemaker, AzureML, etc\r\n2. Update `mlflow model serve` to delegate to TorchServe when serving a PyTorch model.\r\n\r\nNote that these approaches  are not mutually exclusive (i.e. we could do both) -  it'd be great for `mlflow models serve` to locally-serve Pytorch models in the recommended/most-efficient way, and it would also be great to enable users to deploy models to a remote TorchServe server for the reasons above^.\r\n\r\n#### Approach 1: Write a deployment plugin for local/remote deployment to TorchServe\r\nIn option 1, we'd write a deployment plugin [as described in the docs](https://mlflow.org/docs/latest/plugins.html#writing-your-own-mlflow-plugins), so that users could deploy/score PyTorch models with TorchServe via commands like:\r\n\r\n```\r\n# Deploy to local TorchServe server to test out model\r\nmlflow deployments run-local --target torchserve  --name spamclassifier --model-uri s3:/my/spam/classifier\r\n# Deploy model to remote server\r\nmlflow deployments create --target torchserve  --name spamclassifier --model-uri s3:/my/spam/classifier\r\n# Perform inference using our remote-deployed model endpoint\r\nmlflow deployments predict --target torchserve --name spamclassifier -f emails.json\r\n# Delete our remote endpoint\r\nmlflow deployments delete  --target torchserve --name spamclassifier\r\n```\r\n\r\n#### Option 2: Make `mlflow models serve` delegate to TorchServe when locally serving a PyTorch model\r\nIn option 2, we'd define and [register](https://github.com/mlflow/mlflow/blob/master/mlflow/models/flavor_backend_registry.py) a new PytorchFlavorBackend defining a custom `serve` method that delegates to TorchServe when serving MLflow models via `mlflow models serve`, if TorchServe is installed locally. The PytorchFlavorBackend would implement this [FlavorBackend interface](https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/backend.py), and could generally extend the existing [PyfuncBackend implementation](https://github.com/mlflow/mlflow/blob/master/mlflow/pyfunc/backend.py) with custom behavior for `serve`\r\n\r\n\r\n#### Observations\r\nOne apparent challenge with option 2 is that the TorchServe inference endpoint (POST to `/predictions/<model-name>`, see [docs](https://pytorch.org/serve/inference_api.html#id3)) and the endpoint exposed by `mlflow models serve` (POST to `/invocations`, [see docs](https://mlflow.org/docs/latest/cli.html#mlflow-models-serve)) are different. The expected input formats are also different - MLflow expects JSON-serialized records in pandas split or record-oriented formats, while TorchServe seems to support more general binary input (e.g. a jpg)","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3065/reactions","total_count":5,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":2,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/3065/timeline","performed_via_github_app":null,"state_reason":null}