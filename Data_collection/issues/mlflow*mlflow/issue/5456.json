{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5456","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5456/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5456/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5456/events","html_url":"https://github.com/mlflow/mlflow/issues/5456","id":1160073448,"node_id":"I_kwDOCB5Jx85FJVDo","number":5456,"title":"[Proposal] MLflow Pipelines (previously named as MLX)","user":{"login":"mengxr","id":829644,"node_id":"MDQ6VXNlcjgyOTY0NA==","avatar_url":"https://avatars.githubusercontent.com/u/829644?v=4","gravatar_id":"","url":"https://api.github.com/users/mengxr","html_url":"https://github.com/mengxr","followers_url":"https://api.github.com/users/mengxr/followers","following_url":"https://api.github.com/users/mengxr/following{/other_user}","gists_url":"https://api.github.com/users/mengxr/gists{/gist_id}","starred_url":"https://api.github.com/users/mengxr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mengxr/subscriptions","organizations_url":"https://api.github.com/users/mengxr/orgs","repos_url":"https://api.github.com/users/mengxr/repos","events_url":"https://api.github.com/users/mengxr/events{/privacy}","received_events_url":"https://api.github.com/users/mengxr/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":18,"created_at":"2022-03-04T21:39:47Z","updated_at":"2022-07-03T23:29:58Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"## Proposal Summary\r\n\r\n(You can find [the latest version of the proposal in this Google doc](https://docs.google.com/document/d/1KDNxLvmS392LdNpqc-bU9EAyoTvv0U3FiB5vRMfQuzI/edit).)\r\n\r\n## What are you trying to do? Articulate your objectives using absolutely no jargon.\r\n\r\nWe want to introduce MLflow Pipelines (previously named as MLX), an opinionated approach for MLOps, as part of MLflow’s next major release. It adds two key features to MLflow:\r\n- Predefined ML pipelines to address common ML problems at production quality.\r\n- Utilization of efficient pipeline execution engines to accelerate ML development.\r\n\r\nOur objective is to enable data scientists to stay mostly within their comfort zone utilizing their expert knowledge while following the best practices in ML development and delivering production-ready ML projects, with little help from production engineers and DevOps.\r\n\r\nWatch a proof-of-concept video here:\r\n\r\n[![PoC](https://img.youtube.com/vi/ZZIHcnM1EiY/0.jpg)](https://www.youtube.com/watch?v=ZZIHcnM1EiY)\r\n\r\n## How is it done today, and what are the limits of current practice?\r\n\r\nAlmost every company wants to be a Data+AI company. But many are new to ML under production settings, where the core ML component is a relatively small component surrounded by many others like data collection and verification, testing and debugging, automation via CI/CD, resource provision, model management and monitoring, etc. This is where MLOps kicks in and its job is to connect various components needed for ML productionization.\r\n\r\nHowever, despite being [an emerging topic](https://trends.google.com/trends/explore?date=today%205-y&geo=US&q=MLOps), MLOps is hard and there are no widely established approaches for MLOps. In a Databricks blog post [Need for Data-centric ML Platforms](https://databricks.com/blog/2021/06/23/need-for-data-centric-ml-platforms.html), the authors summarized MLOps as ModelOps + DataOps + DevOps. Connecting components from different areas naturally makes MLOps hard. What makes the job even harder is that in many companies the ownership of MLOps usually falls through the cracks between data science teams and production engineering teams. Data scientists are mostly focused on modeling the business problems and reasoning about data, features, and metrics, while the production engineers/ops are mostly focused on traditional DevOps for software development, ignoring ML-specific Ops like ML development cycles, experiment tracking, data/model validation, etc.\r\n\r\nOne solution to this problem is to hire “full-stack” data scientists or engineers who are capable of doing everything end-to-end. But only a few companies can afford this luxury solution.\r\n\r\nAnother candidate solution is to adopt the MLOps frameworks open-sourced by companies that have matured MLOps practices, for example, [TFX](https://www.tensorflow.org/tfx) from Google. However, those frameworks were originally designed for internal users to solve likely complex problems. They are not simple enough for the majority of data scientists to solve less sophisticated ML problems.\r\n\r\nWith ~10 million monthly downloads, MLflow is the most popular open-source MLOps framework. It is used by many users to track ML lifecycle and manage projects, metadata, and models. However, it itself is missing the “flow” part that helps streamline an end-to-end pipeline. This proposal aims to fill in that gap and greatly simplify the end-to-end story.\r\n\r\n## What is new in your approach and why do you think it will be successful?\r\n\r\nWe propose an opinionated approach to simplify and standardize ML development and productionization. On one hand, we offer data scientists production-quality ML pipeline templates they can start with and iterate quickly. On the other hand, we offer production engineers command-line interfaces for easy CI/CD integration. We focus on simplicity and try to address less sophisticated ML use cases, which we believe are the majority.\r\n\r\nBelow are the key differentiators:\r\n\r\n- Pre-defined production-quality ML pipeline templates. We will build pipeline templates that match common ML problem types, e.g., regression, clustering, recommendation, etc, where we embed best practices collected from industry experts. Instead of constructing an ML pipeline end-to-end, data scientists first pick a pipeline template that matches the problem type to start a project and customize its steps to solve the problem. This leads to a declarative and config-driven approach that saves the boilerplate code. So they can focus on the modeling steps while delivering production-ready projects. We believe the pre-defined templates would have a good coverage of the ML problems.\r\n\r\n- Efficient pipeline execution engine. ML development is a very iterative process. Users frequently jump between steps in a pipeline to understand the problem and improve the model. We will adopt an efficient pipeline engine to optimize the execution. After making changes, users only need to declare what they want and leave the engine to figure out what steps need to be executed and what can be reused. So they can quickly iterate during development, e.g., changing some features and then directly verifying feature importance.\r\n\r\n- Command-line interfaces to execute ML pipelines. During production handoff, instead of letting data scientists and production engineers negotiate the scripts, params, and I/O to be used in CI/CD, we want to standardize the interfaces to train and deploy models. The pre-defined pipelines automatically perform checks like schema and model validation. So production engineers have less things to worry about if they adopt our approach.\r\n\r\n- Notebook interfaces to execute ML pipelines. While we promote modular development for production, we love notebooks for data analysis, which is essential during iterative ML development. MLflow Pipelines provides notebook interfaces to trigger pipeline execution and display relevant results in cell outputs for data analysis and comparison.\r\n\r\n## Who cares? If you are successful, what difference will it make?\r\n\r\n- Data scientists. If MLflow Pipelines is successful, data scientists can stay mostly in the comfort zone utilizing their expert knowledge while delivering end-to-end production-quality projects.\r\n- Production engineers. If MLflow Pipelines is successful, production engineers can easily configure CI/CD for ML projects and let data scientists fully own the development cycles.\r\n\r\n## What are the risks?\r\n\r\nMLflow Pipelines is an opinionated approach. The biggest risk is whether the proposed opinionated piece fits the target use cases well. To de-risk, we plan to build the first pipeline template as a proof-of-concept, make it an optional component in MLflow, and collect feedback from the MLflow community to see how it fits and decide the next step.\r\n\r\n## How much will it cost?\r\n\r\nThe initial work will be focused on creating the first pipeline template, e.g., regression, and providing both command-line and notebook interfaces. We will adopt an existing pipeline engine instead of building one to save cost. Our estimate is ~10PWs.\r\n\r\n## How long will it take?\r\n\r\nWe plan to release the first pipeline template in early May. Contributions from the community would help accelerate the development.\r\n\r\n## What are the mid-term and final “exams” to check for success?\r\n\r\n- Mid-term:\r\n  - We receive positive feedback from the community on the first pipeline template.\r\n  - MLflow Pipelines becomes a default component in MLflow’s next major release and ships with more production-quality pipeline templates.\r\n- Final:\r\n  - Wide adoption of MLflow Pipelines.\r\n\r\n## Appendix\r\n\r\n### Terminology\r\n\r\n- Pipeline: An orchestration to solve one kind of machine learning problem end-to-end. It consists of steps, their inputs and outputs, and how they depend on each other. It is usually pre-defined by engineers and used by data scientists, who can configure the pipeline and its steps and customize certain steps to fit the specific problem to solve.\r\n- Step: An MLflow pipeline building block that usually does a single task, e.g., feature transformation. It declares inputs and outputs to chain with other steps in a pipeline. Users can configure its behavior via conf or Python code. Once configured, a step should be deterministic. Step names are verbs.\r\n- Run: A session that tracks the execution of an MLflow pipeline, fully or partially.\r\n- Profile: A named set of configurations users can activate when triggering a pipeline. Common profile names are “local”, “dev”, and “prod”.\r\n\r\n### Example development workflow\r\n\r\nMike is a data scientist. He liked the MLflow Pipelines tutorial and wanted to try MLflow Pipelines on a new ML project.\r\n\r\n- He already installed MLflow w/ MLflow Pipelines from the tutorial.\r\n- His new project is to predict used car sale prices. It is a regression problem. So he ran “mlp init –name autos –pipeline regression” to create a new project folder.\r\n- He used VS Code to open the generated project folder.\r\n- Inside the project folder, he saw a README.md file, a configuration file “pipeline.yaml”, a dependency file “requirements.txt”, and subfolders “steps/”, “notebooks/”, etc.\r\n- He opened README.md first and saw instructions and a list of TODOs:\r\n  - [ ] Check/update “requirements.txt” and run “pip install -r requirements.txt”.\r\n  - [ ] Open “notebooks/runme.ipynb” and try running this project first.\r\n  - [ ] Update the primary evaluation metric to use.\r\n  - [ ] Update data location in “pipeline.yaml” and target column.\r\n  - [ ] Update sklearn estimator in “steps/train.py”.\r\n  - [ ] …\r\n- He took a look at requirements.txt. The packages he needed were all listed. So he ran “pip install -r requirements.txt” directly.\r\n- He opens “notebooks/runme.ipynb” in VS Code. He already installed the Jupyter plugin. He activated the Python environment (kernel) for this project.\r\n- He clicked “Run All” and saw the pipeline visualization and outputs from its steps.\r\n- He checked the primary metric defined in “pipeline.yaml”. It is RMSE, which is good.\r\n- He checked the data path, which points to a local file under “data/”. He moved a parquet file that contains the sample training dataset to “data/” and updated the data path.\r\n- He changed the target column to “price”. \r\n- He switched back to “runme.ipynb” and re-ran the “ingest” cell, which displays the data summary of ingested data.\r\n- Then he re-ran the “evaluate” cell to see model performance on the real training dataset. The RMSE was more than $10,000, which is bad. He saw some training examples that have the worst prediction errors.\r\n- He opened “steps/train.py” and saw it uses an AutoML library to train. It takes a param to limit the cost. He increased the limit in the conf and hoped it would improve the model.\r\n- He switched back to “runme.ipynb”. He created a new cell and triggered the “evaluate” step again. He saw that the new model got RMSE $2,000, which is much better.\r\n- He re-ran the “explain” cell to see feature importance. One feature didn’t show up among the top ones. He knew the feature was very important, but it needed some parsing.\r\n- So he opened “steps/transform.py” and implemented a parser for that feature.\r\n- He found himself switching between the notebook and the source code. So he split the VS Code window and put the notebook and source code windows side by side.\r\n- He created a new cell and re-ran “evaluate”. The RMSE got improved to $1,500. He checked model importance again and confirmed the specific feature was among the top.\r\n- After a few more iterations, he successfully improved the RMSE to $1,000 on the sample training dataset.\r\n- He wanted to test it on the full training dataset.\r\n- He updated the “pipeline.yaml” file and added a new profile called “dev”. He configured the “ingest” step to read from the full dataset and increased the train cost limit again.\r\n- He updated the profile name and reran the notebook. It took much longer. He found the RMSE on the full dataset is $800, which was good enough to ship as the initial version.\r\n- He used git to check in the project folder to a repo shared by the data science team.\r\n- Btw, all the trials and models Mike made were tracked under MLflow automatically:)\r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5456/reactions","total_count":23,"+1":16,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":7},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5456/timeline","performed_via_github_app":null,"state_reason":null}