{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5144","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5144/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5144/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5144/events","html_url":"https://github.com/mlflow/mlflow/issues/5144","id":1070859714,"node_id":"I_kwDOCB5Jx84_1AXC","number":5144,"title":"[FR] Model Input Validation Outside of Pyfunc or MLFlow Deployment","user":{"login":"totalhack","id":43683140,"node_id":"MDQ6VXNlcjQzNjgzMTQw","avatar_url":"https://avatars.githubusercontent.com/u/43683140?v=4","gravatar_id":"","url":"https://api.github.com/users/totalhack","html_url":"https://github.com/totalhack","followers_url":"https://api.github.com/users/totalhack/followers","following_url":"https://api.github.com/users/totalhack/following{/other_user}","gists_url":"https://api.github.com/users/totalhack/gists{/gist_id}","starred_url":"https://api.github.com/users/totalhack/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/totalhack/subscriptions","organizations_url":"https://api.github.com/users/totalhack/orgs","repos_url":"https://api.github.com/users/totalhack/repos","events_url":"https://api.github.com/users/totalhack/events{/privacy}","received_events_url":"https://api.github.com/users/totalhack/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-12-03T18:45:41Z","updated_at":"2021-12-03T19:22:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ X] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nI want to be able to define the signature of my model using mlflow and then at inference time do input format validation based on the signature metadata from the artifact. I know loading as `pyfunc` and then calling `predict` does this automatically, as does packaging the project for deployment with mlflow (which I'm not doing because I need more flexibility in the API), but then I lose access to other methods on my underlying model (such as predict_proba for an sklearn model). \r\n\r\nIt looks like I might be able to work around the issue by using `_model_impl` on the pyfunc object to still have access to predict_proba, but that interface may be subject to change:\r\n\r\n```\r\nmodel = mlflow.pyfunc.load_model(MODEL_URI)\r\nmodel._model_impl.predict_proba(df)\r\n```\r\n\r\nI can then do manual validation in a predict_proba endpoint with something like this perhaps (while still having it automatically done when calling predict() otherwise):\r\n\r\n```\r\nfrom mlflow.pyfunc import _enforce_schema\r\ninput_schema = model.metadata.get_input_schema()\r\ndf = _enforce_schema(df, input_schema)\r\n```\r\n\r\nIt would be nice if there was a built in option to have non-pyfunc models do signature-based validation, or otherwise an officially supported way to load metadata with a loaded model and verify some input based on the signature of that model.\r\n\r\n## Motivation\r\n\r\n- What is the use case for this feature? See above.\r\n- Why is this use case valuable to support for MLflow users in general? Input validation is important, so giving users ways to utilize it outside of the mlflow-specific deployment cases seems like a good idea.\r\n- Why is this use case valuable to support for your project(s) or organization? It would be great to keep the artifact as the source of truth for the input signature so my model serving can leverage that instead of having it externally defined.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient) I had to dig through the code just to find the workaround above, which may be prone to breaking as mlflow evolves. \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5144/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5144/timeline","performed_via_github_app":null,"state_reason":null}