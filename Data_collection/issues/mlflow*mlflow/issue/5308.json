{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5308","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5308/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5308/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5308/events","html_url":"https://github.com/mlflow/mlflow/issues/5308","id":1113835023,"node_id":"I_kwDOCB5Jx85CY8YP","number":5308,"title":"[BUG] ONNX deployment on AzureML not working with onnxruntime > v1.9.0","user":{"login":"ecm200","id":34489160,"node_id":"MDQ6VXNlcjM0NDg5MTYw","avatar_url":"https://avatars.githubusercontent.com/u/34489160?v=4","gravatar_id":"","url":"https://api.github.com/users/ecm200","html_url":"https://github.com/ecm200","followers_url":"https://api.github.com/users/ecm200/followers","following_url":"https://api.github.com/users/ecm200/following{/other_user}","gists_url":"https://api.github.com/users/ecm200/gists{/gist_id}","starred_url":"https://api.github.com/users/ecm200/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ecm200/subscriptions","organizations_url":"https://api.github.com/users/ecm200/orgs","repos_url":"https://api.github.com/users/ecm200/repos","events_url":"https://api.github.com/users/ecm200/events{/privacy}","received_events_url":"https://api.github.com/users/ecm200/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022845866,"node_id":"MDU6TGFiZWwyMDIyODQ1ODY2","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/artifacts","name":"area/artifacts","color":"48eabc","default":false,"description":"Artifact stores and artifact logging"},{"id":2022847714,"node_id":"MDU6TGFiZWwyMDIyODQ3NzE0","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/model-registry","name":"area/model-registry","color":"48eabc","default":false,"description":"Model registry, model registry APIs, and the fluent client calls for model registry"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"},{"id":2022859639,"node_id":"MDU6TGFiZWwyMDIyODU5NjM5","url":"https://api.github.com/repos/mlflow/mlflow/labels/integrations/azure","name":"integrations/azure","color":"ffbce5","default":false,"description":"Azure and Azure ML integrations"},{"id":2022860064,"node_id":"MDU6TGFiZWwyMDIyODYwMDY0","url":"https://api.github.com/repos/mlflow/mlflow/labels/integrations/sagemaker","name":"integrations/sagemaker","color":"ffbce5","default":false,"description":"Sagemaker integrations"},{"id":2114036915,"node_id":"MDU6TGFiZWwyMTE0MDM2OTE1","url":"https://api.github.com/repos/mlflow/mlflow/labels/integrations/databricks","name":"integrations/databricks","color":"ffbce5","default":false,"description":"Databricks integrations"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2022-01-25T12:33:47Z","updated_at":"2022-02-02T12:43:16Z","closed_at":"2022-02-02T12:43:15Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [x] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **MLflow installed from (source or binary)**: Both source and binaries.\r\n- **MLflow version (run ``mlflow --version``)**:  1.18.0 and 1.23.0 (latest\r\n- **Python version**: 3.8.12\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI have saved a PyTorch Detectron 2 Object Detection model as an ONNX format, and registered it with the MLFlow tracking serving as an artifact. I did this using the `mlflow.onnx.log_model()` functionality, so the model artifact was uploaded as a MLFlow pyfunc type model. This process happens correctly and registers a model as expected, with the following meta data in the `MLModel` file:\r\n\r\n```yaml\r\nartifact_path: model/tracing_detectron2_mask_rcnn_R_50_FPN_1X__COCO_trained__model_zoo\r\nflavors:\r\n  onnx:\r\n    data: model.onnx\r\n    onnx_version: 1.10.2\r\n  python_function:\r\n    data: model.onnx\r\n    env: conda.yaml\r\n    loader_module: mlflow.onnx\r\n    python_version: 3.8.12\r\nrun_id: 205584e9973d4077822e5a333fa4aec8\r\nsignature:\r\n  inputs: '[{\"name\": \"data\", \"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"int8\", \"shape\":\r\n    [3, -1, -1]}}]'\r\n  outputs: '[{\"name\": \"bboxes\", \"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"float32\",\r\n    \"shape\": [-1, 4]}}, {\"name\": \"classes\", \"type\": \"tensor\", \"tensor-spec\": {\"dtype\":\r\n    \"uint64\", \"shape\": [-1, 1]}}, {\"name\": \"segm\", \"type\": \"tensor\", \"tensor-spec\":\r\n    {\"dtype\": \"float32\", \"shape\": [-1, 1, 28, 28]}}, {\"name\": \"scores\", \"type\": \"tensor\",\r\n    \"tensor-spec\": {\"dtype\": \"float32\", \"shape\": [-1, -1]}}, {\"name\": \"img_size\",\r\n    \"type\": \"tensor\", \"tensor-spec\": {\"dtype\": \"uint64\", \"shape\": [2, 1]}}]'\r\nutc_time_created: '2022-01-24 17:31:36.602627'\r\n```\r\n\r\nI am testing the ability to transfer registered models in the MLFlow tracking server to a Azure ML workspace for deployment to a Kubernetes inference server.\r\n\r\nTo do this, I am following the standard template to deploying a pyfunc type model and using the `mlflow.azureml.deploy` function to do this.\r\n\r\nThe problem that is occurring, is that the `load_model` function of the `mlflow.onnx` model flavour, which is used in the templated Azure ML deployment entry script, does not appear to be loading the ONNX model correctly. The following error occurs when the end point is deployed onto the Kubernetes cluster.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/tmp/ipykernel_22290/660498313.py in <module>\r\n----> 1 model = load_model(model_path)\r\n\r\n~/git/mlflow_v1_23_0/mlflow/pyfunc/__init__.py in load_model(model_uri, suppress_warnings, dst_path)\r\n    665         mlflow.pyfunc.utils._add_code_to_system_path(code_path=code_path)\r\n    666     data_path = os.path.join(local_path, conf[DATA]) if (DATA in conf) else local_path\r\n--> 667     model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n    668     return PyFuncModel(model_meta=model_meta, model_impl=model_impl)\r\n    669 \r\n\r\n~/git/mlflow_v1_23_0/mlflow/onnx.py in _load_pyfunc(path)\r\n    283     Load PyFunc implementation. Called by ``pyfunc.load_pyfunc``.\r\n    284     \"\"\"\r\n--> 285     return _OnnxModelWrapper(path)\r\n    286 \r\n    287 \r\n\r\n~/git/mlflow_v1_23_0/mlflow/onnx.py in __init__(self, path)\r\n    186         import onnxruntime\r\n    187 \r\n--> 188         self.rt = onnxruntime.InferenceSession(path)\r\n    189         assert len(self.rt.get_inputs()) >= 1\r\n    190         self.inputs = [(inp.name, inp.type) for inp in self.rt.get_inputs()]\r\n\r\n~/.conda/envs/py38_azureml_objdet_dev_mlflow_latest/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py in __init__(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\r\n    333 \r\n    334         try:\r\n--> 335             self._create_inference_session(providers, provider_options, disabled_optimizers)\r\n    336         except ValueError:\r\n    337             if self._enable_fallback:\r\n\r\n~/.conda/envs/py38_azureml_objdet_dev_mlflow_latest/lib/python3.8/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py in _create_inference_session(self, providers, provider_options, disabled_optimizers)\r\n    359         if providers == [] and len(available_providers) > 1:\r\n    360             self.disable_fallback()\r\n--> 361             raise ValueError(\"This ORT build has {} enabled. \".format(available_providers) +\r\n    362                              \"Since ORT 1.9, you are required to explicitly set \" +\r\n    363                              \"the providers parameter when instantiating InferenceSession. For example, \"\r\n\r\nValueError: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)\r\n```\r\n\r\nThe problem originates from the `load_model` function of the `mlflow.pyfunc` module, in the `__init__.py`, line 667 calls the `_load_pyfunc` function of the MLFlow model flavour.\r\n\r\n`model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)`\r\n\r\nIn this case, the MLFlow ONNX model flavour returns and instance of the `_OnnxModelWrapper` class. The `__init__` method of the this class is the operation which loads the ONNX model into the runtime framework, so that it can be used for inference. This class object is returned initialized into the AzureML entry script as a `model` object, where `model.rt`, is the ONNX model runtime object.\r\n\r\n```python\r\nclass _OnnxModelWrapper:\r\n      def __init__(self, path):\r\n              import onnxruntime\r\n      \r\n              self.rt = onnxruntime.InferenceSession(path)\r\n              assert len(self.rt.get_inputs()) >= 1\r\n              self.inputs = [(inp.name, inp.type) for inp in self.rt.get_inputs()]\r\n              self.output_names = [outp.name for outp in self.rt.get_outputs()]\r\n```\r\n\r\nAs the error suggestions, the line loading the ONNX model with the onnxruntime, needs to be adapted to specify the inference provider, such as:\r\n\r\n```python\r\nself.rt = onnxruntime.InferenceSession(path, provider=['CUDAExecutionProvider'])\r\n\r\n\r\n### Code to reproduce issue\r\n\r\nThe input requires an ONNX model registered to an MLFlow tracking server, with it downloaded to a local file. The model should be in MLFlow MLModel format with the following files:\r\n\r\n- MLmodel\r\n- conda.yml\r\n- model.onnx\r\n- requirements.txt\r\n\r\n```python\r\nfrom mlflow.pyfunc import load_model\r\n\r\nmodel_path = \"path/to/the/downloaded/model/artifact\"\r\nmodel = load_model(model_path)\r\n```\r\n\r\nIn this example, the MLModel configuration file takes care of loading the correct model flavour functions.\r\n\r\n### Other info / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [x] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [x] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [x] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [x] `integrations/azure`: Azure and Azure ML integrations\r\n- [x] `integrations/sagemaker`: SageMaker integrations\r\n- [x] `integrations/databricks`: Databricks integrations\r\n","closed_by":{"login":"ecm200","id":34489160,"node_id":"MDQ6VXNlcjM0NDg5MTYw","avatar_url":"https://avatars.githubusercontent.com/u/34489160?v=4","gravatar_id":"","url":"https://api.github.com/users/ecm200","html_url":"https://github.com/ecm200","followers_url":"https://api.github.com/users/ecm200/followers","following_url":"https://api.github.com/users/ecm200/following{/other_user}","gists_url":"https://api.github.com/users/ecm200/gists{/gist_id}","starred_url":"https://api.github.com/users/ecm200/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ecm200/subscriptions","organizations_url":"https://api.github.com/users/ecm200/orgs","repos_url":"https://api.github.com/users/ecm200/repos","events_url":"https://api.github.com/users/ecm200/events{/privacy}","received_events_url":"https://api.github.com/users/ecm200/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5308/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5308/timeline","performed_via_github_app":null,"state_reason":"completed"}