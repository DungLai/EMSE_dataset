{"url":"https://api.github.com/repos/mlflow/mlflow/issues/1921","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/1921/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/1921/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/1921/events","html_url":"https://github.com/mlflow/mlflow/issues/1921","id":505115838,"node_id":"MDU6SXNzdWU1MDUxMTU4Mzg=","number":1921,"title":"[FR] Decouple run from data, allow metric logging by dataset","user":{"login":"karlschriek","id":25316920,"node_id":"MDQ6VXNlcjI1MzE2OTIw","avatar_url":"https://avatars.githubusercontent.com/u/25316920?v=4","gravatar_id":"","url":"https://api.github.com/users/karlschriek","html_url":"https://github.com/karlschriek","followers_url":"https://api.github.com/users/karlschriek/followers","following_url":"https://api.github.com/users/karlschriek/following{/other_user}","gists_url":"https://api.github.com/users/karlschriek/gists{/gist_id}","starred_url":"https://api.github.com/users/karlschriek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/karlschriek/subscriptions","organizations_url":"https://api.github.com/users/karlschriek/orgs","repos_url":"https://api.github.com/users/karlschriek/repos","events_url":"https://api.github.com/users/karlschriek/events{/privacy}","received_events_url":"https://api.github.com/users/karlschriek/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022849295,"node_id":"MDU6TGFiZWwyMDIyODQ5Mjk1","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/tracking","name":"area/tracking","color":"48eabc","default":false,"description":"Tracking service, tracking client APIs, autologging"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-10-10T08:26:10Z","updated_at":"2020-07-30T19:06:24Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"My colleagues and I have been keeping an eye on MLFlow for some time now, as its stated goals would meet many of the typical requirements we have when rolling out Machine Learning solutions. When it just launched we thought that it looked very promising, but we were not yet confident about using it in client projects and have thus far mainly stuck to creating our own bespoke solutions.\r\n\r\nSince launch a lot has happened though (kudos to Databricks and the MLFlow community!) and now seems to be a good time to get on board. We still have several requirements that are not met by MLFlow and I would like to start putting them up for discussion to see if the wider community have the same requirements.\r\n\r\nSo here is the first requirement we have:\r\n\r\n\r\n**Description**\r\n\r\nCurrently the dataset used in a run is **implicit** and any metric logging therefore only refers to the data used in this run. We prefer to be able to separate Training and Evaluation as separate processes (e.g. triggered and scaled as separate microservices on Kubernetes). Generally this would mean that during Training we only have **one** training dataset and **one** validation dataset. (These would be identifiable with their own UUID). \r\n\r\nHowever, once the model is trained and saved we typically run evaluations on **multiple** test datasets. Either because we want to test how well the model generalises to different data or because as time goes by we improve our test set and want to reevaluate the models we have developed thus far. We also have a process for setting existing models to inactive or changing the selected model for a given experiment which feeds from this, but that is a discussion or different FR.\r\n\r\nA typical DB Model might look like this:\r\n\r\n![image](https://user-images.githubusercontent.com/25316920/66550327-733a9280-eb45-11e9-8cb3-7ca568765829.png).\r\n\r\n`databunch` is the named collection of data (such as the Iris or Titanic data). `datasets` are (typically) the `train`, `valid`  and `test` data found within the `databunch`.\r\n\r\nA `run` implicitly produces one and only one model. An `evaluation_run` uses this one model to produce metrics on a specific `dataset`\r\n\r\n\r\n*Proposed Changes*\r\nAs far as I can see this proposal would least require changes here:\r\n- DB Schema\r\n- mflow.log_metric(...) will need something with which to identify the Dataset for which the metric is to be logged\r\n- Possibly it would make sense to create Dataset and Databunch classes to deal with the abstraction client side. \r\n- Something like `mlflow.register_dataset(databunch_name, dataset_name, artifact_location)` and then `data_set = mlflow.get_dataset(databunch_name=\"iris\", dataset_name=\"train\")` could be used to fetch the dataset from the artifact store\r\n- UI would need to be updated to filter between Datasets\r\n\r\n\r\nI hope I've made our use-case clear. I'd also be happy to have a go at a PR that implements these changes if this is something the community would find useful.\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/1921/reactions","total_count":5,"+1":5,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/1921/timeline","performed_via_github_app":null,"state_reason":null}