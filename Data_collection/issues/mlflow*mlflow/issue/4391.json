{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4391","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4391/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4391/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4391/events","html_url":"https://github.com/mlflow/mlflow/issues/4391","id":901029797,"node_id":"MDU6SXNzdWU5MDEwMjk3OTc=","number":4391,"title":"[FR] Stop runs through the UI or MLflow API","user":{"login":"guysmoilov","id":611655,"node_id":"MDQ6VXNlcjYxMTY1NQ==","avatar_url":"https://avatars.githubusercontent.com/u/611655?v=4","gravatar_id":"","url":"https://api.github.com/users/guysmoilov","html_url":"https://github.com/guysmoilov","followers_url":"https://api.github.com/users/guysmoilov/followers","following_url":"https://api.github.com/users/guysmoilov/following{/other_user}","gists_url":"https://api.github.com/users/guysmoilov/gists{/gist_id}","starred_url":"https://api.github.com/users/guysmoilov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/guysmoilov/subscriptions","organizations_url":"https://api.github.com/users/guysmoilov/orgs","repos_url":"https://api.github.com/users/guysmoilov/repos","events_url":"https://api.github.com/users/guysmoilov/events{/privacy}","received_events_url":"https://api.github.com/users/guysmoilov/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1470945519,"node_id":"MDU6TGFiZWwxNDcwOTQ1NTE5","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/uiux","name":"area/uiux","color":"ede978","default":false,"description":"Front-end, user experience, plotting, JavaScript, JavaScript dev server"},{"id":2022849295,"node_id":"MDU6TGFiZWwyMDIyODQ5Mjk1","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/tracking","name":"area/tracking","color":"48eabc","default":false,"description":"Tracking service, tracking client APIs, autologging"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-05-25T16:03:48Z","updated_at":"2021-07-16T08:57:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nThe idea is to let users stop training runs via the MLflow experiment UI (or corresponding REST API).\r\nThe tracking client will detect that the user wants the current run to stop, and will stop the running process.\r\n\r\n## Motivation\r\n\r\nAt [DAGsHub](https://DAGsHub.com/), we've heard from several users who would benefit a lot from the ability to stop runs directly from the experiment visualization UI.  \r\nWe've even had a user say that they use WandB instead of MLflow, just because they have this specific feature.\r\n\r\nThe scenario is:\r\n1. The user is triggering multiple long-running training runs in parallel, in separate processes, probably on some cloud machines and with different hyperparams or code in each run.\r\n1. While the training runs are ongoing, the user checks the MLflow UI or API to check if the metrics are converging nicely, or looks at logged artifacts to see if they're going in the right direction. Example - log some generated GAN images after each epoch, and check which training runs are starting to show good looking images.\r\n1. The user would then like to cancel runs which look hopeless - to save money or free resources for more runs.\r\n    * Note that they might not know in advance the criteria for stopping a run, otherwise it could have been automated in code. The decision to terminate runs can depend on how the specific training run is faring in comparison to other current runs for example.\r\n1. It would be possible for them to do it by SSH'ing directly to the training machine or via whatever cluster manager they're using, but it's:\r\n    a.  Very inconvenient and disconnected from where the user made the actual decision - in the MLflow UI\r\n    b.  Might require permissions the user doesn't have, even though they're the actual data scientists who should be making this decision\r\n    c.  Difficult to find the exact process on the physical machine, which corresponds to a specific training run - meaning it's easy to make a mistake and kill the wrong process.\r\n\r\nAnother interesting corollary:  \r\nThis is hinting at a more general need - realtime monitoring. Right now, a complete page refresh is needed to reload metrics in the UI for a given run. It could be worth it to consider some auto refresh.  \r\nAlso, the user we talked to who uses WandB for this use case also set up a Slack bot to send live updates from WandB to their phone - loss curves, screenshots of generated artifacts, etc. Then they can know right away if a run is doing poorly, and cancel it via a Slack bot which activates the WandB API. This is also an interesting area of development in the future, probably not requiring any changes to core MLflow but instead as new examples, side projects or plugins.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [x] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n\r\n## Details\r\n\r\nMy proposal for how to implement this:\r\n* Use the existing tracking APIs (`log_params`, `log_metrics` etc.)\r\n* Add a new field to the response - this should be backwards compatible by leveraging protobuf, meaning older clients will just ignore it\r\n* This field will be a \"flag\" that indicates what the user wants to do with the run - 0 means nothing, 1 can mean \"stop\", and we can use more bit flags for other possible future functionality\r\n* On each relevant `log_X` function in the tracking client, it will check the returned flag and trigger a handler if required.\r\n* The tracking client will have a handler for the received flag - the default handler will just send a SIGINT to the current process, triggering an orderly shutdown, or else just call `sys.exit`.\r\n* This default handler can be overridden by the user to do something different\r\n* In terms of the backend - a new REST API endpoint will be created to cancel a specific run\r\n* This will store the instruction as a persistent flag in the DB, attached to the run\r\n* Then this flag will be checked and returned to the client on each relevant `log_X` call from the client\r\n\r\nThanks MLflow team! Looking forward to working on this after your design review.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4391/reactions","total_count":11,"+1":11,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4391/timeline","performed_via_github_app":null,"state_reason":null}