{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6211","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/6211/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/6211/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/6211/events","html_url":"https://github.com/mlflow/mlflow/issues/6211","id":1297854315,"node_id":"I_kwDOCB5Jx85NW69r","number":6211,"title":"[BUG] Batch inference with mlflow.pyfunc.spark_udf fails against timestamp input data types","user":{"login":"smurching","id":2358483,"node_id":"MDQ6VXNlcjIzNTg0ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2358483?v=4","gravatar_id":"","url":"https://api.github.com/users/smurching","html_url":"https://github.com/smurching","followers_url":"https://api.github.com/users/smurching/followers","following_url":"https://api.github.com/users/smurching/following{/other_user}","gists_url":"https://api.github.com/users/smurching/gists{/gist_id}","starred_url":"https://api.github.com/users/smurching/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smurching/subscriptions","organizations_url":"https://api.github.com/users/smurching/orgs","repos_url":"https://api.github.com/users/smurching/repos","events_url":"https://api.github.com/users/smurching/events{/privacy}","received_events_url":"https://api.github.com/users/smurching/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"},{"id":4300304016,"node_id":"LA_kwDOCB5Jx88AAAABAFFukA","url":"https://api.github.com/repos/mlflow/mlflow/labels/has-closing-pr","name":"has-closing-pr","color":"fef2c0","default":false,"description":"This issue has a closing PR"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2022-07-07T17:55:59Z","updated_at":"2022-07-18T06:54:31Z","closed_at":"2022-07-15T16:02:01Z","author_association":"COLLABORATOR","active_lock_reason":null,"body":"### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n1.27.0\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nDatabricks ML Runtime 11.0\r\n\r\n\n\n### Describe the problem\n\nI'm trying to score a table using `mlflow.pyfunc.spark_udf`. The table has the schema shown below, with timestamp columns:\r\n![image](https://user-images.githubusercontent.com/2358483/177837792-b77e3d53-a6ce-4f94-b80e-25149ee73475.png)\r\n\r\nI have some wrapper logic around `mlflow.pyfunc.spark_udf` :\r\n\r\n```python\r\nimport mlflow\r\nfrom pyspark.sql.functions import struct\r\n\r\n\r\ndef predict_batch(spark_session, model_uri, input_table_name, output_table_name):\r\n    \"\"\"\r\n    Apply the model at the specified URI for batch inference on the table with name input_table_name,\r\n    writing results to the table with name output_table_name\r\n    \"\"\"\r\n    table = spark_session.table(input_table_name)\r\n    predict = mlflow.pyfunc.spark_udf(spark_session, model_uri, result_type=\"string\", env_manager=\"conda\")\r\n    output_df = table.withColumn(\"prediction\", predict(struct(*table.columns)))\r\n    output_df.display()\r\n    output_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(output_table_name)\r\n```\r\n\r\nWhen I try to call this function, e.g:\r\n```python\r\npredict_batch(spark, model_uri, input_table_name, output_table_name)\r\n```\r\n\r\nI get the following exception:\r\n```\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (ip-10-68-149-252.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'TypeError: Object of type Timestamp is not JSON serializable'. Full traceback below:\r\nTraceback (most recent call last):\r\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1274, in udf\r\n    os.kill(scoring_server_proc.pid, signal.SIGTERM)\r\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1116, in _predict_row_batch\r\n    result = predict_fn(pdf)\r\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 1247, in batch_predict_fn\r\n    return client.invoke(pdf)\r\n  File \"/databricks/python/lib/python3.9/site-packages/mlflow/pyfunc/scoring_server/client.py\", line 39, in invoke\r\n    post_data = json.dumps(scoring_server._get_jsonable_obj(data, pandas_orient=\"split\"))\r\n  File \"/usr/lib/python3.9/json/__init__.py\", line 231, in dumps\r\n    return _default_encoder.encode(obj)\r\n  File \"/usr/lib/python3.9/json/encoder.py\", line 199, in encode\r\n    chunks = self.iterencode(o, _one_shot=True)\r\n  File \"/usr/lib/python3.9/json/encoder.py\", line 257, in iterencode\r\n    return _iterencode(o, 0)\r\n  File \"/usr/lib/python3.9/json/encoder.py\", line 179, in default\r\n    raise TypeError(f'Object of type {o.__class__.__name__} '\r\nTypeError: Object of type Timestamp is not JSON serializable\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:696)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:649)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n...\r\n```\r\n\r\nHowever, performing predictions locally using `mlflow.pyfunc.predict` doesn't hit the same JSON serialization issue:\r\n```\r\nimport pandas as pd\r\npdf = spark.read.table(\"nyc_taxi_data_train\").toPandas()\r\np.predict(pdf)\r\n```\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\nSee above\n\n### Other info / logs\n\n_No response_\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/pipelines`: Pipelines, Pipeline APIs, Pipeline configs, Pipeline Templates\n- [ ] `area/projects`: MLproject format, project running backends\n- [X] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations","closed_by":{"login":"harupy","id":17039389,"node_id":"MDQ6VXNlcjE3MDM5Mzg5","avatar_url":"https://avatars.githubusercontent.com/u/17039389?v=4","gravatar_id":"","url":"https://api.github.com/users/harupy","html_url":"https://github.com/harupy","followers_url":"https://api.github.com/users/harupy/followers","following_url":"https://api.github.com/users/harupy/following{/other_user}","gists_url":"https://api.github.com/users/harupy/gists{/gist_id}","starred_url":"https://api.github.com/users/harupy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harupy/subscriptions","organizations_url":"https://api.github.com/users/harupy/orgs","repos_url":"https://api.github.com/users/harupy/repos","events_url":"https://api.github.com/users/harupy/events{/privacy}","received_events_url":"https://api.github.com/users/harupy/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6211/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/6211/timeline","performed_via_github_app":null,"state_reason":"completed"}