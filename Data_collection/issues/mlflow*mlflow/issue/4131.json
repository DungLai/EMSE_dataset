{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4131","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4131/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4131/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4131/events","html_url":"https://github.com/mlflow/mlflow/issues/4131","id":813836665,"node_id":"MDU6SXNzdWU4MTM4MzY2NjU=","number":4131,"title":"[BUG] Spark UDF throws IllegalArgumentException","user":{"login":"gs-alt","id":75452261,"node_id":"MDQ6VXNlcjc1NDUyMjYx","avatar_url":"https://avatars.githubusercontent.com/u/75452261?v=4","gravatar_id":"","url":"https://api.github.com/users/gs-alt","html_url":"https://github.com/gs-alt","followers_url":"https://api.github.com/users/gs-alt/followers","following_url":"https://api.github.com/users/gs-alt/following{/other_user}","gists_url":"https://api.github.com/users/gs-alt/gists{/gist_id}","starred_url":"https://api.github.com/users/gs-alt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gs-alt/subscriptions","organizations_url":"https://api.github.com/users/gs-alt/orgs","repos_url":"https://api.github.com/users/gs-alt/repos","events_url":"https://api.github.com/users/gs-alt/events{/privacy}","received_events_url":"https://api.github.com/users/gs-alt/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022847277,"node_id":"MDU6TGFiZWwyMDIyODQ3Mjc3","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/examples","name":"area/examples","color":"48eabc","default":false,"description":"Example code"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"},{"id":2022851725,"node_id":"MDU6TGFiZWwyMDIyODUxNzI1","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/docker","name":"area/docker","color":"ede978","default":false,"description":"Docker use anywhere, such as MLprojects and MLmodels"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-22T20:26:12Z","updated_at":"2022-06-02T00:43:07Z","closed_at":"2022-06-02T00:43:07Z","author_association":"NONE","active_lock_reason":null,"body":"**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, running a Jupyter Notebook\r\n- **MLflow installed from (source or binary)**: Binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.10.0\r\n- **Python version**: 3.7.3\r\n- **npm version, if running the dev UI**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI've created a PySpark model in Python and logged it using MLflow. Then, I tried to create a Spark UDF and run a prediction with it on a toy dataset. It's throwing an error on the columns when I try to show the resulting DataFrame. Also, if I try to use `.toPandas()` to convert it to Pandas, it returns an empty DataFrame even though the number of rows and columns is (2, 5)\r\n\r\n### Code to reproduce issue\r\n#### First create the model and log it with MLflow\r\n```python\r\nfrom pyspark.ml.classification import LogisticRegression\r\nfrom pyspark.ml import Pipeline\r\nfrom pyspark.sql import Row, SparkSession\r\n\r\nspark_session = SparkSession.builder.appName('spark-udf-deployment').getOrCreate()\r\n\r\nFEATURES = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\r\n\r\nSPARK_DATA = spark_session.createDataFrame(\r\n    [\r\n        (5.1, 3.5, 1.4, 0.2, \"A\"),\r\n        (6.7, 3.3, 5.7, 2.1, \"B\"),\r\n    ],\r\n    FEATURES\r\n)\r\n\r\nfeature_cols = SPARK_DATA.columns[:-1]\r\n\r\nassembler = pyspark.ml.feature.VectorAssembler(inputCols=feature_cols, outputCol='features')\r\nlabel_indexer = pyspark.ml.feature.StringIndexer(inputCol='species', outputCol='label')\r\nmodel = LogisticRegression(maxIter=2, regParam=0.001)\r\npipeline = Pipeline(stages=[assembler, label_indexer, model])\r\n\r\nmodel = pipeline.fit(SPARK_DATA)\r\n\r\nwith mlflow.start_run() as run:\r\n    mlflow.spark.log_model(model, \"sparkmodel\")\r\n```\r\n\r\n#### Now create the UDF and run it\r\n```python\r\nimport mlflow.pyfunc\r\nimport pandas as pd\r\nfrom pyspark.sql.types import ArrayType, FloatType\r\nfrom pyspark.sql import Row, SparkSession\r\n\r\nMODEL_PATH = \"s3://mlflow/0/5a27e53916914a66af08266ba04a75d1/artifacts/sparkmodel\"\r\nFEATURES = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\r\n\r\nspark_session = SparkSession.builder.appName('spark-udf-deployment').getOrCreate()\r\nSPARK_DATA = spark_session.createDataFrame(\r\n    [\r\n        (5.1, 3.5, 1.4, 0.2),\r\n        (6.7, 3.3, 5.7, 2.1),\r\n    ],\r\n    FEATURES\r\n)\r\n\r\npred_udf = mlflow.pyfunc.spark_udf(spark=spark_session, model_uri=MODEL_PATH, result_type=ArrayType(FloatType()))\r\nSPARK_DATA.show()\r\n\r\nresult = SPARK_DATA.withColumn(\"prediction\", pred_udf(*FEATURES))\r\n\r\nprint(result.count(), len(result.columns)) # prints 2 5 \r\ndisplay(result) # prints nothing\r\nresult.toPandas() # prints DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, prediction: array<float>]\r\nprint(result.toPandas()) # prints an empty Pandas DataFrame\r\nresult.show() # throws the error\r\n```\r\n\r\n\r\n\r\n### Other info / logs\r\nThe last line of `result.show()` throws a large exception. I think the important part is:\r\n\r\n```\r\npyspark.sql.utils.IllegalArgumentException: 'Field \"sepal_length\" does not exist.\\nAvailable fields: 0, 1, 2, 3'\r\n```\r\n\r\nHere is the full logs if useful:\r\n\r\n```\r\nPy4JJavaError                             Traceback (most recent call last)\r\n<ipython-input-93-4ac216c0e3a8> in <module>\r\n     25 result.toPandas()\r\n     26 print(result.toPandas())\r\n---> 27 result.show()\r\n\r\n/usr/local/spark/python/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)\r\n    376         \"\"\"\r\n    377         if isinstance(truncate, bool) and truncate:\r\n--> 378             print(self._jdf.showString(n, 20, vertical))\r\n    379         else:\r\n    380             print(self._jdf.showString(n, int(truncate), vertical))\r\n\r\n/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)\r\n   1255         answer = self.gateway_client.send_command(command)\r\n   1256         return_value = get_return_value(\r\n-> 1257             answer, self.gateway_client, self.target_id, self.name)\r\n   1258 \r\n   1259         for temp_arg in temp_args:\r\n\r\n/usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n     61     def deco(*a, **kw):\r\n     62         try:\r\n---> 63             return f(*a, **kw)\r\n     64         except py4j.protocol.Py4JJavaError as e:\r\n     65             s = e.java_exception.toString()\r\n\r\n/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    326                 raise Py4JJavaError(\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n--> 328                     format(target_id, \".\", name), value)\r\n    329             else:\r\n    330                 raise Py4JError(\r\n\r\nPy4JJavaError: An error occurred while calling o6659.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 739.0 failed 1 times, most recent failure: Lost task 0.0 in stage 739.0 (TID 841, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o63.transform.\r\n: java.lang.IllegalArgumentException: Field \"sepal_length\" does not exist.\r\nAvailable fields: 0, 1, 2, 3\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 286, in dump_stream\r\n    for series in iterator:\r\n  File \"<string>\", line 1, in <lambda>\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\r\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\r\n    result = f(*a)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\", line 644, in predict\r\n    result = model.predict(pdf)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\", line 418, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/spark.py\", line 542, in predict\r\n    self.spark_model.transform(spark_df).select(\"prediction\").collect()]\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\r\n    return self._transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 262, in _transform\r\n    dataset = t.transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\r\n    return self._transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 312, in _transform\r\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\r\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\r\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\r\npyspark.sql.utils.IllegalArgumentException: 'Field \"sepal_length\" does not exist.\\nAvailable fields: 0, 1, 2, 3'\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\r\n\tat sun.reflect.GeneratedMethodAccessor171.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o63.transform.\r\n: java.lang.IllegalArgumentException: Field \"sepal_length\" does not exist.\r\nAvailable fields: 0, 1, 2, 3\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\r\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\r\n    serializer.dump_stream(func(split_index, iterator), outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 286, in dump_stream\r\n    for series in iterator:\r\n  File \"<string>\", line 1, in <lambda>\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\r\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\r\n    result = f(*a)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\", line 644, in predict\r\n    result = model.predict(pdf)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/pyfunc/__init__.py\", line 418, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"/opt/conda/lib/python3.7/site-packages/mlflow/spark.py\", line 542, in predict\r\n    self.spark_model.transform(spark_df).select(\"prediction\").collect()]\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\r\n    return self._transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\", line 262, in _transform\r\n    dataset = t.transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\r\n    return self._transform(dataset)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 312, in _transform\r\n    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\r\n  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\r\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\r\npyspark.sql.utils.IllegalArgumentException: 'Field \"sepal_length\" does not exist.\\nAvailable fields: 0, 1, 2, 3'\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\r\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:98)\r\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:96)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:801)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n```\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ x ] `area/examples`: Example code\r\n- [ x ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n\r\nInterface \r\n- [ x ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n\r\n","closed_by":{"login":"dbczumar","id":39497902,"node_id":"MDQ6VXNlcjM5NDk3OTAy","avatar_url":"https://avatars.githubusercontent.com/u/39497902?v=4","gravatar_id":"","url":"https://api.github.com/users/dbczumar","html_url":"https://github.com/dbczumar","followers_url":"https://api.github.com/users/dbczumar/followers","following_url":"https://api.github.com/users/dbczumar/following{/other_user}","gists_url":"https://api.github.com/users/dbczumar/gists{/gist_id}","starred_url":"https://api.github.com/users/dbczumar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbczumar/subscriptions","organizations_url":"https://api.github.com/users/dbczumar/orgs","repos_url":"https://api.github.com/users/dbczumar/repos","events_url":"https://api.github.com/users/dbczumar/events{/privacy}","received_events_url":"https://api.github.com/users/dbczumar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4131/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4131/timeline","performed_via_github_app":null,"state_reason":"completed"}