{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2151","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/2151/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/2151/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/2151/events","html_url":"https://github.com/mlflow/mlflow/issues/2151","id":530294948,"node_id":"MDU6SXNzdWU1MzAyOTQ5NDg=","number":2151,"title":"[Example request] examples on saving a spark submit job into a mlflow and serving the saved spark model with the ability to specify the spark configurations when serving the model ","user":{"login":"y-tee","id":26523874,"node_id":"MDQ6VXNlcjI2NTIzODc0","avatar_url":"https://avatars.githubusercontent.com/u/26523874?v=4","gravatar_id":"","url":"https://api.github.com/users/y-tee","html_url":"https://github.com/y-tee","followers_url":"https://api.github.com/users/y-tee/followers","following_url":"https://api.github.com/users/y-tee/following{/other_user}","gists_url":"https://api.github.com/users/y-tee/gists{/gist_id}","starred_url":"https://api.github.com/users/y-tee/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/y-tee/subscriptions","organizations_url":"https://api.github.com/users/y-tee/orgs","repos_url":"https://api.github.com/users/y-tee/repos","events_url":"https://api.github.com/users/y-tee/events{/privacy}","received_events_url":"https://api.github.com/users/y-tee/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022866842,"node_id":"MDU6TGFiZWwyMDIyODY2ODQy","url":"https://api.github.com/repos/mlflow/mlflow/labels/priority/awaiting-more-evidence","name":"priority/awaiting-more-evidence","color":"534cb5","default":false,"description":"Lowest priority. Possibly useful, but not yet enough support to actually get it done."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-11-29T11:21:47Z","updated_at":"2020-07-22T18:28:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, \r\nI understand that there is an example in the github on pyspark codes, namely the multistep workflow, however, I have problem understanding how a saved spark model should be served (with their spark configuration specified). If would be great if you can provide some examples since it is actually not written in the documentations. Many many thanks!\r\n\r\nCurrently when i am trying to serve the spark model with \r\n```mlflow models serve -m runs:/6162603c99234ff6801c8346d6871630/model --port 5002```\r\nI get errors of the python environment in worker nodes is not the same as the python environment in driver:\r\n```\r\n19/11/29 18:10:58 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\r\n[2019-11-29 18:10:58 +0800] [28178] [ERROR] Exception in worker process\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/arbiter.py\", line 583, in spawn_worker\r\n    worker.init_process()\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/workers/base.py\", line 129, in init_process\r\n    self.load_wsgi()\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/workers/base.py\", line 138, in load_wsgi\r\n    self.wsgi = self.app.wsgi()\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/app/base.py\", line 67, in wsgi\r\n    self.callable = self.load()\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py\", line 52, in load\r\n    return self.load_wsgiapp()\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/app/wsgiapp.py\", line 41, in load_wsgiapp\r\n    return util.import_app(self.app_uri)\r\n  File \"/usr/local/lib/python3.6/site-packages/gunicorn/util.py\", line 350, in import_app\r\n    __import__(module)\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/mlflow/pyfunc/scoring_server/wsgi.py\", line 6, in <module>\r\n    app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/mlflow/pyfunc/__init__.py\", line 290, in load_model\r\n    return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/mlflow/spark.py\", line 436, in _load_pyfunc\r\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/mlflow/spark.py\", line 378, in _load_model\r\n    return PipelineModel.load(model_path)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/ml/util.py\", line 362, in load\r\n    return cls.read().load(path)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/ml/pipeline.py\", line 240, in load\r\n    metadata = DefaultParamsReader.loadMetadata(path, self.sc)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/ml/util.py\", line 558, in loadMetadata\r\n    metadataStr = sc.textFile(metadataPath, 1).first()\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/rdd.py\", line 1378, in first\r\n    rs = self.take(1)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/rdd.py\", line 1360, in take\r\n    res = self.context.runJob(self, takeUpToNumLeft, p)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/context.py\", line 1069, in runJob\r\n    sock_info = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/py4j/java_gateway.py\", line 1257, in __call__\r\n    answer, self.gateway_client, self.target_id, self.name)\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/home/davidooi/.local/lib/python3.6/site-packages/py4j/protocol.py\", line 328, in get_return_value\r\n    format(target_id, \".\", name), value)\r\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\r\n    (\"%d.%d\" % sys.version_info[:2], version))\r\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1893)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1881)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1880)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1880)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2114)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2063)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2052)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/opt/apps/ecm/service/spark/2.4.3-1.2.0/package/spark-2.4.3-1.2.0-bin-hadoop2.8/python/lib/pyspark.zip/pyspark/worker.py\", line 267, in main\r\n    (\"%d.%d\" % sys.version_info[:2], version))\r\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n\r\n[2019-11-29 18:10:58 +0800] [28178] [INFO] Worker exiting (pid: 28178)\r\n[2019-11-29 18:10:58 +0800] [28169] [INFO] Shutting down: Master\r\n[2019-11-29 18:10:58 +0800] [28169] [INFO] Reason: Worker failed to boot.\r\nTraceback (most recent call last):\r\n  File \"/home/davidooi/.conda/envs/mlflow/bin/mlflow\", line 10, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 764, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 717, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 1137, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 956, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/davidooi/.local/lib/python3.7/site-packages/click/core.py\", line 555, in invoke\r\n    return callback(*args, **kwargs)\r\n  File \"/home/davidooi/.conda/envs/mlflow/lib/python3.7/site-packages/mlflow/models/cli.py\", line 56, in serve\r\n    host=host)\r\n  File \"/home/davidooi/.conda/envs/mlflow/lib/python3.7/site-packages/mlflow/pyfunc/backend.py\", line 84, in serve\r\n    command_env=command_env)\r\n  File \"/home/davidooi/.conda/envs/mlflow/lib/python3.7/site-packages/mlflow/pyfunc/backend.py\", line 164, in _execute_in_conda_env\r\n    command, rc\r\nException: Command 'source /opt/anaconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-9ebeece9d3ad59af4a788f4ffa82db1db63f7d99 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5002 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 3\r\n```\r\nAny idea what i did wrong when submitting or saving the spark model itself? I did a spark submit on the python file with the right PYSPARK_PYTHON and DRIVER_PYSPARK_PYTHON set ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2151/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/2151/timeline","performed_via_github_app":null,"state_reason":null}