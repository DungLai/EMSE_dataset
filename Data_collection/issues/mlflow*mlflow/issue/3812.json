{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3812","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/3812/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/3812/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/3812/events","html_url":"https://github.com/mlflow/mlflow/issues/3812","id":761235170,"node_id":"MDU6SXNzdWU3NjEyMzUxNzA=","number":3812,"title":"Unable to terminate mlflow models serve process started from python with subprocess.Popen","user":{"login":"rawkm","id":15920671,"node_id":"MDQ6VXNlcjE1OTIwNjcx","avatar_url":"https://avatars.githubusercontent.com/u/15920671?v=4","gravatar_id":"","url":"https://api.github.com/users/rawkm","html_url":"https://github.com/rawkm","followers_url":"https://api.github.com/users/rawkm/followers","following_url":"https://api.github.com/users/rawkm/following{/other_user}","gists_url":"https://api.github.com/users/rawkm/gists{/gist_id}","starred_url":"https://api.github.com/users/rawkm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rawkm/subscriptions","organizations_url":"https://api.github.com/users/rawkm/orgs","repos_url":"https://api.github.com/users/rawkm/repos","events_url":"https://api.github.com/users/rawkm/events{/privacy}","received_events_url":"https://api.github.com/users/rawkm/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2020-12-10T13:19:39Z","updated_at":"2020-12-14T13:03:29Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [x ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **MLflow installed from (source or binary)**: binary\r\n- **MLflow version (run ``mlflow --version``)**: 1.11.0\r\n- **Python version**: 3.8.5\r\n- **npm version, if running the dev UI**: -\r\n- **Exact command to reproduce**: p = subprocess.Popen(\r\n            [\"mlflow\", \"models\", \"serve\", \"-m\", remote_model_uri, \"-h\", host, \"-p\",\r\n             port, \"--no-conda\"])\r\n\r\n### Describe the problem\r\nI'm trying to serve a model locally within a unittest test case using subprocess.Popen() and to terminate the serving process at the end of the test. I'm able to start the subprocess and get predictions from the served model, but unable to terminate the process that runs the served model, i.e., after the test completes the model serving gunicorn process as well as the worker process it spawns stay alive and can be queried for predictions.\r\n\r\nI've tried to terminate the process with all of the following\r\n\r\np.terminate()\r\np.kill()\r\nos.kill(p.pid, signal.SIGTERM)\r\nos.kill(p.pid, signal.SIGKILL)\r\n\r\nas well as calling different combinations of\r\n\r\np.wait()\r\np.poll()\r\np.communicate(timeout=20)\r\ndel p\r\n\r\nafter the termination attempt.\r\n\r\nHowever, the two gunicorn processes never get killed and persist after the tests end. \r\n\r\nI'm looking for a way to run model serving from Python script and to terminate the model serving processes when the script finishes.\r\n\r\n### Code to reproduce issue\r\n\r\np = subprocess.Popen(\r\n            [\"mlflow\", \"models\", \"serve\", \"-m\", remote_model_uri, \"-h\", host, \"-p\",\r\n             port, \"--no-conda\"])\r\n\r\n### Other info / logs\r\n\r\n\r\n### What component(s), interfaces, languages, and integrations does this bug affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterface \r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguage \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3812/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/3812/timeline","performed_via_github_app":null,"state_reason":null}