{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4245","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4245/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4245/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4245/events","html_url":"https://github.com/mlflow/mlflow/issues/4245","id":857096561,"node_id":"MDU6SXNzdWU4NTcwOTY1NjE=","number":4245,"title":"[FR] Add Transformers Pretrained Model Flavor","user":{"login":"twolffpiggott","id":13351115,"node_id":"MDQ6VXNlcjEzMzUxMTE1","avatar_url":"https://avatars.githubusercontent.com/u/13351115?v=4","gravatar_id":"","url":"https://api.github.com/users/twolffpiggott","html_url":"https://github.com/twolffpiggott","followers_url":"https://api.github.com/users/twolffpiggott/followers","following_url":"https://api.github.com/users/twolffpiggott/following{/other_user}","gists_url":"https://api.github.com/users/twolffpiggott/gists{/gist_id}","starred_url":"https://api.github.com/users/twolffpiggott/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/twolffpiggott/subscriptions","organizations_url":"https://api.github.com/users/twolffpiggott/orgs","repos_url":"https://api.github.com/users/twolffpiggott/repos","events_url":"https://api.github.com/users/twolffpiggott/events{/privacy}","received_events_url":"https://api.github.com/users/twolffpiggott/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-04-13T15:49:37Z","updated_at":"2022-12-07T20:29:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nThis proposal would add a [Transformers](https://github.com/huggingface/transformers) [pretrained model](https://huggingface.co/transformers/pretrained_models.html) flavor to MLflow.\r\n\r\n## Motivation\r\n\r\n### What is the use case for this feature?\r\n\r\nThe custom model flavor will enable full integration of Transformer pretrained model artifacts with tracking server runs and the MLflow model registry, and enable downstream integration with [Seldon](https://docs.seldon.io/projects/seldon-core/en/v1.1.0/servers/mlflow.html) via the resulting MLmodel. Users will be able to serialise transformers model artifacts via a simple `mlflow.log_model` call, and load via `mlflow.pyfunc.load_model`.\r\n\r\n### Why is this use case valuable to support for MLflow users in general?\r\n\r\nTransformers is a very popular NLP library (it has significantly more stars, for example, than [Catboost](https://github.com/catboost/catboost), for which a flavor was recently added). Its integration with the MLflow tracking server and model registry will support many NLP use cases for experimentation and deployment. Many MLflow users will have interests in NLP projects.\r\n\r\n### Why is this use case valuable to support for your project(s) or organization?\r\n\r\nMy projects involve fine-tuning transformers pretrained models for specific use cases. This involves experimentation using the MLflow tracking server and publishing new model versions via the MLflow model registry, which is connected to production pipelines. A transformers pretrained model flavor supports seamless integration of the full model artifacts with the tracking server runs and model registry.\r\n\r\n### Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\nTransformers pretrained models use [custom serialisation methods](https://github.com/huggingface/transformers/blob/4b919657313103f1ee903e32a9213b48e6433afe/src/transformers/modeling_utils.py#L784), that save and load a number of model dependencies in addition to the Pytorch model binary. Transformers pretrained models are also generally expected to be serialised together with a [tokenizer](https://github.com/huggingface/transformers/blob/cb38ffcc5e0ae2fac653342ac36dc75c15ea178f/src/transformers/tokenization_utils_base.py#L1839).\r\n\r\nAdditionally, the PyFunc wrapper for a transformers pretrained model should ideally include tokenisation. By contrast, the [existing Pytorch model flavor](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/__init__.py) assumes only the bare `torch.save` and `torch.load` functions are used for (de)serialisation, and has a parsimonious `_PyTorchWrapper` that cannot currently include tokenisation.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThis proposal would add a [Transformers](https://github.com/huggingface/transformers) [pretrained model](https://huggingface.co/transformers/pretrained_models.html) flavor to MLflow.\r\n\r\nSince the underlying models are Pytorch-based, the implementation would be similar to the [existing Pytorch model flavor](https://github.com/mlflow/mlflow/blob/master/mlflow/pytorch/__init__.py).\r\n\r\nHowever, it would specifically add saving and loading functionality for [Transformers pretrained models](https://huggingface.co/transformers/pretrained_models.html) and [tokenizers](https://huggingface.co/transformers/main_classes/tokenizer.html).\r\n\r\nInstead of using the bare `torch.save` and `torch.load`, it would invoke the `save_pretrained` methods for [Transformers pretrained models](https://github.com/huggingface/transformers/blob/4b919657313103f1ee903e32a9213b48e6433afe/src/transformers/modeling_utils.py#L784) and [tokenizers](https://github.com/huggingface/transformers/blob/cb38ffcc5e0ae2fac653342ac36dc75c15ea178f/src/transformers/tokenization_utils_base.py#L1839).\r\n\r\nThis flavor would also implement a custom `_TransformerPretrainedWrapper` for the PyFunc implementation.\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4245/reactions","total_count":8,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":4,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4245/timeline","performed_via_github_app":null,"state_reason":null}