{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4571","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4571/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4571/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4571/events","html_url":"https://github.com/mlflow/mlflow/issues/4571","id":946736152,"node_id":"MDU6SXNzdWU5NDY3MzYxNTI=","number":4571,"title":"[BUG]pyfunc prediction with sklearn column transformer fails with pyspark udf but the same works fine when used via the REST API","user":{"login":"uvnikgupta","id":20485662,"node_id":"MDQ6VXNlcjIwNDg1NjYy","avatar_url":"https://avatars.githubusercontent.com/u/20485662?v=4","gravatar_id":"","url":"https://api.github.com/users/uvnikgupta","html_url":"https://github.com/uvnikgupta","followers_url":"https://api.github.com/users/uvnikgupta/followers","following_url":"https://api.github.com/users/uvnikgupta/following{/other_user}","gists_url":"https://api.github.com/users/uvnikgupta/gists{/gist_id}","starred_url":"https://api.github.com/users/uvnikgupta/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uvnikgupta/subscriptions","organizations_url":"https://api.github.com/users/uvnikgupta/orgs","repos_url":"https://api.github.com/users/uvnikgupta/repos","events_url":"https://api.github.com/users/uvnikgupta/events{/privacy}","received_events_url":"https://api.github.com/users/uvnikgupta/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-07-17T06:36:49Z","updated_at":"2021-07-18T07:25:48Z","closed_at":"2021-07-18T07:20:04Z","author_association":"NONE","active_lock_reason":null,"body":"I have created the following training pipeline:\r\n```\r\ntargetCol = \"median_house_value\"</br>\r\ncatCols = [\"ocean_proximity\"]\r\nnumCols = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\r\nnum_pipeline = Pipeline([\r\n                    (\"Imputer\", SimpleImputer()),\r\n                    (\"Scaler\", StandardScaler())\r\n                ])\r\nfull_pipeline = ColumnTransformer([\r\n                    (\"Numerical_Pipeline\", num_pipeline, numCols),\r\n                    (\"OneHot\", OneHotEncoder(), catCols)\r\n                ])\r\n```\r\nand am using the following training code:\r\n```\r\nx = housing[numCols + catCols]\r\ny = housing[targetCol]\r\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=True)\r\nx_train = full_pipeline.fit_transform(x_train)\r\nx_test = full_pipeline.transform(x_test)\r\nmodel = LinearRegression()\r\nmodel.fit(x_train, y_train)\r\n```\r\nThen I dump the pipeline and the model using:\r\n```\r\njoblib.dump(model, model_path)\r\njoblib.dump(full_pipeline, pipeline_path);\r\n```\r\nI create a MLFlow pyfunc model:\r\n```\r\nartifacts = {\r\n    \"pipeline\": pipeline_path,\r\n    \"model\": model_path\r\n}\r\n\r\n# Define the model class\r\nclass ModelWrapper(mlflow.pyfunc.PythonModel):\r\n    def load_context(self, context):\r\n        self.pipeline = joblib.load(context.artifacts[\"pipeline\"])\r\n        self.model = joblib.load(context.artifacts[\"model\"])\r\n\r\n    def predict(self, context, model_input):\r\n        input_matrix = self.pipeline.transform(model_input)\r\n        return self.model.predict(input_matrix)\r\n```\r\nAnd log this model.\r\nI use two different methods for getting the prediction from the logged model:\r\n1. REST API \r\n```\r\ncatCols = [\"ocean_proximity\"]\r\nnumCols = [\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\"]\r\nx = housing[numCols + catCols]\r\ninput_json = x[:100].to_json(orient=\"split\")\r\nproc = subprocess.run([\"curl\",  \"-X\", \"POST\", \"-H\", \r\n                       \"Content-Type:application/json; format=pandas-split\", \r\n                       \"--data\", input_json, \"http://127.0.0.1:31236/invocations\"], \r\n                      stdout=subprocess.PIPE, encoding='utf-8')\r\noutput = proc.stdout\r\npredictions = np.array(json.loads(output)).reshape(-1,1)\r\n```\r\n2. Pyspark UDF\r\n```\r\nmodel = mlflow.pyfunc.spark_udf(spark, logged_model)\r\ndf = housing.withColumn('predictions', model(\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"ocean_proximity\"))\r\ndf.show()\r\n```\r\nI get the predictions from the REST API call successfully. \r\nBut the pyspark UDF returns the following error:\r\n```\r\n21/07/17 06:02:16 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 19)1]\r\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\r\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\r\n    for batch in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\r\n    for series in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 105, in <lambda>\r\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 844, in predict\r\n    result = model.predict(pdf)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 596, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/model.py\", line 257, in predict\r\n    return self.python_model.predict(self.context, model_input)\r\n  File \"/tmp/ipykernel_151/12639501.py\", line 16, in predict\r\n  File \"/opt/conda/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 560, in transform\r\n    raise RuntimeError(\r\nRuntimeError: Given feature/column names do not match the ones for the data given during fit.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n21/07/17 06:02:16 WARN TaskSetManager: Lost task 0.0 in stage 19.0 (TID 19) (mltoolkit-6b7455f648-hhwg2 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\r\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\r\n    for batch in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\r\n    for series in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 105, in <lambda>\r\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 844, in predict\r\n    result = model.predict(pdf)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 596, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/model.py\", line 257, in predict\r\n    return self.python_model.predict(self.context, model_input)\r\n  File \"/tmp/ipykernel_151/12639501.py\", line 16, in predict\r\n  File \"/opt/conda/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 560, in transform\r\n    raise RuntimeError(\r\nRuntimeError: Given feature/column names do not match the ones for the data given during fit.\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\n21/07/17 06:02:16 ERROR TaskSetManager: Task 0 in stage 19.0 failed 1 times; aborting job\r\n---------------------------------------------------------------------------\r\nPythonException                           Traceback (most recent call last)\r\n/tmp/ipykernel_214/555224018.py in <module>\r\n      3 \r\n      4 df = housing.withColumn('predictions', model(\"housing_median_age\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\", \"median_income\", \"ocean_proximity\"))\r\n----> 5 df.show()\r\n\r\n/opt/conda/lib/python3.9/site-packages/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)\r\n    482         \"\"\"\r\n    483         if isinstance(truncate, bool) and truncate:\r\n--> 484             print(self._jdf.showString(n, 20, vertical))\r\n    485         else:\r\n    486             print(self._jdf.showString(n, int(truncate), vertical))\r\n\r\n/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args)\r\n   1302 \r\n   1303         answer = self.gateway_client.send_command(command)\r\n-> 1304         return_value = get_return_value(\r\n   1305             answer, self.gateway_client, self.target_id, self.name)\r\n   1306 \r\n\r\n/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\r\n    115                 # Hide where the exception came from that shows a non-Pythonic\r\n    116                 # JVM exception message.\r\n--> 117                 raise converted from None\r\n    118             else:\r\n    119                 raise\r\n\r\nPythonException: \r\n  An exception was thrown from the Python worker. Please see the stack trace below.\r\nTraceback (most recent call last):\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\r\n    process()\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\r\n    serializer.dump_stream(out_iter, outfile)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 273, in dump_stream\r\n    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 81, in dump_stream\r\n    for batch in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 266, in init_stream_yield_batches\r\n    for series in iterator:\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\r\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 105, in <lambda>\r\n    verify_result_type(f(*a)), len(a[0])), arrow_return_type)\r\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 844, in predict\r\n    result = model.predict(pdf)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/__init__.py\", line 596, in predict\r\n    return self._model_impl.predict(data)\r\n  File \"/opt/conda/lib/python3.9/site-packages/mlflow/pyfunc/model.py\", line 257, in predict\r\n    return self.python_model.predict(self.context, model_input)\r\n  File \"/tmp/ipykernel_151/12639501.py\", line 16, in predict\r\n  File \"/opt/conda/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py\", line 560, in transform\r\n    raise RuntimeError(\r\nRuntimeError: Given feature/column names do not match the ones for the data given during fit.\r\n```\r\n\r\nAny ideas what could be the issue and the fix?","closed_by":{"login":"uvnikgupta","id":20485662,"node_id":"MDQ6VXNlcjIwNDg1NjYy","avatar_url":"https://avatars.githubusercontent.com/u/20485662?v=4","gravatar_id":"","url":"https://api.github.com/users/uvnikgupta","html_url":"https://github.com/uvnikgupta","followers_url":"https://api.github.com/users/uvnikgupta/followers","following_url":"https://api.github.com/users/uvnikgupta/following{/other_user}","gists_url":"https://api.github.com/users/uvnikgupta/gists{/gist_id}","starred_url":"https://api.github.com/users/uvnikgupta/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uvnikgupta/subscriptions","organizations_url":"https://api.github.com/users/uvnikgupta/orgs","repos_url":"https://api.github.com/users/uvnikgupta/repos","events_url":"https://api.github.com/users/uvnikgupta/events{/privacy}","received_events_url":"https://api.github.com/users/uvnikgupta/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4571/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4571/timeline","performed_via_github_app":null,"state_reason":"completed"}