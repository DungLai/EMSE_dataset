{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6008","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/6008/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/6008/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/6008/events","html_url":"https://github.com/mlflow/mlflow/issues/6008","id":1254557892,"node_id":"I_kwDOCB5Jx85KxwjE","number":6008,"title":"[BUG]spark serve model fails with custom function added to the pyspark MLpipeline ","user":{"login":"Hiteshsaai","id":29530488,"node_id":"MDQ6VXNlcjI5NTMwNDg4","avatar_url":"https://avatars.githubusercontent.com/u/29530488?v=4","gravatar_id":"","url":"https://api.github.com/users/Hiteshsaai","html_url":"https://github.com/Hiteshsaai","followers_url":"https://api.github.com/users/Hiteshsaai/followers","following_url":"https://api.github.com/users/Hiteshsaai/following{/other_user}","gists_url":"https://api.github.com/users/Hiteshsaai/gists{/gist_id}","starred_url":"https://api.github.com/users/Hiteshsaai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Hiteshsaai/subscriptions","organizations_url":"https://api.github.com/users/Hiteshsaai/orgs","repos_url":"https://api.github.com/users/Hiteshsaai/repos","events_url":"https://api.github.com/users/Hiteshsaai/events{/privacy}","received_events_url":"https://api.github.com/users/Hiteshsaai/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022845866,"node_id":"MDU6TGFiZWwyMDIyODQ1ODY2","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/artifacts","name":"area/artifacts","color":"48eabc","default":false,"description":"Artifact stores and artifact logging"},{"id":2022847714,"node_id":"MDU6TGFiZWwyMDIyODQ3NzE0","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/model-registry","name":"area/model-registry","color":"48eabc","default":false,"description":"Model registry, model registry APIs, and the fluent client calls for model registry"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2022-06-01T00:12:50Z","updated_at":"2022-06-02T09:18:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Willingness to contribute\n\nNo. I cannot contribute a bug fix at this time.\n\n### MLflow version\n\n1.25.1\n\n### System information\n\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS\r\n- **Python version**: 3.8.12\r\n- **yarn version, if running the dev UI**: NA\r\n\n\n### Describe the problem\n\nI created a custom Transformer and added that to the MLlib pipeline in pypark and saved the model using \r\n```mlflow.spark.save_model()``` , I am able to load the model using ```mlflow.spark.load_model()``` and test a sample input successfully in the pipeline but when i try to run it on mlfow server i am getting \r\n\r\n```AttributeError: module '__main__' has no attribute 'SetValueTransformer'```\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\n```\r\nfrom pyspark.sql import SparkSession\r\nfrom pyspark.sql import DataFrame\r\nfrom pyspark.ml import Pipeline, Transformer\r\nfrom pyspark.ml.feature import Imputer\r\nfrom pyspark.ml.feature import StringIndexer\r\nfrom pyspark.ml.feature import OneHotEncoder\r\nfrom pyspark.ml.feature import VectorAssembler\r\nfrom pyspark.ml.classification import RandomForestClassifier\r\nimport mlflow \r\nfrom pyspark import keyword_only\r\nfrom pyspark.ml import Transformer\r\nfrom pyspark.ml.param.shared import HasOutputCols, Param, Params, TypeConverters\r\nfrom pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\r\nfrom pyspark.sql.functions import lit # for the dummy _transform\r\n\r\n\r\nspark = SparkSession.builder.appName('ml').config(\"spark.driver.bindAddress\", \"127.0.0.1\").getOrCreate()\r\n\r\n\r\ndf  = spark.read.csv(\"drug200.csv\", header=True, inferSchema=True)\r\n\r\n\r\n# CUSTOM TRANSFORMER WITH SAVING --------------\r\nclass SetValueTransformer(\r\n    Transformer, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable,\r\n):\r\n    colName = Param(\r\n        Params._dummy(),\r\n        \"colName\",\r\n        \"value which we need to replace the null with\",\r\n        typeConverter=TypeConverters.toString\r\n    )\r\n\r\n    Na_thresh = Param(\r\n        Params._dummy(),\r\n        \"Na_thresh\",\r\n        \"used to filter the rows\",\r\n        typeConverter=TypeConverters.toFloat,\r\n    )\r\n\r\n    # custom_params =  Param(\r\n    #     Params._dummy(),\r\n    #     \"Na_thresh\",\r\n    #     \"used to filter the rows\",\r\n    #     typeConverter=TypeConverters.toDict\r\n    # )\r\n\r\n    @keyword_only\r\n    def __init__(self, colName=None, Na_thresh=0.0):\r\n        super(SetValueTransformer, self).__init__()\r\n        self._setDefault(colName=None)\r\n        self._setDefault(Na_thresh=0.0)\r\n        kwargs = self._input_kwargs\r\n        self._set(**kwargs)\r\n        # custom_stage0 = Custom_transformation(inputCols = ['Na_to_k'], outputCols = '', custom_params = {'na_thresh': 9})\r\n\r\n\r\n    @keyword_only\r\n    def setParams(self, colName=None, Na_thresh=0.0):\r\n        \"\"\"\r\n        setParams(self, outputCols=None, value=0.0)\r\n        Sets params for this SetValueTransformer.\r\n        \"\"\"\r\n        kwargs = self._input_kwargs\r\n        return self._set(**kwargs)\r\n\r\n    def setThresh(self, thresh):\r\n        \"\"\"\r\n        Sets the value of :py:attr:`value`.\r\n        \"\"\"\r\n        return self._set(Na_thresh=thresh)\r\n\r\n    def getThresh(self):\r\n        \"\"\"\r\n        Gets the value of :py:attr:`value` or its default value.\r\n        \"\"\"\r\n        return self.getOrDefault(self.Na_thresh)\r\n\r\n    def setColName(self, col):\r\n        return self._set(colName=col)\r\n    \r\n    \r\n    def getColName(self):\r\n        return self.getOrDefault(self.colName)\r\n\r\n    def _transform(self, df):\r\n\r\n        df = df.na.drop(subset=[\"Sex\",\"BP\", \"Cholesterol\", 'Drug'])\r\n        df = df.filter(df[self.getColName()] > self.getThresh())\r\n        # df = df.drop(*[x for x in df.columns if any(y in x for y in self.banned_list)])\r\n        return df\r\n\r\n\r\n\r\n## Initiating the custom preprocess \r\n# stage0 = CustomDataPreprocess(\"Na_to_K\", 9)\r\n\r\n\r\nstage1 = SetValueTransformer(colName = \"Na_to_k\", Na_thresh = 8)\r\nstage2 = Imputer(inputCols= ['Na_to_K'], outputCols = [ \"impute_{}\".format(c) for c in ['Na_to_K']]).setStrategy('mean')\r\nstage3 = StringIndexer(inputCol = \"Sex\", outputCol = 'sexIndex') \r\nstage4 = StringIndexer(inputCol=\"Cholesterol\", outputCol='cholesterolIndex') \r\nstage5 = StringIndexer(inputCol=\"Drug\", outputCol='drugIndex') \r\nstage6 = StringIndexer(inputCol=\"BP\", outputCol='bpIndex') \r\nstage7 = OneHotEncoder(inputCol= 'bpIndex', outputCol='bpOHE')\r\nstage8 = VectorAssembler(inputCols=[\"Age\", \"sexIndex\",\"bpOHE\",\"cholesterolIndex\",\"impute_Na_to_K\"], outputCol= 'features')\r\nstage9  = RandomForestClassifier(featuresCol='features', labelCol='drugIndex',  numTrees= 100)\r\n\r\n# custom_stage0 = Custom_transformation(inputCols = ['Na_to_k'], outputCols = '', custom_params = {'na_thresh': 9})\r\n\r\n\r\n\r\n## Adding the custom preprocess with other steps\r\npipeline = Pipeline(stages=[stage1, stage2, stage3, stage4, stage5, stage6, stage7, stage8, stage9])\r\n\r\n## Split the data \r\ntrain_df, test_df = df.randomSplit([0.75,0.25])\r\n\r\n\r\n## Fitting Training data\r\nmodel = pipeline.fit(train_df)\r\n\r\n# ## Getting prediction from hold out test data\r\n# predictions = model.transform(test_df)\r\n# predictResult = predictions.select(\"prediction\")\r\n# predictResult.show()\r\n\r\n\r\nmlflow.spark.save_model(model, 'model_with_custom_preprocess')\r\n```\r\n\r\n\n\n### Other info / logs\n\n```/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py:2322: FutureWarning: `--no-conda` is deprecated and will be removed in a future MLflow release. Use `--env-manager=local` instead.\r\n  value = self.callback(ctx, self, value)\r\n2022/05/31 17:08:45 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\r\n2022/05/31 17:08:45 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b 0.0.0.0:15 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\r\n[2022-05-31 17:08:45 -0700] [30992] [INFO] Starting gunicorn 20.1.0\r\n[2022-05-31 17:08:45 -0700] [30992] [INFO] Listening at: http://0.0.0.0:15 (30992)\r\n[2022-05-31 17:08:45 -0700] [30992] [INFO] Using worker: sync\r\n[2022-05-31 17:08:45 -0700] [30994] [INFO] Booting worker with pid: 30994\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n22/05/31 17:08:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2022/05/31 17:08:49 INFO mlflow.spark: File '/Users/i29140/Documents/MLE/spark/TESTING_spark_mlflow_preprocess_/sparkml' is already on DFS, copy is not necessary.\r\n[2022-05-31 17:08:51 -0700] [30994] [ERROR] Exception in worker process         \r\nTraceback (most recent call last):\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/arbiter.py\", line 589, in spawn_worker\r\n    worker.init_process()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 134, in init_process\r\n    self.load_wsgi()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 146, in load_wsgi\r\n    self.wsgi = self.app.wsgi()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/base.py\", line 67, in wsgi\r\n    self.callable = self.load()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 58, in load\r\n    return self.load_wsgiapp()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 48, in load_wsgiapp\r\n    return util.import_app(self.app_uri)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/util.py\", line 359, in import_app\r\n    mod = importlib.import_module(module)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/wsgi.py\", line 6, in <module>\r\n    app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 733, in load_model\r\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/spark.py\", line 737, in _load_pyfunc\r\n    return _PyFuncModelWrapper(spark, _load_model(model_uri=path))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/spark.py\", line 656, in _load_model\r\n    return PipelineModel.load(model_uri)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/util.py\", line 332, in load\r\n    return cls.read().load(path)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/pipeline.py\", line 260, in load\r\n    uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/pipeline.py\", line 396, in load\r\n    stage = DefaultParamsReader.loadParamsInstance(stagePath, sc)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/util.py\", line 590, in loadParamsInstance\r\n    py_type = DefaultParamsReader.__get_class(pythonClassName)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/ml/util.py\", line 501, in __get_class\r\n    m = getattr(m, comp)\r\n**AttributeError: module '__main__' has no attribute 'SetValueTransformer'**\r\n[2022-05-31 17:08:51 -0700] [30994] [INFO] Worker exiting (pid: 30994)\r\n[2022-05-31 17:08:51 -0700] [30992] [INFO] Shutting down: Master\r\n[2022-05-31 17:08:51 -0700] [30992] [INFO] Reason: Worker failed to boot.\r\nTraceback (most recent call last):\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/bin/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/models/cli.py\", line 65, in serve\r\n    return _get_flavor_backend(\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/backend.py\", line 197, in serve\r\n    raise Exception(\r\nException: Command '['bash', '-c', 'exec gunicorn --timeout=60 -b 0.0.0.0:15 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app']' returned non zero return code. Return code = 3\r\n```\n\n### What component(s) does this bug affect?\n\n- [X] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [X] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6008/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/6008/timeline","performed_via_github_app":null,"state_reason":null}