{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3499","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/3499/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/3499/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/3499/events","html_url":"https://github.com/mlflow/mlflow/issues/3499","id":716142477,"node_id":"MDU6SXNzdWU3MTYxNDI0Nzc=","number":3499,"title":"How to serve model manually without using mlfow model serve command","user":{"login":"sivakumarl","id":24239452,"node_id":"MDQ6VXNlcjI0MjM5NDUy","avatar_url":"https://avatars.githubusercontent.com/u/24239452?v=4","gravatar_id":"","url":"https://api.github.com/users/sivakumarl","html_url":"https://github.com/sivakumarl","followers_url":"https://api.github.com/users/sivakumarl/followers","following_url":"https://api.github.com/users/sivakumarl/following{/other_user}","gists_url":"https://api.github.com/users/sivakumarl/gists{/gist_id}","starred_url":"https://api.github.com/users/sivakumarl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sivakumarl/subscriptions","organizations_url":"https://api.github.com/users/sivakumarl/orgs","repos_url":"https://api.github.com/users/sivakumarl/repos","events_url":"https://api.github.com/users/sivakumarl/events{/privacy}","received_events_url":"https://api.github.com/users/sivakumarl/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-10-07T02:18:39Z","updated_at":"2020-10-07T02:18:39Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi Team,\r\n\r\n         I have created the conda environment with help of **_mlflow prepare-env_** command , after that i am trying to launch/serve the model manually with the command that being used by **_mlflow models serve_**   i.e., ` gunicorn --timeout=60 -b 0.0.0.0:8080 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app` \r\n\r\n      As per my analysis i understand that for serving the models the artifacts should be available some where in temporary location, so mlflow model serve command is helping to download the artifacts and saving into temp location.\r\n    \r\n     If we run manually the gunicorn command doesn't know about the temporary artifacts location. So please let me know how to server the model manually after preparing the environment. Thanks\r\n\r\nRegards,\r\nSiva ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3499/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/3499/timeline","performed_via_github_app":null,"state_reason":null}