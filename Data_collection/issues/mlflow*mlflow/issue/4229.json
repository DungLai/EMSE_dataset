{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4229","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4229/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4229/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4229/events","html_url":"https://github.com/mlflow/mlflow/issues/4229","id":851327781,"node_id":"MDU6SXNzdWU4NTEzMjc3ODE=","number":4229,"title":"Spark session creation is flaky in GitHub Actions","user":{"login":"harupy","id":17039389,"node_id":"MDQ6VXNlcjE3MDM5Mzg5","avatar_url":"https://avatars.githubusercontent.com/u/17039389?v=4","gravatar_id":"","url":"https://api.github.com/users/harupy","html_url":"https://github.com/harupy","followers_url":"https://api.github.com/users/harupy/followers","following_url":"https://api.github.com/users/harupy/following{/other_user}","gists_url":"https://api.github.com/users/harupy/gists{/gist_id}","starred_url":"https://api.github.com/users/harupy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harupy/subscriptions","organizations_url":"https://api.github.com/users/harupy/orgs","repos_url":"https://api.github.com/users/harupy/repos","events_url":"https://api.github.com/users/harupy/events{/privacy}","received_events_url":"https://api.github.com/users/harupy/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-04-06T11:25:17Z","updated_at":"2021-04-11T15:15:44Z","closed_at":"2021-04-11T15:15:44Z","author_association":"MEMBER","active_lock_reason":null,"body":"https://github.com/mlflow/mlflow/blob/8e74cdd108ff324175e38d621b12d9b2aced05bd/tests/pyfunc/test_spark.py#L56-L67\r\n\r\n`get_spark_session` sometimes fails in GitHub Actions with the following error:\r\n\r\n```\r\ntests/pyfunc/test_spark.py:66: in get_spark_session\r\n    .master(\"local-cluster[2, 1, 1024]\")\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/pyspark/sql/session.py:173: in getOrCreate\r\n    sc = SparkContext.getOrCreate(sparkConf)\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/pyspark/context.py:349: in getOrCreate\r\n    SparkContext(conf=conf or SparkConf())\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/pyspark/context.py:118: in __init__\r\n    conf, jsc, profiler_cls)\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/pyspark/context.py:180: in _do_init\r\n    self._jsc = jsc or self._initialize_context(self._conf._jconf)\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/pyspark/context.py:288: in _initialize_context\r\n    return self._jvm.JavaSparkContext(jconf)\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/py4j/java_gateway.py:1525: in __call__\r\n    answer, self._gateway_client, None, self._fqn)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nanswer = 'xro65'\r\ngateway_client = <py4j.java_gateway.GatewayClient object at 0x7f5d460c2828>\r\ntarget_id = None, name = 'org.apache.spark.api.java.JavaSparkContext'\r\n\r\n    def get_return_value(answer, gateway_client, target_id=None, name=None):\r\n        \"\"\"Converts an answer received from the Java gateway into a Python object.\r\n    \r\n        For example, string representation of integers are converted to Python\r\n        integer, string representation of objects are converted to JavaObject\r\n        instances, etc.\r\n    \r\n        :param answer: the string returned by the Java gateway\r\n        :param gateway_client: the gateway client used to communicate with the Java\r\n            Gateway. Only necessary if the answer is a reference (e.g., object,\r\n            list, map)\r\n        :param target_id: the name of the object from which the answer comes from\r\n            (e.g., *object1* in `object1.hello()`). Optional.\r\n        :param name: the name of the member from which the answer comes from\r\n            (e.g., *hello* in `object1.hello()`). Optional.\r\n        \"\"\"\r\n        if is_error(answer)[0]:\r\n            if len(answer) > 1:\r\n                type = answer[1]\r\n                value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n                if answer[1] == REFERENCE_TYPE:\r\n                    raise Py4JJavaError(\r\n                        \"An error occurred while calling {0}{1}{2}.\\n\".\r\n>                       format(target_id, \".\", name), value)\r\nE                   py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\r\nE                   : java.net.BindException: Cannot assign requested address: Service 'sparkMaster' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkMaster' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\r\nE                   \tat sun.nio.ch.Net.bind0(Native Method)\r\nE                   \tat sun.nio.ch.Net.bind(Net.java:461)\r\nE                   \tat sun.nio.ch.Net.bind(Net.java:453)\r\nE                   \tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222)\r\nE                   \tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:128)\r\nE                   \tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:558)\r\nE                   \tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1283)\r\nE                   \tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501)\r\nE                   \tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486)\r\nE                   \tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:989)\r\nE                   \tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:254)\r\nE                   \tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:364)\r\nE                   \tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\r\nE                   \tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\r\nE                   \tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\r\nE                   \tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\r\nE                   \tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\r\nE                   \tat java.lang.Thread.run(Thread.java:748)\r\n\r\nanswer     = 'xro65'\r\ngateway_client = <py4j.java_gateway.GatewayClient object at 0x7f5d460c2828>\r\nname       = 'org.apache.spark.api.java.JavaSparkContext'\r\ntarget_id  = None\r\ntype       = 'r'\r\nvalue      = JavaObject id=o65\r\n\r\n/usr/share/miniconda/envs/test-environment/lib/python3.6/site-packages/py4j/protocol.py:328: Py4JJavaError\r\n---------------------------- Captured stdout setup -----------------------------\r\n2021-04-06 05:07:33 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 WARN  Utils:66 - Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n2021-04-06 05:07:34 ERROR SparkContext:91 - Error initializing SparkContext.\r\n```\r\n\r\n\r\n# Logs:\r\n\r\n[logs_22914.zip](https://github.com/mlflow/mlflow/files/6264429/logs_22914.zip)","closed_by":{"login":"harupy","id":17039389,"node_id":"MDQ6VXNlcjE3MDM5Mzg5","avatar_url":"https://avatars.githubusercontent.com/u/17039389?v=4","gravatar_id":"","url":"https://api.github.com/users/harupy","html_url":"https://github.com/harupy","followers_url":"https://api.github.com/users/harupy/followers","following_url":"https://api.github.com/users/harupy/following{/other_user}","gists_url":"https://api.github.com/users/harupy/gists{/gist_id}","starred_url":"https://api.github.com/users/harupy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harupy/subscriptions","organizations_url":"https://api.github.com/users/harupy/orgs","repos_url":"https://api.github.com/users/harupy/repos","events_url":"https://api.github.com/users/harupy/events{/privacy}","received_events_url":"https://api.github.com/users/harupy/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4229/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4229/timeline","performed_via_github_app":null,"state_reason":"completed"}