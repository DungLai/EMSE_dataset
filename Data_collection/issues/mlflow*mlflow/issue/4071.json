{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4071","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4071/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4071/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4071/events","html_url":"https://github.com/mlflow/mlflow/issues/4071","id":802572068,"node_id":"MDU6SXNzdWU4MDI1NzIwNjg=","number":4071,"title":"[FR] Use model-specific Shap explainers in mlflow.shap.log_explanations","user":{"login":"coinflip112","id":18250940,"node_id":"MDQ6VXNlcjE4MjUwOTQw","avatar_url":"https://avatars.githubusercontent.com/u/18250940?v=4","gravatar_id":"","url":"https://api.github.com/users/coinflip112","html_url":"https://github.com/coinflip112","followers_url":"https://api.github.com/users/coinflip112/followers","following_url":"https://api.github.com/users/coinflip112/following{/other_user}","gists_url":"https://api.github.com/users/coinflip112/gists{/gist_id}","starred_url":"https://api.github.com/users/coinflip112/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coinflip112/subscriptions","organizations_url":"https://api.github.com/users/coinflip112/orgs","repos_url":"https://api.github.com/users/coinflip112/repos","events_url":"https://api.github.com/users/coinflip112/events{/privacy}","received_events_url":"https://api.github.com/users/coinflip112/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022845866,"node_id":"MDU6TGFiZWwyMDIyODQ1ODY2","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/artifacts","name":"area/artifacts","color":"48eabc","default":false,"description":"Artifact stores and artifact logging"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-06T01:14:46Z","updated_at":"2021-02-11T19:12:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [x] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\nCurrently, `mlflow.shap.log_explanation` utilizes `shap.KernelExplainer` to generate the relevant artifacts. This, while being model agnostic comes at a significant computational cost. However, [Shap](https://github.com/slundberg/shap) offers model-specific explainers, which offer non-trivial benefits. I propose designing a mechanism to leverage the specific explainers and using shap.KernelExplainer as a fallback.\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nArtifacts produced by `mlflow.shap.log_explanation` would be generated significantly faster for specific model types. Additionally, often, no sampling would be required and the resulting shap values or feature importances would be exact instead of an approximation (under some assumptions).   \r\n- Why is this use case valuable to support for MLflow users in general?\r\nCurrently `mlflow.shap.log_explanation` is rather computationally demanding. This is caused by the fact that `shap.kmeans` is used under the hood to summarize the background data. Apart from being expensive, it brings a host of other issues as described in #3985. In order for `mlflow.shap.log_explanation` to be useful in practice, it needs to be able to scale to large problems, without having to rely on approximation and sampling, unless necessary. \r\n- Why is this use case valuable to support for your project(s) or organization?\r\nEase of logging explanations is invaluable during early modeling stages. This is precisely the time, when quick iteration is required. However, the computational demand of the current implementation makes that difficult or even impossible for mid to large size datasets.  \r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nScaling to large problems is impossible due to the computational demands of the current implementation.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [x] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\nThe change I'm proposing would require (among other things) a change in the `shap.log_explanation` signature. More specifically, passing the predict_function of the _to-be-explained model_ would be replaced by passing the model itself. Internally a mapping between model types and explainer types would be used to yield a suitable explainer. This explainer would then produce all the artifacts, which the current implementation provides. \r\n \r\nThe mapping itself would be rather simple (there are not that many explainers available). Essentially:\r\n- Tree based models (RF, GBDT, DT) in the frameworks (XGBoost/LightGBM/CatBoost/scikit-learn/Pyspark) -> `TreeExplainer`\r\n- Tensorflow/Keras/Pytorch  -> `DeepExplainer`\r\n- Linear models of supported frameworks (really anything, which can provide coefficients) -> `LinearExplainer`\r\n\r\nIf a different model type is provided, `shap.log_explanation` would fall back to using the `Kernel Explainer`.  \r\n\r\nA simple benchmarks example: It takes ~1h in order for `shap.log_explanation` to complete on [Collab](https://colab.research.google.com/drive/18OGcOnFoTj01dLRhsGK0eORlBHfwcMYp?usp=sharing) (2 Intel(R) Xeon(R) CPU @ 2.20GHz) on a relatively small dataset (california_housing with dimensions (20640, 8)). This renders the functionality unusable in  any, even slightly more complex, real world application. This is especially true if the user would like to log explanations, let's say during hyperparameter optimization. ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4071/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4071/timeline","performed_via_github_app":null,"state_reason":null}