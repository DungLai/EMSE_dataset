{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5658","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5658/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5658/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5658/events","html_url":"https://github.com/mlflow/mlflow/issues/5658","id":1199912335,"node_id":"I_kwDOCB5Jx85HhTWP","number":5658,"title":"MLFLOW Server or MLFLOW BUILD DOCKER","user":{"login":"eriktavares","id":6141088,"node_id":"MDQ6VXNlcjYxNDEwODg=","avatar_url":"https://avatars.githubusercontent.com/u/6141088?v=4","gravatar_id":"","url":"https://api.github.com/users/eriktavares","html_url":"https://github.com/eriktavares","followers_url":"https://api.github.com/users/eriktavares/followers","following_url":"https://api.github.com/users/eriktavares/following{/other_user}","gists_url":"https://api.github.com/users/eriktavares/gists{/gist_id}","starred_url":"https://api.github.com/users/eriktavares/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eriktavares/subscriptions","organizations_url":"https://api.github.com/users/eriktavares/orgs","repos_url":"https://api.github.com/users/eriktavares/repos","events_url":"https://api.github.com/users/eriktavares/events{/privacy}","received_events_url":"https://api.github.com/users/eriktavares/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-04-11T12:38:30Z","updated_at":"2022-08-12T20:26:35Z","closed_at":"2022-08-12T20:26:35Z","author_association":"NONE","active_lock_reason":null,"body":"\r\nos.environ['MLFLOW_TRACKING_URI'] = 'http://127.0.0.1:5000'\r\n\r\n\r\n!mlflow models serve -m \"models:/modelo_cancer/Production\"  -p 5001\r\n========================================================\r\n\r\n\r\n**mlflow, version 1.24.0**\r\n\r\n2022/04/11 08:28:36 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\r\n2022/04/11 08:28:38 INFO mlflow.pyfunc.backend: === Running command 'conda activate mlflow-c7641d6d575a322abd5f83e8ebbe52cfa6344f6d & waitress-serve --host=127.0.0.1 --port=5001 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\r\nC:\\Users\\Erik\\.conda\\envs\\mlflow-c7641d6d575a322abd5f83e8ebbe52cfa6344f6d\\lib\\site-packages\\waitress\\runner.py:218: FutureWarning: MLflow support for Python 3.6 is deprecated and will be dropped in an upcoming release. At that point, existing Python 3.6 workflows that use MLflow will continue to work without modification, but Python 3.6 users will no longer get access to the latest MLflow features and bugfixes. We recommend that you upgrade to Python 3.7 or newer.\r\n  obj = __import__(module_name, fromlist=segments[:1])\r\nError: Bad module 'mlflow.pyfunc.scoring_server.wsgi'\r\n\r\nUsage:\r\n\r\n    waitress-serve [OPTS] MODULE:OBJECT\r\n\r\nStandard options:\r\n\r\n    --help\r\n        Show this information.\r\n\r\n    --call\r\n        Call the given object to get the WSGI application.\r\n\r\n    --host=ADDR\r\n        Hostname or IP address on which to listen, default is '0.0.0.0',\r\n        which means \"all IP addresses on this host\".\r\n\r\n        Note: May not be used together with --listen\r\n\r\n    --port=PORT\r\n        TCP port on which to listen, default is '8080'\r\n\r\n        Note: May not be used together with --listen\r\n\r\n    --listen=ip:port\r\n        Tell waitress to listen on an ip port combination.\r\n\r\n        Example:\r\n\r\n            --listen=127.0.0.1:8080\r\n            --listen=[::1]:8080\r\n            --listen=*:8080\r\n\r\n        This option may be used multiple times to listen on multiple sockets.\r\n        A wildcard for the hostname is also supported and will bind to both\r\n        IPv4/IPv6 depending on whether they are enabled or disabled.\r\n\r\n    --[no-]ipv4\r\n        Toggle on/off IPv4 support.\r\n\r\n        Example:\r\n\r\n            --no-ipv4\r\n\r\n        This will disable IPv4 socket support. This affects wildcard matching\r\n        when generating the list of sockets.\r\n\r\n    --[no-]ipv6\r\n        Toggle on/off IPv6 support.\r\n\r\n        Example:\r\n\r\n            --no-ipv6\r\n\r\n        This will turn on IPv6 socket support. This affects wildcard matching\r\n        when generating a list of sockets.\r\n\r\n    --unix-socket=PATH\r\n        Path of Unix socket. If a socket path is specified, a Unix domain\r\n        socket is made instead of the usual inet domain socket.\r\n\r\n        Not available on Windows.\r\n\r\n    --unix-socket-perms=PERMS\r\n        Octal permissions to use for the Unix domain socket, default is\r\n        '600'.\r\n\r\n    --url-scheme=STR\r\n        Default wsgi.url_scheme value, default is 'http'.\r\n\r\n   --url-prefix=STR\r\n        The ``SCRIPT_NAME`` WSGI environment value.  Setting this to anything\r\n        except the empty string will cause the WSGI ``SCRIPT_NAME`` value to be\r\n        the value passed minus any trailing slashes you add, and it will cause\r\n        the ``PATH_INFO`` of any request which is prefixed with this value to\r\n        be stripped of the prefix.  Default is the empty string.\r\n\r\n    --ident=STR\r\n        Server identity used in the 'Server' header in responses. Default\r\n        is 'waitress'.\r\n\r\nTuning options:\r\n\r\n    --threads=INT\r\n        Number of threads used to process application logic, default is 4.\r\n\r\n    --backlog=INT\r\n        Connection backlog for the server. Default is 1024.\r\n\r\n    --recv-bytes=INT\r\n        Number of bytes to request when calling socket.recv(). Default is\r\n        8192.\r\n\r\n    --send-bytes=INT\r\n        Number of bytes to send to socket.send(). Default is 18000.\r\n        Multiples of 9000 should avoid partly-filled TCP packets.\r\n\r\n    --outbuf-overflow=INT\r\n        A temporary file should be created if the pending output is larger\r\n        than this. Default is 1048576 (1MB).\r\n\r\n    --outbuf-high-watermark=INT\r\n        The app_iter will pause when pending output is larger than this value\r\n        and will resume once enough data is written to the socket to fall below\r\n        this threshold. Default is 16777216 (16MB).\r\n\r\n    --inbuf-overflow=INT\r\n        A temporary file should be created if the pending input is larger\r\n        than this. Default is 524288 (512KB).\r\n\r\n    --connection-limit=INT\r\n        Stop creating new channels if too many are already active.\r\n        Default is 100.\r\n\r\n    --cleanup-interval=INT\r\n        Minimum seconds between cleaning up inactive channels. Default\r\n        is 30. See '--channel-timeout'.\r\n\r\n    --channel-timeout=INT\r\n        Maximum number of seconds to leave inactive connections open.\r\n        Default is 120. 'Inactive' is defined as 'has received no data\r\n        from the client and has sent no data to the client'.\r\n\r\n    --[no-]log-socket-errors\r\n        Toggle whether premature client disconnect tracebacks ought to be\r\n        logged. On by default.\r\n\r\n    --max-request-header-size=INT\r\n        Maximum size of all request headers combined. Default is 262144\r\n        (256KB).\r\n\r\n    --max-request-body-size=INT\r\n        Maximum size of request body. Default is 1073741824 (1GB).\r\n\r\n    --[no-]expose-tracebacks\r\n        Toggle whether to expose tracebacks of unhandled exceptions to the\r\n        client. Off by default.\r\n\r\n    --asyncore-loop-timeout=INT\r\n        The timeout value in seconds passed to asyncore.loop(). Default is 1.\r\n\r\n    --asyncore-use-poll\r\n        The use_poll argument passed to ``asyncore.loop()``. Helps overcome\r\n        open file descriptors limit. Default is False.\r\n\r\n    --channel-request-lookahead=INT\r\n        Allows channels to stay readable and buffer more requests up to the\r\n        given maximum even if a request is already being processed. This allows\r\n        detecting if a client closed the connection while its request is being\r\n        processed. Default is 0.\r\n\r\n\r\nThere was an exception (ModuleNotFoundError) importing your module.\r\n\r\nIt had these arguments: \r\n1. No module named 'pycaret'\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\mlflow-script.py\", line 10, in <module>\r\n    sys.exit(cli())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\models\\cli.py\", line 57, in serve\r\n    return _get_flavor_backend(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\pyfunc\\backend.py\", line 78, in serve\r\n    return _execute_in_conda_env(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\pyfunc\\backend.py\", line 167, in _execute_in_conda_env\r\n    raise Exception(\r\nException: Command 'conda activate mlflow-c7641d6d575a322abd5f83e8ebbe52cfa6344f6d & waitress-serve --host=127.0.0.1 --port=5001 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app' returned non zero return code. Return code = 1\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n!mlflow models serve -m \"models:/modelo_cancer/Production\"  -p 5001 --no-conda\r\n========================================================\r\n\r\n\r\n\r\n\r\n2022/04/11 08:35:27 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\r\n2022/04/11 08:35:28 INFO mlflow.pyfunc.backend: === Running command 'waitress-serve --host=127.0.0.1 --port=5001 --ident=mlflow mlflow.pyfunc.scoring_server.wsgi:app'\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator SimpleImputer from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\r\n  warnings.warn(\r\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator OneHotEncoder from version 0.23.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\r\n  warnings.warn(\r\nError: Bad object name 'app'\r\n\r\nUsage:\r\n\r\n    waitress-serve [OPTS] MODULE:OBJECT\r\n\r\nStandard options:\r\n\r\n    --help\r\n        Show this information.\r\n\r\n    --call\r\n        Call the given object to get the WSGI application.\r\n\r\n    --host=ADDR\r\n        Hostname or IP address on which to listen, default is '0.0.0.0',\r\n        which means \"all IP addresses on this host\".\r\n\r\n        Note: May not be used together with --listen\r\n\r\n    --port=PORT\r\n        TCP port on which to listen, default is '8080'\r\n\r\n        Note: May not be used together with --listen\r\n\r\n    --listen=ip:port\r\n        Tell waitress to listen on an ip port combination.\r\n\r\n        Example:\r\n\r\n            --listen=127.0.0.1:8080\r\n            --listen=[::1]:8080\r\n            --listen=*:8080\r\n\r\n        This option may be used multiple times to listen on multiple sockets.\r\n        A wildcard for the hostname is also supported and will bind to both\r\n        IPv4/IPv6 depending on whether they are enabled or disabled.\r\n\r\n    --[no-]ipv4\r\n        Toggle on/off IPv4 support.\r\n\r\n        Example:\r\n\r\n            --no-ipv4\r\n\r\n        This will disable IPv4 socket support. This affects wildcard matching\r\n        when generating the list of sockets.\r\n\r\n    --[no-]ipv6\r\n        Toggle on/off IPv6 support.\r\n\r\n        Example:\r\n\r\n            --no-ipv6\r\n\r\n        This will turn on IPv6 socket support. This affects wildcard matching\r\n        when generating a list of sockets.\r\n\r\n    --unix-socket=PATH\r\n        Path of Unix socket. If a socket path is specified, a Unix domain\r\n        socket is made instead of the usual inet domain socket.\r\n\r\n        Not available on Windows.\r\n\r\n    --unix-socket-perms=PERMS\r\n        Octal permissions to use for the Unix domain socket, default is\r\n        '600'.\r\n\r\n    --url-scheme=STR\r\n        Default wsgi.url_scheme value, default is 'http'.\r\n\r\n   --url-prefix=STR\r\n        The ``SCRIPT_NAME`` WSGI environment value.  Setting this to anything\r\n        except the empty string will cause the WSGI ``SCRIPT_NAME`` value to be\r\n        the value passed minus any trailing slashes you add, and it will cause\r\n        the ``PATH_INFO`` of any request which is prefixed with this value to\r\n        be stripped of the prefix.  Default is the empty string.\r\n\r\n    --ident=STR\r\n        Server identity used in the 'Server' header in responses. Default\r\n        is 'waitress'.\r\n\r\nTuning options:\r\n\r\n    --threads=INT\r\n        Number of threads used to process application logic, default is 4.\r\n\r\n    --backlog=INT\r\n        Connection backlog for the server. Default is 1024.\r\n\r\n    --recv-bytes=INT\r\n        Number of bytes to request when calling socket.recv(). Default is\r\n        8192.\r\n\r\n    --send-bytes=INT\r\n        Number of bytes to send to socket.send(). Default is 18000.\r\n        Multiples of 9000 should avoid partly-filled TCP packets.\r\n\r\n    --outbuf-overflow=INT\r\n        A temporary file should be created if the pending output is larger\r\n        than this. Default is 1048576 (1MB).\r\n\r\n    --outbuf-high-watermark=INT\r\n        The app_iter will pause when pending output is larger than this value\r\n        and will resume once enough data is written to the socket to fall below\r\n        this threshold. Default is 16777216 (16MB).\r\n\r\n    --inbuf-overflow=INT\r\n        A temporary file should be created if the pending input is larger\r\n        than this. Default is 524288 (512KB).\r\n\r\n    --connection-limit=INT\r\n        Stop creating new channels if too many are already active.\r\n        Default is 100.\r\n\r\n    --cleanup-interval=INT\r\n        Minimum seconds between cleaning up inactive channels. Default\r\n        is 30. See '--channel-timeout'.\r\n\r\n    --channel-timeout=INT\r\n        Maximum number of seconds to leave inactive connections open.\r\n        Default is 120. 'Inactive' is defined as 'has received no data\r\n        from the client and has sent no data to the client'.\r\n\r\n    --[no-]log-socket-errors\r\n        Toggle whether premature client disconnect tracebacks ought to be\r\n        logged. On by default.\r\n\r\n    --max-request-header-size=INT\r\n        Maximum size of all request headers combined. Default is 262144\r\n        (256KB).\r\n\r\n    --max-request-body-size=INT\r\n        Maximum size of request body. Default is 1073741824 (1GB).\r\n\r\n    --[no-]expose-tracebacks\r\n        Toggle whether to expose tracebacks of unhandled exceptions to the\r\n        client. Off by default.\r\n\r\n    --asyncore-loop-timeout=INT\r\n        The timeout value in seconds passed to asyncore.loop(). Default is 1.\r\n\r\n    --asyncore-use-poll\r\n        The use_poll argument passed to ``asyncore.loop()``. Helps overcome\r\n        open file descriptors limit. Default is False.\r\n\r\n    --channel-request-lookahead=INT\r\n        Allows channels to stay readable and buffer more requests up to the\r\n        given maximum even if a request is already being processed. This allows\r\n        detecting if a client closed the connection while its request is being\r\n        processed. Default is 0.\r\n\r\n\r\nThere was an exception (AttributeError) importing your module.\r\n\r\nIt had these arguments: \r\n1. Can't get attribute 'CustomProbabilityThresholdClassifier' on <module 'pycaret.internal.meta_estimators' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\pycaret\\\\internal\\\\meta_estimators.py'>\r\n\r\n\r\n\r\n!mlflow models build-docker -m 'models:/modelo_cancer/Staging' -n \"model_cancer\" \r\n===============================================================\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\ProgramData\\Anaconda3\\Scripts\\mlflow-script.py\", line 10, in <module>\r\n    sys.exit(cli())\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\click\\core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\models\\cli.py\", line 163, in build_docker\r\n    _get_flavor_backend(model_uri, docker_build=True).build_image(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\models\\cli.py\", line 180, in _get_flavor_backend\r\n    local_path = _download_artifact_from_uri(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\tracking\\artifact_utils.py\", line 95, in _download_artifact_from_uri\r\n    return get_artifact_repository(artifact_uri=root_uri).download_artifacts(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\store\\artifact\\local_artifact_repo.py\", line 74, in download_artifacts\r\n    return super().download_artifacts(artifact_path, dst_path)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\mlflow\\store\\artifact\\artifact_repo.py\", line 265, in download_artifacts\r\n    raise MlflowException(\r\nmlflow.exceptions.MlflowException: The following failures occurred while downloading one or more artifacts from 'models:/modelo_cancer/Staging': {'MLmodel': \"FileNotFoundError(2, 'No such file or directory')\"}\r\n​\r\n","closed_by":{"login":"dbczumar","id":39497902,"node_id":"MDQ6VXNlcjM5NDk3OTAy","avatar_url":"https://avatars.githubusercontent.com/u/39497902?v=4","gravatar_id":"","url":"https://api.github.com/users/dbczumar","html_url":"https://github.com/dbczumar","followers_url":"https://api.github.com/users/dbczumar/followers","following_url":"https://api.github.com/users/dbczumar/following{/other_user}","gists_url":"https://api.github.com/users/dbczumar/gists{/gist_id}","starred_url":"https://api.github.com/users/dbczumar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbczumar/subscriptions","organizations_url":"https://api.github.com/users/dbczumar/orgs","repos_url":"https://api.github.com/users/dbczumar/repos","events_url":"https://api.github.com/users/dbczumar/events{/privacy}","received_events_url":"https://api.github.com/users/dbczumar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5658/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5658/timeline","performed_via_github_app":null,"state_reason":"completed"}