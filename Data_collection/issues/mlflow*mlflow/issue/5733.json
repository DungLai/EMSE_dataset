{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5733","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5733/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5733/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5733/events","html_url":"https://github.com/mlflow/mlflow/issues/5733","id":1210308563,"node_id":"I_kwDOCB5Jx85II9fT","number":5733,"title":"[BUG] 'sparkDriver' Error by running MLflow server for spark saved model ","user":{"login":"Hiteshsaai","id":29530488,"node_id":"MDQ6VXNlcjI5NTMwNDg4","avatar_url":"https://avatars.githubusercontent.com/u/29530488?v=4","gravatar_id":"","url":"https://api.github.com/users/Hiteshsaai","html_url":"https://github.com/Hiteshsaai","followers_url":"https://api.github.com/users/Hiteshsaai/followers","following_url":"https://api.github.com/users/Hiteshsaai/following{/other_user}","gists_url":"https://api.github.com/users/Hiteshsaai/gists{/gist_id}","starred_url":"https://api.github.com/users/Hiteshsaai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Hiteshsaai/subscriptions","organizations_url":"https://api.github.com/users/Hiteshsaai/orgs","repos_url":"https://api.github.com/users/Hiteshsaai/repos","events_url":"https://api.github.com/users/Hiteshsaai/events{/privacy}","received_events_url":"https://api.github.com/users/Hiteshsaai/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-04-20T23:04:43Z","updated_at":"2022-04-26T08:14:45Z","closed_at":"2022-04-26T08:14:45Z","author_association":"NONE","active_lock_reason":null,"body":"Thank you for submitting an issue. Please refer to our [issue policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md) for additional information about bug reports. For help with debugging your code, please refer to [Stack Overflow](https://stackoverflow.com/questions/tagged/mlflow).\r\n\r\n**Please fill in this bug report template to ensure a timely and thorough response.**\r\n\r\n### Willingness to contribute\r\nThe MLflow Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the MLflow code base?\r\n\r\n- [ ] Yes. I can contribute a fix for this bug independently.\r\n- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n- [x] No. I cannot contribute a bug fix at this time.\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in MLflow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MAC OS \r\n- **MLflow installed from (source or binary)**: pip install\r\n- **MLflow version (run ``mlflow --version``)**: 1.25.1\r\n- **Python version**: 3.8.12\r\n- **npm version, if running the dev UI**: 8.5.0\r\n- **Exact command to reproduce**: ```mlflow models serve -m spark_model --no-conda --host 0.0.0.0 --port 15```\r\n\r\n### Describe the problem\r\n\r\nwhen i try to run this command in terminal mlflow models serve -m spark_model --no-conda --host 0.0.0.0 --port 15\r\nand getting this as error\r\nCan't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)\r\n\r\nspark_model was saved using mlflow.spark.save_model()\r\n\r\nWould like to know the solution for this, Thanks !\r\n\r\nBelow is the whole error:\r\n\r\n(spark) i29140@LVVCHD7QMD6R spark % mlflow models serve -m spark_model --no-conda --host 0.0.0.0 --port 15\r\n/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py:2322: FutureWarning: `--no-conda` is deprecated and will be removed in a future MLflow release. Use `--env-manager=local` instead.\r\n  value = self.callback(ctx, self, value)\r\n2022/04/20 09:43:11 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\r\n2022/04/20 09:43:11 INFO mlflow.pyfunc.backend: === Running command 'gunicorn --timeout=60 -b 0.0.0.0:15 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\r\n[2022-04-20 09:43:11 -0700] [1359] [INFO] Starting gunicorn 20.1.0\r\n[2022-04-20 09:43:11 -0700] [1359] [INFO] Listening at: http://0.0.0.0:15 (1359)\r\n[2022-04-20 09:43:11 -0700] [1359] [INFO] Using worker: sync\r\n[2022-04-20 09:43:11 -0700] [1361] [INFO] Booting worker with pid: 1361\r\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\nSetting default log level to \"WARN\".\r\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\r\n22/04/20 09:43:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\r\n22/04/20 09:43:14 ERROR SparkContext: Error initializing SparkContext.\r\njava.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\r\n\tat sun.nio.ch.Net.bind0(Native Method)\r\n\tat sun.nio.ch.Net.bind(Net.java:438)\r\n\tat sun.nio.ch.Net.bind(Net.java:430)\r\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n[2022-04-20 09:43:14 -0700] [1361] [ERROR] Exception in worker process\r\nTraceback (most recent call last):\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/arbiter.py\", line 589, in spawn_worker\r\n    worker.init_process()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 134, in init_process\r\n    self.load_wsgi()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/workers/base.py\", line 146, in load_wsgi\r\n    self.wsgi = self.app.wsgi()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/base.py\", line 67, in wsgi\r\n    self.callable = self.load()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 58, in load\r\n    return self.load_wsgiapp()\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/app/wsgiapp.py\", line 48, in load_wsgiapp\r\n    return util.import_app(self.app_uri)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/gunicorn/util.py\", line 359, in import_app\r\n    mod = importlib.import_module(module)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/importlib/__init__.py\", line 127, in import_module\r\n    return _bootstrap._gcd_import(name[level:], package, level)\r\n  File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\r\n  File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\r\n  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\r\n  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/scoring_server/wsgi.py\", line 6, in <module>\r\n    app = scoring_server.init(load_model(os.environ[scoring_server._SERVER_MODEL_PATH]))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 733, in load_model\r\n    model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/spark.py\", line 729, in _load_pyfunc\r\n    pyspark.sql.SparkSession.builder.config(\"spark.python.worker.reuse\", True)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/session.py\", line 228, in getOrCreate\r\n    sc = SparkContext.getOrCreate(sparkConf)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/context.py\", line 392, in getOrCreate\r\n    SparkContext(conf=conf or SparkConf())\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/context.py\", line 146, in __init__\r\n    self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/context.py\", line 209, in _do_init\r\n    self._jsc = jsc or self._initialize_context(self._conf._jconf)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/context.py\", line 329, in _initialize_context\r\n    return self._jvm.JavaSparkContext(jconf)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1585, in __call__\r\n    return_value = get_return_value(\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\r\n    raise Py4JJavaError(\r\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\r\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\r\n\tat sun.nio.ch.Net.bind0(Native Method)\r\n\tat sun.nio.ch.Net.bind(Net.java:438)\r\n\tat sun.nio.ch.Net.bind(Net.java:430)\r\n\tat sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:225)\r\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:134)\r\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:506)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:491)\r\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\r\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\r\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\r\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\r\n[2022-04-20 09:43:14 -0700] [1361] [INFO] Worker exiting (pid: 1361)\r\n[2022-04-20 09:43:14 -0700] [1359] [INFO] Shutting down: Master\r\n[2022-04-20 09:43:14 -0700] [1359] [INFO] Reason: Worker failed to boot.\r\nTraceback (most recent call last):\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/bin/mlflow\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1130, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1055, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1657, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/click/core.py\", line 760, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/models/cli.py\", line 65, in serve\r\n    return _get_flavor_backend(\r\n  File \"/Users/i29140/opt/anaconda3/envs/spark/lib/python3.8/site-packages/mlflow/pyfunc/backend.py\", line 197, in serve\r\n    raise Exception(\r\nException: Command '['bash', '-c', 'exec gunicorn --timeout=60 -b 0.0.0.0:15 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app']' returned non zero return code. Return code = 3\r\n(spark) i29140@LVVCHD7QMD6R spark % mlflow models serve -m spark_model --no-conda --host 0.0.0.0 --port 15\r\n","closed_by":{"login":"WeichenXu123","id":19235986,"node_id":"MDQ6VXNlcjE5MjM1OTg2","avatar_url":"https://avatars.githubusercontent.com/u/19235986?v=4","gravatar_id":"","url":"https://api.github.com/users/WeichenXu123","html_url":"https://github.com/WeichenXu123","followers_url":"https://api.github.com/users/WeichenXu123/followers","following_url":"https://api.github.com/users/WeichenXu123/following{/other_user}","gists_url":"https://api.github.com/users/WeichenXu123/gists{/gist_id}","starred_url":"https://api.github.com/users/WeichenXu123/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/WeichenXu123/subscriptions","organizations_url":"https://api.github.com/users/WeichenXu123/orgs","repos_url":"https://api.github.com/users/WeichenXu123/repos","events_url":"https://api.github.com/users/WeichenXu123/events{/privacy}","received_events_url":"https://api.github.com/users/WeichenXu123/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5733/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5733/timeline","performed_via_github_app":null,"state_reason":"completed"}