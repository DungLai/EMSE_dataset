{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2791","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/2791/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/2791/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/2791/events","html_url":"https://github.com/mlflow/mlflow/issues/2791","id":613404280,"node_id":"MDU6SXNzdWU2MTM0MDQyODA=","number":2791,"title":"Using HDFS as artifact store","user":{"login":"mustafa-pit","id":58014691,"node_id":"MDQ6VXNlcjU4MDE0Njkx","avatar_url":"https://avatars.githubusercontent.com/u/58014691?v=4","gravatar_id":"","url":"https://api.github.com/users/mustafa-pit","html_url":"https://github.com/mustafa-pit","followers_url":"https://api.github.com/users/mustafa-pit/followers","following_url":"https://api.github.com/users/mustafa-pit/following{/other_user}","gists_url":"https://api.github.com/users/mustafa-pit/gists{/gist_id}","starred_url":"https://api.github.com/users/mustafa-pit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mustafa-pit/subscriptions","organizations_url":"https://api.github.com/users/mustafa-pit/orgs","repos_url":"https://api.github.com/users/mustafa-pit/repos","events_url":"https://api.github.com/users/mustafa-pit/events{/privacy}","received_events_url":"https://api.github.com/users/mustafa-pit/received_events","type":"User","site_admin":false},"labels":[{"id":2022845866,"node_id":"MDU6TGFiZWwyMDIyODQ1ODY2","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/artifacts","name":"area/artifacts","color":"48eabc","default":false,"description":"Artifact stores and artifact logging"},{"id":2022863969,"node_id":"MDU6TGFiZWwyMDIyODYzOTY5","url":"https://api.github.com/repos/mlflow/mlflow/labels/priority/important-soon","name":"priority/important-soon","color":"534cb5","default":false,"description":"The issue is worked on by the community currently or will be very soon, ideally in time for the"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-05-06T15:20:37Z","updated_at":"2022-06-03T00:22:19Z","closed_at":"2022-06-03T00:21:47Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, \r\nI need some help to configure setting up hdfs as the artifact store for mlflow. I have mlflow and hdfs all running in separate containers across a docket network. When I try to log the model I get the following error:\r\n\r\n```---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-35-e54b25688d8e> in <module>\r\n      1 # log model artifacts\r\n----> 2 pyfunc.log_model('hdfs://hdfs:8020/', python_model=LGBWrapper(), artifacts=artifacts, conda_env=conda_env)\r\n      3 # pyfunc.save_model('prediction_model8', python_model=LGBWrapper(), artifacts=artifacts, conda_env=conda_env)\r\n      4 \r\n      5 # set tag for selecting model\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/pyfunc/__init__.py in log_model(artifact_path, loader_module, data_path, code_path, conda_env, python_model, artifacts, registered_model_name)\r\n    697                      artifacts=artifacts,\r\n    698                      conda_env=conda_env,\r\n--> 699                      registered_model_name=registered_model_name)\r\n    700 \r\n    701 \r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/models/__init__.py in log(cls, artifact_path, flavor, registered_model_name, **kwargs)\r\n    100             mlflow_model = cls(artifact_path=artifact_path, run_id=run_id)\r\n    101             flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\r\n--> 102             mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\r\n    103             try:\r\n    104                 mlflow.tracking.fluent._record_logged_model(mlflow_model)\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/tracking/fluent.py in log_artifacts(local_dir, artifact_path)\r\n    321     \"\"\"\r\n    322     run_id = _get_or_start_run().info.run_id\r\n--> 323     MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\r\n    324 \r\n    325 \r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/tracking/client.py in log_artifacts(self, run_id, local_dir, artifact_path)\r\n    265         :param artifact_path: If provided, the directory in ``artifact_uri`` to write to.\r\n    266         \"\"\"\r\n--> 267         self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\r\n    268 \r\n    269     def _record_logged_model(self, run_id, mlflow_model):\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/tracking/_tracking_service/client.py in log_artifacts(self, run_id, local_dir, artifact_path)\r\n    266         run = self.get_run(run_id)\r\n    267         artifact_repo = get_artifact_repository(run.info.artifact_uri)\r\n--> 268         artifact_repo.log_artifacts(local_dir, artifact_path)\r\n    269 \r\n    270     def list_artifacts(self, run_id, path=None):\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/store/artifact/hdfs_artifact_repo.py in log_artifacts(self, local_dir, artifact_path)\r\n     47         hdfs_base_path = _resolve_base_path(self.path, artifact_path)\r\n     48 \r\n---> 49         with hdfs_system(host=self.host, port=self.port) as hdfs:\r\n     50 \r\n     51             if not hdfs.exists(hdfs_base_path):\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/contextlib.py in __enter__(self)\r\n     79     def __enter__(self):\r\n     80         try:\r\n---> 81             return next(self.gen)\r\n     82         except StopIteration:\r\n     83             raise RuntimeError(\"generator didn't yield\") from None\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/mlflow/store/artifact/hdfs_artifact_repo.py in hdfs_system(host, port)\r\n    175                                 driver=driver,\r\n    176                                 kerb_ticket=kerb_ticket,\r\n--> 177                                 extra_conf=extra_conf)\r\n    178     yield connected\r\n    179     connected.close()\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/pyarrow/hdfs.py in connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n    213     fs = HadoopFileSystem(host=host, port=port, user=user,\r\n    214                           kerb_ticket=kerb_ticket, driver=driver,\r\n--> 215                           extra_conf=extra_conf)\r\n    216     return fs\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/pyarrow/hdfs.py in __init__(self, host, port, user, kerb_ticket, driver, extra_conf)\r\n     36                  driver='libhdfs', extra_conf=None):\r\n     37         if driver == 'libhdfs':\r\n---> 38             _maybe_set_hadoop_classpath()\r\n     39 \r\n     40         self._connect(host, port, user, kerb_ticket, driver, extra_conf)\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/pyarrow/hdfs.py in _maybe_set_hadoop_classpath()\r\n    138             classpath = _hadoop_classpath_glob(hadoop_bin)\r\n    139     else:\r\n--> 140         classpath = _hadoop_classpath_glob('hadoop')\r\n    141 \r\n    142     os.environ['CLASSPATH'] = classpath.decode('utf-8')\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/site-packages/pyarrow/hdfs.py in _hadoop_classpath_glob(hadoop_bin)\r\n    163 \r\n    164     hadoop_classpath_args = (hadoop_bin, 'classpath', '--glob')\r\n--> 165     return subprocess.check_output(hadoop_classpath_args)\r\n    166 \r\n    167 \r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/subprocess.py in check_output(timeout, *popenargs, **kwargs)\r\n    354 \r\n    355     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n--> 356                **kwargs).stdout\r\n    357 \r\n    358 \r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/subprocess.py in run(input, timeout, check, *popenargs, **kwargs)\r\n    421         kwargs['stdin'] = PIPE\r\n    422 \r\n--> 423     with Popen(*popenargs, **kwargs) as process:\r\n    424         try:\r\n    425             stdout, stderr = process.communicate(input, timeout=timeout)\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\r\n    727                                 c2pread, c2pwrite,\r\n    728                                 errread, errwrite,\r\n--> 729                                 restore_signals, start_new_session)\r\n    730         except:\r\n    731             # Cleanup if the child failed starting.\r\n\r\n~/opt/anaconda3/envs/soptai/lib/python3.6/subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\r\n   1362                         if errno_num == errno.ENOENT:\r\n   1363                             err_msg += ': ' + repr(err_filename)\r\n-> 1364                     raise child_exception_type(errno_num, err_msg, err_filename)\r\n   1365                 raise child_exception_type(err_msg)\r\n   1366 \r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'hadoop': 'hadoop'\r\n```\r\n\r\n\r\nAccess to hdfs is not an issue as they are in the same network and other services running on the same network can access hdfs as well. \r\nMaybe there are some changes to be made to the core-site.xml or hdfs-site.xml as someone who reported a similar issue suggested (https://github.com/mlflow/mlflow/issues/1466). Unfortunately, I have no idea what those changes need to be. Please assist! ","closed_by":{"login":"dbczumar","id":39497902,"node_id":"MDQ6VXNlcjM5NDk3OTAy","avatar_url":"https://avatars.githubusercontent.com/u/39497902?v=4","gravatar_id":"","url":"https://api.github.com/users/dbczumar","html_url":"https://github.com/dbczumar","followers_url":"https://api.github.com/users/dbczumar/followers","following_url":"https://api.github.com/users/dbczumar/following{/other_user}","gists_url":"https://api.github.com/users/dbczumar/gists{/gist_id}","starred_url":"https://api.github.com/users/dbczumar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbczumar/subscriptions","organizations_url":"https://api.github.com/users/dbczumar/orgs","repos_url":"https://api.github.com/users/dbczumar/repos","events_url":"https://api.github.com/users/dbczumar/events{/privacy}","received_events_url":"https://api.github.com/users/dbczumar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2791/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/2791/timeline","performed_via_github_app":null,"state_reason":"completed"}