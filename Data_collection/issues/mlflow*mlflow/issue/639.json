{"url":"https://api.github.com/repos/mlflow/mlflow/issues/639","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/639/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/639/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/639/events","html_url":"https://github.com/mlflow/mlflow/issues/639","id":371093212,"node_id":"MDU6SXNzdWUzNzEwOTMyMTI=","number":639,"title":"Reinforcement Learning","user":{"login":"slavakurilyak","id":6625584,"node_id":"MDQ6VXNlcjY2MjU1ODQ=","avatar_url":"https://avatars.githubusercontent.com/u/6625584?v=4","gravatar_id":"","url":"https://api.github.com/users/slavakurilyak","html_url":"https://github.com/slavakurilyak","followers_url":"https://api.github.com/users/slavakurilyak/followers","following_url":"https://api.github.com/users/slavakurilyak/following{/other_user}","gists_url":"https://api.github.com/users/slavakurilyak/gists{/gist_id}","starred_url":"https://api.github.com/users/slavakurilyak/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/slavakurilyak/subscriptions","organizations_url":"https://api.github.com/users/slavakurilyak/orgs","repos_url":"https://api.github.com/users/slavakurilyak/repos","events_url":"https://api.github.com/users/slavakurilyak/events{/privacy}","received_events_url":"https://api.github.com/users/slavakurilyak/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2018-10-17T14:02:48Z","updated_at":"2019-08-02T18:12:45Z","closed_at":"2019-08-02T18:12:44Z","author_association":"NONE","active_lock_reason":null,"body":"# Goals\r\n\r\nAs an artificial intelligence developer, I want to use MLflow in my reinforcement learning (RL) models, so that I can freely experiment with wild ideas, also known as speculative research.\r\n\r\n# Consider\r\n\r\n- Consider using Google's [Dopamine](https://github.com/google/dopamine), a research framework for fast prototyping of reinforcement learning algorithms.\r\n\r\n# Inspiration\r\n\r\n## Google's Dopamine\r\n\r\nMost existing RL frameworks do not provide the combination of flexibility and stability that enables researchers to iterate on RL methods effectively, and thus explore new research directions that may not have immediately obvious benefits. Further, reproducing the results from existing frameworks is often too time consuming, which can lead to scientific reproducibility issues down the line. ([Google, 2018](https://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html))\r\n\r\nReproducing results is one of the strengths of MLflow.\r\n\r\n## DeepMind's AlphaGo Zero\r\n\r\nIn order to understand the potential of reinforcement learning, it is important to look at the AlphaGo Zero case study.\r\n\r\nAlphaGo Zero is the strongest Go player in the world. It outperformed all previous versions of AlphaGo. It defeated the version of AlphaGo that won against the world champion Lee Sedol by 100 games to 0.\r\n\r\nWhat was the difference between AlphaGo and AlphaGo Zero? AlphaGo was trained by supervised learning from human expert moves, and by reinforcement learning from self-play. AlphaGo Zero was trained solely on reinforcement learning, without human data.\r\n\r\nIf you want to learn more about AlphaGo Zero, [watch this video](https://www.youtube.com/watch?v=tXlM99xPQC8) (2 min).\r\n\r\nFeeling inspired yet? Great! Start coding.","closed_by":{"login":"ankit-db","id":52183359,"node_id":"MDQ6VXNlcjUyMTgzMzU5","avatar_url":"https://avatars.githubusercontent.com/u/52183359?v=4","gravatar_id":"","url":"https://api.github.com/users/ankit-db","html_url":"https://github.com/ankit-db","followers_url":"https://api.github.com/users/ankit-db/followers","following_url":"https://api.github.com/users/ankit-db/following{/other_user}","gists_url":"https://api.github.com/users/ankit-db/gists{/gist_id}","starred_url":"https://api.github.com/users/ankit-db/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ankit-db/subscriptions","organizations_url":"https://api.github.com/users/ankit-db/orgs","repos_url":"https://api.github.com/users/ankit-db/repos","events_url":"https://api.github.com/users/ankit-db/events{/privacy}","received_events_url":"https://api.github.com/users/ankit-db/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/639/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/639/timeline","performed_via_github_app":null,"state_reason":"completed"}