{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4548","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4548/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4548/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4548/events","html_url":"https://github.com/mlflow/mlflow/issues/4548","id":941381156,"node_id":"MDU6SXNzdWU5NDEzODExNTY=","number":4548,"title":"[BUG] Loading a registered model in PySpark and executing it as UDF results in AttributeError exception","user":{"login":"uvnikgupta","id":20485662,"node_id":"MDQ6VXNlcjIwNDg1NjYy","avatar_url":"https://avatars.githubusercontent.com/u/20485662?v=4","gravatar_id":"","url":"https://api.github.com/users/uvnikgupta","html_url":"https://github.com/uvnikgupta","followers_url":"https://api.github.com/users/uvnikgupta/followers","following_url":"https://api.github.com/users/uvnikgupta/following{/other_user}","gists_url":"https://api.github.com/users/uvnikgupta/gists{/gist_id}","starred_url":"https://api.github.com/users/uvnikgupta/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uvnikgupta/subscriptions","organizations_url":"https://api.github.com/users/uvnikgupta/orgs","repos_url":"https://api.github.com/users/uvnikgupta/repos","events_url":"https://api.github.com/users/uvnikgupta/events{/privacy}","received_events_url":"https://api.github.com/users/uvnikgupta/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-07-11T06:04:59Z","updated_at":"2021-07-17T04:03:13Z","closed_at":"2021-07-17T04:02:54Z","author_association":"NONE","active_lock_reason":null,"body":"I executed the following steps:\r\n1. I am using Python v3.9.5, pyspark v3.0.3 and mlflow v1.18.0\r\n2. Created a simple linear regression model using <code>sklearn.linear_model.LineraRegression</code> and housing_median_age as the only feature from the sklearn housing data set. \r\n3. Used <code>mlflow.sklearn.log_model()</code> to log this model\r\n4. From the mlflow UI, registered this model as \"linreg\" and promoted it to Staging\r\n5. In the pyspark code, loaded this model using <code>mlflow.pyfunc.spark_udf()</code>\r\n6. Called this model using <code>df.withColumn('predictions', model(f.col(\"housing_median_age\")))</code>\r\n7. Executed <code>df.show()</code> which resulted in the following error:\r\n\r\n<code>\r\n      21/07/11 05:46:20 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)\r\n      org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\r\n          func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\r\n          udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\r\n          f, return_type = read_command(pickleSer, infile)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\r\n          command = serializer._read_with_length(file)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\r\n          return self.loads(obj)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\r\n          return pickle.loads(obj, encoding=encoding)\r\n      AttributeError: Can't get attribute '_fill_function' on \\<module 'pyspark.cloudpickle' from '/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'\\>\r\n</code>\r\n      \r\n\t      at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\t      at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\t      at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\t      at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\t      at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\t      at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\t      at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\t      at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\t      at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t      at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\t      at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\t      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\t      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\t      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\t      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\t      at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\t      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\t      at org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\t      at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\t      at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\t      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\t      at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t      at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t      at java.base/java.lang.Thread.run(Thread.java:829)\r\n      21/07/11 05:46:20 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (mltoolkit-6b7455f648-6lkxr executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\r\n          func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\r\n          udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\r\n          f, return_type = read_command(pickleSer, infile)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\r\n          command = serializer._read_with_length(file)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\r\n          return self.loads(obj)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\r\n          return pickle.loads(obj, encoding=encoding)\r\n      AttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>\r\n      \r\n\t      at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\t      at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:99)\r\n\t      at org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:49)\r\n\t      at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\t      at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\t      at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\t      at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\t      at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\t      at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\t      at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\t      at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\t      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\t      at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\t      at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\t      at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\t      at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\t      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\t      at org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\t      at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\t      at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\t      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\t      at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\t      at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t      at java.base/java.lang.Thread.run(Thread.java:829)\r\n      \r\n      21/07/11 05:46:20 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job\r\n      ---------------------------------------------------------------------------\r\n      PythonException                           Traceback (most recent call last)\r\n      /tmp/ipykernel_145/2168557162.py in <module>\r\n            3 \r\n            4 df = housing.withColumn('predictions', model(f.col(\"housing_median_age\")))\r\n      ----> 5 df.show()\r\n      \r\n      /opt/conda/lib/python3.9/site-packages/pyspark/sql/dataframe.py in show(self, n, truncate, vertical)\r\n          438         \"\"\"\r\n          439         if isinstance(truncate, bool) and truncate:\r\n      --> 440             print(self._jdf.showString(n, 20, vertical))\r\n          441         else:\r\n          442             print(self._jdf.showString(n, int(truncate), vertical))\r\n      \r\n      /opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args)\r\n         1302 \r\n         1303         answer = self.gateway_client.send_command(command)\r\n      -> 1304         return_value = get_return_value(\r\n         1305             answer, self.gateway_client, self.target_id, self.name)\r\n         1306 \r\n      \r\n      /opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py in deco(*a, **kw)\r\n          132                 # Hide where the exception came from that shows a non-Pythonic\r\n          133                 # JVM exception message.\r\n      --> 134                 raise_from(converted)\r\n          135             else:\r\n          136                 raise\r\n      \r\n      /opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py in raise_from(e)\r\n      \r\n      PythonException: \r\n        An exception was thrown from the Python worker. Please see the stack trace below.\r\n      Traceback (most recent call last):\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 588, in main\r\n          func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\r\n          udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\r\n          f, return_type = read_command(pickleSer, infile)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 69, in read_command\r\n          command = serializer._read_with_length(file)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\r\n          return self.loads(obj)\r\n        File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\r\n          return pickle.loads(obj, encoding=encoding)\r\n      AttributeError: Can't get attribute '_fill_function' on <module 'pyspark.cloudpickle' from '/usr/local/spark/python/lib/pyspark.zip/pyspark/cloudpickle/__init__.py'>","closed_by":{"login":"uvnikgupta","id":20485662,"node_id":"MDQ6VXNlcjIwNDg1NjYy","avatar_url":"https://avatars.githubusercontent.com/u/20485662?v=4","gravatar_id":"","url":"https://api.github.com/users/uvnikgupta","html_url":"https://github.com/uvnikgupta","followers_url":"https://api.github.com/users/uvnikgupta/followers","following_url":"https://api.github.com/users/uvnikgupta/following{/other_user}","gists_url":"https://api.github.com/users/uvnikgupta/gists{/gist_id}","starred_url":"https://api.github.com/users/uvnikgupta/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/uvnikgupta/subscriptions","organizations_url":"https://api.github.com/users/uvnikgupta/orgs","repos_url":"https://api.github.com/users/uvnikgupta/repos","events_url":"https://api.github.com/users/uvnikgupta/events{/privacy}","received_events_url":"https://api.github.com/users/uvnikgupta/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4548/timeline","performed_via_github_app":null,"state_reason":"completed"}