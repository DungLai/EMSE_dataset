{"url":"https://api.github.com/repos/mlflow/mlflow/issues/7316","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/7316/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/7316/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/7316/events","html_url":"https://github.com/mlflow/mlflow/issues/7316","id":1445719052,"node_id":"I_kwDOCB5Jx85WK-wM","number":7316,"title":"[FR] Enable support for mlflow evaluate() on Keras multiclass models","user":{"login":"paprocki-r","id":3410289,"node_id":"MDQ6VXNlcjM0MTAyODk=","avatar_url":"https://avatars.githubusercontent.com/u/3410289?v=4","gravatar_id":"","url":"https://api.github.com/users/paprocki-r","html_url":"https://github.com/paprocki-r","followers_url":"https://api.github.com/users/paprocki-r/followers","following_url":"https://api.github.com/users/paprocki-r/following{/other_user}","gists_url":"https://api.github.com/users/paprocki-r/gists{/gist_id}","starred_url":"https://api.github.com/users/paprocki-r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/paprocki-r/subscriptions","organizations_url":"https://api.github.com/users/paprocki-r/orgs","repos_url":"https://api.github.com/users/paprocki-r/repos","events_url":"https://api.github.com/users/paprocki-r/events{/privacy}","received_events_url":"https://api.github.com/users/paprocki-r/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2022-11-11T16:28:25Z","updated_at":"2022-11-21T08:59:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Issues Policy acknowledgement\r\n\r\n- [X] I have read and agree to submit bug reports in accordance with the [issues policy](https://www.github.com/mlflow/mlflow/blob/master/ISSUE_POLICY.md)\r\n\r\n### Willingness to contribute\r\n\r\nNo. I cannot contribute a bug fix at this time.\r\n\r\n### MLflow version\r\n\r\n- Client: 1.30.0\r\n\r\n\r\n\r\n### System information\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04.5 LTS\r\n- **Python version**: 3.8.10\r\n\r\n\r\n\r\n### Describe the problem\r\n\r\nHardcoded metrics in default evaluator forbid from using mlflow.evaluate() on tf.keras multiclass model. \r\nIt works well for sklearn multiclass models or xgboost and so on.\r\n\r\nit has something to do with the fact that this doesn't work:\r\n\r\n```python\r\nfrom sklearn.metrics import accuracy_score\r\n\r\ny_pred = [[0.5, 1], [-1, 1], [7, -6]]\r\ny_true = [[0, 2], [-1, 2], [8, -5]]\r\naccuracy_score(y_true, y_pred)\r\n```\r\n\r\nand how it should be used in that case:\r\n\r\n```python\r\naccuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\r\n```\r\n\r\nI suppose the way to work it around would be to write my own evaluator. However, it feels that it could be solved if our own metrics could be provided, e.g. through evaluator_config:Dict \r\n\r\n### Tracking information\r\n\r\n_No response_\r\n\r\n### Code to reproduce issue\r\n\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom tensorflow.keras.utils import to_categorical\r\nfrom keras.datasets import fashion_mnist\r\nfrom keras.models import Sequential, Model\r\nfrom keras.layers import Dense, Input\r\n\r\n# Import fashion MNIST dataset\r\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\r\n\r\n# Display the first 7 images\r\nfig, axes = plt.subplots(ncols=7, sharex=False,\r\n\t\t\t\t\t\t sharey=True, figsize=(16, 4))\r\nfor i in range(7):\r\n\taxes[i].set_title(y_train[i])\r\n\taxes[i].imshow(X_train[i], cmap='gray')\r\n\taxes[i].get_xaxis().set_visible(False)\r\n\taxes[i].get_yaxis().set_visible(False)\r\nplt.show()\r\n\r\nprint(\"Original shape of X_train =\", X_train.shape)\r\nprint(\"Original shape of X_test =\", X_test.shape, end='\\n')\r\n\r\n# Reshape X_train to (60000, 784) and X_test to (10000, 784)\r\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2])\r\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2])\r\n\r\nprint(\"New X_train shape\", X_train.shape)\r\nprint(\"New X_test shape\", X_test.shape, end='\\n')\r\n\r\n# Convert target (y_train and y_test) into one-hot\r\ntemp = []\r\nfor i in range(len(y_train)):\r\n    temp.append(to_categorical(y_train[i], num_classes=10))\r\n    \r\ny_train = np.array(temp)\r\n\r\ntemp = []\r\nfor i in range(len(y_test)):\r\n    temp.append(to_categorical(y_test[i], num_classes=10))\r\n\r\ny_test = np.array(temp)\r\n\r\n# Create and train sequential model\r\nmodel_seq = Sequential()\r\nmodel_seq.add(Dense(5, activation='sigmoid', input_shape=(X_train.shape[1],)))\r\nmodel_seq.add(Dense(4, activation='sigmoid'))\r\nmodel_seq.add(Dense(10, activation='softmax'))\r\n\r\nmodel_seq.summary()\r\n\r\nmodel_seq.compile(loss='categorical_crossentropy', \r\n                  optimizer='adam', \r\n                  metrics=['acc'])\r\n\r\nmodel_seq.fit(X_train, y_train, epochs=3, \r\n              validation_data=(X_test,y_test))\r\n\r\n# Create and train functional model\r\ninput1 = Input(shape=(X_train.shape[1],))\r\nhidden1 = Dense(5, activation='sigmoid')(input1)\r\nhidden2 = Dense(4, activation='sigmoid')(hidden1)\r\noutput = Dense(10, activation='softmax')(hidden2)\r\nmodel_func = Model(inputs=input1, outputs=output)\r\n\r\nmodel_func.summary()\r\n\r\nmodel_func.compile(loss='categorical_crossentropy', \r\n                   optimizer='adam', \r\n                   metrics=['acc'])\r\n\r\nmodel_func.fit(X_train, y_train, epochs=3, \r\n               validation_data=(X_test,y_test))\r\n# create eval pd.df \r\ntargets = np.where(y_test == np.amax(y_test))[1]\r\n\r\ndf = pd.DataFrame(X_test)\r\ndf['target'] = targets\r\n\r\n\r\n#mlflow evaluation\r\nwith mlflow.start_run() as run:\r\n    model_info = mlflow.sklearn.log_model(model_func, \"model\")\r\n    result = mlflow.evaluate(\r\n       model_info.model_uri,\r\n       df,\r\n       targets=\"target\",\r\n       model_type=\"classifier\",\r\n       dataset_name=\"adult\",\r\n       evaluators=[\"default\"],\r\n    )\r\n```\r\n\r\n### Stack trace\r\n\r\n```\r\nValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<command-1847767594277258> in <module>\r\n     83 with mlflow.start_run() as run:\r\n     84     model_info = mlflow.sklearn.log_model(model_func, \"model\")\r\n---> 85     result = mlflow.evaluate(\r\n     86        model_info.model_uri,\r\n     87        df,\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/base.py in evaluate(model, data, targets, model_type, dataset_name, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, validation_thresholds, baseline_model, env_manager)\r\n   1241     with _start_run_or_reuse_active_run() as run_id:\r\n   1242         try:\r\n-> 1243             evaluate_result = _evaluate(\r\n   1244                 model=model,\r\n   1245                 model_type=model_type,\r\n\r\n/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_evaluation.py in patched_evaluate(model, model_type, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, **kwargs)\r\n     38             try:\r\n     39                 original_succeeded = False\r\n---> 40                 original_result = original_evaluate_fn(\r\n     41                     model=model,\r\n     42                     model_type=model_type,\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/base.py in _evaluate(model, model_type, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, custom_metrics, baseline_model)\r\n    814         if evaluator.can_evaluate(model_type=model_type, evaluator_config=config):\r\n    815             _logger.info(f\"Evaluating the model with the {evaluator_name} evaluator.\")\r\n--> 816             eval_result = evaluator.evaluate(\r\n    817                 model=model,\r\n    818                 model_type=model_type,\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/default_evaluator.py in evaluate(self, model, model_type, dataset, run_id, evaluator_config, custom_metrics, baseline_model, **kwargs)\r\n   1226             if baseline_model:\r\n   1227                 _logger.info(\"Evaluating candidate model:\")\r\n-> 1228             evaluation_result = self._evaluate(model, is_baseline_model=False)\r\n   1229 \r\n   1230         if not baseline_model:\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/default_evaluator.py in _evaluate(self, model, is_baseline_model, **kwargs)\r\n   1175             with mlflow.utils.autologging_utils.disable_autologging():\r\n   1176                 self._generate_model_predictions()\r\n-> 1177                 self._compute_builtin_metrics()\r\n   1178                 self._evaluate_custom_metrics_and_log_produced_artifacts(\r\n   1179                     log_to_mlflow_tracking=not is_baseline_model\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/default_evaluator.py in _compute_builtin_metrics(self)\r\n   1114                 average = self.evaluator_config.get(\"average\", \"weighted\")\r\n   1115                 self.metrics.update(\r\n-> 1116                     _get_multiclass_classifier_metrics(\r\n   1117                         y_true=self.y,\r\n   1118                         y_pred=self.y_pred,\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/default_evaluator.py in _get_multiclass_classifier_metrics(y_true, y_pred, y_proba, labels, average, sample_weights)\r\n    225     sample_weights=None,\r\n    226 ):\r\n--> 227     return _get_common_classifier_metrics(\r\n    228         y_true=y_true,\r\n    229         y_pred=y_pred,\r\n\r\n/databricks/python/lib/python3.8/site-packages/mlflow/models/evaluation/default_evaluator.py in _get_common_classifier_metrics(y_true, y_pred, y_proba, labels, average, pos_label, sample_weights)\r\n    164     metrics = {\r\n    165         \"example_count\": len(y_true),\r\n--> 166         \"accuracy_score\": sk_metrics.accuracy_score(y_true, y_pred, sample_weight=sample_weights),\r\n    167         \"recall_score\": sk_metrics.recall_score(\r\n    168             y_true,\r\n\r\n/databricks/python/lib/python3.8/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)\r\n     61             extra_args = len(args) - len(all_args)\r\n     62             if extra_args <= 0:\r\n---> 63                 return f(*args, **kwargs)\r\n     64 \r\n     65             # extra_args > 0\r\n\r\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py in accuracy_score(y_true, y_pred, normalize, sample_weight)\r\n    200 \r\n    201     # Compute accuracy for each possible representation\r\n--> 202     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n    203     check_consistent_length(y_true, y_pred, sample_weight)\r\n    204     if y_type.startswith('multilabel'):\r\n\r\n/databricks/python/lib/python3.8/site-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred)\r\n     90 \r\n     91     if len(y_type) > 1:\r\n---> 92         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\r\n     93                          \"and {1} targets\".format(type_true, type_pred))\r\n     94 \r\n\r\nValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets\r\n```\r\n\r\n### Other info / logs\r\n\r\n_No response_\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [X] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/7316/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/7316/timeline","performed_via_github_app":null,"state_reason":null}