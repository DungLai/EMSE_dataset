{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5815","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5815/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5815/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5815/events","html_url":"https://github.com/mlflow/mlflow/issues/5815","id":1224352603,"node_id":"I_kwDOCB5Jx85I-iNb","number":5815,"title":"[BUG] Computing SHAP explanations fails during mlflow.evaluate()","user":{"login":"AnastasiaProkaieva","id":88480495,"node_id":"MDQ6VXNlcjg4NDgwNDk1","avatar_url":"https://avatars.githubusercontent.com/u/88480495?v=4","gravatar_id":"","url":"https://api.github.com/users/AnastasiaProkaieva","html_url":"https://github.com/AnastasiaProkaieva","followers_url":"https://api.github.com/users/AnastasiaProkaieva/followers","following_url":"https://api.github.com/users/AnastasiaProkaieva/following{/other_user}","gists_url":"https://api.github.com/users/AnastasiaProkaieva/gists{/gist_id}","starred_url":"https://api.github.com/users/AnastasiaProkaieva/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AnastasiaProkaieva/subscriptions","organizations_url":"https://api.github.com/users/AnastasiaProkaieva/orgs","repos_url":"https://api.github.com/users/AnastasiaProkaieva/repos","events_url":"https://api.github.com/users/AnastasiaProkaieva/events{/privacy}","received_events_url":"https://api.github.com/users/AnastasiaProkaieva/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2022-05-03T16:52:43Z","updated_at":"2022-08-12T13:34:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Willingness to contribute\n\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\n\n### System information\n\n- **Running MlFlow on Databricks runtime 10.4 ML LTS**:\r\n- **MLflow version is '1.24.0'**:\r\n- **Python version is '3.8.10'**:\r\n\n\n### Describe the problem\n\nWhile executing mlflow.evaluate on Databricks runtime 10.4 get an error : \r\n> Exception: Additivity check failed in TreeExplainer! \r\n\r\nThe error is coming from SHAP package, the check_additivity=False should be specified. \r\nCan this be updated in options or set to default or should there be an downsampling then? \r\n\r\nThe related SHAP issue is here: \r\nhttps://github.com/slundberg/shap/issues/941\n\n### Tracking information\n\n_No response_\n\n### Code to reproduce issue\n\n```\r\nmodel = RandomForestClassifier(**params_rf)\r\n# construct an evaluation dataset from the test set\r\neval_data = X_test\r\neval_data[\"target\"] = y_test\r\n\r\nwith mlflow.start_run(run_name=f'untuned_random_forest_{user_name}'):\r\n  \r\n  model.fit(X_train, y_train)\r\n  # predict_proba returns [prob_negative, prob_positive], so slice the output with [:, 1]\r\n  predictions_test = model.predict_proba(X_test)[:,1]\r\n  auc_score = roc_auc_score(y_test, predictions_test)\r\n  # Logging parameters used to train our model \r\n  mlflow.log_params(params_rf)\r\n  # Use the area under the ROC curve as a metric.\r\n  mlflow.log_metric('auc', auc_score)\r\n \r\n  model_info = mlflow.sklearn.log_model(model, \"sklearn_rf_model\")\r\n  result = mlflow.evaluate(\r\n       model_info.model_uri,\r\n       eval_data,\r\n       targets='target',\r\n       model_type=\"classifier\",\r\n       dataset_name=\"wine_dataset\",\r\n       evaluators=[\"default\"],\r\n   )\r\n```\n\n### Other info / logs\n\n ``` Exception: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was 0.547230, while the model output was 0.527094. If this difference is acceptable you can set check_additivity=False to disable this check.```\r\n\r\nPartial log:\r\n```\r\n/databricks/python/lib/python3.8/site-packages/shap/explainers/_tree.py in shap_values(self, X, y, tree_limit, approximate, check_additivity, from_call)\r\n    406         out = self._get_shap_output(phi, flat_output)\r\n    407         if check_additivity and self.model.model_output == \"raw\":\r\n--> 408             self.assert_additivity(out, self.model.predict(X))\r\n    409 \r\n    410         return out\r\n\r\n/databricks/python/lib/python3.8/site-packages/shap/explainers/_tree.py in assert_additivity(self, phi, model_output)\r\n    537         if type(phi) is list:\r\n    538             for i in range(len(phi)):\r\n--> 539                 check_sum(self.expected_value[i] + phi[i].sum(-1), model_output[:,i])\r\n    540         else:\r\n    541             check_sum(self.expected_value + phi.sum(-1), model_output)\r\n\r\n/databricks/python/lib/python3.8/site-packages/shap/explainers/_tree.py in check_sum(sum_val, model_output)\r\n    533                            \" was %f, while the model output was %f. If this difference is acceptable\" \\\r\n    534                            \" you can set check_additivity=False to disable this check.\" % (sum_val[ind], model_output[ind])\r\n--> 535                 raise Exception(err_msg)\r\n    536 \r\n    537         if type(phi) is list:\r\n```\n\n### What component(s) does this bug affect?\n\n- [ ] `area/artifacts`: Artifact stores and artifact logging\n- [ ] `area/build`: Build and test infrastructure for MLflow\n- [ ] `area/docs`: MLflow documentation pages\n- [ ] `area/examples`: Example code\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\n- [ ] `area/projects`: MLproject format, project running backends\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\n- [ ] `area/server-infra`: MLflow Tracking server backend\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\n\n### What interface(s) does this bug affect?\n\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\n- [ ] `area/windows`: Windows support\n\n### What language(s) does this bug affect?\n\n- [ ] `language/r`: R APIs and clients\n- [ ] `language/java`: Java APIs and clients\n- [ ] `language/new`: Proposals for new client languages\n\n### What integration(s) does this bug affect?\n\n- [ ] `integrations/azure`: Azure and Azure ML integrations\n- [ ] `integrations/sagemaker`: SageMaker integrations\n- [ ] `integrations/databricks`: Databricks integrations","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5815/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5815/timeline","performed_via_github_app":null,"state_reason":null}