{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2428","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/2428/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/2428/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/2428/events","html_url":"https://github.com/mlflow/mlflow/issues/2428","id":564592547,"node_id":"MDU6SXNzdWU1NjQ1OTI1NDc=","number":2428,"title":"Struggling to deploy model to Azure ML","user":{"login":"djrscally","id":4592235,"node_id":"MDQ6VXNlcjQ1OTIyMzU=","avatar_url":"https://avatars.githubusercontent.com/u/4592235?v=4","gravatar_id":"","url":"https://api.github.com/users/djrscally","html_url":"https://github.com/djrscally","followers_url":"https://api.github.com/users/djrscally/followers","following_url":"https://api.github.com/users/djrscally/following{/other_user}","gists_url":"https://api.github.com/users/djrscally/gists{/gist_id}","starred_url":"https://api.github.com/users/djrscally/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djrscally/subscriptions","organizations_url":"https://api.github.com/users/djrscally/orgs","repos_url":"https://api.github.com/users/djrscally/repos","events_url":"https://api.github.com/users/djrscally/events{/privacy}","received_events_url":"https://api.github.com/users/djrscally/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-02-13T10:42:19Z","updated_at":"2020-02-13T14:10:45Z","closed_at":"2020-02-13T14:10:45Z","author_association":"NONE","active_lock_reason":null,"body":"I'm struggling with deployment to AzureML. I have a NN built in Keras, but pipelined with SKlearn preprocessors. When I train it for a single epoch on a small amount of data, the deployment to Azure ML works ok. I use this code to perform the deployment:\r\n\r\n```Python\r\nimport azureml\r\nfrom azureml.core import Workspace\r\nimport mlflow.azureml\r\nfrom azureml.exceptions import WebserviceException\r\nfrom azureml.core.webservice import AciWebservice, Webservice\r\n\r\nrun_id='2a043368ee1b48c781a18ea3d2c0fb75'\r\n\r\n# Connect to the EMEA BI Azure ML Workspace\r\n\r\nworkspace_name = \"uks1bimlws01\"\r\nworkspace_location=\"UK South\"\r\nresource_group = \"Rg_BI_Prod_EMEA_01\"\r\nsubscription_id = \"< redacted > \"\r\n\r\nworkspace = Workspace.create(name = workspace_name,\r\n                             location = workspace_location,\r\n                             resource_group = resource_group,\r\n                             subscription_id = subscription_id,\r\n                             exist_ok=True)\r\n\r\n# Build the docker image to serve the model saved against the ML Flow run.\r\n\r\nmodel_uri = 'runs:/{0}/Model'.format(run_id)\r\nmodel_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \r\n                                                      workspace=workspace,\r\n                                                      model_name=\"Product-Recommender-DNN\",\r\n                                                      image_name=\"prcg-emea-bi-dev\",\r\n                                                      description=\"Product Recommendation Engine, Candidate Generation DNN\",\r\n                                                      synchronous=False)\r\n\r\nmodel_image.wait_for_creation(show_output=True)\r\n\r\ndev_webservice_name = \"prcg-emea-bi-dev\"\r\n\r\ntry:\r\n  Webservice(workspace, dev_webservice_name).delete()\r\nexcept WebserviceException as e:\r\n  print(e.message)\r\n  pass\r\n\r\ndev_webservice_deployment_config = AciWebservice.deploy_configuration(\r\n  cpu_cores=0.1,\r\n  memory_gb=0.5,\r\n  description=\"Product Recommendation Engine, Candidate Generation Endpoint\"\r\n)\r\n\r\ndev_webservice = Webservice.deploy_from_image(\r\n  name=dev_webservice_name,\r\n  image=model_image,\r\n  deployment_config=dev_webservice_deployment_config,\r\n  workspace=workspace,\r\n  overwrite=True\r\n)\r\n\r\ndev_webservice.wait_for_deployment()\r\n```\r\n\r\nAnd yeah that works; the model is hosted and I can use the Scoring endpoint to perform real time inference, fantastic.\r\n\r\nWhen I retrain the model on the full whack of data until the EarlyStopping callback kills it, the same code (with the appropriate run ID of course) won't deploy the model. the call to `.wait_for_deployment()` eventually returns a timeout exception:\r\n\r\n```\r\nERROR - Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\nOperation ID: 7aad999a-e4bf-42ed-9f70-af43e503518c\r\nMore information can be found using '.get_logs()'\r\nError:\r\n{\r\n  \"code\": \"DeploymentTimedOut\",\r\n  \"statusCode\": 504,\r\n  \"message\": \"The deployment operation polling has TimedOut. The service creation is taking longer than our normal time. We are still trying to achieve the desired state for the web service. Please check the webservice state for the current webservice health. You can run print(service.state) from the python SDK to retrieve the current state of the webservice.\"\r\n}\r\n\r\nERROR - Service deployment polling reached non-successful terminal state, current service state: Unhealthy\r\n```\r\n\r\nThe output of `dev_webservice.get_logs()` doesn't seem to give any explicit error messages, but seems to imply the container is in a boot loop. The logs start like this:\r\n\r\n```\r\n2020-02-13T09:00:49,039652829+00:00 - gunicorn/run \r\n2020-02-13T09:00:49,040364733+00:00 - rsyslog/run \r\n2020-02-13T09:00:49,039110426+00:00 - iot-server/run \r\n2020-02-13T09:00:49,108711314+00:00 - nginx/run \r\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\r\n2020-02-13T09:00:52,106415897+00:00 - iot-server/finish 1 0\r\n2020-02-13T09:00:52,201007324+00:00 - Exit code 1 is normal. Not restarting iot-server.\r\nStarting gunicorn 19.6.0\r\nListening at: http://127.0.0.1:31311 (9)\r\nUsing worker: sync\r\nworker timeout is set to 300\r\nBooting worker with pid: 43\r\nInitializing logger\r\nStarting up app insights client\r\nStarting up request id generator\r\nStarting up app insight hooks\r\nInvoking user's init function\r\n2020-02-13 09:01:43,003 | azureml.core.run | DEBUG | Could not load run context RunEnvironmentException:\r\n\tMessage: Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\r\n\tInnerException None\r\n\tErrorResponse \r\n{\r\n    \"error\": {\r\n        \"message\": \"Could not load a submitted run, if outside of an execution context, use experiment.start_logging to initialize an azureml.core.Run.\"\r\n    }\r\n}, switching offline: False\r\n2020-02-13 09:01:43,003 | azureml.core.run | DEBUG | Could not load the run context and allow_offline set to False\r\n2020-02-13 09:01:43,003 | azureml.core.model | DEBUG | Using passed in version 12\r\n2020-02-13 09:01:43,004 | azureml.core.model | DEBUG | Found model path at azureml-models/Product-Recommender-DNN/12/Model\r\n/var/azureml-app/execution_script.py:12: DeprecationWarning: .. Warning:: ``mlflow.pyfunc.load_pyfunc`` is deprecated since 1.0. This method will be removed in a near future release. Use ``mlflow.pyfunc.load_model`` instead.\r\n  model = load_pyfunc(model_path)\r\n2020/02/13 09:01:43 WARNING mlflow.pyfunc: The version of CloudPickle that was used to save the model, `CloudPickle 1.3.0`, differs from the version of CloudPickle that is currently running, `CloudPickle 1.2.2`, and may be incompatible\r\nFrom /opt/miniconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nFrom /opt/miniconda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\r\n2020-02-13 09:02:31.009156: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2020-02-13 09:02:31.108618: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2294685000 Hz\r\n2020-02-13 09:02:31.108899: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559b2524dd80 executing computations on platform Host. Devices:\r\n2020-02-13 09:02:31.108957: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\r\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0\r\nOMP: Info #213: KMP_AFFINITY: decoding x2APIC ids.\r\nOMP: Info #276: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\r\nOMP: Info #156: KMP_AFFINITY: 1 available OS procs\r\nOMP: Info #157: KMP_AFFINITY: Uniform topology\r\nOMP: Info #159: KMP_AFFINITY: 1 packages x 1 cores/pkg x 1 threads/core (1 total cores)\r\nOMP: Info #215: KMP_AFFINITY: OS proc to physical thread map:\r\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 \r\nOMP: Info #251: KMP_AFFINITY: pid 43 tid 43 thread 0 bound to OS proc set 0\r\n2020-02-13 09:02:31.109486: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n```\r\n\r\nEverything after the line `worker timeout is set to 300` then just repeats itself (different times and PIDs but everything else matches) over and over until I give up and kill the Webservice with `.delete()`\r\n\r\nI'm not really sure how to even begin to debug this unfortunately; any help would be appreciated.","closed_by":{"login":"djrscally","id":4592235,"node_id":"MDQ6VXNlcjQ1OTIyMzU=","avatar_url":"https://avatars.githubusercontent.com/u/4592235?v=4","gravatar_id":"","url":"https://api.github.com/users/djrscally","html_url":"https://github.com/djrscally","followers_url":"https://api.github.com/users/djrscally/followers","following_url":"https://api.github.com/users/djrscally/following{/other_user}","gists_url":"https://api.github.com/users/djrscally/gists{/gist_id}","starred_url":"https://api.github.com/users/djrscally/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djrscally/subscriptions","organizations_url":"https://api.github.com/users/djrscally/orgs","repos_url":"https://api.github.com/users/djrscally/repos","events_url":"https://api.github.com/users/djrscally/events{/privacy}","received_events_url":"https://api.github.com/users/djrscally/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2428/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/2428/timeline","performed_via_github_app":null,"state_reason":"completed"}