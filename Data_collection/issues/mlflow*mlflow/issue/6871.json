{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6871","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/6871/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/6871/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/6871/events","html_url":"https://github.com/mlflow/mlflow/issues/6871","id":1383801076,"node_id":"I_kwDOCB5Jx85SeyD0","number":6871,"title":"[BUG] Conflict columns when Spark Model loading","user":{"login":"NastasiaSaby","id":8245071,"node_id":"MDQ6VXNlcjgyNDUwNzE=","avatar_url":"https://avatars.githubusercontent.com/u/8245071?v=4","gravatar_id":"","url":"https://api.github.com/users/NastasiaSaby","html_url":"https://github.com/NastasiaSaby","followers_url":"https://api.github.com/users/NastasiaSaby/followers","following_url":"https://api.github.com/users/NastasiaSaby/following{/other_user}","gists_url":"https://api.github.com/users/NastasiaSaby/gists{/gist_id}","starred_url":"https://api.github.com/users/NastasiaSaby/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NastasiaSaby/subscriptions","organizations_url":"https://api.github.com/users/NastasiaSaby/orgs","repos_url":"https://api.github.com/users/NastasiaSaby/repos","events_url":"https://api.github.com/users/NastasiaSaby/events{/privacy}","received_events_url":"https://api.github.com/users/NastasiaSaby/received_events","type":"User","site_admin":false},"labels":[{"id":955449428,"node_id":"MDU6TGFiZWw5NTU0NDk0Mjg=","url":"https://api.github.com/repos/mlflow/mlflow/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2114036915,"node_id":"MDU6TGFiZWwyMTE0MDM2OTE1","url":"https://api.github.com/repos/mlflow/mlflow/labels/integrations/databricks","name":"integrations/databricks","color":"ffbce5","default":false,"description":"Databricks integrations"}],"state":"closed","locked":false,"assignee":{"login":"BenWilson2","id":39283302,"node_id":"MDQ6VXNlcjM5MjgzMzAy","avatar_url":"https://avatars.githubusercontent.com/u/39283302?v=4","gravatar_id":"","url":"https://api.github.com/users/BenWilson2","html_url":"https://github.com/BenWilson2","followers_url":"https://api.github.com/users/BenWilson2/followers","following_url":"https://api.github.com/users/BenWilson2/following{/other_user}","gists_url":"https://api.github.com/users/BenWilson2/gists{/gist_id}","starred_url":"https://api.github.com/users/BenWilson2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BenWilson2/subscriptions","organizations_url":"https://api.github.com/users/BenWilson2/orgs","repos_url":"https://api.github.com/users/BenWilson2/repos","events_url":"https://api.github.com/users/BenWilson2/events{/privacy}","received_events_url":"https://api.github.com/users/BenWilson2/received_events","type":"User","site_admin":false},"assignees":[{"login":"BenWilson2","id":39283302,"node_id":"MDQ6VXNlcjM5MjgzMzAy","avatar_url":"https://avatars.githubusercontent.com/u/39283302?v=4","gravatar_id":"","url":"https://api.github.com/users/BenWilson2","html_url":"https://github.com/BenWilson2","followers_url":"https://api.github.com/users/BenWilson2/followers","following_url":"https://api.github.com/users/BenWilson2/following{/other_user}","gists_url":"https://api.github.com/users/BenWilson2/gists{/gist_id}","starred_url":"https://api.github.com/users/BenWilson2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BenWilson2/subscriptions","organizations_url":"https://api.github.com/users/BenWilson2/orgs","repos_url":"https://api.github.com/users/BenWilson2/repos","events_url":"https://api.github.com/users/BenWilson2/events{/privacy}","received_events_url":"https://api.github.com/users/BenWilson2/received_events","type":"User","site_admin":false}],"milestone":null,"comments":11,"created_at":"2022-09-23T13:21:16Z","updated_at":"2022-11-02T17:31:42Z","closed_at":"2022-11-02T17:31:41Z","author_association":"NONE","active_lock_reason":null,"body":"### Willingness to contribute\r\n\r\nYes. I would be willing to contribute a fix for this bug with guidance from the MLflow community.\r\n\r\n### MLflow version\r\n\r\nmlflow-skinny 1.28\r\n\r\n### System information\r\n\r\n- **Databricks platform Release 11.2 ML**:\r\n- **Python 3.**:\r\n- **yarn version, if running the dev UI**:\r\n\r\n\r\n### Describe the problem\r\n\r\nWhen passing from 1.27 to 1.28, there's a bug with loading a Spark model that says: \r\n`java.lang.AssertionError: assertion failed: Conflicting partition column names detected:`\r\n\r\n### Tracking information\r\n\r\nMLflow version: 1.28.0\r\nTracking URI: databricks\r\nArtifact URI: dbfs:/databricks/mlflow-tracking/3773864572821124/c149c22b99f34bb5ac023d2ae0f679a8/artifacts\r\n\r\n### Code to reproduce issue\r\n\r\n```python\r\n\r\n%pip install mlflow-skinny==1.28\r\n\r\nfrom typing import List\r\nfrom pyspark.sql import DataFrame\r\nfrom pyspark.sql.types import StructType, StructField, IntegerType\r\n\r\ndata: List = [\r\n              (1, 3, 1),\r\n  (1, 3, 1),\r\n  (1, 3, 1),\r\n  (1, 3, 1),\r\n  (1, 3, 1),\r\n              (2, 4, 1),\r\n              (2, 3, 1),\r\n              (3, 3, 1),\r\n              (3, 4, 1)\r\n  ]\r\n\r\nschema: StructType = StructType([ \\\r\n    StructField(\"utilisateur_identifiant\", IntegerType(), True), \\\r\n    StructField(\"diamant_identifiant\", IntegerType(), True),\r\n    StructField(\"nombre_de_fois_achetes\", IntegerType(), True)\r\n  ])\r\n\r\ndiamants_pre_features: DataFrame = spark.createDataFrame(data=data,schema=schema)\r\n\r\n\r\nfrom pyspark.ml.recommendation import ALS, ALSModel\r\n\r\nals: ALS = ALS(\r\n  userCol=\"utilisateur_identifiant\", \r\n  itemCol=\"diamant_identifiant\", \r\n  ratingCol=\"nombre_de_fois_achetes\",\r\n  implicitPrefs=True,\r\n  alpha=40,\r\n  nonnegative=True\r\n)\r\nmodel: ALSModel = als.fit(diamants_pre_features)\r\n\r\nimport mlflow\r\nmlflow.set_experiment(\"/Users/nastasia/ALS_experiment\")\r\n\r\nwith mlflow.start_run() as last_run:\r\n  mlflow.spark.log_model(model, \"als_exp\")\r\n\r\nfrom mlflow.tracking import MlflowClient\r\n# Get last run from Mlflow experiment\r\nclient = MlflowClient()\r\n\r\nmodel_experiment_id = client.get_experiment_by_name(\"/Users/nastasia/ALS_experiment\").experiment_id\r\n\r\nruns = client.search_runs(\r\n        model_experiment_id, order_by=[\"start_time DESC\"]\r\n)\r\n\r\nrun_uuid = runs[0].info.run_uuid\r\n\r\n# can be loaded from s3\r\n# model = ALSModel.load(sources_jobs['ALS_model'])\r\nloaded_model = mlflow.spark.load_model(f\"runs:/{run_uuid}/als_exp\")\r\n```\r\n\r\n### Stack trace\r\n\r\n```python\r\n\r\nPy4JJavaError                             Traceback (most recent call last)\r\n<command-3935055487045470> in <cell line: 15>()\r\n     13 # can be loaded from s3\r\n     14 # model = ALSModel.load(sources_jobs['ALS_model'])\r\n---> 15 loaded_model = mlflow.spark.load_model(f\"runs:/{run_uuid}/als_exp\")\r\n\r\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-704f188f-34a4-414e-9394-fe156dae6392/lib/python3.9/site-packages/mlflow/spark.py in load_model(model_uri, dfs_tmpdir)\r\n    784             get_databricks_profile_uri_from_artifact_uri(root_uri)\r\n    785         ):\r\n--> 786             return PipelineModel.load(mlflowdbfs_path)\r\n    787 \r\n    788     return _load_model(\r\n\r\n/databricks/spark/python/pyspark/ml/util.py in load(cls, path)\r\n    444     def load(cls, path: str) -> RL:\r\n    445         \"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\r\n--> 446         return cls.read().load(path)\r\n    447 \r\n    448 \r\n\r\n/databricks/spark/python/pyspark/ml/pipeline.py in load(self, path)\r\n    282         metadata = DefaultParamsReader.loadMetadata(path, self.sc)\r\n    283         if \"language\" not in metadata[\"paramMap\"] or metadata[\"paramMap\"][\"language\"] != \"Python\":\r\n--> 284             return JavaMLReader(cast(Type[\"JavaMLReadable[PipelineModel]\"], self.cls)).load(path)\r\n    285         else:\r\n    286             uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)\r\n\r\n/databricks/spark/python/pyspark/ml/util.py in load(self, path)\r\n    393         if not isinstance(path, str):\r\n    394             raise TypeError(\"path should be a string, got type %s\" % type(path))\r\n--> 395         java_obj = self._jread.load(path)\r\n    396         if not hasattr(self._clazz, \"_from_java\"):\r\n    397             raise NotImplementedError(\r\n\r\n/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py in __call__(self, *args)\r\n   1319 \r\n   1320         answer = self.gateway_client.send_command(command)\r\n-> 1321         return_value = get_return_value(\r\n   1322             answer, self.gateway_client, self.target_id, self.name)\r\n   1323 \r\n\r\n/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)\r\n    194     def deco(*a: Any, **kw: Any) -> Any:\r\n    195         try:\r\n--> 196             return f(*a, **kw)\r\n    197         except Py4JJavaError as e:\r\n    198             converted = convert_exception(e.java_exception)\r\n\r\n/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)\r\n    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325             if answer[1] == REFERENCE_TYPE:\r\n--> 326                 raise Py4JJavaError(\r\n    327                     \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328                     format(target_id, \".\", name), value)\r\n\r\nPy4JJavaError: An error occurred while calling o631.load.\r\n: java.lang.AssertionError: assertion failed: Conflicting partition column names detected:\r\n\r\n\tPartition column name list #0: part-00000-tid-2225352087800616616-e8b36a0f-887c-4049-96ad-de143491c840-1160-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\r\n\tPartition column name list #1: part-00001-tid-2225352087800616616-e8b36a0f-887c-4049-96ad-de143491c840-1161-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\r\n\tPartition column name list #2: part-00002-tid-2225352087800616616-e8b36a0f-887c-4049-96ad-de143491c840-1162-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\r\n\tPartition column name list #3: part-00003-tid-2225352087800616616-e8b36a0f-887c-4049-96ad-de143491c840-1163-1-c000.snappy.parquet?X-Amz-Security-Token, _cloud_type_, _file_size_\r\n\r\nFor partitioned table directories, data files should only live in leaf directories.\r\nAnd directories at the same level should have the same partition column name.\r\n\tat scala.Predef$.assert(Predef.scala:223)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:482)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:212)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:106)\r\n\tat org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:53)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:368)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:324)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:324)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:558)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:548)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$5(Pipeline.scala:277)\r\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\r\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\r\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:277)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\r\n\tat org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356)\r\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:161)\r\n\tat org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:156)\r\n\tat org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:43)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:284)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:284)\r\n\tat org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\r\n\tat py4j.Gateway.invoke(Gateway.java:306)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\n### Other info / logs\r\n\r\n_No response_\r\n\r\n### What component(s) does this bug affect?\r\n\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/pipelines`: Pipelines, Pipeline APIs, Pipeline configs, Pipeline Templates\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\n### What interface(s) does this bug affect?\r\n\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\n### What language(s) does this bug affect?\r\n\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\n### What integration(s) does this bug affect?\r\n\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [X] `integrations/databricks`: Databricks integrations","closed_by":{"login":"dbczumar","id":39497902,"node_id":"MDQ6VXNlcjM5NDk3OTAy","avatar_url":"https://avatars.githubusercontent.com/u/39497902?v=4","gravatar_id":"","url":"https://api.github.com/users/dbczumar","html_url":"https://github.com/dbczumar","followers_url":"https://api.github.com/users/dbczumar/followers","following_url":"https://api.github.com/users/dbczumar/following{/other_user}","gists_url":"https://api.github.com/users/dbczumar/gists{/gist_id}","starred_url":"https://api.github.com/users/dbczumar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbczumar/subscriptions","organizations_url":"https://api.github.com/users/dbczumar/orgs","repos_url":"https://api.github.com/users/dbczumar/repos","events_url":"https://api.github.com/users/dbczumar/events{/privacy}","received_events_url":"https://api.github.com/users/dbczumar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/6871/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/6871/timeline","performed_via_github_app":null,"state_reason":"completed"}