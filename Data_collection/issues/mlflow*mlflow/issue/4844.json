{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4844","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4844/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4844/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4844/events","html_url":"https://github.com/mlflow/mlflow/issues/4844","id":1005475614,"node_id":"I_kwDOCB5Jx8477lce","number":4844,"title":"[FR] Add support to serve MLflow models through MLServer","user":{"login":"adriangonz","id":1577620,"node_id":"MDQ6VXNlcjE1Nzc2MjA=","avatar_url":"https://avatars.githubusercontent.com/u/1577620?v=4","gravatar_id":"","url":"https://api.github.com/users/adriangonz","html_url":"https://github.com/adriangonz","followers_url":"https://api.github.com/users/adriangonz/followers","following_url":"https://api.github.com/users/adriangonz/following{/other_user}","gists_url":"https://api.github.com/users/adriangonz/gists{/gist_id}","starred_url":"https://api.github.com/users/adriangonz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adriangonz/subscriptions","organizations_url":"https://api.github.com/users/adriangonz/orgs","repos_url":"https://api.github.com/users/adriangonz/repos","events_url":"https://api.github.com/users/adriangonz/events{/privacy}","received_events_url":"https://api.github.com/users/adriangonz/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-23T14:03:04Z","updated_at":"2021-10-22T20:05:25Z","closed_at":"2021-10-22T20:05:25Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [x] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [ ] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\n[MLServer](https://mlserver.readthedocs.io/en/latest/) is a fully-featured inference server built on top of FastAPI / AsyncIO, aimed towards production use cases. The [MLServer project](https://mlserver.readthedocs.io/en/latest/) has recently added full support for MLflow, which allows users to serve models through MLServer. This work has been driven between both the MLflow (@tomasatdatabricks) and MLServer teams, including participation from the community. As part of this collaboration, we took into account integrating deeply with some of the core MLflow features, including:\r\n\r\n- Native support for the [existing MLflow protocol](https://www.mlflow.org/docs/latest/models.html#deploy-mlflow-models), which can be used side-to-side with the [V2 Inference Protocol](https://github.com/kubeflow/kfserving/tree/master/docs/predict-api/v2).\r\n- Support for the MLflow’s model signature, which is used to decode the incoming requests into the right Python data types. The model signature is also exposed as the _“model metadata”_ of the V2 Inference Protocol. \r\n- Loading of custom Conda environments, which can be generated from the `conda.yaml` file exported by MLflow. \r\n\r\nThis allows users to get the most out of their models at serving time.\r\n\r\nFollowing this initial work, and after discussions between both teams, it was agreed by the MLFlow team that the best next step would be to contribute this work upstream to MLflow. The full initial design for this can be seen in [this design document](https://docs.google.com/document/d/1ITRBAleZUzS8DxZC0WZjmh0q6dlxNE2q8Yk0X20A2zs/edit?usp=sharing). The gist of it, is that it could be great for users if they could serve models through MLServer directly from the `mlflow` CLI, doing something like:\r\n\r\n```bash\r\n$ mlflow models serve -m my_model --mlserver\r\n```\r\n\r\nAdditionally, it was also agreed that it would be good to support building a Docker image which already includes MLServer. This could be added to the existing `mlflow models build-docker` subcommand, allowing the users to do something like:\r\n\r\n```bash\r\n$ mlflow models build-docker -m my_model -n my-model-image:0.1.0 --mlserver\r\n```\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\n\r\n  MLServer offers a production-grade inference server, currently used as a core component of some Kubernetes-native deployment frameworks, like Seldon Core, KFServing and KServe. Currently, MLflow users can serve models locally through the local scoring server. However, they may still face some challenges, like horizontal / vertical scalability, ease of deployment, etc. Allowing users to serve models through MLServer directly from MLflow could ease some of these pain points.\r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\n\r\n  MLServer is built with production use cases in mind, including:\r\n\r\n  * Multi-model serving, letting users run multiple models within the same process.\r\n  * Ability to run [inference in parallel for vertical scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html) across multiple models through a pool of inference workers.\r\n  * Support for [adaptive batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html), to group inference requests together on the fly.\r\n  * Support for the standard [V2 Inference Protocol](https://github.com/kubeflow/kfserving/tree/master/docs/predict-api/v2)  on both the gRPC and REST flavours, which has been standardised and adopted by various model serving frameworks.\r\n\r\n  Additionally, following the initial work to integrate with MLflow, MLServer should now provide a drop-in replacement for the local development server, exposing a similar API through the `/invocations` endpoint. Therefore, these features could be easy to integrate within existing user workflows. The full design document for this integration can be seen in [this link](https://docs.google.com/document/d/1yO-NqbdvpQzijaQXurDXPOb65Xh57f2odIa9_fAI_Qo/edit).\r\n\r\n  On top of this, MLServer is also used as the core Python inference server to serve machine learning models in Kubernetes native frameworks, including [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol), [KFServing](https://github.com/kubeflow/kfserving/tree/master/docs/samples/v1beta1/sklearn/v2) and [KServe](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/). Therefore, MLflow models served through MLServer should be easily deployed in these frameworks.\r\n\r\n- Why is this use case valuable to support for your project(s) or organization?\r\n\r\n  The goal of the MLServer project is to offer an easy-to-use, performant and extensible inference server. Therefore, it's on the project's best interest to make the user's life as easy as possible. As such, improving the model's lifecycle by simplifying the transition between training and serving is a key use case.\r\n\r\n  This includes Seldon Core and KFServing's current user base, where a large chunk of users already leveraging MLflow to train and serialise their models.\r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\n\r\n  At the moment, getting a model ready for production usually involves saving its artifacts in a centralised storage (e.g. S3, MLflow Model Registry, etc.) and then sharing its location with a 3rd party tool (e.g. Seldon Core, KFServing). Alternatively, a user can also manually build a Docker image including a 3rd party inference server (e.g. Triton, MLServer, etc.). The latter option would usually involve (depending on the server) some extra code to link the MLflow artifact with the inference server.\r\n\r\n  On both cases, this adds an extra manual step in the transition between training and serving of the model's lifecycle which can be cumbersome. Additionally, depending on their background, this extra step may also involve a relatively steep learning curve for data scientists.\r\n\r\n  Supporting MLServer within MLflow would mean that users can, instead, test their models directly as how they'd run in production: using the same tooling.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents\r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [x] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages\r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThe MLflow (@tomasatdatabricks) and MLServer teams have drafted an [initial design doc](https://docs.google.com/document/d/1ITRBAleZUzS8DxZC0WZjmh0q6dlxNE2q8Yk0X20A2zs/edit) which covers how this feature could be implemented in the MLflow codebase. This issue is also accompanied by a PR (#4845) which already gets a first stab at implement this design doc.","closed_by":{"login":"tomasatdatabricks","id":33237569,"node_id":"MDQ6VXNlcjMzMjM3NTY5","avatar_url":"https://avatars.githubusercontent.com/u/33237569?v=4","gravatar_id":"","url":"https://api.github.com/users/tomasatdatabricks","html_url":"https://github.com/tomasatdatabricks","followers_url":"https://api.github.com/users/tomasatdatabricks/followers","following_url":"https://api.github.com/users/tomasatdatabricks/following{/other_user}","gists_url":"https://api.github.com/users/tomasatdatabricks/gists{/gist_id}","starred_url":"https://api.github.com/users/tomasatdatabricks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tomasatdatabricks/subscriptions","organizations_url":"https://api.github.com/users/tomasatdatabricks/orgs","repos_url":"https://api.github.com/users/tomasatdatabricks/repos","events_url":"https://api.github.com/users/tomasatdatabricks/events{/privacy}","received_events_url":"https://api.github.com/users/tomasatdatabricks/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4844/timeline","performed_via_github_app":null,"state_reason":"completed"}