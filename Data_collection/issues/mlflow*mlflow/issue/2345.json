{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2345","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/2345/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/2345/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/2345/events","html_url":"https://github.com/mlflow/mlflow/issues/2345","id":554815546,"node_id":"MDU6SXNzdWU1NTQ4MTU1NDY=","number":2345,"title":".load_context() does not seem to run imports or assign global variables","user":{"login":"djrscally","id":4592235,"node_id":"MDQ6VXNlcjQ1OTIyMzU=","avatar_url":"https://avatars.githubusercontent.com/u/4592235?v=4","gravatar_id":"","url":"https://api.github.com/users/djrscally","html_url":"https://github.com/djrscally","followers_url":"https://api.github.com/users/djrscally/followers","following_url":"https://api.github.com/users/djrscally/following{/other_user}","gists_url":"https://api.github.com/users/djrscally/gists{/gist_id}","starred_url":"https://api.github.com/users/djrscally/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djrscally/subscriptions","organizations_url":"https://api.github.com/users/djrscally/orgs","repos_url":"https://api.github.com/users/djrscally/repos","events_url":"https://api.github.com/users/djrscally/events{/privacy}","received_events_url":"https://api.github.com/users/djrscally/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2020-01-24T15:55:29Z","updated_at":"2020-01-29T15:07:08Z","closed_at":"2020-01-29T15:07:08Z","author_association":"NONE","active_lock_reason":null,"body":"Weird one maybe. I have a DNN built in Keras which I'm wrapping in an Sklearn wrapper so that I can use it with Pipeline. This means I have to use the `mlflow.pyfunc` module, which is fine. \r\n\r\nOne problem I need to solve is that 4 of my features are multi-variate; I.E. each of those features contains an array of values, like the Item History feature below:\r\n\r\n```\r\n  CustomerAccountNum                        Item History\r\n0          100378466                            [964538]\r\n1          100383315  [443583, 3508270, 4021856, 955424]\r\n2          100383315  [443583, 3508270, 4021856, 955424]\r\n3          100383315  [443583, 3508270, 4021856, 955424]\r\n4          100383315  [443583, 3508270, 4021856, 955424]\r\n```\r\n\r\nThe problem is that that needs encoding obviously, and sklearn's preprocessors don't accept partial-fit (which would let me sweep over the arrays and partially fit on each row). For that reason I'm pre-fitting ordinal encoders, and then building [FunctionTransformers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) using a helper function that transforms each of those features in the way I want (transforms using the pre-fitted estimators, then pads to 200 items). Like this:\r\n\r\n```Python\r\n# Pre-fit the encoder\r\nle = LabelEncoder()\r\nle.fit(df.Item)\r\n\r\n# Helper Functions\r\n\r\ndef PlusOneTransformer(X):\r\n    \"\"\"\r\n    Just increases every encoded value by one; necessary to avoid labels mapped to 0 being discarded by Keras\r\n    masking\r\n    \"\"\"\r\n    return X + 1\r\n\r\ndef _pad(x):\r\n    \"\"\"\r\n    Wrapped function defining max pad length\r\n    \"\"\"\r\n    return pad_sequences([x], maxlen=200)[0]\r\n\r\ndef PrePadder(X):\r\n    \"\"\"\r\n    Pads Item History to shape.\r\n    \"\"\"\r\n    return X.applymap(_pad)\r\n\r\ndef ItemLabelEncoder(x):\r\n    \"\"\"\r\n    Converts Item History to prefitted encoder values\r\n    \"\"\"\r\n    return x.applymap(le.transform)\r\n\r\npreprocessor = Pipeline(steps=[\r\n    ('LabelEncode', FunctionTransformer(ItemLabelEncoder)),\r\n    ('PlusOne', FunctionTransformer(PlusOneTransformer)),\r\n    ('PrePadder', FunctionTransformer(PrePadder))\r\n])\r\n```\r\n\r\nNow; all that works fine in the training script. I create a class that inherits from `mlflow.pyfunc.PythonModel` that (in the training script) has a `.save()` method to dump the constituent parts to disk and has a `.load_context()` method defined that **should** import everything back in hunky dory, like so:\r\n\r\n```Python\r\nclass ProductRecommender(PythonModel):\r\n    \r\n    def __init__(self):\r\n        return\r\n        \r\n    def load_context(self, context):\r\n        \r\n        import keras\r\n        from keras.metrics import sparse_top_k_categorical_accuracy\r\n        from helper_functions import \\\r\n            PlusOneTransformer, \\\r\n            ItemLabelEncoder, \\\r\n            ManufacturerLabelEncoder, \\\r\n            CategoryLabelEncoder, \\\r\n            SubCategoryLabelEncoder, \\\r\n            _pad, \\\r\n            PrePadder, \\\r\n            add_sq_cols, \\\r\n            add_cbrt_cols, \\\r\n            top_10_categorical_accuracy\r\n        \r\n        global le\r\n        global mle\r\n        global cle\r\n        global scle\r\n        \r\n        le = joblib.load(context.artifacts['item_encoder'])\r\n        mle = joblib.load(context.artifacts['manufacturer_encoder'])\r\n        cle = joblib.load(context.artifacts['category_encoder'])\r\n        scle = joblib.load(context.artifacts['subcategory_encoder'])\r\n        \r\n        self.keras_model = keras.models.load_model(context.artifacts[\"keras_model\"], custom_objects={'top_10_categorical_accuracy':top_10_categorical_accuracy})\r\n        self.sklearn_preprocessor = joblib.load(context.artifacts[\"sklearn_preprocessor\"])\r\n        \r\n        self.sklearn_model = KerasModelRegressor(self.keras_model, epochs=5, validation_split=0.2)\r\n        \r\n        self.pipeline = Pipeline(steps=[\r\n            ('preprocessor', self.sklearn_preprocessor),\r\n            ('estimator', self.sklearn_model)\r\n        ])\r\n    \r\n    def save(self, pipeline, le, mle, cle, scle):\r\n        pipeline.named_steps.estimator.model.save('artifacts/keras_model.h5')\r\n        joblib.dump(pipeline.named_steps.preprocessor, 'artifacts/sklearn_preprocessor.joblib')\r\n        joblib.dump(le, 'artifacts/le.joblib')\r\n        joblib.dump(mle, 'artifacts/mle.joblib')\r\n        joblib.dump(cle, 'artifacts/cle.joblib')\r\n        joblib.dump(scle, 'artifacts/scle.joblib')\r\n        \r\n    def predict(self, context, X):\r\n        return self.pipeline.predict(X)\r\n```\r\n\r\nand then I log that against the run using `.log_model()`:\r\n\r\n```Python\r\nwith mlflow.start_run() as run:\r\n    \r\n    print('This Run ID: ', run.info.run_id)\r\n    \r\n    pipeline.fit(df, le.transform(df.Item))\r\n    pr = ProductRecommender()\r\n    pr.save(\r\n        pipeline,\r\n        le,\r\n        mle,\r\n        cle,\r\n        scle\r\n    )\r\n    \r\n    conda_env = {\r\n        'name': 'mlflow-env',\r\n        'channels': [\r\n            'defaults',\r\n            'anaconda',\r\n            'conda-forge'\r\n        ],\r\n        'dependencies': [\r\n            'python=3.7.0',\r\n            'cloudpickle',\r\n            'keras==2.2.5',\r\n            'joblib==0.13.2',\r\n            'scikit-learn==0.20.3',\r\n            'sys'\r\n        ]\r\n    }\r\n\r\n    artifacts = {\r\n        'keras_model':'artifacts/keras_model.h5',\r\n        'sklearn_preprocessor':'artifacts/sklearn_preprocessor.joblib',\r\n        'item_encoder':'artifacts/le.joblib',\r\n        'manufacturer_encoder':'artifacts/mle.joblib',\r\n        'category_encoder':'artifacts/cle.joblib',\r\n        'subcategory_encoder':'artifacts/scle.joblib'\r\n    }\r\n\r\n    mlflow.pyfunc.log_model(\r\n        artifact_path='Model',\r\n        code_path=['artifacts/sklearn_wrapper.py', 'artifacts/helper_functions.py'],\r\n        python_model=pr,\r\n        conda_env=conda_env,\r\n        artifacts=artifacts\r\n    )\r\n```\r\n\r\nNote that in addition to passing the pickled objects as artifacts, I'm also passing the file defining the helper functions to `code_path`. That `log_model()` operation seems to be fine.\r\n\r\nMy problem in a nutshell is that when I called `model = mlflow.pyfunc.load_model()` to import that saved mode, the import statements and assignment of global variables inside `.load_context()` don't seem to happen; so if I run this as my loading command:\r\n\r\n```Python\r\nimport mlflow\r\nimport mlflow.pyfunc\r\nmodel = mlflow.pyfunc.load_model('runs:/292c8c28f3ce468cb89e4aa9e1dded25/Model')\r\n```\r\n\r\nAn exception is thrown:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-1-19d417ca6b69> in <module>\r\n      1 import mlflow\r\n      2 import mlflow.pyfunc\r\n----> 3 model = mlflow.pyfunc.load_model('runs:/292c8c28f3ce468cb89e4aa9e1dded25/Model')\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mlflow\\pyfunc\\__init__.py in load_model(model_uri, suppress_warnings)\r\n    290         mlflow.pyfunc.utils._add_code_to_system_path(code_path=code_path)\r\n    291     data_path = os.path.join(local_model_path, conf[DATA]) if (DATA in conf) else local_model_path\r\n--> 292     return importlib.import_module(conf[MAIN])._load_pyfunc(data_path)\r\n    293 \r\n    294 \r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mlflow\\pyfunc\\model.py in _load_pyfunc(model_path)\r\n    221 \r\n    222     context = PythonModelContext(artifacts=artifacts)\r\n--> 223     python_model.load_context(context=context)\r\n    224     return _PythonModelPyfuncWrapper(python_model=python_model, context=context)\r\n    225 \r\n\r\n<ipython-input-22-56b57b832a62> in load_context(self, context)\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\numpy_pickle.py in load(filename, mmap_mode)\r\n    603                     return load_compatibility(fobj)\r\n    604 \r\n--> 605                 obj = _unpickle(fobj, filename, mmap_mode)\r\n    606 \r\n    607     return obj\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\joblib\\numpy_pickle.py in _unpickle(fobj, filename, mmap_mode)\r\n    527     obj = None\r\n    528     try:\r\n--> 529         obj = unpickler.load()\r\n    530         if unpickler.compat_mode:\r\n    531             warnings.warn(\"The file '%s' has been generated with a \"\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\pickle.py in load(self)\r\n   1086                     raise EOFError\r\n   1087                 assert isinstance(key, bytes_types)\r\n-> 1088                 dispatch[key[0]](self)\r\n   1089         except _Stop as stopinst:\r\n   1090             return stopinst.value\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\pickle.py in load_global(self)\r\n   1374         module = self.readline()[:-1].decode(\"utf-8\")\r\n   1375         name = self.readline()[:-1].decode(\"utf-8\")\r\n-> 1376         klass = self.find_class(module, name)\r\n   1377         self.append(klass)\r\n   1378     dispatch[GLOBAL[0]] = load_global\r\n\r\nc:\\users\\dscally\\appdata\\local\\programs\\python\\python37\\lib\\pickle.py in find_class(self, module, name)\r\n   1428             return _getattribute(sys.modules[module], name)[0]\r\n   1429         else:\r\n-> 1430             return getattr(sys.modules[module], name)\r\n   1431 \r\n   1432     def load_reduce(self):\r\n\r\nAttributeError: module '__main__' has no attribute 'ItemLabelEncoder'\r\n```\r\n\r\nSo the `from helper_functions import blah blah` bit of `.load_context()` isn't working. If I define those functions (or locally import them from the actual helper_functions.py) then the `model = mlflow.pyfunc.load_model()` call completes properly, but the le, mle, cle and scle objects that are defined as global variables in `.load_context()` are not instantiated:\r\n\r\n```Python\r\n>>> le\r\n---------------------------------------------------------------------------\r\nNameError                                 Traceback (most recent call last)\r\n<ipython-input-4-286ef15a1c47> in <module>\r\n----> 1 le\r\n\r\nNameError: name 'le' is not defined\r\n```\r\n\r\nBut the assignments of class attributes (`self.keras_model` etc) DO work fine. \r\n\r\nI'm not sure if this is a bug, something unimplemented yet or if I'm just using it completely wrong (which wouldn't be the first time). Can anyone give any advice on a way forward? I hope I've given enough detail here to show what the problem is but please let me know if you need anything more.","closed_by":{"login":"djrscally","id":4592235,"node_id":"MDQ6VXNlcjQ1OTIyMzU=","avatar_url":"https://avatars.githubusercontent.com/u/4592235?v=4","gravatar_id":"","url":"https://api.github.com/users/djrscally","html_url":"https://github.com/djrscally","followers_url":"https://api.github.com/users/djrscally/followers","following_url":"https://api.github.com/users/djrscally/following{/other_user}","gists_url":"https://api.github.com/users/djrscally/gists{/gist_id}","starred_url":"https://api.github.com/users/djrscally/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/djrscally/subscriptions","organizations_url":"https://api.github.com/users/djrscally/orgs","repos_url":"https://api.github.com/users/djrscally/repos","events_url":"https://api.github.com/users/djrscally/events{/privacy}","received_events_url":"https://api.github.com/users/djrscally/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/2345/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/2345/timeline","performed_via_github_app":null,"state_reason":"completed"}