{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3392","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/3392/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/3392/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/3392/events","html_url":"https://github.com/mlflow/mlflow/issues/3392","id":694152349,"node_id":"MDU6SXNzdWU2OTQxNTIzNDk=","number":3392,"title":"[FR] Add model explainability with SHAP","user":{"login":"smurching","id":2358483,"node_id":"MDQ6VXNlcjIzNTg0ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2358483?v=4","gravatar_id":"","url":"https://api.github.com/users/smurching","html_url":"https://github.com/smurching","followers_url":"https://api.github.com/users/smurching/followers","following_url":"https://api.github.com/users/smurching/following{/other_user}","gists_url":"https://api.github.com/users/smurching/gists{/gist_id}","starred_url":"https://api.github.com/users/smurching/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smurching/subscriptions","organizations_url":"https://api.github.com/users/smurching/orgs","repos_url":"https://api.github.com/users/smurching/repos","events_url":"https://api.github.com/users/smurching/events{/privacy}","received_events_url":"https://api.github.com/users/smurching/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022849295,"node_id":"MDU6TGFiZWwyMDIyODQ5Mjk1","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/tracking","name":"area/tracking","color":"48eabc","default":false,"description":"Tracking service, tracking client APIs, autologging"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2020-09-05T19:24:33Z","updated_at":"2022-06-03T00:27:49Z","closed_at":"2022-06-03T00:27:49Z","author_association":"COLLABORATOR","active_lock_reason":null,"body":"## Willingness to contribute\r\nThe MLflow Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature (either as an MLflow Plugin or an enhancement to the MLflow code base)?\r\n\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time. <- happy to help with design/code review\r\n\r\n## Proposal Summary\r\n\r\n[SHAP](https://github.com/slundberg/shap) is a popular library for model explanability. These explanations are useful for both:\r\n* Understanding feature importance during model training (e.g. to guide further improvements to the model)\r\n* Understanding the predictions made by the model on fresh data (e.g. for providing post-hoc justification for/insight into a prediction)\r\n\r\nWe should consider an extension to the MLflow APIs that simplifies logging model explanations with SHAP. \r\n\r\n## Motivation\r\nSee above\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n- [ ] `area/server-infra`: MLflow server, JavaScript dev server\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, JavaScript, plotting\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\n\r\n### Initial Investigation [WIP]\r\nSHAP provides an API for generating a \"model explainer\" given a fitted model, which can then be applied to an input dataset (`explainer.shap_values(input_dataset)`) to explain the model's predictions. For each data point, the \"shap value\" of each of its features is a coefficient illustrating the impact of that feature value in increasing or decreasing the model output.\r\n\r\nFor certain model types, e.g. trees ([link](https://github.com/slundberg/shap#tree-ensemble-example-with-treeexplainer-xgboostlightgbmcatboostscikit-learnpyspark-models)), SHAP is able to compute explanations using only the fitted model and model input:\r\n\r\n```\r\nmodel = xgboost.train({\"learning_rate\": 0.01}, xgboost.DMatrix(X, label=y), 100)\r\n\r\n# explain the model's predictions using SHAP\r\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\r\nexplainer = shap.TreeExplainer(model)\r\nshap_values = explainer.shap_values(X)\r\n\r\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\r\nshap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\r\n```\r\n\r\nIn other cases, e.g. deep learning, more robust explanations can be generated by passing \"background\" data constructing a model explainer ([link](https://github.com/slundberg/shap#deep-learning-example-with-deepexplainer-tensorflowkeras-models)):\r\n\r\n```\r\n# select a set of background examples to take an expectation over\r\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\r\n\r\n# explain predictions of the model on four images\r\ne = shap.DeepExplainer(model, background)\r\nshap_values = e.shap_values(x_test[1:5])\r\n\r\n# plot the feature attributions\r\nshap.image_plot(shap_values, -x_test[1:5])\r\n```\r\n\r\nThe same holds for SHAP's generic [KernelExplainer](https://github.com/slundberg/shap#model-agnostic-example-with-kernelexplainer-explains-any-function). Note that in the example below we pass a function to represent the model (``svm.predict_proba``):\r\n\r\n```\r\n# use Kernel SHAP to explain test set predictions\r\nexplainer = shap.KernelExplainer(svm.predict_proba, X_train, link=\"logit\")\r\nshap_values = explainer.shap_values(X_test, nsamples=100)\r\n\r\n# plot the SHAP values for the Setosa output of the first instance\r\nshap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")\r\n```\r\n\r\nIn general, it seems that constructing an explainer always requires the fitted model (or some attribute thereof) and often also an input dataset, while computing and plotting explanations requires the explainer and an input dataset. Thus in the future, we could autogenerate an explainer at model fit time (e.g. in our autologging integrations) and persist it alongside the model, so that users can later load it back and compute explanations on fresh data.\r\n\r\nAs a stepping stone, we could provide  ``mlflow.shap.log_explainer(explainer, path)`` and ``mlflow.shap.load_explainer(path)`` APIs that persist a user-constructed ``explainer`` to a specified artifact subpath and load it back for future use. Note that SHAP doesn't appear to have any built-in persistence APIs (TODO investigate, but [see docs](https://shap.readthedocs.io/en/latest/#plots)), so we may have to e.g. just pickle the explainer","closed_by":{"login":"harupy","id":17039389,"node_id":"MDQ6VXNlcjE3MDM5Mzg5","avatar_url":"https://avatars.githubusercontent.com/u/17039389?v=4","gravatar_id":"","url":"https://api.github.com/users/harupy","html_url":"https://github.com/harupy","followers_url":"https://api.github.com/users/harupy/followers","following_url":"https://api.github.com/users/harupy/following{/other_user}","gists_url":"https://api.github.com/users/harupy/gists{/gist_id}","starred_url":"https://api.github.com/users/harupy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harupy/subscriptions","organizations_url":"https://api.github.com/users/harupy/orgs","repos_url":"https://api.github.com/users/harupy/repos","events_url":"https://api.github.com/users/harupy/events{/privacy}","received_events_url":"https://api.github.com/users/harupy/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/3392/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/3392/timeline","performed_via_github_app":null,"state_reason":"completed"}