{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5612","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/5612/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/5612/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/5612/events","html_url":"https://github.com/mlflow/mlflow/issues/5612","id":1193673679,"node_id":"I_kwDOCB5Jx85HJgPP","number":5612,"title":"[FR] Enable python clients to serve models and collect predictions from python code","user":{"login":"efagerberg","id":7632073,"node_id":"MDQ6VXNlcjc2MzIwNzM=","avatar_url":"https://avatars.githubusercontent.com/u/7632073?v=4","gravatar_id":"","url":"https://api.github.com/users/efagerberg","html_url":"https://github.com/efagerberg","followers_url":"https://api.github.com/users/efagerberg/followers","following_url":"https://api.github.com/users/efagerberg/following{/other_user}","gists_url":"https://api.github.com/users/efagerberg/gists{/gist_id}","starred_url":"https://api.github.com/users/efagerberg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/efagerberg/subscriptions","organizations_url":"https://api.github.com/users/efagerberg/orgs","repos_url":"https://api.github.com/users/efagerberg/repos","events_url":"https://api.github.com/users/efagerberg/events{/privacy}","received_events_url":"https://api.github.com/users/efagerberg/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2022-04-05T20:15:22Z","updated_at":"2022-08-25T14:37:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\n- [ ] Yes. I can contribute this feature independently.\r\n- [ ] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n- [x] No. I cannot contribute this feature at this time.\r\n\r\n## Proposal Summary\r\n\r\nEnable python clients to serve models and collect predictions from python code\r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nThis is a very similar use case to what sagemaker does, basically code base A defines this model, and code base B needs to consume that model's predictions. Code base B could just load the model, but then code base B inherits the dependencies of code base A. If code base B needs to get predictions from another model defined in code base C, then code base A and code base C have to have compatible dependencies in order to both be used in code base B.\r\n- Why is this use case valuable to support for MLflow users in general?\r\nSometimes you don't need to provision a whole other machine whose whole purpose is to serve the model. Sometimes you have pipelines that just need to consume some predictions for a period of time and then turn off.\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nBasically for the same reason as the general use case value.\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nIn the master version of this code you can get pretty close but it is very repetitive, and relies on technically protected functionality that may change.\r\n```python\r\nimport pandas as pd\r\nimport mlflow\r\nfrom mlflow.models.cli import _get_flavor_backend\r\n\r\nbackend = _get_flavor_backend('models:/ElasticnetWineModel/1')\r\nbackend.serve('models:/ElasticnetWineModel/1', port='9090', host='127.0.0.1', enable_mlserver=False, synchronous=False)\r\ntime.sleep(2)\r\ndf = pd.DataFrame(\r\n    columns=[\"alcohol\", \"chlorides\", \"citric acid\", \"density\", \"fixed acidity\", \"free sulfur dioxide\", \"pH\", \"residual sugar\", \"sulphates\", \"total sulfur dioxide\", \"volatile acidity\"],\r\n    data=[[12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66], [12.8, 0.029, 0.48, 0.98, 6.2, 29, 3.33, 1.2, 0.39, 75, 0.66]]\r\n)\r\nbackend.predict('models:/ElasticnetWineModel/1', ...)\r\n```\r\n\r\nNotice that you have to resupply the model URI multiple times. It would also be nice if you could just pass a DataFrame, but `backend.predict` expects an input file, and delivers an output file.\r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [ ] `area/artifacts`: Artifact stores and artifact logging\r\n- [ ] `area/build`: Build and test infrastructure for MLflow\r\n- [ ] `area/docs`: MLflow documentation pages\r\n- [ ] `area/examples`: Example code\r\n- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry\r\n- [x] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [ ] `area/projects`: MLproject format, project running backends\r\n- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs\r\n- [ ] `area/server-infra`: MLflow Tracking server backend\r\n- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging\r\n\r\nInterfaces\r\n- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server\r\n- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry\r\n- [ ] `area/windows`: Windows support\r\n\r\nLanguages \r\n- [ ] `language/r`: R APIs and clients\r\n- [ ] `language/java`: Java APIs and clients\r\n- [ ] `language/new`: Proposals for new client languages\r\n\r\nIntegrations\r\n- [ ] `integrations/azure`: Azure and Azure ML integrations\r\n- [ ] `integrations/sagemaker`: SageMaker integrations\r\n- [ ] `integrations/databricks`: Databricks integrations\r\n\r\n## Details\r\n\r\nThis feature would make more basic, pipeline driven applications much easier. Ideally as a User I can supply a known URI to some functionality that serves the model in an isolated environment, and has the capability to submit input, all in the same process.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/5612/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/5612/timeline","performed_via_github_app":null,"state_reason":null}