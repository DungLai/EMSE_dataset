{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4107","repository_url":"https://api.github.com/repos/mlflow/mlflow","labels_url":"https://api.github.com/repos/mlflow/mlflow/issues/4107/labels{/name}","comments_url":"https://api.github.com/repos/mlflow/mlflow/issues/4107/comments","events_url":"https://api.github.com/repos/mlflow/mlflow/issues/4107/events","html_url":"https://github.com/mlflow/mlflow/issues/4107","id":810473182,"node_id":"MDU6SXNzdWU4MTA0NzMxODI=","number":4107,"title":"[FR] Harmonize Dockerization in projects and models ","user":{"login":"QuantumPlumber","id":44450703,"node_id":"MDQ6VXNlcjQ0NDUwNzAz","avatar_url":"https://avatars.githubusercontent.com/u/44450703?v=4","gravatar_id":"","url":"https://api.github.com/users/QuantumPlumber","html_url":"https://github.com/QuantumPlumber","followers_url":"https://api.github.com/users/QuantumPlumber/followers","following_url":"https://api.github.com/users/QuantumPlumber/following{/other_user}","gists_url":"https://api.github.com/users/QuantumPlumber/gists{/gist_id}","starred_url":"https://api.github.com/users/QuantumPlumber/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/QuantumPlumber/subscriptions","organizations_url":"https://api.github.com/users/QuantumPlumber/orgs","repos_url":"https://api.github.com/users/QuantumPlumber/repos","events_url":"https://api.github.com/users/QuantumPlumber/events{/privacy}","received_events_url":"https://api.github.com/users/QuantumPlumber/received_events","type":"User","site_admin":false},"labels":[{"id":955449434,"node_id":"MDU6TGFiZWw5NTU0NDk0MzQ=","url":"https://api.github.com/repos/mlflow/mlflow/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":2022848043,"node_id":"MDU6TGFiZWwyMDIyODQ4MDQz","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/models","name":"area/models","color":"48eabc","default":false,"description":"MLmodel format, model serialization/deserialization, flavors"},{"id":2022848510,"node_id":"MDU6TGFiZWwyMDIyODQ4NTEw","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/projects","name":"area/projects","color":"48eabc","default":false,"description":"MLproject format, project running backends"},{"id":2022848902,"node_id":"MDU6TGFiZWwyMDIyODQ4OTAy","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/scoring","name":"area/scoring","color":"48eabc","default":false,"description":"MLflow Model server, model deployment tools, Spark UDFs"},{"id":2022851725,"node_id":"MDU6TGFiZWwyMDIyODUxNzI1","url":"https://api.github.com/repos/mlflow/mlflow/labels/area/docker","name":"area/docker","color":"ede978","default":false,"description":"Docker use anywhere, such as MLprojects and MLmodels"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-17T19:32:03Z","updated_at":"2021-07-30T17:34:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Willingness to contribute\r\n- [X] Yes. I would be willing to contribute this feature with guidance from the MLflow community.\r\n\r\n## Proposal Summary\r\nThe proposal is to standardize the way MLFlow builds docker images for training and serving. Currently MLFlow has different utilities for building Docker images under the projects module and models module. \r\n\r\nIn projects, the user must provide a base image that captures all Experiment dependencies. There is no support for activating a Conda environment within the Docker training image. A hack solution is to use a shell script as Docker entry point that activates the Conda environment before passing the arguments to the actual ML project entry point.\r\n\r\nIn contrast, the models build process for the server image demands that all requirements be installed via Conda. Additionally, the base image is fixed as Ubuntu 18.04 LTS. \r\n\r\nMLFlow should standardize image building across both modules to ensure that projects can seamlessly move from training to serving. \r\n\r\n## Motivation\r\n- What is the use case for this feature?\r\nMy company wants to use MLFlow as a complete training and deployment solution but is having trouble automating training given the inconsistency between training and serving images.\r\n\r\n- Why is this use case valuable to support for MLflow users in general?\r\nMLFlow users should be able to train using docker environments (for deployment on docker swarm, kubernetes ..) without abandoning Conda training environments. Additionally the server images created by MLFlow should support dependencies outside of Conda. \r\n\r\n- Why is this use case valuable to support for your project(s) or organization?\r\nThis would allow us to deploy our AutoML infrastructure using docker images while logging all results using MLFlow. We can seamlessly move to serving the models with the MLFlow server. \r\n\r\n- Why is it currently difficult to achieve this use case? (please be as specific as possible about why related MLflow features and components are insufficient)\r\nTo achieve the desired workflow, we have to use shell scripts to activate Conda environments within training images. We also need to be sure that the Conda training environment is captured accurately in the server build step.  \r\n\r\n### What component(s), interfaces, languages, and integrations does this feature affect?\r\nComponents \r\n- [X] `area/models`: MLmodel format, model serialization/deserialization, flavors\r\n- [X] `area/projects`: MLproject format, project running backends\r\n- [X] `area/scoring`: Local serving, model deployment tools, spark UDFs\r\n\r\nInterfaces\r\n- [X] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models\r\n\r\n## Details\r\n\r\nImplementation seems fairly straightforward. \r\n\r\nFor the model module, the build should accept an argument for a different base image. It can remain up to the user to verify that the base image will support the installed server dependencies. Perhaps this is as easy as demanding the base image be built from Ubuntu 18.04 LTS or similar. \r\n\r\nFor the project module, the build should make use of the same code used in the model module to install miniconda and configure a custom environment (if a Conda environment is requested). MLFlow can then generate a shell script in the working directory when building the image that activates the Conda environment and calls the project entry point. \r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mlflow/mlflow/issues/4107/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mlflow/mlflow/issues/4107/timeline","performed_via_github_app":null,"state_reason":null}