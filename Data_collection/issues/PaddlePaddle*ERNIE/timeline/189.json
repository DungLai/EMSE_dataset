[{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508018596","html_url":"https://github.com/PaddlePaddle/ERNIE/issues/189#issuecomment-508018596","issue_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189","id":508018596,"node_id":"MDEyOklzc3VlQ29tbWVudDUwODAxODU5Ng==","user":{"login":"tianxin1860","id":6829601,"node_id":"MDQ6VXNlcjY4Mjk2MDE=","avatar_url":"https://avatars.githubusercontent.com/u/6829601?v=4","gravatar_id":"","url":"https://api.github.com/users/tianxin1860","html_url":"https://github.com/tianxin1860","followers_url":"https://api.github.com/users/tianxin1860/followers","following_url":"https://api.github.com/users/tianxin1860/following{/other_user}","gists_url":"https://api.github.com/users/tianxin1860/gists{/gist_id}","starred_url":"https://api.github.com/users/tianxin1860/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tianxin1860/subscriptions","organizations_url":"https://api.github.com/users/tianxin1860/orgs","repos_url":"https://api.github.com/users/tianxin1860/repos","events_url":"https://api.github.com/users/tianxin1860/events{/privacy}","received_events_url":"https://api.github.com/users/tianxin1860/received_events","type":"User","site_admin":false},"created_at":"2019-07-03T09:33:04Z","updated_at":"2019-07-03T09:33:04Z","author_association":"COLLABORATOR","body":"> 在使用[输入句子经过 ERNIE 编码后的 Embedding 表示](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE#faq) 的功能时，在ChnSentiCorp数据集中，我分别打印了test.tsv中每句话的token embedding(即ernie_encoder.py中的unpad_top_layer_emb)的shape，以及使用tokenization.py中FullTokenizer加载./config/vocab.txt后对每句话进行分词的结果，发现模型返回的token embedding的长度全部都比使用FullTokenizer进行分词后的长度多出2个token。\r\n> \r\n> 理论上来说，模型返回的token embedding中第一维对应的embedding应该是[CLS]的token embedding，这是每句话相较原句分词结果多出来的其中一个token，那多出来的另一个token是什么？它在模型返回的token embedding中位于哪个位置？\r\n> \r\n> 还是说我的分词方法有问题，不该用FullTokenizer加载./config/vocab.txt来分词？（但是数了一下纯中文的句子的字数，确实全都要比模型返回的token embedding少2个token）\r\n> \r\n> 希望解答一下我的疑惑，谢谢！\r\n\r\n原始数据在预处理阶段，会在每句话的头部加上 `[CLS]` , 在每句话的 尾部加上 `[SEP]`, 所以 token_embedding 的长度相比原始数据经过 FullTokenizer 进行 token 化后的长度多出2个 token","reactions":{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508018596/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"tianxin1860","id":6829601,"node_id":"MDQ6VXNlcjY4Mjk2MDE=","avatar_url":"https://avatars.githubusercontent.com/u/6829601?v=4","gravatar_id":"","url":"https://api.github.com/users/tianxin1860","html_url":"https://github.com/tianxin1860","followers_url":"https://api.github.com/users/tianxin1860/followers","following_url":"https://api.github.com/users/tianxin1860/following{/other_user}","gists_url":"https://api.github.com/users/tianxin1860/gists{/gist_id}","starred_url":"https://api.github.com/users/tianxin1860/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tianxin1860/subscriptions","organizations_url":"https://api.github.com/users/tianxin1860/orgs","repos_url":"https://api.github.com/users/tianxin1860/repos","events_url":"https://api.github.com/users/tianxin1860/events{/privacy}","received_events_url":"https://api.github.com/users/tianxin1860/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508020034","html_url":"https://github.com/PaddlePaddle/ERNIE/issues/189#issuecomment-508020034","issue_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189","id":508020034,"node_id":"MDEyOklzc3VlQ29tbWVudDUwODAyMDAzNA==","user":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false},"created_at":"2019-07-03T09:37:02Z","updated_at":"2019-07-03T09:37:02Z","author_association":"NONE","body":"> > 在使用[输入句子经过 ERNIE 编码后的 Embedding 表示](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE#faq) 的功能时，在ChnSentiCorp数据集中，我分别打印了test.tsv中每句话的token embedding(即ernie_encoder.py中的unpad_top_layer_emb)的shape，以及使用tokenization.py中FullTokenizer加载./config/vocab.txt后对每句话进行分词的结果，发现模型返回的token embedding的长度全部都比使用FullTokenizer进行分词后的长度多出2个token。\r\n> > 理论上来说，模型返回的token embedding中第一维对应的embedding应该是[CLS]的token embedding，这是每句话相较原句分词结果多出来的其中一个token，那多出来的另一个token是什么？它在模型返回的token embedding中位于哪个位置？\r\n> > 还是说我的分词方法有问题，不该用FullTokenizer加载./config/vocab.txt来分词？（但是数了一下纯中文的句子的字数，确实全都要比模型返回的token embedding少2个token）\r\n> > 希望解答一下我的疑惑，谢谢！\r\n> \r\n> 原始数据在预处理阶段，会在每句话的头部加上 `[CLS]` , 在每句话的 尾部加上 `[SEP]`, 所以 token_embedding 的长度相比原始数据经过 FullTokenizer 进行 token 化后的长度多出2个 token\r\n\r\n好的明白，感谢回复！\r\nbtw确认一下，超过max_seq_len的句子，模型所返回的token embedding的最后一位也是[SEP]对应的embedding吧？","reactions":{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508020034/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508028284","html_url":"https://github.com/PaddlePaddle/ERNIE/issues/189#issuecomment-508028284","issue_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189","id":508028284,"node_id":"MDEyOklzc3VlQ29tbWVudDUwODAyODI4NA==","user":{"login":"tianxin1860","id":6829601,"node_id":"MDQ6VXNlcjY4Mjk2MDE=","avatar_url":"https://avatars.githubusercontent.com/u/6829601?v=4","gravatar_id":"","url":"https://api.github.com/users/tianxin1860","html_url":"https://github.com/tianxin1860","followers_url":"https://api.github.com/users/tianxin1860/followers","following_url":"https://api.github.com/users/tianxin1860/following{/other_user}","gists_url":"https://api.github.com/users/tianxin1860/gists{/gist_id}","starred_url":"https://api.github.com/users/tianxin1860/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tianxin1860/subscriptions","organizations_url":"https://api.github.com/users/tianxin1860/orgs","repos_url":"https://api.github.com/users/tianxin1860/repos","events_url":"https://api.github.com/users/tianxin1860/events{/privacy}","received_events_url":"https://api.github.com/users/tianxin1860/received_events","type":"User","site_admin":false},"created_at":"2019-07-03T10:00:22Z","updated_at":"2019-07-03T10:00:22Z","author_association":"COLLABORATOR","body":"> > > 在使用[输入句子经过 ERNIE 编码后的 Embedding 表示](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE#faq) 的功能时，在ChnSentiCorp数据集中，我分别打印了test.tsv中每句话的token embedding(即ernie_encoder.py中的unpad_top_layer_emb)的shape，以及使用tokenization.py中FullTokenizer加载./config/vocab.txt后对每句话进行分词的结果，发现模型返回的token embedding的长度全部都比使用FullTokenizer进行分词后的长度多出2个token。\r\n> > > 理论上来说，模型返回的token embedding中第一维对应的embedding应该是[CLS]的token embedding，这是每句话相较原句分词结果多出来的其中一个token，那多出来的另一个token是什么？它在模型返回的token embedding中位于哪个位置？\r\n> > > 还是说我的分词方法有问题，不该用FullTokenizer加载./config/vocab.txt来分词？（但是数了一下纯中文的句子的字数，确实全都要比模型返回的token embedding少2个token）\r\n> > > 希望解答一下我的疑惑，谢谢！\r\n> > \r\n> > \r\n> > 原始数据在预处理阶段，会在每句话的头部加上 `[CLS]` , 在每句话的 尾部加上 `[SEP]`, 所以 token_embedding 的长度相比原始数据经过 FullTokenizer 进行 token 化后的长度多出2个 token\r\n> \r\n> 好的明白，感谢回复！\r\n> btw确认一下，超过max_seq_len的句子，模型所返回的token embedding的最后一位也是[SEP]对应的embedding吧？\r\n\r\n是的","reactions":{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508028284/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"tianxin1860","id":6829601,"node_id":"MDQ6VXNlcjY4Mjk2MDE=","avatar_url":"https://avatars.githubusercontent.com/u/6829601?v=4","gravatar_id":"","url":"https://api.github.com/users/tianxin1860","html_url":"https://github.com/tianxin1860","followers_url":"https://api.github.com/users/tianxin1860/followers","following_url":"https://api.github.com/users/tianxin1860/following{/other_user}","gists_url":"https://api.github.com/users/tianxin1860/gists{/gist_id}","starred_url":"https://api.github.com/users/tianxin1860/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tianxin1860/subscriptions","organizations_url":"https://api.github.com/users/tianxin1860/orgs","repos_url":"https://api.github.com/users/tianxin1860/repos","events_url":"https://api.github.com/users/tianxin1860/events{/privacy}","received_events_url":"https://api.github.com/users/tianxin1860/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508035659","html_url":"https://github.com/PaddlePaddle/ERNIE/issues/189#issuecomment-508035659","issue_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189","id":508035659,"node_id":"MDEyOklzc3VlQ29tbWVudDUwODAzNTY1OQ==","user":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false},"created_at":"2019-07-03T10:23:16Z","updated_at":"2019-07-03T10:23:16Z","author_association":"NONE","body":"> > > > 在使用[输入句子经过 ERNIE 编码后的 Embedding 表示](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE#faq) 的功能时，在ChnSentiCorp数据集中，我分别打印了test.tsv中每句话的token embedding(即ernie_encoder.py中的unpad_top_layer_emb)的shape，以及使用tokenization.py中FullTokenizer加载./config/vocab.txt后对每句话进行分词的结果，发现模型返回的token embedding的长度全部都比使用FullTokenizer进行分词后的长度多出2个token。\r\n> > > > 理论上来说，模型返回的token embedding中第一维对应的embedding应该是[CLS]的token embedding，这是每句话相较原句分词结果多出来的其中一个token，那多出来的另一个token是什么？它在模型返回的token embedding中位于哪个位置？\r\n> > > > 还是说我的分词方法有问题，不该用FullTokenizer加载./config/vocab.txt来分词？（但是数了一下纯中文的句子的字数，确实全都要比模型返回的token embedding少2个token）\r\n> > > > 希望解答一下我的疑惑，谢谢！\r\n> > > \r\n> > > \r\n> > > 原始数据在预处理阶段，会在每句话的头部加上 `[CLS]` , 在每句话的 尾部加上 `[SEP]`, 所以 token_embedding 的长度相比原始数据经过 FullTokenizer 进行 token 化后的长度多出2个 token\r\n> > \r\n> > \r\n> > 好的明白，感谢回复！\r\n> > btw确认一下，超过max_seq_len的句子，模型所返回的token embedding的最后一位也是[SEP]对应的embedding吧？\r\n> \r\n> 是的\r\n\r\n非常感谢！","reactions":{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/comments/508035659/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false}},{"id":2457086293,"node_id":"MDExOkNsb3NlZEV2ZW50MjQ1NzA4NjI5Mw==","url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/events/2457086293","actor":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2019-07-03T10:23:16Z","state_reason":null,"performed_via_github_app":null}]