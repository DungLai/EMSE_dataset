{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189","repository_url":"https://api.github.com/repos/PaddlePaddle/ERNIE","labels_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189/labels{/name}","comments_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189/comments","events_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189/events","html_url":"https://github.com/PaddlePaddle/ERNIE/issues/189","id":463580886,"node_id":"MDU6SXNzdWU0NjM1ODA4ODY=","number":189,"title":"模型返回的token embedding除了[CLS]还有什么？","user":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-07-03T07:04:46Z","updated_at":"2019-07-03T10:23:17Z","closed_at":"2019-07-03T10:23:16Z","author_association":"NONE","active_lock_reason":null,"body":"在使用[输入句子经过 ERNIE 编码后的 Embedding 表示](https://github.com/PaddlePaddle/LARK/tree/develop/ERNIE#faq) 的功能时，在ChnSentiCorp数据集中，我分别打印了test.tsv中每句话的token embedding(即ernie_encoder.py中的unpad_top_layer_emb)的shape，以及使用tokenization.py中FullTokenizer加载./config/vocab.txt后对每句话进行分词的结果，发现模型返回的token embedding的长度全部都比使用FullTokenizer进行分词后的长度多出2个token。\r\n\r\n理论上来说，模型返回的token embedding中第一维对应的embedding应该是[CLS]的token embedding，这是每句话相较原句分词结果多出来的其中一个token，那多出来的另一个token是什么？它在模型返回的token embedding中位于哪个位置？\r\n\r\n还是说我的分词方法有问题，不该用FullTokenizer加载./config/vocab.txt来分词？（但是数了一下纯中文的句子的字数，确实全都要比模型返回的token embedding少2个token）\r\n\r\n希望解答一下我的疑惑，谢谢！","closed_by":{"login":"bb9696aa","id":14299519,"node_id":"MDQ6VXNlcjE0Mjk5NTE5","avatar_url":"https://avatars.githubusercontent.com/u/14299519?v=4","gravatar_id":"","url":"https://api.github.com/users/bb9696aa","html_url":"https://github.com/bb9696aa","followers_url":"https://api.github.com/users/bb9696aa/followers","following_url":"https://api.github.com/users/bb9696aa/following{/other_user}","gists_url":"https://api.github.com/users/bb9696aa/gists{/gist_id}","starred_url":"https://api.github.com/users/bb9696aa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bb9696aa/subscriptions","organizations_url":"https://api.github.com/users/bb9696aa/orgs","repos_url":"https://api.github.com/users/bb9696aa/repos","events_url":"https://api.github.com/users/bb9696aa/events{/privacy}","received_events_url":"https://api.github.com/users/bb9696aa/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/PaddlePaddle/ERNIE/issues/189/timeline","performed_via_github_app":null,"state_reason":"completed"}