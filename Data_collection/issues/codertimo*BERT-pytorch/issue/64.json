{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/64","id":453880374,"node_id":"MDU6SXNzdWU0NTM4ODAzNzQ=","number":64,"title":"OOM error in cuda while passing large corpus of wikipedia text files ? how to manage big files to train","user":{"login":"MohamedLotfyElrefai","id":45141667,"node_id":"MDQ6VXNlcjQ1MTQxNjY3","avatar_url":"https://avatars.githubusercontent.com/u/45141667?v=4","gravatar_id":"","url":"https://api.github.com/users/MohamedLotfyElrefai","html_url":"https://github.com/MohamedLotfyElrefai","followers_url":"https://api.github.com/users/MohamedLotfyElrefai/followers","following_url":"https://api.github.com/users/MohamedLotfyElrefai/following{/other_user}","gists_url":"https://api.github.com/users/MohamedLotfyElrefai/gists{/gist_id}","starred_url":"https://api.github.com/users/MohamedLotfyElrefai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MohamedLotfyElrefai/subscriptions","organizations_url":"https://api.github.com/users/MohamedLotfyElrefai/orgs","repos_url":"https://api.github.com/users/MohamedLotfyElrefai/repos","events_url":"https://api.github.com/users/MohamedLotfyElrefai/events{/privacy}","received_events_url":"https://api.github.com/users/MohamedLotfyElrefai/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-06-09T09:33:11Z","updated_at":"2019-06-09T09:33:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have used this parameter \r\n\r\n> bert -c /home/ai/LM_fit/bert/bert_pytorch/dataset/wiki_arabic.txt -v \r\n\r\n> /home/ai/LM_fit/bert/bert_pytorch/dataset/wiki_vocab.small -o \r\n\r\n> /home/ai/LM_fit/bert/bert_pytorch/dataset/wiki_model_cpu -hs 240 -l 3 -a 3 -s 30 -b 8 \r\n\r\n> --on_memory False  --with_cuda True -w 4\r\n\r\n`Loading Vocab /home/ai/LM_fit/bert/bert_pytorch/dataset/wiki_vocab.small\r\nVocab Size:  2135556\r\nLoading Train Dataset /home/ai/LM_fit/bert/bert_pytorch/dataset/wiki_arabic.txt\r\nLoading Dataset: 1760404it [00:03, 497901.29it/s]\r\nLoading Test Dataset None\r\nCreating Dataloader\r\nBuilding BERT model\r\nCreating BERT Trainer\r\nTotal Parameters: 1029286598\r\nTraining Start\r\nEP_train:0:   0%|| 0/220051 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"/home/ai/py3.6/bin/bert\", line 11, in <module>\r\n    sys.exit(train())\r\n  File \"/home/ai/py3.6/lib/python3.6/site-packages/bert_pytorch/__main__.py\", line 67, in train\r\n    trainer.train(epoch)\r\n  File \"/home/ai/py3.6/lib/python3.6/site-packages/bert_pytorch/trainer/pretrain.py\", line 81, in train\r\n    self.iteration(epoch, self.train_data)\r\n  File \"/home/ai/py3.6/lib/python3.6/site-packages/bert_pytorch/trainer/pretrain.py\", line 132, in iteration\r\n    loss.backward()\r\n  File \"/home/ai/py3.6/lib/python3.6/site-packages/torch/tensor.py\", line 107, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n  File \"/home/ai/py3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\", line 93, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\nRuntimeError: CUDA out of memory. Tried to allocate 1.91 GiB (GPU 0; 10.92 GiB total capacity; 9.59 GiB already allocated; 230.62 MiB free; 13.23 MiB cached)\r\n`","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/64/timeline","performed_via_github_app":null,"state_reason":null}