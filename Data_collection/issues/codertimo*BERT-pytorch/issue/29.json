{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/29","id":372849644,"node_id":"MDU6SXNzdWUzNzI4NDk2NDQ=","number":29,"title":"when training the masked LM, the unmasked words (have label 0) were trained together with masked words?","user":{"login":"coddinglxf","id":8458182,"node_id":"MDQ6VXNlcjg0NTgxODI=","avatar_url":"https://avatars.githubusercontent.com/u/8458182?v=4","gravatar_id":"","url":"https://api.github.com/users/coddinglxf","html_url":"https://github.com/coddinglxf","followers_url":"https://api.github.com/users/coddinglxf/followers","following_url":"https://api.github.com/users/coddinglxf/following{/other_user}","gists_url":"https://api.github.com/users/coddinglxf/gists{/gist_id}","starred_url":"https://api.github.com/users/coddinglxf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coddinglxf/subscriptions","organizations_url":"https://api.github.com/users/coddinglxf/orgs","repos_url":"https://api.github.com/users/coddinglxf/repos","events_url":"https://api.github.com/users/coddinglxf/events{/privacy}","received_events_url":"https://api.github.com/users/coddinglxf/received_events","type":"User","site_admin":false},"labels":[{"id":1091696787,"node_id":"MDU6TGFiZWwxMDkxNjk2Nzg3","url":"https://api.github.com/repos/codertimo/BERT-pytorch/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1091696791,"node_id":"MDU6TGFiZWwxMDkxNjk2Nzkx","url":"https://api.github.com/repos/codertimo/BERT-pytorch/labels/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2018-10-23T07:35:37Z","updated_at":"2018-10-30T07:47:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"According to the code\r\n\r\n\r\n```\r\n    def random_word(self, sentence):\r\n        tokens = sentence.split()\r\n        output_label = []\r\n\r\n        for i, token in enumerate(tokens):\r\n            prob = random.random()\r\n            if prob < 0.15:\r\n                # 80% randomly change token to make token\r\n                if prob < prob * 0.8:\r\n                    tokens[i] = self.vocab.mask_index\r\n\r\n                # 10% randomly change token to random token\r\n                elif prob * 0.8 <= prob < prob * 0.9:\r\n                    tokens[i] = random.randrange(len(self.vocab))\r\n\r\n                # 10% randomly change token to current token\r\n                elif prob >= prob * 0.9:\r\n                    tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\r\n\r\n                output_label.append(self.vocab.stoi.get(token, self.vocab.unk_index))\r\n\r\n            else:\r\n                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\r\n                output_label.append(0)\r\n\r\n        return tokens, output_label\r\n```\r\n\r\nDo we need to exclude the unmasked words when training the LM?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/29/timeline","performed_via_github_app":null,"state_reason":null}