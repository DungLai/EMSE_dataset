{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/83","id":707092432,"node_id":"MDU6SXNzdWU3MDcwOTI0MzI=","number":83,"title":"transformer.py 中的forword方法调用的SublayerConnection类。实现残差链接和标准化的实现","user":{"login":"dshwei","id":42167236,"node_id":"MDQ6VXNlcjQyMTY3MjM2","avatar_url":"https://avatars.githubusercontent.com/u/42167236?v=4","gravatar_id":"","url":"https://api.github.com/users/dshwei","html_url":"https://github.com/dshwei","followers_url":"https://api.github.com/users/dshwei/followers","following_url":"https://api.github.com/users/dshwei/following{/other_user}","gists_url":"https://api.github.com/users/dshwei/gists{/gist_id}","starred_url":"https://api.github.com/users/dshwei/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dshwei/subscriptions","organizations_url":"https://api.github.com/users/dshwei/orgs","repos_url":"https://api.github.com/users/dshwei/repos","events_url":"https://api.github.com/users/dshwei/events{/privacy}","received_events_url":"https://api.github.com/users/dshwei/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-09-23T06:42:01Z","updated_at":"2020-10-16T08:27:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"sublayerout =  layerNorm(x +sublayer(x))\r\n**首先是残差链接然后是层标准化**\r\n在你代码中：sublayer.py中 应该是\r\n     def forward(self, x, sublayer):\r\n        \"Apply residual connection to any sublayer with the same size.\"\r\n              # return x + self.dropout(sublayer(self.norm(x)))\r\n        return self.norm( x + self.dropout(sublayer(x)))\r\n\r\ntranformer.py中：\r\n    def forward(self, x, mask):\r\n             x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\r\n            x = self.output_sublayer(x, **lambda _x: self.feed_forward.forward(_x))**\r\n        return self.dropout(x)\r\n\r\n此处我对论文立即额和你不一样，有错误的地方请指教","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83/reactions","total_count":2,"+1":0,"-1":0,"laugh":2,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/83/timeline","performed_via_github_app":null,"state_reason":null}