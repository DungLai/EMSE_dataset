{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/60","id":430182878,"node_id":"MDU6SXNzdWU0MzAxODI4Nzg=","number":60,"title":"self.d_k = d_model // h gives 64 dimension ?","user":{"login":"BerenLuthien","id":20260688,"node_id":"MDQ6VXNlcjIwMjYwNjg4","avatar_url":"https://avatars.githubusercontent.com/u/20260688?v=4","gravatar_id":"","url":"https://api.github.com/users/BerenLuthien","html_url":"https://github.com/BerenLuthien","followers_url":"https://api.github.com/users/BerenLuthien/followers","following_url":"https://api.github.com/users/BerenLuthien/following{/other_user}","gists_url":"https://api.github.com/users/BerenLuthien/gists{/gist_id}","starred_url":"https://api.github.com/users/BerenLuthien/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BerenLuthien/subscriptions","organizations_url":"https://api.github.com/users/BerenLuthien/orgs","repos_url":"https://api.github.com/users/BerenLuthien/repos","events_url":"https://api.github.com/users/BerenLuthien/events{/privacy}","received_events_url":"https://api.github.com/users/BerenLuthien/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-04-07T19:31:09Z","updated_at":"2019-11-06T09:45:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"https://github.com/codertimo/BERT-pytorch/blob/d10dc4f9d5a6f2ca74380f62039526eb7277c671/bert_pytorch/model/attention/multi_head.py#L15\r\n\r\nLooks that  **self.d_k = d_model // h ---> embed size 768 dividing number of heads 12 = 64**\r\n```\r\n        self.d_k = d_model // h # 64\r\n        self.h = h # 12 heads\r\n        # 1) Do all the linear projections in batch from d_model => h x d_k\r\n        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\r\n                             for l, x in zip(self.linear_layers, (query, key, value))]\r\n```\r\nwhy convert 768 dimensional [q,v,k] into 64 dimension embedding ? \r\n\r\n\r\n**Reference**:\r\nhttp://nlp.seas.harvard.edu/2018/04/03/attention.html\r\nI put some comments on the shape:\r\n```\r\nclass MultiHeadedAttention(nn.Module): # d_model=512, h=8\r\n    def __init__(self, h, d_model, dropout=0.1):\r\n        \"Take in model size and number of heads.\"\r\n        super(MultiHeadedAttention, self).__init__()\r\n        assert d_model % h == 0\r\n        # We assume d_v always equals d_k\r\n        self.d_k = d_model // h # 512//8=64\r\n        self.h = h # 8\r\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\r\n        self.attn = None\r\n        self.dropout = nn.Dropout(p=dropout)\r\n        \r\n    def forward(self, query, key, value, mask=None):\r\n        \"Implements Figure 2\"\r\n        if mask is not None:\r\n            # Same mask applied to all h heads.\r\n            mask = mask.unsqueeze(1)\r\n        nbatches = query.size(0)\r\n        \r\n        # 1) Do all the linear projections in batch from d_model => h x d_k \r\n        query, key, value = \\\r\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\r\n             for l, x in zip(self.linears, (query, key, value))]\r\n        \r\n        # 2) Apply attention on all the projected vectors in batch. \r\n        x, self.attn = attention(query, key, value, mask=mask, \r\n                                 dropout=self.dropout)\r\n        \r\n        # 3) \"Concat\" using a view and apply a final linear. \r\n        x = x.transpose(1, 2).contiguous() \\\r\n             .view(nbatches, -1, self.h * self.d_k) # (nbatches, -1, 512)\r\n        return self.linears[-1](x)\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/60/timeline","performed_via_github_app":null,"state_reason":null}