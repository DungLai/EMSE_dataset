{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/94","id":1045824609,"node_id":"I_kwDOCSBSd84-VgRh","number":94,"title":"It keeps trying to use CUDA despite --with_cuda False option","user":{"login":"hanyangii","id":13743051,"node_id":"MDQ6VXNlcjEzNzQzMDUx","avatar_url":"https://avatars.githubusercontent.com/u/13743051?v=4","gravatar_id":"","url":"https://api.github.com/users/hanyangii","html_url":"https://github.com/hanyangii","followers_url":"https://api.github.com/users/hanyangii/followers","following_url":"https://api.github.com/users/hanyangii/following{/other_user}","gists_url":"https://api.github.com/users/hanyangii/gists{/gist_id}","starred_url":"https://api.github.com/users/hanyangii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hanyangii/subscriptions","organizations_url":"https://api.github.com/users/hanyangii/orgs","repos_url":"https://api.github.com/users/hanyangii/repos","events_url":"https://api.github.com/users/hanyangii/events{/privacy}","received_events_url":"https://api.github.com/users/hanyangii/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-05T12:59:25Z","updated_at":"2021-11-05T12:59:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\n\r\nI have tried to run _bert_ with _--with_cuda False_, but the model keeps running \"forward\" function on cuda. These are my command line and the error message I got. \r\n\r\n_bert -c corpus.small -v vocab.small -o bert.model --with_cuda False -e 5_\r\n\r\nLoading Vocab vocab.small\r\nVocab Size:  262\r\nLoading Train Dataset corpus.small\r\nLoading Dataset: 113it [00:00, 560232.09it/s]\r\nLoading Test Dataset None\r\nCreating Dataloader\r\nBuilding BERT model\r\nCreating BERT Trainer\r\nTotal Parameters: 6453768\r\nTraining Start\r\nEP_train:0:   0%|| 0/2 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/yuni/anaconda3/envs/py3/bin/bert\", line 8, in <module>\r\n    sys.exit(train())\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/__main__.py\", line 67, in train\r\n    trainer.train(epoch)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/trainer/pretrain.py\", line 69, in train\r\n    self.iteration(epoch, self.train_data)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/trainer/pretrain.py\", line 102, in iteration\r\n    next_sent_output, mask_lm_output = self.model.forward(data[\"bert_input\"], data[\"segment_label\"])\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/language_model.py\", line 24, in forward\r\n    x = self.bert(x, segment_label)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/bert.py\", line 46, in forward\r\n    x = transformer.forward(x, mask)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/transformer.py\", line 29, in forward\r\n    x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/utils/sublayer.py\", line 18, in forward\r\n    return x + self.dropout(sublayer(self.norm(x)))\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/transformer.py\", line 29, in <lambda>\r\n    x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/attention/multi_head.py\", line 32, in forward\r\n    x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\r\n    return forward_call(*input, **kwargs)\r\n  File \"/home/yuni/anaconda3/envs/py3/lib/python3.6/site-packages/bert_pytorch/model/attention/single.py\", line 25, in forward\r\n    return torch.matmul(p_attn, value), p_attn\r\n**RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 1.95 GiB total capacity; 309.18 MiB already allocated; 125.62 MiB free; 312.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF**\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/94/timeline","performed_via_github_app":null,"state_reason":null}