{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/67","id":466509092,"node_id":"MDU6SXNzdWU0NjY1MDkwOTI=","number":67,"title":"ONNX conversion: TransformerBlock problem","user":{"login":"jbmaxwell","id":15166432,"node_id":"MDQ6VXNlcjE1MTY2NDMy","avatar_url":"https://avatars.githubusercontent.com/u/15166432?v=4","gravatar_id":"","url":"https://api.github.com/users/jbmaxwell","html_url":"https://github.com/jbmaxwell","followers_url":"https://api.github.com/users/jbmaxwell/followers","following_url":"https://api.github.com/users/jbmaxwell/following{/other_user}","gists_url":"https://api.github.com/users/jbmaxwell/gists{/gist_id}","starred_url":"https://api.github.com/users/jbmaxwell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jbmaxwell/subscriptions","organizations_url":"https://api.github.com/users/jbmaxwell/orgs","repos_url":"https://api.github.com/users/jbmaxwell/repos","events_url":"https://api.github.com/users/jbmaxwell/events{/privacy}","received_events_url":"https://api.github.com/users/jbmaxwell/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-07-10T20:40:13Z","updated_at":"2019-07-10T20:40:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm wondering if there's any workaround for an error when trying to convert BERT-pytorch to ONNX. The problem occurs when trying to trace through the TransformerBlock. I'm wondering if there's a way to rewrite the `forward()` to get around this error? \r\n\r\n```\r\ndef forward(self, x, mask):\r\n        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask)) // <--- Error happens here!\r\n        x = self.output_sublayer(x, self.feed_forward)\r\n        return self.dropout(x)\r\n```\r\n\r\nThe error is:\r\n\r\n\r\n```\r\nbuiltins.ValueError: Auto nesting doesn't know how to process an input object of type bert_pytorch.model.transformer.TransformerBlock.forward.<locals>.<lambda>. Accepted types: Tensors, or lists/tuples of them\r\n```\r\nThe trace can't handle the type of the Lambda, so I'm wondering if rewriting without the Lambda would fix it? (Sorry, but I'm not good enough with Python yet to know how to do that without breaking anything.)","closed_by":null,"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/67/timeline","performed_via_github_app":null,"state_reason":null}