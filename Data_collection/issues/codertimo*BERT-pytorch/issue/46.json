{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46","repository_url":"https://api.github.com/repos/codertimo/BERT-pytorch","labels_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46/labels{/name}","comments_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46/comments","events_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46/events","html_url":"https://github.com/codertimo/BERT-pytorch/issues/46","id":380967552,"node_id":"MDU6SXNzdWUzODA5Njc1NTI=","number":46,"title":"Attention maybe changed","user":{"login":"soobinseo","id":18044285,"node_id":"MDQ6VXNlcjE4MDQ0Mjg1","avatar_url":"https://avatars.githubusercontent.com/u/18044285?v=4","gravatar_id":"","url":"https://api.github.com/users/soobinseo","html_url":"https://github.com/soobinseo","followers_url":"https://api.github.com/users/soobinseo/followers","following_url":"https://api.github.com/users/soobinseo/following{/other_user}","gists_url":"https://api.github.com/users/soobinseo/gists{/gist_id}","starred_url":"https://api.github.com/users/soobinseo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/soobinseo/subscriptions","organizations_url":"https://api.github.com/users/soobinseo/orgs","repos_url":"https://api.github.com/users/soobinseo/repos","events_url":"https://api.github.com/users/soobinseo/events{/privacy}","received_events_url":"https://api.github.com/users/soobinseo/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2018-11-15T02:05:00Z","updated_at":"2018-11-26T08:31:00Z","closed_at":"2018-11-26T08:31:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, Thanks for your great job.\r\nI wonder that the attention mechanism of your code seems to be changed.\r\nThe shape of attention vector should be (**batch, timestep, timestep)**, but according to your code, the shape of self attention vector is (**batch, timestep, hidden_size**). There is new code that I fixed below.  Please review it and appreciate your comments. Thank you.\r\n\r\n`\r\nclass Attention(nn.Module):\r\n    def __init__(self, num_hidden, h=8):\r\n        super(Attention, self).__init__()\r\n        \r\n        self.num_hidden_per_attn = num_hidden // h\r\n        self.h = h\r\n        \r\n        self.key = nn.Linear(num_hidden, num_hidden)\r\n        self.value = nn.Linear(num_hidden, num_hidden)\r\n        self.query = nn.Linear(num_hidden, num_hidden)\r\n        \r\n        self.layer_norm_1 = LayerNorm(num_hidden)\r\n        self.layer_norm_2 = LayerNorm(num_hidden)\r\n        self.out_linear = nn.Linear(num_hidden, num_hidden)\r\n        \r\n        self.dropout = nn.Dropout(p=0.1)\r\n        \r\n    def forward(self, input_):\r\n        batch_size = input_.size(0)\r\n        \r\n        key = F.relu(self.key(input_))\r\n        value = F.relu(self.value(input_))\r\n        query = F.relu(self.query(input_))\r\n        \r\n        key, value, query = list(map(lambda x: x.view(batch_size, -1, self.h, self.num_hidden_per_attn), (key, value, query)))\r\n        params = [(key[:,:,i,:], value[:,:,i,:], query[:,:,i,:]) for i in range(self.h)]\r\n\r\n        _attn = list(map(self._multihead, params))\r\n        attn = list(map(lambda x: x[0], _attn))\r\n        probs = list(map(lambda x: x[1], _attn))\r\n        result = t.cat(attn, -1)\r\n\r\n        result = self.dropout(result)\r\n        result = result.view(batch_size, -1, self.h * self.num_hidden_per_attn)\r\n        \r\n        # residual connection\r\n        result = self.layer_norm_1(F.relu(input_ + result))\r\n        \r\n        out = self.out_linear(result)\r\n        out = self.layer_norm_2(F.relu(result + out))\r\n        \r\n        return result, probs\r\n    \r\n    def _multihead(self, params):\r\n\r\n        key, value, query = params[0], params[1], params[2]\r\n\r\n        attn = t.bmm(query, key.transpose(1,2)) / math.sqrt(key.shape[-1])\r\n\r\n        attn = F.softmax(attn, dim=-1)\r\n        result = t.bmm(attn, value)\r\n\r\n        return result, attn\r\n`\r\n\r\n\r\n","closed_by":{"login":"soobinseo","id":18044285,"node_id":"MDQ6VXNlcjE4MDQ0Mjg1","avatar_url":"https://avatars.githubusercontent.com/u/18044285?v=4","gravatar_id":"","url":"https://api.github.com/users/soobinseo","html_url":"https://github.com/soobinseo","followers_url":"https://api.github.com/users/soobinseo/followers","following_url":"https://api.github.com/users/soobinseo/following{/other_user}","gists_url":"https://api.github.com/users/soobinseo/gists{/gist_id}","starred_url":"https://api.github.com/users/soobinseo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/soobinseo/subscriptions","organizations_url":"https://api.github.com/users/soobinseo/orgs","repos_url":"https://api.github.com/users/soobinseo/repos","events_url":"https://api.github.com/users/soobinseo/events{/privacy}","received_events_url":"https://api.github.com/users/soobinseo/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/codertimo/BERT-pytorch/issues/46/timeline","performed_via_github_app":null,"state_reason":"completed"}