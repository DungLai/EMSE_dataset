{"url":"https://api.github.com/repos/huggingface/hmtl/issues/15","repository_url":"https://api.github.com/repos/huggingface/hmtl","labels_url":"https://api.github.com/repos/huggingface/hmtl/issues/15/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/hmtl/issues/15/comments","events_url":"https://api.github.com/repos/huggingface/hmtl/issues/15/events","html_url":"https://github.com/huggingface/hmtl/issues/15","id":445729517,"node_id":"MDU6SXNzdWU0NDU3Mjk1MTc=","number":15,"title":"Issue with only train data being used for vocab creation","user":{"login":"rangwani-harsh","id":15148765,"node_id":"MDQ6VXNlcjE1MTQ4NzY1","avatar_url":"https://avatars.githubusercontent.com/u/15148765?v=4","gravatar_id":"","url":"https://api.github.com/users/rangwani-harsh","html_url":"https://github.com/rangwani-harsh","followers_url":"https://api.github.com/users/rangwani-harsh/followers","following_url":"https://api.github.com/users/rangwani-harsh/following{/other_user}","gists_url":"https://api.github.com/users/rangwani-harsh/gists{/gist_id}","starred_url":"https://api.github.com/users/rangwani-harsh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rangwani-harsh/subscriptions","organizations_url":"https://api.github.com/users/rangwani-harsh/orgs","repos_url":"https://api.github.com/users/rangwani-harsh/repos","events_url":"https://api.github.com/users/rangwani-harsh/events{/privacy}","received_events_url":"https://api.github.com/users/rangwani-harsh/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-05-18T16:03:42Z","updated_at":"2019-05-18T18:00:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi team,\r\nThanks for this wonderful repo . The code in the repo is generic and can easily be reused. I wanted to ask that during creation of the vocab in all the models only training tokens are being used.\r\n```\r\n\"datasets_for_vocab_creation\": [\"train\"]\r\n```\r\nSo in cases when we are using the multitask model we have a large coverage of tokens as we have a large vocab that consists of tokens from all datasets. So there is a high probability of test token to be found in that vocab. Whereas in case of using only single model the vocab size is less and there is a large chance of a token being OOV (Out of Vocab). \r\nSo how do we make sure that the improvements are due to multitask learning rather then due to large coverage of vocabulary in case of multitask learning?\r\n\r\nThe other point was that if we only consider vocab made from training data we make our model work well on only tokens that are present in training data which makes us loose important token information that is present in the word embeddings for those tokens which are not present in the training data.\r\n\r\nIt would be great to hear your thoughts on it.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/huggingface/hmtl/issues/15/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/hmtl/issues/15/timeline","performed_via_github_app":null,"state_reason":null}