{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/499","repository_url":"https://api.github.com/repos/ultralytics/yolov3","labels_url":"https://api.github.com/repos/ultralytics/yolov3/issues/499/labels{/name}","comments_url":"https://api.github.com/repos/ultralytics/yolov3/issues/499/comments","events_url":"https://api.github.com/repos/ultralytics/yolov3/issues/499/events","html_url":"https://github.com/ultralytics/yolov3/issues/499","id":493706220,"node_id":"MDU6SXNzdWU0OTM3MDYyMjA=","number":499,"title":"Training was interrupted after the first epoch","user":{"login":"TOMLEUNGKS","id":51043693,"node_id":"MDQ6VXNlcjUxMDQzNjkz","avatar_url":"https://avatars.githubusercontent.com/u/51043693?v=4","gravatar_id":"","url":"https://api.github.com/users/TOMLEUNGKS","html_url":"https://github.com/TOMLEUNGKS","followers_url":"https://api.github.com/users/TOMLEUNGKS/followers","following_url":"https://api.github.com/users/TOMLEUNGKS/following{/other_user}","gists_url":"https://api.github.com/users/TOMLEUNGKS/gists{/gist_id}","starred_url":"https://api.github.com/users/TOMLEUNGKS/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TOMLEUNGKS/subscriptions","organizations_url":"https://api.github.com/users/TOMLEUNGKS/orgs","repos_url":"https://api.github.com/users/TOMLEUNGKS/repos","events_url":"https://api.github.com/users/TOMLEUNGKS/events{/privacy}","received_events_url":"https://api.github.com/users/TOMLEUNGKS/received_events","type":"User","site_admin":false},"labels":[{"id":1035696370,"node_id":"MDU6TGFiZWwxMDM1Njk2Mzcw","url":"https://api.github.com/repos/ultralytics/yolov3/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2019-09-15T07:36:47Z","updated_at":"2019-09-18T10:09:45Z","closed_at":"2019-09-18T10:09:45Z","author_association":"NONE","active_lock_reason":null,"body":"I am using AWS EC2 to train a custom model with my own dataset (6 classes, 12594 images in total). After the first epoch, the training was interrupted. And here is the error log:\r\n\r\n\r\n[ec2-user@ip-172-31-4-237 yolo_retrain_v2]$ python3 train.py --cfg cfg/yolov3.cfg --weights weights/yolov3.weights --epochs 500\r\nFontconfig warning: ignoring UTF-8: not a valid region tag\r\nNamespace(accumulate=2, adam=False, arc='defaultpw', batch_size=32, bucket='', cache_images=False, cfg='cfg/yolov3.cfg', data='data/coco.data', device='', epochs=500, evolve=False, img_size=416, img_weights=False, multi_scale=False, name='', nosave=False, notest=False, prebias=False, rect=False, resume=False, transfer=False, var=None, weights='weights/yolov3.weights')\r\nUsing CUDA device0 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device1 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device2 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device3 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device4 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device5 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device6 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n           device7 _CudaDeviceProperties(name='Tesla K80', total_memory=11441MB)\r\n\r\nReading labels (12594 found, 0 missing, 0 empty for 12594 images): 100%|##########################################################| 12594/12594 [00:00<00:00, 16385.53it/s]\r\nModel Summary: 222 layers, 6.15507e+07 parameters, 6.15507e+07 gradients\r\nStarting training for 500 epochs...\r\n\r\n     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\r\n     0/499     2.95G      1.55      2.87      13.7      18.1        18       416: 100%|##################################################| 394/394 [07:04<00:00,  1.08s/it]\r\n               Class    Images   Targets         P         R       mAP        F1: 100%|######################################################| 2/2 [00:06<00:00,  3.04s/it]\r\n                 all        60        64     0.503     0.896     0.827     0.614\r\n\r\n     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\r\n  0%|                                                                                                                                              | 0/394 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"train.py\", line 415, in <module>\r\n    train()  # train normally\r\n  File \"train.py\", line 261, in train\r\n    pred = model(imgs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/nn/parallel/distributed.py\", line 459, in forward\r\n    self.reducer.prepare_for_backward([])\r\nRuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by (1) passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`; (2) making sure all `forward` function outputs participate in calculating loss. If you already have done the above two steps, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable). (prepare_for_backward at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:518)\r\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x33 (0x7f08ce370273 in /home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/lib/libc10.so)\r\nframe #1: c10d::Reducer::prepare_for_backward(std::vector<torch::autograd::Variable, std::allocator<torch::autograd::Variable> > const&) + 0x734 (0x7f0918c3c9e4 in /home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #2: <unknown function> + 0x691a4c (0x7f0918c2ba4c in /home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\nframe #3: <unknown function> + 0x1d3ef4 (0x7f091876def4 in /home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\r\n<omitting python frames>\r\nframe #33: __libc_start_main + 0xf5 (0x7f092c5af445 in /lib64/libc.so.6)\r\n\r\nException in thread Thread-3172:\r\nTraceback (most recent call last):\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/pin_memory.py\", line 21, in _pin_memory_loop\r\n    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 113, in get\r\n    return _ForkingPickler.loads(res)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 284, in rebuild_storage_fd\r\n    fd = df.detach()\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\r\n    with _resource_sharer.get_connection(self._id) as conn:\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\r\n    c = Client(address, authkey=process.current_process().authkey)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\r\n    answer_challenge(c, authkey)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 732, in answer_challenge\r\n    message = connection.recv_bytes(256)         # reject large message\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/home/ec2-user/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\r\n    chunk = read(handle, remaining)\r\nConnectionResetError: [Errno 104] Connection reset by peer\r\n\r\n\r\nMay I know how this error can be fixed? I tried --nosave, but the error was the same. Thank you so much for your generous help!\r\n\r\nRegards,\r\nTom\r\n","closed_by":{"login":"glenn-jocher","id":26833433,"node_id":"MDQ6VXNlcjI2ODMzNDMz","avatar_url":"https://avatars.githubusercontent.com/u/26833433?v=4","gravatar_id":"","url":"https://api.github.com/users/glenn-jocher","html_url":"https://github.com/glenn-jocher","followers_url":"https://api.github.com/users/glenn-jocher/followers","following_url":"https://api.github.com/users/glenn-jocher/following{/other_user}","gists_url":"https://api.github.com/users/glenn-jocher/gists{/gist_id}","starred_url":"https://api.github.com/users/glenn-jocher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/glenn-jocher/subscriptions","organizations_url":"https://api.github.com/users/glenn-jocher/orgs","repos_url":"https://api.github.com/users/glenn-jocher/repos","events_url":"https://api.github.com/users/glenn-jocher/events{/privacy}","received_events_url":"https://api.github.com/users/glenn-jocher/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/499/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ultralytics/yolov3/issues/499/timeline","performed_via_github_app":null,"state_reason":"completed"}