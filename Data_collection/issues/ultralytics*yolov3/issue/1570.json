{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570","repository_url":"https://api.github.com/repos/ultralytics/yolov3","labels_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570/labels{/name}","comments_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570/comments","events_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570/events","html_url":"https://github.com/ultralytics/yolov3/issues/1570","id":751821975,"node_id":"MDU6SXNzdWU3NTE4MjE5NzU=","number":1570,"title":"Train Custom Data Tutorial ðŸŒŸ","user":{"login":"glenn-jocher","id":26833433,"node_id":"MDQ6VXNlcjI2ODMzNDMz","avatar_url":"https://avatars.githubusercontent.com/u/26833433?v=4","gravatar_id":"","url":"https://api.github.com/users/glenn-jocher","html_url":"https://github.com/glenn-jocher","followers_url":"https://api.github.com/users/glenn-jocher/followers","following_url":"https://api.github.com/users/glenn-jocher/following{/other_user}","gists_url":"https://api.github.com/users/glenn-jocher/gists{/gist_id}","starred_url":"https://api.github.com/users/glenn-jocher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/glenn-jocher/subscriptions","organizations_url":"https://api.github.com/users/glenn-jocher/orgs","repos_url":"https://api.github.com/users/glenn-jocher/repos","events_url":"https://api.github.com/users/glenn-jocher/events{/privacy}","received_events_url":"https://api.github.com/users/glenn-jocher/received_events","type":"User","site_admin":false},"labels":[{"id":1035696372,"node_id":"MDU6TGFiZWwxMDM1Njk2Mzcy","url":"https://api.github.com/repos/ultralytics/yolov3/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1295180840,"node_id":"MDU6TGFiZWwxMjk1MTgwODQw","url":"https://api.github.com/repos/ultralytics/yolov3/labels/tutorial","name":"tutorial","color":"3cba0b","default":false,"description":"Tutorial or example"}],"state":"open","locked":false,"assignee":{"login":"glenn-jocher","id":26833433,"node_id":"MDQ6VXNlcjI2ODMzNDMz","avatar_url":"https://avatars.githubusercontent.com/u/26833433?v=4","gravatar_id":"","url":"https://api.github.com/users/glenn-jocher","html_url":"https://github.com/glenn-jocher","followers_url":"https://api.github.com/users/glenn-jocher/followers","following_url":"https://api.github.com/users/glenn-jocher/following{/other_user}","gists_url":"https://api.github.com/users/glenn-jocher/gists{/gist_id}","starred_url":"https://api.github.com/users/glenn-jocher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/glenn-jocher/subscriptions","organizations_url":"https://api.github.com/users/glenn-jocher/orgs","repos_url":"https://api.github.com/users/glenn-jocher/repos","events_url":"https://api.github.com/users/glenn-jocher/events{/privacy}","received_events_url":"https://api.github.com/users/glenn-jocher/received_events","type":"User","site_admin":false},"assignees":[{"login":"glenn-jocher","id":26833433,"node_id":"MDQ6VXNlcjI2ODMzNDMz","avatar_url":"https://avatars.githubusercontent.com/u/26833433?v=4","gravatar_id":"","url":"https://api.github.com/users/glenn-jocher","html_url":"https://github.com/glenn-jocher","followers_url":"https://api.github.com/users/glenn-jocher/followers","following_url":"https://api.github.com/users/glenn-jocher/following{/other_user}","gists_url":"https://api.github.com/users/glenn-jocher/gists{/gist_id}","starred_url":"https://api.github.com/users/glenn-jocher/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/glenn-jocher/subscriptions","organizations_url":"https://api.github.com/users/glenn-jocher/orgs","repos_url":"https://api.github.com/users/glenn-jocher/repos","events_url":"https://api.github.com/users/glenn-jocher/events{/privacy}","received_events_url":"https://api.github.com/users/glenn-jocher/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2020-11-26T20:51:00Z","updated_at":"2022-02-03T17:57:09Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"ðŸš€  This guide explains how to train your own **custom dataset** with YOLOv3.\r\n\r\n## Before You Start\r\n\r\nClone this repo, download tutorial dataset, and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) dependencies, including **Python>=3.8** and **PyTorch>=1.7**.\r\n\r\n```bash\r\n$ git clone https://github.com/ultralytics/yolov3  # clone repo\r\n$ cd yolov3\r\n$ pip install -r requirements.txt  # install dependencies\r\n```\r\n\r\n## Train On Custom Data\r\n\r\n### 1. Create dataset.yaml\r\n\r\n[COCO128](https://www.kaggle.com/ultralytics/coco128) is a small tutorial dataset composed of the first 128 images in [COCO](http://cocodataset.org/#home) train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. [data/coco128.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), shown below, is the dataset configuration file that defines 1) an optional download command/URL for auto-downloading, 2) a path to a directory of training images (or path to a *.txt file with a list of training images), 3) the same for our validation images, 4) the number of classes, 5) a list of class names:\r\n```yaml\r\n# download command/URL (optional)\r\ndownload: https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\r\n\r\n# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\r\ntrain: ../coco128/images/train2017/\r\nval: ../coco128/images/train2017/\r\n\r\n# number of classes\r\nnc: 80\r\n\r\n# class names\r\nnames: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\r\n        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\r\n        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\r\n        'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\r\n        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\r\n        'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\r\n        'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', \r\n        'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \r\n        'teddy bear', 'hair drier', 'toothbrush']\r\n```\r\n\r\n\r\n### 2. Create Labels\r\n\r\nAfter using a tool like [CVAT](https://github.com/opencv/cvat), [makesense.ai](https://www.makesense.ai/) or [Labelbox](https://labelbox.com/)  to label your images, export your labels to **YOLO format**, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The `*.txt` file specifications are:\r\n\r\n- One row per object\r\n- Each row is `class x_center y_center width height` format.\r\n- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels, divide `x_center` and `width` by image width, and `y_center` and `height` by image height.\r\n- Class numbers are zero-indexed (start from 0).\r\n\r\n<img width=\"800\" alt=\"Image Labels\" src=\"https://user-images.githubusercontent.com/26833433/91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg\">\r\n\r\nThe label file corresponding to the above image contains 2 persons (class `0`) and a tie (class `27`):\r\n\r\n<p align=\"center\"><img width=\"428\" src=\"https://user-images.githubusercontent.com/26833433/112467037-d2568c00-8d66-11eb-8796-55402ac0d62f.png\"></p>\r\n\r\n\r\n### 3. Organize Directories\r\n\r\nOrganize your train and val images and labels according to the example below. In this example we assume `/coco128` is **next to** the `/yolov3` directory. **YOLOv3 locates labels automatically for each image** by replacing the last instance of `/images/` in each image path with `/labels/`. For example: \r\n```bash\r\ndataset/images/im0.jpg  # image\r\ndataset/labels/im0.txt  # label\r\n```\r\n\r\n<p align=\"center\"><img width=\"698\" src=\"https://user-images.githubusercontent.com/26833433/112467887-e18a0980-8d67-11eb-93af-6505620ff8aa.png\"></p>\r\n\r\n### 4. Select a Model\r\n\r\nSelect a pretrained model to start training from. Here we select [YOLOv3](https://github.com/ultralytics/yolov3/blob/master/models/yolov3.yaml), the smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov3#pretrained-checkpoints) for a full comparison of all models.\r\n\r\n<img width=\"800\" alt=\"YOLOv3 Models\" src=\"https://user-images.githubusercontent.com/26833433/100390612-4e4afc80-3031-11eb-9555-439cb07f455e.png\">\r\n\r\n### 5. Train\r\n\r\nTrain a YOLOv3 model on COCO128 by specifying dataset, batch-size, image size and either pretrained `--weights yolov3.pt` (recommended), or randomly initialized `--weights '' --cfg yolov3.yaml` (not recommended). Pretrained weights are auto-downloaded from the [latest YOLOv3 release](https://github.com/ultralytics/yolov3/releases).\r\n\r\n```bash\r\n# Train YOLOv3 on COCO128 for 5 epochs\r\n$ python train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov3.pt\r\n```\r\n\r\nAll training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc. For more details see the Training section of our Google Colab Notebook. <a href=\"https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov3\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n\r\n\r\n## Visualize\r\n\r\n### Weights & Biases Logging (ðŸš€ NEW)\r\n\r\n[Weights & Biases](https://www.wandb.com/) (W&B) is now integrated with YOLOv3 for real-time visualization and cloud logging of training runs. This allows for better run comparison and introspection, as well improved visibility and collaboration among team members. To enable W&B logging install `wandb`, and then train normally (you will be guided setup on first use).\r\n```bash\r\n$ pip install wandb\r\n```\r\n\r\nDuring training you will see live updates at [https://www.wandb.com/](https://www.wandb.com/), and you can create [Detailed Reports](https://wandb.ai/glenn-jocher/yolov5_tutorial/reports/YOLOv5-COCO128-Tutorial-Results--VmlldzozMDI5OTY) of your results using the W&B Reports tool.\r\n\r\n<p align=\"center\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/112469341-a8eb2f80-8d69-11eb-959a-dd85d3997bcf.jpg\"></p>\r\n\r\n\r\n### Local Logging\r\n\r\nAll results are logged by default to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc. View train and test jpgs to see mosaics, labels, predictions and augmentation effects. Note a **Mosaic Dataloader** is used for training (shown below), a new concept developed by Ultralytics and first featured in [YOLOv4](https://arxiv.org/abs/2004.10934).\r\n\r\n\r\n`train_batch0.jpg` shows train batch 0 mosaics and labels:  \r\n> <img src=\"https://user-images.githubusercontent.com/26833433/83667642-90fcb200-a583-11ea-8fa3-338bbf7da194.jpeg\" width=\"600\">\r\n\r\n`test_batch0_labels.jpg` shows test batch 0 labels:  \r\n> <img src=\"https://user-images.githubusercontent.com/26833433/83667626-8c37fe00-a583-11ea-997b-0923fe59b29b.jpeg\" width=\"600\">\r\n\r\n`test_batch0_pred.jpg` shows test batch 0 _predictions_:  \r\n> <img src=\"https://user-images.githubusercontent.com/26833433/83667635-90641b80-a583-11ea-8075-606316cebb9c.jpeg\" width=\"600\">\r\n\r\n\r\nTraining losses and performance metrics are also logged to [Tensorboard](https://www.tensorflow.org/tensorboard) and a custom `results.txt` logfile which is plotted as `results.png` (below) after training completes. Here we show YOLOv3 trained on COCO128 to 300 epochs, starting from scratch (blue), and from pretrained `--weights yolov3.pt` (orange).\r\n\r\n```python\r\nfrom utils.plots import plot_results \r\nplot_results(save_dir='runs/train/exp')  # plot results.txt as results.png\r\n```\r\n<img src=\"https://user-images.githubusercontent.com/26833433/97808309-8182b180-1c66-11eb-8461-bffe1a79511d.png\" width=\"800\">\r\n\r\n\r\n## Environments\r\n\r\nYOLOv3 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\r\n\r\n- **Google Colab and Kaggle** notebooks with free GPU: <a href=\"https://colab.research.google.com/github/ultralytics/yolov3/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov3\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\r\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/GCP-Quickstart)\r\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://github.com/ultralytics/yolov5/wiki/AWS-Quickstart)\r\n- **Docker Image**. See [Docker Quickstart Guide](https://github.com/ultralytics/yolov3/wiki/Docker-Quickstart) <a href=\"https://hub.docker.com/r/ultralytics/yolov3\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov3?logo=docker\" alt=\"Docker Pulls\"></a>\r\n\r\n\r\n## Status\r\n\r\n![CI CPU testing](https://github.com/ultralytics/yolov3/workflows/CI%20CPU%20testing/badge.svg)\r\n\r\nIf this badge is green, all [YOLOv3 GitHub Actions](https://github.com/ultralytics/yolov3/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv3 training ([train.py](https://github.com/ultralytics/yolov3/blob/master/train.py)), testing ([test.py](https://github.com/ultralytics/yolov3/blob/master/test.py)), inference ([detect.py](https://github.com/ultralytics/yolov3/blob/master/detect.py)) and export ([export.py](https://github.com/ultralytics/yolov3/blob/master/models/export.py)) on MacOS, Windows, and Ubuntu every 24 hours and on every commit.\r\n","closed_by":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1570/timeline","performed_via_github_app":null,"state_reason":null}