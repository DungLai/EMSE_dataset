{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699","repository_url":"https://api.github.com/repos/ultralytics/yolov3","labels_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699/labels{/name}","comments_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699/comments","events_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699/events","html_url":"https://github.com/ultralytics/yolov3/issues/1699","id":818176198,"node_id":"MDU6SXNzdWU4MTgxNzYxOTg=","number":1699,"title":"CUDA out of memory","user":{"login":"ardeal","id":6227348,"node_id":"MDQ6VXNlcjYyMjczNDg=","avatar_url":"https://avatars.githubusercontent.com/u/6227348?v=4","gravatar_id":"","url":"https://api.github.com/users/ardeal","html_url":"https://github.com/ardeal","followers_url":"https://api.github.com/users/ardeal/followers","following_url":"https://api.github.com/users/ardeal/following{/other_user}","gists_url":"https://api.github.com/users/ardeal/gists{/gist_id}","starred_url":"https://api.github.com/users/ardeal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ardeal/subscriptions","organizations_url":"https://api.github.com/users/ardeal/orgs","repos_url":"https://api.github.com/users/ardeal/repos","events_url":"https://api.github.com/users/ardeal/events{/privacy}","received_events_url":"https://api.github.com/users/ardeal/received_events","type":"User","site_admin":false},"labels":[{"id":1035696376,"node_id":"MDU6TGFiZWwxMDM1Njk2Mzc2","url":"https://api.github.com/repos/ultralytics/yolov3/labels/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"},{"id":1890885613,"node_id":"MDU6TGFiZWwxODkwODg1NjEz","url":"https://api.github.com/repos/ultralytics/yolov3/labels/Stale","name":"Stale","color":"ededed","default":false,"description":null}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":17,"created_at":"2021-02-28T10:42:49Z","updated_at":"2021-07-26T22:43:57Z","closed_at":"2021-04-24T00:13:33Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\n\r\nmy environment is:\r\n<pre>\r\nwindows 10  \r\n10700K CPU   with 16GB ram \r\n3090 GPU with 24G memory  \r\ndriver version: 461.40  \r\ncuda version: 11.0  \r\ncudnn version: cudnn-11.0-windows-x64-v8.0.5.39  \r\nSSD 512GB  \r\ntorch 1.7.1  \r\n</pre>\r\n\r\n\r\ndatasets information:\r\n<pre>\r\n10180 images with 1080P resolution  \r\nepoch 100\r\nbatch size 32\r\nbatch count 319\r\n</pre>\r\n\r\n|   |num_workers=1 | num_workers = 2| num_workers = 4 |\r\n| :-----:|:----: |:----: |:----: |\r\n|CPU RAM used|9.5G| 9.5G |  unknown as cuda out of memory issue |\r\n|GPU RAM used|17.5G| 17.5G | unknown as cuda out of memory issue|\r\n|GPU Power(w)|257| 357 | unknown as cuda out of memory issue|\r\n| epoch |100| 100 | 100  |\r\n| batch size |32| 32 | 32 |\r\n| batch count | 319| 319 | 319 |\r\n| total time for training(hours) |15 | 6.459 | unknown |\r\n| time for each epoch(minutes) |9 | 3.875| unknown |\r\n| mean time for batch(second) | 1.69 | 0.73| unknown |\r\n\r\n**my doubts are:**\r\n1) time needed for num_workers=2 is more than twice of num_workers =1.\r\n2) much time is wasted by CPU as num_workers=2 will used 2 threads to load a batch of images.\r\n3) for num_workers=2, much ram reamined. why CUDA out of memory issue happened for num_workers = 4?\r\n4) num_workers will only change the threads used to load images, which will not change batch size. That is to say, GPU memory needed will not be changed for num_workers=2 and num_workers=4. why the CUDA out of memory issue happened? \r\n5) on my computer, the biggest num_workers=2, however, in your code <code>nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers </code>, num_workers is set to 8. Dose this work on your computer? what is computer configuration?\r\n\r\n\r\n\r\n```python\r\nAnalyzing anchors... anchors/target = 5.86, Best Possible Recall (BPR) = 1.0000\r\nImage sizes 640 train, 640 test\r\nUsing 4 dataloader workers\r\nLogging results to runs\\train\\exp7\r\nStarting training for 100 epochs...\r\n     Epoch   gpu_mem       box       obj       cls     total   targets  img_size\r\n  0%|          | 0/319 [00:00<?, ?it/s]Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\r\nNumExpr defaulting to 8 threads.\r\n  0%|          | 0/319 [00:02<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"D:/code_python/har_hailiang/har_hdd/algo/train.py\", line 288, in train\r\n    pred = model(imgs)  # forward\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\code_python\\har_hailiang\\har_hdd\\algo\\models\\yolo.py\", line 122, in forward\r\n    return self.forward_once(x, profile)  # single-scale inference, train\r\n  File \"D:\\code_python\\har_hailiang\\har_hdd\\algo\\models\\yolo.py\", line 138, in forward_once\r\n    x = m(x)  # run\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"D:\\code_python\\har_hailiang\\har_hdd\\algo\\models\\common.py\", line 35, in forward\r\n    return self.act(self.bn(self.conv(x)))\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 423, in forward\r\n    return self._conv_forward(input, self.weight)\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 419, in _conv_forward\r\n    return F.conv2d(input, weight, self.bias, self.stride,\r\nRuntimeError: CUDA out of memory. Tried to allocate 3.85 GiB (GPU 0; 24.00 GiB total capacity; 1.47 GiB already allocated; 20.35 GiB free; 1.54 GiB reserved in total by PyTorch)\r\npython-BaseException\r\n```\r\n","closed_by":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1699/timeline","performed_via_github_app":null,"state_reason":"completed"}