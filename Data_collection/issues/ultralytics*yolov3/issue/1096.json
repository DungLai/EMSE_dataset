{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096","repository_url":"https://api.github.com/repos/ultralytics/yolov3","labels_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096/labels{/name}","comments_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096/comments","events_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096/events","html_url":"https://github.com/ultralytics/yolov3/issues/1096","id":606839919,"node_id":"MDU6SXNzdWU2MDY4Mzk5MTk=","number":1096,"title":"Better training image plots ","user":{"login":"jveitchmichaelis","id":3159591,"node_id":"MDQ6VXNlcjMxNTk1OTE=","avatar_url":"https://avatars.githubusercontent.com/u/3159591?v=4","gravatar_id":"","url":"https://api.github.com/users/jveitchmichaelis","html_url":"https://github.com/jveitchmichaelis","followers_url":"https://api.github.com/users/jveitchmichaelis/followers","following_url":"https://api.github.com/users/jveitchmichaelis/following{/other_user}","gists_url":"https://api.github.com/users/jveitchmichaelis/gists{/gist_id}","starred_url":"https://api.github.com/users/jveitchmichaelis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jveitchmichaelis/subscriptions","organizations_url":"https://api.github.com/users/jveitchmichaelis/orgs","repos_url":"https://api.github.com/users/jveitchmichaelis/repos","events_url":"https://api.github.com/users/jveitchmichaelis/events{/privacy}","received_events_url":"https://api.github.com/users/jveitchmichaelis/received_events","type":"User","site_admin":false},"labels":[{"id":1035696372,"node_id":"MDU6TGFiZWwxMDM1Njk2Mzcy","url":"https://api.github.com/repos/ultralytics/yolov3/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":38,"created_at":"2020-04-25T19:33:45Z","updated_at":"2020-04-30T23:10:21Z","closed_at":"2020-04-30T23:10:21Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"## ðŸš€ Feature\r\nImproved image batch plots during training\r\n\r\n## Motivation\r\n\r\nIt's nice to see labeling performance as training progresses, but there are a few issues:\r\n\r\n* Boxes are not coloured per-class\r\n* It would be useful to see confidence and class per-box\r\n* Support for dealing with N-channel images or 1-channel images\r\n* If the images are normalised (for example zero-centred) prior to the forward pass then they get clipped and the display is black\r\n* This is because images are automatically scaled by 255 (hardcoded) which is fine for RGB, but breaks other datasets for example if you use a dataloader that has normalisation as part of its pipeline\r\n*Currently only ground truth is plotted. For test data, the plot is overwritten every epoch so you can't see progress\r\n\r\nThe last point is really important I think. Seeing what the model is labeling is arguably more informative than looking at the curves in tensorboard, and it's also a sanity check to see that your labels are loading correctly.\r\n\r\nCurrently with a non-standard dataset, plotting images looks a bit like this:\r\n\r\n![image](https://user-images.githubusercontent.com/3159591/80261251-6f053b00-8679-11ea-9adb-20495c5b376a.png)\r\n\r\n## Pitch\r\n\r\nHere is the result on the FLIR ADAS dataset - for context after an epoch of training from scratch:\r\n\r\n![image](https://user-images.githubusercontent.com/3159591/80288790-d7135a00-8729-11ea-9ad1-6df36a2e3792.png)\r\n\r\nThis function improves the above. If means and stds are provided, then we invert the normalisation. If you provide a multi-dimensional image, for example RGBD, you can select which channels are plotted. Boxes are coloured using matplotlib's property cycle, so it should be consistent within a dataset. Boxes are plotted with confidence labelled as well, and by default the alpha of the box is set to the confidence (or this can be disabled).\r\n\r\nNote we need to pass the actual tensor in from `imgs` in the training loop because we may have performed augmentation (therefore the path alone isn't enough).\r\n\r\n```\r\ndef plot_images(imgs, targets, paths=None, fname='images.jpg', max_subplots=16, normalised_coords=True, means=None, stds=None, plot_channels=[0,1,2], conf_as_alpha=True, label_fontsize=5):\r\n    \"\"\"\r\n    Plots training images overlaid with targets\r\n\r\n    Parameters:\r\n        imgs: input array of images Numpy array or Tensor\r\n        targets: input array of bounding boxes in xywh order\r\n        paths: paths to images\r\n        fname: output filename\r\n        max_subplots: max number of images in batch to plot\r\n        normalised_coords: transform bounding boxes to image coordinates\r\n        means: dataset channel means\r\n        stds: dataset channel stds\r\n        plot_channels: image channels to plot\r\n        conf_as_alpha: plot bounding boxes with confidence as alpha channel\r\n        label_fontsize: size of text (5 works fine with 16)\r\n    \"\"\"\r\n\r\n    # Move data to CPU\r\n    if isinstance(imgs, torch.Tensor):\r\n        if imgs.is_cuda:\r\n            imgs = imgs.cpu()\r\n        imgs = imgs.numpy()\r\n\r\n    fig = plt.figure(figsize=(10, 10))\r\n    bs, c, h, w = imgs.shape  # batch size, channels, height, width\r\n    bs = min(bs, max_subplots)  # limit plot to `subplots` images\r\n    ns = np.ceil(bs ** 0.5)  # number of subplots\r\n\r\n    # Fix colour per class\r\n    prop_cycle = plt.rcParams['axes.prop_cycle']\r\n    colors = prop_cycle.by_key()['color']\r\n\r\n    for i in range(bs):\r\n\r\n        if c == 1:\r\n            img = imgs[i][0]\r\n        else:\r\n            img = imgs[i].transpose(1,2,0) # channel last\r\n\r\n        # Reverse normalisation\r\n        if stds is not None:\r\n            img /= np.reciprocal(stds)\r\n\r\n        if means is not None:\r\n            img += means\r\n\r\n        # If we have an n-channel image, plot 3 channels only\r\n        if c > 3:\r\n            img = img[:,:,plot_channels]\r\n\r\n        ax = plt.subplot(ns, ns, i + 1)\r\n        \r\n        # Plot gray as gray\r\n        if c == 1:\r\n            ax.imshow(img.astype(int), cmap='gray')\r\n        else:\r\n            ax.imshow(img.astype(int))\r\n\r\n        boxes = targets[i]\r\n        if isinstance(boxes, torch.Tensor):\r\n            if boxes.is_cuda:\r\n                boxes = boxes.cpu()\r\n            boxes = boxes.numpy()\r\n\r\n        if len(boxes.shape) == 1:\r\n            boxes = np.expand_dims(boxes, axis=0)\r\n\r\n        if len(boxes) > 0:\r\n            for box in boxes:\r\n                obj_conf = 1\r\n                # Model prediction\r\n                if len(box) > 5:\r\n                    cls_id, x, y, bw, bh, obj_conf = box\r\n                # Ground truth\r\n                else:\r\n                    cls_id, x, y, bw, bh = box\r\n\r\n                # Could also use xywh2xyxy here maybe?\r\n                x1 = x - bw/2\r\n                x2 = x + bw/2\r\n                y1 = y-bh/2\r\n                y2 = y+bh/2\r\n\r\n                if normalised_coords:\r\n                    x1 *= w\r\n                    x2 *= w\r\n                    y1 *= h\r\n                    y2 *= h\r\n                    bw = x2-x1\r\n                    bh = y2-y1\r\n                \r\n                if conf_as_alpha:\r\n                    obj_alpha = obj_conf\r\n                else:\r\n                    obj_alpha = 1\r\n\r\n                # Confidence/class label\r\n                ax.add_patch(plt.Rectangle((x1, y1), bw, 10, color=colors[int(cls_id) % len(colors)], alpha=obj_alpha))\r\n                plt.text(x1+2, y1+10, \"{:.2f}\".format(obj_conf), fontsize=label_fontsize, color='white', alpha=obj_alpha)\r\n\r\n                # Bounding box\r\n                ax.add_patch(plt.Rectangle((x1, y1), bw, bh, color=colors[int(cls_id) % len(colors)], alpha=obj_alpha, fill=False))\r\n\r\n        ax.axis('off')\r\n\r\n        # Filename as subplot title\r\n        if paths is not None:\r\n            s = Path(paths[i]).name\r\n            plt.title(s[:min(len(s), 40)], fontdict={'size': 8})  # limit to 40 characters\r\n```\r\n\r\nIt handles ground truth/predictions appropriately.\r\n\r\nI've opted to pass in the mean/std and not have a default value. We could just set 255 and 1 by default, but this may still break for custom datasets due to this line:\r\n\r\nhttps://github.com/ultralytics/yolov3/blob/3554ab07fbedc05d91d9e6907b96a62512d931d5/train.py#L237\r\n\r\n## Required modifications\r\n\r\nThis should work out of the box as a drop-in replacement, with some extra code to adjust the target array above. However, for best results (and for extensibility on other datasets) there needs to be a few extra infrastructure changes when calling test code.\r\n\r\nMainly `test()` function needs to accept some extra parameters, for example data set means/standard deviation if used, the summary writer so we can push to the same tensorboard instance, etc. This is mostly because these parameters are now out of scope when `train` is called. \r\n\r\nI would suggest looking into a more tightly coupled train/test framework so that these sorts of things could be shared more efficiently, but I guess that's coming anyway! https://github.com/ultralytics/yolov3/issues/1093\r\n\r\n* Callling code (training), not I've also added an arg for output folder:\r\n\r\n```\r\n# Plot images with bounding boxes\r\n            if ni < 1:\r\n                f = os.path.join(opt.output_dir, 'train_batch%g_gt.png' % epoch)  # filename\r\n\r\n                plot_targets = []\r\n                for i in range(len(imgs)):\r\n                    mask = targets[:,0].int() == i\r\n                    plot_targets.append(targets[mask][:,1:])\r\n\r\n                plot_images(imgs=imgs, targets=plot_targets, paths=paths, fname=f, means=means, stds=stds)\r\n                if tb_writer:\r\n                    tb_writer.add_image(\"example_batch\", cv2.imread(f)[:, :, ::-1], dataformats='HWC', step=epoch)\r\n```\r\n\r\n* Testing, a bit hacky because we need to adjust the bounding box format, but it works:\r\n\r\n```\r\n# construct targets for plotting, iterate over batch\r\n            plot_targets = []\r\n            for i, o in enumerate(output):\r\n\r\n                image_targets = []\r\n                if i is not None:\r\n                    for pred in o.cpu().numpy():\r\n                        box = pred[:4]\r\n                        w = box[2]-box[0]\r\n                        h = box[3]-box[1]\r\n                        x = box[0] + w/2\r\n                        y = box[1] + h/2\r\n                        conf = pred[4]\r\n                        cls = int(pred[5])\r\n\r\n                        image_targets.append([cls, x, y, w, h, conf])\r\n                plot_targets.append(np.array(image_targets))\r\n\r\n            plot_targets = np.array(plot_targets)\r\n```\r\n\r\n## Other thoughts\r\n\r\n* Should plotting be done per epoch (or every N epochs)? I think yes, this is really useful to visually track progress and it's more informative for real-world problems than a mAP curve. Every epoch is a bit excessive, so I've done this with an argument.\r\n* I think the overhead is pretty low so there's no need to only do this once at the beginning, and for debugging it's super useful.\r\n* This is a topic for another issue, but I think augmentation/transformations should probably be done in one place using a composition or something in the dataloader, not in the training loop. For example I've modified the repo to use Albumentations which is great. e.g.\r\n\r\n```\r\n        transforms = Compose(augmentation+[\r\n                        # Note that internally, this function does:\r\n                        # mean *= max_pixel_value, std *= max_pixel_value\r\n                        Normalize(\r\n                            mean=means,\r\n                            std=stds,\r\n                            max_pixel_value=1.0\r\n                        ),\r\n                        ToTensor()\r\n                    ], bbox_params=BboxParams(format='coco', label_fields=['category_id']))\r\n```\r\n\r\nin fact I would suggest we could move all the current augmentation code over to Torch's built in stuff (for flips, hsv, affine). The main reason I don't like the transforms in PyTorch is that the image transformations force you to use PIL, which craps out with > 3 channels. Albumentations does most stuff in Numpy and is also very fast.\r\n\r\n## Summary\r\n\r\n\r\nBasically this is good to PR, but it'd be good to get your input on how you want to handle e.g. adding extra args to `train()` and whether this is OK.","closed_by":{"login":"jveitchmichaelis","id":3159591,"node_id":"MDQ6VXNlcjMxNTk1OTE=","avatar_url":"https://avatars.githubusercontent.com/u/3159591?v=4","gravatar_id":"","url":"https://api.github.com/users/jveitchmichaelis","html_url":"https://github.com/jveitchmichaelis","followers_url":"https://api.github.com/users/jveitchmichaelis/followers","following_url":"https://api.github.com/users/jveitchmichaelis/following{/other_user}","gists_url":"https://api.github.com/users/jveitchmichaelis/gists{/gist_id}","starred_url":"https://api.github.com/users/jveitchmichaelis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jveitchmichaelis/subscriptions","organizations_url":"https://api.github.com/users/jveitchmichaelis/orgs","repos_url":"https://api.github.com/users/jveitchmichaelis/repos","events_url":"https://api.github.com/users/jveitchmichaelis/events{/privacy}","received_events_url":"https://api.github.com/users/jveitchmichaelis/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ultralytics/yolov3/issues/1096/timeline","performed_via_github_app":null,"state_reason":"completed"}