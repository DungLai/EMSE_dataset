{"url":"https://api.github.com/repos/EmuKit/emukit/issues/373","repository_url":"https://api.github.com/repos/EmuKit/emukit","labels_url":"https://api.github.com/repos/EmuKit/emukit/issues/373/labels{/name}","comments_url":"https://api.github.com/repos/EmuKit/emukit/issues/373/comments","events_url":"https://api.github.com/repos/EmuKit/emukit/issues/373/events","html_url":"https://github.com/EmuKit/emukit/issues/373","id":914150159,"node_id":"MDU6SXNzdWU5MTQxNTAxNTk=","number":373,"title":"Updating multi_fidelity_dp run_adam function for gpflow 2.x","user":{"login":"izsahara","id":62043990,"node_id":"MDQ6VXNlcjYyMDQzOTkw","avatar_url":"https://avatars.githubusercontent.com/u/62043990?v=4","gravatar_id":"","url":"https://api.github.com/users/izsahara","html_url":"https://github.com/izsahara","followers_url":"https://api.github.com/users/izsahara/followers","following_url":"https://api.github.com/users/izsahara/following{/other_user}","gists_url":"https://api.github.com/users/izsahara/gists{/gist_id}","starred_url":"https://api.github.com/users/izsahara/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/izsahara/subscriptions","organizations_url":"https://api.github.com/users/izsahara/orgs","repos_url":"https://api.github.com/users/izsahara/repos","events_url":"https://api.github.com/users/izsahara/events{/privacy}","received_events_url":"https://api.github.com/users/izsahara/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-06-08T01:58:08Z","updated_at":"2021-06-08T11:56:01Z","closed_at":"2021-06-08T11:55:32Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, how do i update the Adam optimizer to cater to the format of gpflow 2? \r\n(https://github.com/EmuKit/emukit/blob/master/emukit/examples/multi_fidelity_dgp/multi_fidelity_deep_gp.py)\r\n\r\n```\r\ndef run_adam(self, lr, iterations):\r\n    adam = AdamOptimizer(lr).make_optimize_action(self)           \r\n    actions = [adam, PrintAction(self, 'MF-DGP with Adam')]      \r\n    loop = Loop(actions, stop=iterations)()\r\n```\r\nThis is my `DGP_Base` class (the `run_adam` function is at the end)\r\n\r\n```\r\nclass DGP(Module):\r\n    def __init__(self, X: list, Y: list, layers: list, likelihood, num_samples=10):\r\n        super().__init__(name=\"DeepGP\")\r\n\r\n        self.Y_list = Y\r\n        self.X_list = X\r\n        self.num_samples = num_samples\r\n        for i, (x, y) in enumerate(zip(X, Y)):\r\n            setattr(self, 'num_data' + str(i), x.shape[0])\r\n            setattr(self, 'X' + str(i), Parameter(x))\r\n            setattr(self, 'Y' + str(i), Parameter(y))\r\n\r\n        self._train_upto_fidelity = -1\r\n        self.num_layers = len(layers)\r\n        self.likelihood = BroadcastingLikelihood(likelihood)\r\n        self.layers = layers\r\n        self.L = 0\r\n        self.KL = 0\r\n\r\n    @staticmethod\r\n    def _likelihood_at_fidelity(Fmu, Fvar, Y, variance):\r\n        \"\"\"\r\n        Calculate likelihood term for observations corresponding to one fidelity\r\n\r\n        :param Fmu: Posterior mean\r\n        :param Fvar: Posterior variance\r\n        :param Y: training observations\r\n        :param variance: likelihood variance\r\n        :return:\r\n        \"\"\"\r\n        return -0.5 * np.log(2 * np.pi) - 0.5 * tf.math.log(variance) \\\r\n               - 0.5 * (tf.square(Y - Fmu) + Fvar) / variance\r\n\r\n    def _build_predict(self, X, full_cov=False, num_samples=10, fidelity=None):\r\n        \"\"\"\r\n        Predicts from the fidelity level specified. If fidelity is not specified, return prediction at highest fidelity.\r\n\r\n        :param X: Location at which to predict\r\n        :param full_cov: Whether to predict full covariance matrix\r\n        :param num_samples: Number of samples to use for MC sampling between layers\r\n        :param fidelity: zero based fidelity index at which to predict\r\n        :return: (mean, variance) where each is [num_samples, N, 1] where num_samples is number of samples and N is number of predicted points.\r\n        \"\"\"\r\n\r\n        if fidelity is None:\r\n            fidelity = -1\r\n\r\n        _, Fmeans, Fvars = self.propagate(X, full_cov=full_cov, num_samples=num_samples)\r\n        return Fmeans[fidelity], Fvars[fidelity]\r\n\r\n    def E_log_p_Y(self, X_f, Y_f, fidelity=None):\r\n        \"\"\"\r\n        Calculate the expectation of the data log likelihood under the variational distribution with MC samples\r\n\r\n        :param X_f: Training inputs for a given\r\n        :param Y_f:\r\n        :param fidelity:\r\n        :return:\r\n        \"\"\"\r\n\r\n        Fmean, Fvar = self._build_predict(X_f, full_cov=False, num_samples=self.num_samples, fidelity=fidelity)\r\n\r\n        if fidelity == (self.num_layers - 1):\r\n            \"\"\"\r\n            KC - The likelihood of the observations at the last layer is computed using the model's 'likelihood' object\r\n            \"\"\"\r\n            var_exp = self.likelihood.variational_expectations(Fmean, Fvar, Y_f)  # S, N, D\r\n        else:\r\n            \"\"\"\r\n            KC - The Gaussian likelihood of the observations at the intermediate layers is computed using the noise \r\n            parameter pertaining to the White noise kernel.\r\n\r\n            This assumes that a White kernel should be added to all layers except for the last!\r\n            If no noise is desired, the variance parameter in the White kernel should be set to zero and fixed.\r\n            \"\"\"\r\n            variance = self.layers[fidelity].kern.kernels[-1].variance\r\n\r\n            f = lambda vars_SND, vars_ND, vars_N: self._likelihood_at_fidelity(vars_SND[0],\r\n                                                                               vars_SND[1],\r\n                                                                               vars_ND[0],\r\n                                                                               vars_N)\r\n\r\n            var_exp = f([Fmean, Fvar], [tf.expand_dims(Y_f, 0)], variance)\r\n\r\n        return tf.reduce_mean(var_exp, 0)\r\n\r\n    def propagate(self, X, full_cov=False, num_samples=10, zs=None):\r\n        sX = tf.tile(tf.expand_dims(X, 0), [num_samples, 1, 1])\r\n        Fs, Fmeans, Fvars = [], [], []\r\n        F = sX\r\n        zs = zs or [None, ] * len(self.layers)\r\n        for i, (layer, z) in enumerate(zip(self.layers, zs)):\r\n            if i == 0:\r\n                F, Fmean, Fvar = layer.sample_from_conditional(F, z=z, full_cov=full_cov)\r\n            else:\r\n                '''\r\n                KC - At all layers 1..L, the input to the next layer is original input augmented with \r\n                the realisation of the function at the previous layer at that input.\r\n                '''\r\n                F_aug = tf.concat([sX, F], 2)\r\n                F, Fmean, Fvar = layer.sample_from_conditional(F_aug, z=z, full_cov=full_cov)\r\n\r\n            Fs.append(F)\r\n            Fmeans.append(Fmean)\r\n            Fvars.append(Fvar)\r\n        return Fs, Fmeans, Fvars\r\n\r\n    def predict_f(self, Xnew, num_samples, fidelity=None):\r\n        return self._build_predict(Xnew, full_cov=False, num_samples=num_samples, fidelity=fidelity)\r\n\r\n    def predict_f_full_cov(self, Xnew, num_samples, fidelity=None):\r\n        return self._build_predict(Xnew, full_cov=True, num_samples=num_samples, fidelity=fidelity)\r\n\r\n    def predict_all_layers(self, predict_at, num_samples, full_cov=False):\r\n        return self.propagate(predict_at, full_cov=full_cov,\r\n                              num_samples=num_samples)\r\n\r\n    def predict_y(self, Xnew, num_samples):\r\n        Fmean, Fvar = self._build_predict(Xnew, full_cov=False, num_samples=num_samples)\r\n        return self.likelihood.predict_mean_and_var(Fmean, Fvar)\r\n\r\n    def predict_log_density(self, Xnew, Ynew, num_samples):\r\n        Fmean, Fvar = self._build_predict(Xnew, full_cov=False, num_samples=num_samples)\r\n        like = self.likelihood.predict_log_density(Fmean, Fvar, Ynew)\r\n        log_num_samples = tf.math.log(tf.cast(num_samples, tf.float64))\r\n        return tf.reduce_logsumexp(like - log_num_samples, axis=0)\r\n\r\n    def mf_elbo(self):\r\n        \"\"\"\r\n        ELBO calculation\r\n        :return: MC estimate of lower bound\r\n        \"\"\"\r\n        L = 0.\r\n        KL = 0.\r\n        for fidelity in range(self.num_layers):\r\n\r\n            if (self._train_upto_fidelity != -1) and (fidelity > self._train_upto_fidelity):\r\n                continue\r\n\r\n            X_l = getattr(self, 'X' + str(fidelity))\r\n            Y_l = getattr(self, 'Y' + str(fidelity))\r\n\r\n            n_data = getattr(self, 'num_data' + str(fidelity))\r\n            scale = tf.cast(n_data, tf.float64) / tf.cast(tf.shape(X_l)[0], tf.float64)\r\n\r\n            L += (tf.reduce_sum(self.E_log_p_Y(X_l, Y_l, fidelity)) * scale)\r\n            KL += tf.reduce_sum(self.layers[fidelity].KL())\r\n\r\n        self.L = L\r\n        self.KL = KL\r\n\r\n        return self.L - self.KL\r\n\r\n    def multi_step_training(self, lr_1 = 3e-3, lr_2= 1e-3, n_iter=5000, n_iter_2=15000):\r\n        \"\"\"\r\n        Train with variational covariance fixed to be small first, then free up and train covariance alongside other\r\n        parameters. Inducing point locations are fixed throughout.\r\n        \"\"\"\r\n        for layer in self.layers[:-1]:\r\n            layer.q_sqrt.assign(layer.q_sqrt * 1e-8)\r\n            set_trainable(layer.q_sqrt, False)\r\n\r\n        self.layers[-1].q_sqrt.assign(self.layers[-1].q_sqrt * self.Y_list[-1].var() * 0.01)\r\n        set_trainable(self.layers[-1].q_sqrt, False)\r\n\r\n        self.likelihood.likelihood.variance.assign(self.Y_list[-1].var() * 0.01)\r\n        set_trainable(self.likelihood.likelihood.variance, False)\r\n\r\n        # Run with covariance fixed\r\n        print('-' * 100)\r\n        print('Optimization with Fixed Covariance: {} Iterations'.format(n_iter))\r\n        print('-' * 100)\r\n        self.run_adam(lr_1, n_iter)\r\n\r\n        # Run with covariance free\r\n        print('-' * 100)\r\n        print('Optimization with Free Covariance: {} Iterations'.format(n_iter_2))\r\n        print('-' * 100)\r\n        set_trainable(self.likelihood.likelihood.variance, True)\r\n\r\n        for layer in self.layers:\r\n            set_trainable(layer.q_sqrt, True)\r\n\r\n        self.run_adam(lr_2, n_iter_2)\r\n\r\n    def fix_inducing_point_locations(self):\r\n        \"\"\"\r\n        Fix all inducing point locations\r\n        \"\"\"\r\n        for layer in self.layers:\r\n            set_trainable(layer.feature.Z, False)\r\n\r\n    def run_adam(self, lr, iter_):\r\n        \"\"\"\r\n        Utility function running the Adam optimizer\r\n        \"\"\"\r\n        optimizer = tf.optimizers.Adam(learning_rate=lr)\r\n        for ii in range(iter_):\r\n            with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n                tape.watch(self.trainable_variables)\r\n                objective = -self.mf_elbo()\r\n                gradients = tape.gradient(objective, self.trainable_variables)\r\n            optimizer.apply_gradients(zip(gradients, self.trainable_variables))\r\n            print(\"Iteration: %3d | ELBO = %5.5f | KL = %5.5f | OBJ = %5.5f\" % (ii + 1, self.L, self.KL, objective))\r\n\r\n```\r\nSo far is my implementation correct? the code runs, however im afraid i might be doing something wrong \r\n  \r\n","closed_by":{"login":"apaleyes","id":2852301,"node_id":"MDQ6VXNlcjI4NTIzMDE=","avatar_url":"https://avatars.githubusercontent.com/u/2852301?v=4","gravatar_id":"","url":"https://api.github.com/users/apaleyes","html_url":"https://github.com/apaleyes","followers_url":"https://api.github.com/users/apaleyes/followers","following_url":"https://api.github.com/users/apaleyes/following{/other_user}","gists_url":"https://api.github.com/users/apaleyes/gists{/gist_id}","starred_url":"https://api.github.com/users/apaleyes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apaleyes/subscriptions","organizations_url":"https://api.github.com/users/apaleyes/orgs","repos_url":"https://api.github.com/users/apaleyes/repos","events_url":"https://api.github.com/users/apaleyes/events{/privacy}","received_events_url":"https://api.github.com/users/apaleyes/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/EmuKit/emukit/issues/373/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/EmuKit/emukit/issues/373/timeline","performed_via_github_app":null,"state_reason":"completed"}