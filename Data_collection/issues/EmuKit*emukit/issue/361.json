{"url":"https://api.github.com/repos/EmuKit/emukit/issues/361","repository_url":"https://api.github.com/repos/EmuKit/emukit","labels_url":"https://api.github.com/repos/EmuKit/emukit/issues/361/labels{/name}","comments_url":"https://api.github.com/repos/EmuKit/emukit/issues/361/comments","events_url":"https://api.github.com/repos/EmuKit/emukit/issues/361/events","html_url":"https://github.com/EmuKit/emukit/issues/361","id":837980524,"node_id":"MDU6SXNzdWU4Mzc5ODA1MjQ=","number":361,"title":"The proper way of combining HMC with batch acquisition strategies for Bayesian Optimization","user":{"login":"BrunoKM","id":22356384,"node_id":"MDQ6VXNlcjIyMzU2Mzg0","avatar_url":"https://avatars.githubusercontent.com/u/22356384?v=4","gravatar_id":"","url":"https://api.github.com/users/BrunoKM","html_url":"https://github.com/BrunoKM","followers_url":"https://api.github.com/users/BrunoKM/followers","following_url":"https://api.github.com/users/BrunoKM/following{/other_user}","gists_url":"https://api.github.com/users/BrunoKM/gists{/gist_id}","starred_url":"https://api.github.com/users/BrunoKM/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BrunoKM/subscriptions","organizations_url":"https://api.github.com/users/BrunoKM/orgs","repos_url":"https://api.github.com/users/BrunoKM/repos","events_url":"https://api.github.com/users/BrunoKM/events{/privacy}","received_events_url":"https://api.github.com/users/BrunoKM/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-22T18:05:21Z","updated_at":"2021-05-21T12:46:15Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This issue is meant to ask two questions: (1) Does the combination of Local Penalization and `IntegratedHyperparameterAcquisition` behave in the expected way, and (1) What's the right way to approach Batch Bayesian Optimization using multiple Hamiltonian Monte-Carlo (HMC) samples for model hyperparameters in EmuKit?\r\n\r\n## (1) Combining Local Penalization and IntegratedHyperparameterAcquisition\r\n\r\nCurrently, when doing sequential Bayesian Optimization (BO) in EmuKit, one can wrap the acquisition function in a wrapper – `IntegratedHyperParameterAcquisition` – and that wrapper remains responsible for generating and marginalizing-out the HMC samples:\r\n```\r\nacquisition = IntegratedHyperparameterAcquisition(emukit_model, ExpectedImprovement, n_burnin=100, n_samples=20)\r\n```\r\n\r\nWhen using the `IntegratedHyperparameterAcquisition` abstraction, the samples are only being marginalised-out when the acquisition function is called:\r\n```\r\n# All the HMC samples are used to evaluate the one-point acquisition\r\nacquisition.evaluate(x)\r\n```\r\nBut not when any of the model's predict functions are called:\r\n```\r\n# Only 1 HMC sample is used (whichever one the model happened to be updated with last) \r\nemukit_model.predict(x)\r\n```\r\n\r\nThis means that when an  `IntegratedHyperParameterAcquisition` is passed into a batch point calculator — such as the `LocalPenalizaitonPointCalculator` — the HMC samples won't necessarily be used in all places during computation of the batch acquisition.\r\n\r\n---\r\n### What happens when `IntegratedHyperparameterAcquisitionFunction` is combined with `LocalPenlizationPointCalculator`\r\n\r\nIn the specific case of the `LocalPenlizationPointCalculator`, this means that HMC samples will be used to evaluate the single-point acquisition function that's being penalized, but only a single arbitrary sample will be used to estimate the Lipschitz constant:\r\nhttps://github.com/EmuKit/emukit/blob/b7939897ebcff2281d0bb017186fd4ecf059df93/emukit/bayesian_optimization/local_penalization_calculator.py#L68\r\n\r\nor to evaluate the penalizer:\r\nhttps://github.com/EmuKit/emukit/blob/b7939897ebcff2281d0bb017186fd4ecf059df93/emukit/bayesian_optimization/local_penalization_calculator.py#L54\r\n\r\nThis doesn't seem like the behaviour the user would intend in this case. Using an arbitrary HMC sample for estimation of the Lipschitz constant could lead to undesirable, arbitrary behaviour. Depending on which HMC sample happens to be last in the collection of generated samples, the effect of local penalization will be vastly different. This seems to have a significant effect on performance of Batch BO + HMC.\r\n\r\n At the same time, there isn't an obvious way to properly consider all HMC samples in combination with the `LocalPenlizationPointCalculator` in Emukit, without rewriting that class yourself.\r\n\r\n---\r\nThis brings me to another question: where in the hierarchy of abstractions in EmuKIt should generating samples for hyperparameters, and marginalising them out, fit in when doing Batch Bayesian Optimisation? This is an issue not just for the Local Penalization calculator, but also for hypothetical new batch point calculators the user might want to implement:\r\n\r\n### Combining `IntegratedHyperparameterAcquisition` with other batch acquisition methods\r\n\r\nIn another use-case, when someone might want to implement their own batch point calculator. As an example, they might want to use the `MultipointExpectedImprovement`, but select points for the batch by sequentially optimizing 1-point Expected Improvement (EI), then 2-point EI wrt. the second point, 3-point EI wrt. the third point, etc. (as described in [Fast Computation of Expected Improvement](https://hal.archives-ouvertes.fr/hal-00732512v2/document)).\r\n\r\nIn such cases, as well as in the case of a locally penalized acquisition function, the acquisition function changes at each iteration of the batch. One could wrap this new acquisition in an `IntegratedHyperParameterAcquisition` wrapper for every time, but this would require re-running HMC every time. One could also try to construct an ` acquisition_generator` that returns the right acquisition function at each batch-step, but this seems feels like a complicated work-around. `IntegratedHyperparameterAcquisition` seems unsuitable for marginalising out hyperparameter samples for Batch Bayesian Optimisation, including for Local Penalization.\r\n\r\n**This to me hints at the possibility that storing the HMC samples in an abstraction related to the model, rather than the acquisition, might be a more generalisable approach?** (I don't have a concrete suggestion for what that might look like at the moment) If there is a relatively simple way to overcome these difficulties with Emukit methods, I'd love to know.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/EmuKit/emukit/issues/361/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/EmuKit/emukit/issues/361/timeline","performed_via_github_app":null,"state_reason":null}