{"url":"https://api.github.com/repos/EmuKit/emukit/issues/403","repository_url":"https://api.github.com/repos/EmuKit/emukit","labels_url":"https://api.github.com/repos/EmuKit/emukit/issues/403/labels{/name}","comments_url":"https://api.github.com/repos/EmuKit/emukit/issues/403/comments","events_url":"https://api.github.com/repos/EmuKit/emukit/issues/403/events","html_url":"https://github.com/EmuKit/emukit/issues/403","id":1148976051,"node_id":"I_kwDOCMd5485Ee_uz","number":403,"title":"Bayesian optimization gives flat latent function?","user":{"login":"iRove108","id":12959037,"node_id":"MDQ6VXNlcjEyOTU5MDM3","avatar_url":"https://avatars.githubusercontent.com/u/12959037?v=4","gravatar_id":"","url":"https://api.github.com/users/iRove108","html_url":"https://github.com/iRove108","followers_url":"https://api.github.com/users/iRove108/followers","following_url":"https://api.github.com/users/iRove108/following{/other_user}","gists_url":"https://api.github.com/users/iRove108/gists{/gist_id}","starred_url":"https://api.github.com/users/iRove108/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iRove108/subscriptions","organizations_url":"https://api.github.com/users/iRove108/orgs","repos_url":"https://api.github.com/users/iRove108/repos","events_url":"https://api.github.com/users/iRove108/events{/privacy}","received_events_url":"https://api.github.com/users/iRove108/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2022-02-24T08:01:27Z","updated_at":"2022-03-04T11:17:58Z","closed_at":"2022-03-04T11:17:58Z","author_association":"NONE","active_lock_reason":null,"body":"I'm learning a noisy function. I've been getting a flat latent function, and I'm not sure why.\r\n\r\nFor a self-contained working example, I'm abstracting away a run from my actual application. That is, I'm providing precomputed values for my initial design and my objective function.\r\n\r\n**Define my function and my initial design**:\r\n```python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom emukit.core import ParameterSpace, ContinuousParameter\r\nfrom emukit.core.loop import FixedIterationsStoppingCondition, UserFunctionWrapper\r\nfrom emukit.bayesian_optimization.acquisitions import ExpectedImprovement\r\nfrom emukit.bayesian_optimization.loops import BayesianOptimizationLoop\r\nfrom emukit.model_wrappers import GPyModelWrapper\r\n\r\nimport GPy\r\nfrom GPy.models import GPRegression\r\n\r\n# Define my function, which loops through 30 values in order\r\n# (values precomputed from actually running BayesOpt on my real function)\r\nclass SimFunction:\r\n    def __init__(self):\r\n        self.times_called = 0\r\n        self.arr = np.array([151.41014033,  62.09023478  ,62.52426835,  61.8066145,  153.27467322, 61.47440947,  61.66229474 , 62.23729366 , 62.66330217 , 61.37228135, 63.71660287 , 60.80217923,  61.55372866 , 61.85417587,  61.29182696 ,62.01017402  ,63.35483382  ,60.78094205,  62.70740196  ,63.44405674,62.34055014,  61.26645109 , 61.18795096,  62.27117287,  62.60566014,60.67162767,  61.84573099 , 62.86046971,  61.38240327 , 62.15477646])\r\n    def __call__(self, x):\r\n        value = self.arr[self.times_called]\r\n        self.times_called += 1\r\n\r\n        if self.times_called >= len(self.arr):\r\n            self.times_called = 0\r\n        return value\r\n\r\nsim_function = SimFunction()\r\nf = np.frompyfunc(sim_function, 1, 1)\r\nparameter_space = ParameterSpace([ContinuousParameter('x1', 5, 100)])\r\n\r\n# Define my initial design\r\n# (values precomputed from actually running BayesOpt on my real function)\r\nX = np.array([59.33766262,  66.87705975,   9.99274413,  91.77151485,\r\n         15.98334223,  75.47932193,  59.4018363 ,  28.62138164,\r\n         73.61994809,  18.89241105])[:,None]\r\nY = np.array([41.462845288004075, 45.1335386921359, 40.595514445980804,\r\n        55.882947142107895, 19.57217258906048, 49.775813658233105,\r\n        41.639729759946434, 25.650601902591085, 48.58427959331652,\r\n        20.838451230479237])[:,None]\r\n\r\n# Train GP model on initial evaluations\r\nmodel_gpy = GPRegression(X, Y)\r\nmodel_gpy.optimize()\r\nmodel = GPyModelWrapper(model_gpy)\r\n\r\nmodel_gpy.plot()\r\nplt.show()\r\n```\r\n**GP regression on initial points**:\r\n![plot1](https://user-images.githubusercontent.com/12959037/155466680-bd1042b0-ceae-4cf3-85e4-1ae97b68a9c3.png)\r\n\r\n**Run bayesian optimization loop and display final latent function**:\r\n```python\r\nITER = 30\r\n\r\n# initialize bayesian optimization loop\r\nexpected_improvement = ExpectedImprovement(model=model)\r\nloop = BayesianOptimizationLoop(model=model,\r\n                                space=parameter_space,\r\n                                acquisition=expected_improvement,\r\n                                batch_size=1)\r\n\r\n# run bayesian optimization\r\nloop.run_loop(UserFunctionWrapper(f), FixedIterationsStoppingCondition(ITER))\r\n\r\nmodel_gpy.plot()\r\nplt.show()\r\n```\r\n![Screen Shot 2022-02-24 at 12 00 50 AM](https://user-images.githubusercontent.com/12959037/155467053-23b05f2f-1804-4346-8db9-1aeeeeeabe10.jpg)\r\n\r\nThe latent function is completely flat! Its parameters also seem out of wack:\r\n```\r\nName : GP regression\r\nObjective : 187.15753229070074\r\nNumber of Parameters : 3\r\nNumber of Optimization Parameters : 3\r\nUpdates : True\r\nParameters:\r\n  GP_regression.           |               value  |  constraints  |  priors\r\n  rbf.variance             |   3769.474111833743  |      +ve      |        \r\n  rbf.lengthscale          |  2762.2467570215567  |      +ve      |        \r\n  Gaussian_noise.variance  |    590.833747684825  |      +ve      |        \r\n```\r\n\r\nHowever, if I fit another GP regression model on the _same_ data points as in the above graph, I get a _different_ function that also seems more reasonable:\r\n```python\r\n# GP regression on experimental points, run separately\r\nmodel_gpy2 = GPRegression(loop.loop_state.X, loop.loop_state.Y)\r\nmodel_gpy2.optimize()\r\nmodel_gpy2.plot()\r\nplt.show()\r\n```\r\n![Screen Shot 2022-02-24 at 12 09 15 AM](https://user-images.githubusercontent.com/12959037/155467941-18ade1e6-9088-49cc-b2fb-4edf3fc76536.jpg)\r\n\r\n```\r\nName : GP regression\r\nObjective : 95.81686749520499\r\nNumber of Parameters : 3\r\nNumber of Optimization Parameters : 3\r\nUpdates : True\r\nParameters:\r\n  GP_regression.           |               value  |  constraints  |  priors\r\n  rbf.variance             |   4170.330107829418  |      +ve      |        \r\n  rbf.lengthscale          |   5.611544854612646  |      +ve      |        \r\n  Gaussian_noise.variance  |  0.6604288598827055  |      +ve      |        \r\n```\r\n\r\nI believe the two above graphs should be identical. I think the model is not fitting in the optimization loop correctly. I wonder if this has to do with the `RuntimeWarning: overflow encountered in expm1` from `paramz/transformations.py:111`.\r\n\r\nI'd greatly appreciate any ideas on this. Thanks!","closed_by":{"login":"apaleyes","id":2852301,"node_id":"MDQ6VXNlcjI4NTIzMDE=","avatar_url":"https://avatars.githubusercontent.com/u/2852301?v=4","gravatar_id":"","url":"https://api.github.com/users/apaleyes","html_url":"https://github.com/apaleyes","followers_url":"https://api.github.com/users/apaleyes/followers","following_url":"https://api.github.com/users/apaleyes/following{/other_user}","gists_url":"https://api.github.com/users/apaleyes/gists{/gist_id}","starred_url":"https://api.github.com/users/apaleyes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apaleyes/subscriptions","organizations_url":"https://api.github.com/users/apaleyes/orgs","repos_url":"https://api.github.com/users/apaleyes/repos","events_url":"https://api.github.com/users/apaleyes/events{/privacy}","received_events_url":"https://api.github.com/users/apaleyes/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/EmuKit/emukit/issues/403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/EmuKit/emukit/issues/403/timeline","performed_via_github_app":null,"state_reason":"completed"}