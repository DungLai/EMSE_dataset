{"url":"https://api.github.com/repos/EmuKit/emukit/issues/387","repository_url":"https://api.github.com/repos/EmuKit/emukit","labels_url":"https://api.github.com/repos/EmuKit/emukit/issues/387/labels{/name}","comments_url":"https://api.github.com/repos/EmuKit/emukit/issues/387/comments","events_url":"https://api.github.com/repos/EmuKit/emukit/issues/387/events","html_url":"https://github.com/EmuKit/emukit/issues/387","id":963009621,"node_id":"MDU6SXNzdWU5NjMwMDk2MjE=","number":387,"title":"Issue with running multi-fidelity BO notebook.","user":{"login":"PC-FSU","id":58872472,"node_id":"MDQ6VXNlcjU4ODcyNDcy","avatar_url":"https://avatars.githubusercontent.com/u/58872472?v=4","gravatar_id":"","url":"https://api.github.com/users/PC-FSU","html_url":"https://github.com/PC-FSU","followers_url":"https://api.github.com/users/PC-FSU/followers","following_url":"https://api.github.com/users/PC-FSU/following{/other_user}","gists_url":"https://api.github.com/users/PC-FSU/gists{/gist_id}","starred_url":"https://api.github.com/users/PC-FSU/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PC-FSU/subscriptions","organizations_url":"https://api.github.com/users/PC-FSU/orgs","repos_url":"https://api.github.com/users/PC-FSU/repos","events_url":"https://api.github.com/users/PC-FSU/events{/privacy}","received_events_url":"https://api.github.com/users/PC-FSU/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-08-06T19:59:15Z","updated_at":"2021-12-09T20:31:02Z","closed_at":"2021-12-09T20:31:02Z","author_association":"NONE","active_lock_reason":null,"body":"System and package information \r\n\r\nOS : Windows 10, verision 1909, build : 18363.1440\r\nPython : Python 3.7.10, (MSC v.1916 64 bit (AMD64))\r\nScipy : 1.7.0\r\nNumpy : '1.21.1'\r\nSetuptools : '52.0.0.post20210125'\r\nGPy :  '1.10.0'\r\nemcee : 3.1.0\r\n\r\n\r\n\r\nI have a version of \"Emukit-tutorial-multi-fidelity-bayesian-optimization.ipynb\" adapted in two different .py scripts, and I am trying to replicate the result of the Forrester example.  File emukit_wrapper.py contains the functionality for Initializing the model, defining the acquisition function, and running the bayesian optimization loop.  \r\n\r\n        `import numpy as np\r\n        np.random.seed(12345)\r\n        from emukit.multi_fidelity.convert_lists_to_array import convert_x_list_to_array\r\n        from emukit.core import ParameterSpace, ContinuousParameter, InformationSourceParameter\r\n        from emukit.multi_fidelity.models.linear_model import GPyLinearMultiFidelityModel\r\n        import GPy\r\n        from emukit.multi_fidelity.kernels.linear_multi_fidelity_kernel import LinearMultiFidelityKernel\r\n        from emukit.multi_fidelity.convert_lists_to_array import convert_xy_lists_to_arrays\r\n        from emukit.model_wrappers import GPyMultiOutputWrapper\r\n        from GPy.models.gp_regression import GPRegression\r\n        from emukit.bayesian_optimization.acquisitions.entropy_search import MultiInformationSourceEntropySearch\r\n        from emukit.core.acquisition import Acquisition\r\n        from emukit.core.loop import FixedIntervalUpdater, OuterLoop, SequentialPointCalculator\r\n        from emukit.core.loop.loop_state import create_loop_state\r\n        from emukit.core.optimization.multi_source_acquisition_optimizer import MultiSourceAcquisitionOptimizer\r\n        from emukit.core.optimization import GradientAcquisitionOptimizer\r\n        from emukit.core.loop.user_function import MultiSourceFunctionWrapper\r\n        from scipy.optimize import minimize\r\n        from emukit.bayesian_optimization.loops import BayesianOptimizationLoop\r\n        from emukit.bayesian_optimization.acquisitions.entropy_search import EntropySearch\r\n        from emukit.model_wrappers import GPyModelWrapper\r\n        import GPy\r\n        \r\n        class EmukitMultifidelity():\r\n        \r\n            def __init__(self, fidelities, n_fidelities, costs,\r\n                         n_optimization_restarts=5, verbose_optimization=False):\r\n        \r\n                assert len(fidelities) == n_fidelities, \"number of fidelity functions should be same as n_fidelities\"\r\n                # for fid in fidelities: assert type(fid) is function\r\n        \r\n                self.fidelities = fidelities\r\n                self.n_fidelities = n_fidelities\r\n                self.cost_list = costs\r\n                self.x_array = None\r\n                self.y_array = None\r\n                self.model = None\r\n                self.acquisition = None\r\n                self.loop = None\r\n                self.input_dimensions = None\r\n                self.n_optimization_restarts = n_optimization_restarts\r\n                self.verbose_optimization = verbose_optimization\r\n        \r\n                print('Warning! the following limitations/assumptions hold: \\n'\r\n                      '1. assumes discrete set of fidelities \\n'\r\n                      '2. assumes an RBF kernel for all fidelities \\n'\r\n                      '3. all RBF kernels assume a lengthscale bounded by [0.01, 0.5] \\n'\r\n                      '4. input space is assumed to be in [0,1]^d. Be sure to normalize inputs \\n'\r\n                      '5. the input parameter space is assumed to be 1d. Change param_space() to modify this \\n'\r\n                      )\r\n                return\r\n        \r\n            def fidelity_func_wrapper(self):\r\n                user_function = MultiSourceFunctionWrapper([\r\n                    lambda x: self.fidelities[i](x) for i in range(self.n_fidelities)\r\n                ])\r\n                return user_function\r\n        \r\n            def param_space(self):\r\n                # HARD CODE alert: need to change for >1 dimensions\r\n                return ParameterSpace([ContinuousParameter('x', 0, 1),\r\n                                       InformationSourceParameter(self.n_fidelities)])\r\n        \r\n            def initialize(self, x_list, y_list):\r\n                assert len(x_list) == self.n_fidelities, \"you should provide initial data for each fidelity\"\r\n                assert len(y_list) == self.n_fidelities, \"you should provide initial data for each fidelity\"\r\n                self.x_array, self.y_array = convert_xy_lists_to_arrays(x_list, y_list)\r\n                return\r\n        \r\n            def mfmodel(self):\r\n                input_dimensions = self.x_array.shape[-1]-1\r\n        \r\n                kernels = []\r\n                for i in range(self.n_fidelities):\r\n                    kernel = GPy.kern.RBF(input_dimensions)\r\n                    kernel.lengthscale.constrain_bounded(0.01, 0.5) # create\r\n                    kernels.append(kernel)\r\n        \r\n                multi_fidelity_kernel = LinearMultiFidelityKernel(kernels)\r\n                gpy_model = GPyLinearMultiFidelityModel(self.x_array, self.y_array, multi_fidelity_kernel, self.n_fidelities)\r\n        \r\n                # needs to be modified for > 2 fidelities\r\n                gpy_model.likelihood.Gaussian_noise.fix(0.1)\r\n                gpy_model.likelihood.Gaussian_noise_1.fix(0.1)\r\n        \r\n                self.model = GPyMultiOutputWrapper(gpy_model, n_outputs=self.n_fidelities,\r\n                                                   n_optimization_restarts=self.n_optimization_restarts,\r\n                                                   verbose_optimization=self.verbose_optimization)\r\n                self.model.optimize()\r\n                self.input_dimensions = input_dimensions\r\n        \r\n                return\r\n        \r\n            def multifidelity_acquisition_function(self):\r\n                # Define cost of different fidelities as acquisition function\r\n                class Cost(Acquisition):\r\n                    def __init__(Cself, costs):\r\n                        Cself.costs = costs\r\n        \r\n                    def evaluate(Cself, x):\r\n                        fidelity_index = x[:, -1].astype(int)\r\n                        x_cost = np.array([Cself.costs[i] for i in fidelity_index])\r\n                        return x_cost[:, None]\r\n        \r\n                    @property\r\n                    def has_gradients(Cself):\r\n                        return True\r\n        \r\n                    def evaluate_with_gradients(Cself, x):\r\n                        return Cself.evalute(x), np.zeros(x.shape)\r\n        \r\n                cost_acquisition = Cost(self.cost_list)\r\n                self.cost_acquisition = cost_acquisition\r\n                self.acquisition = MultiInformationSourceEntropySearch(self.model, self.param_space()) / self.cost_acquisition\r\n                return\r\n        \r\n            def mfbo_loop(self, num_iters):\r\n                initial_loop_state = create_loop_state(self.x_array, self.y_array)\r\n                acquisition_optimizer = MultiSourceAcquisitionOptimizer(GradientAcquisitionOptimizer(self.param_space()),\r\n                                                                        self.param_space())\r\n                candidate_point_calculator = SequentialPointCalculator(self.acquisition, acquisition_optimizer)\r\n                model_updater = FixedIntervalUpdater(self.model)\r\n                self.loop = OuterLoop(candidate_point_calculator, model_updater, initial_loop_state)\r\n                self.pmean_minimizer = []\r\n                self.pmean_minimum = []\r\n                self.cost = []\r\n                def optimize_posterior_mean_event(loop, loop_state):\r\n                    pmean_minimizer = self.optimize_posterior_mean(opt='minimize')\r\n                    self.pmean_minimizer.append((pmean_minimizer))\r\n                self.loop.iteration_end_event.append(optimize_posterior_mean_event)\r\n                self.loop.run_loop(self.fidelity_func_wrapper(), num_iters)\r\n        \r\n                return\r\n        \r\n            def gp_model_hf(self, x, fidelity=1.0):\r\n                # self.mfmodel()\r\n                # Expose the GP posterior mean and variance\r\n                assert x.shape[-1] == self.input_dimensions, \"x should not include fidelity parameter\"\r\n                x = np.array(x).reshape(-1, self.input_dimensions)\r\n                fid = np.ones(x.shape[0]).reshape(-1,1) * fidelity\r\n                x_aug = np.hstack((x, fid))\r\n                return self.loop.model_updaters[0].model.predict(x_aug)\r\n        \r\n            def optimize_posterior_mean(self, opt='minimize'):\r\n                # max/min the posterior mean at any fidelity. Usually we are interested in highest fidelity\r\n                if opt.lower() == 'minimize':\r\n                    print('minimizing posterior mean at highest fidelity')\r\n                    obj = lambda x: self.gp_model_hf(x, 1.0)[0] # [0] returns mean, [1] returns var\r\n                elif opt.lower() == 'maximize':\r\n                    print('maximizing posterior mean at highest fidelity')\r\n                    obj = lambda x: -1. * self.gp_model_hf(x, 1.0)[0]\r\n                res = minimize(obj, np.ones(self.input_dimensions) * 0.5, method='Nelder-Mead', tol=1e-6)\r\n                return res.x\r\n`\r\n\r\n\r\nI am calling `emukit_wrapper.py` from another file called `emukit_forrester_example.py` in which the problem routine for the two-fidelity Forrester function is defined.\r\n\r\n          `import sys\r\n          #sys.path.append('../src/')\r\n          import os\r\n          from emukit_wrapper import *\r\n          import emukit.test_functions.forrester # test function\r\n          import numpy as np\r\n          import argparse\r\n          \r\n          parser = argparse.ArgumentParser()\r\n          parser.add_argument('niters', type=int, default=1, help='number of BO iterations')\r\n          parser.add_argument('repetitions', type=int, default=1, help='number of independent BO repetitions')\r\n          args = parser.parse_args()\r\n          niters = args.niters\r\n          repetitions = args.repetitions\r\n          \r\n          # define test function\r\n          forrester_fcn, _ = emukit.test_functions.forrester.multi_fidelity_forrester_function()\r\n          forrester_fcn_low = forrester_fcn.f[0]\r\n          forrester_fcn_high = forrester_fcn.f[1]\r\n          \r\n          \r\n          \r\n          ## Assign costs\r\n          low_fidelity_cost = 1\r\n          high_fidelity_cost = 5\r\n          \r\n          print(niters,repetitions,forrester_fcn.f[0])\r\n          \r\n          fidelities = [forrester_fcn_low, forrester_fcn_high]\r\n          n_fidelities = 2\r\n          costs = [low_fidelity_cost, high_fidelity_cost]\r\n          verbose_optimization = True\r\n          ## create object instance\r\n          mf = EmukitMultifidelity(fidelities, n_fidelities, costs,verbose_optimization)\r\n          \r\n          for rep in range(repetitions):\r\n              \r\n              np.random.seed(rep + 123)\r\n              post_maximizer = []\r\n              final_rec = []\r\n          \r\n              x_low  = np.random.rand(12)[:, None]\r\n              x_high = x_low[:6, :]\r\n              y_low  = forrester_fcn_low(x_low)\r\n              y_high = forrester_fcn_high(x_high)\r\n              x_list = [x_low.astype(float), x_high.astype(float)]\r\n              y_list = [y_low.astype(float), y_high.astype(float)]\r\n              \r\n              print(\"\\n x : \\n\",x_low,x_high,x_list)\r\n              print(\"\\n y : \\n\",y_low,y_high,y_list)\r\n              \r\n              mf.initialize(x_list, y_list)\r\n              mf.mfmodel()\r\n              mf.multifidelity_acquisition_function()\r\n              mf.mfbo_loop(niters)\r\n          \r\n          \r\n              # save to file\r\n              X = np.concatenate(mf.loop.loop_state.X).reshape(-1, mf.input_dimensions+1)\r\n              Y = np.concatenate(mf.loop.loop_state.Y)\r\n              muxstar = np.concatenate(mf.pmean_minimizer).reshape(-1, mf.input_dimensions) # posterior mean minimizer\\\r\n              costs = mf.cost_acquisition.evaluate(X)\r\n              np.save('rep' + str(rep) + '_' + 'posterior_mean_minimizer.npy', muxstar)\r\n              np.save('rep' + str(rep) + '_' + 'X.npy', X)\r\n              np.save('rep' + str(rep) + '_' + 'Y.npy', Y)\r\n              np.save('rep' + str(rep) + '_' + 'costs.npy', costs)\r\n`\r\n Running `emukit_forrester_example.py` throws the following error \r\n\r\n        `Traceback (most recent call last):\r\n          File \"emukit_forrester_example.py\", line 55, in <module>\r\n            mf.mfbo_loop(niters)\r\n          File \"C:\\Users\\18503\\emukit\\emukit_wrapper.py\", line 131, in mfbo_loop\r\n            self.loop.run_loop(self.fidelity_func_wrapper(), num_iters)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\loop\\outer_loop.py\", line 92, in run_loop\r\n            new_x = self.candidate_point_calculator.compute_next_points(self.loop_state, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\loop\\candidate_point_calculators.py\", line 50, in compute_next_points\r\n            x, _ = self.acquisition_optimizer.optimize(self.acquisition, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\multi_source_acquisition_optimizer.py\", line 74, in optimize\r\n            x, f_maxs[i] = self.acquisition_optimizer.optimize(acquisition, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\acquisition_optimizer.py\", line 65, in optimize\r\n            max_x, max_value = self._optimize(acquisition, context_manager)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\gradient_acquisition_optimizer.py\", line 58, in _optimize\r\n            anchor_points = anchor_points_generator.get(num_anchor=1, context_manager=context_manager)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\anchor_points_generator.py\", line 62, in get\r\n            scores = self.get_anchor_point_scores(X)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\anchor_points_generator.py\", line 90, in get_anchor_point_scores\r\n            scores[are_constraints_satisfied] = self.acquisition.evaluate(X[are_constraints_satisfied, :])[:, 0]\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\acquisition\\acquisition.py\", line 96, in evaluate\r\n            return self.numerator.evaluate(x) / self.denominator.evaluate(x)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\bayesian_optimization\\acquisitions\\entropy_search.py\", line 157, in evaluate\r\n            results[j] = self.evaluate(x[j, None, :])\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\bayesian_optimization\\acquisitions\\entropy_search.py\", line 165, in evaluate\r\n            dMdx, dVdx = self._innovations(x)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\bayesian_optimization\\acquisitions\\entropy_search.py\", line 207, in _innovations\r\n            sigma_x_rep = self.model.get_covariance_between_points(self.representer_points, x)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\model_wrappers\\gpy_model_wrappers.py\", line 313, in get_covariance_between_points\r\n            return self.gpy_model.posterior_covariance_between_points(X1, X2)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\GPy\\core\\gp.py\", line 774, in posterior_covariance_between_points\r\n            Y_metadata=Y_metadata)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\GPy\\likelihoods\\mixed_noise.py\", line 44, in predictive_values\r\n            ind = Y_metadata['output_index'].flatten()\r\n        TypeError: 'NoneType' object is not subscriptable`\r\n\r\n\r\nIn previous issues, i saw that this can be fixed by setting  `include_likelihood = False` in the function definition of  posterior_covariance_between_points ( in file `~/lib/python3.7/site-packages/GPy/core/gp.py`)\r\n\r\n` posterior_covariance_between_points(self, X1, X2, Y_metadata=None, \r\n                                              likelihood=None, include_likelihood=False)`\r\n\r\nAfter setting  `include_likelihood = False`, I face a compatibility issue with scipy. \r\n\r\n        `  File \"emukit_forrester_example.py\", line 58, in <module>\r\n            mf.mfbo_loop(niters)\r\n          File \"C:\\Users\\18503\\emukit\\emukit_wrapper.py\", line 131, in mfbo_loop\r\n            self.loop.run_loop(self.fidelity_func_wrapper(), num_iters)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\loop\\outer_loop.py\", line 92, in run_loop\r\n            new_x = self.candidate_point_calculator.compute_next_points(self.loop_state, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\loop\\candidate_point_calculators.py\", line 50, in compute_next_points\r\n            x, _ = self.acquisition_optimizer.optimize(self.acquisition, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\multi_source_acquisition_optimizer.py\", line 74, in optimize\r\n            x, f_maxs[i] = self.acquisition_optimizer.optimize(acquisition, context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\acquisition_optimizer.py\", line 65, in optimize\r\n            max_x, max_value = self._optimize(acquisition, context_manager)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\gradient_acquisition_optimizer.py\", line 64, in _optimize\r\n            context_manager=context_manager)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\optimizer.py\", line 117, in apply_optimizer\r\n            optimized_x, _ = optimizer.optimize(problem.x0_no_context, f_no_context, df_no_context, f_df_no_context)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\emukit\\core\\optimization\\optimizer.py\", line 62, in optimize\r\n            res = scipy.optimize.fmin_l_bfgs_b(f, x0=x0, bounds=self.bounds, approx_grad=True, maxiter=self.max_iterations)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 198, in fmin_l_bfgs_b\r\n            **opts)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\", line 308, in _minimize_lbfgsb\r\n            finite_diff_rel_step=finite_diff_rel_step)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\optimize.py\", line 262, in _prepare_scalar_function\r\n            finite_diff_rel_step, bounds, epsilon=epsilon)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 159, in __init__\r\n            self._update_grad()\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 238, in _update_grad\r\n            self._update_grad_impl()\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\", line 156, in update_grad\r\n            **finite_diff_options)\r\n          File \"C:\\Users\\18503\\AppData\\Local\\Continuum\\anaconda3\\envs\\dragonfly\\lib\\site-packages\\scipy\\optimize\\_numdiff.py\", line 448, in approx_derivative\r\n            raise ValueError(\"`f0` passed has more than 1 dimension.\")\r\n        ValueError: `f0` passed has more than 1 dimension.`\r\n \r\nAny suggestion on how could I fix that. ","closed_by":{"login":"apaleyes","id":2852301,"node_id":"MDQ6VXNlcjI4NTIzMDE=","avatar_url":"https://avatars.githubusercontent.com/u/2852301?v=4","gravatar_id":"","url":"https://api.github.com/users/apaleyes","html_url":"https://github.com/apaleyes","followers_url":"https://api.github.com/users/apaleyes/followers","following_url":"https://api.github.com/users/apaleyes/following{/other_user}","gists_url":"https://api.github.com/users/apaleyes/gists{/gist_id}","starred_url":"https://api.github.com/users/apaleyes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/apaleyes/subscriptions","organizations_url":"https://api.github.com/users/apaleyes/orgs","repos_url":"https://api.github.com/users/apaleyes/repos","events_url":"https://api.github.com/users/apaleyes/events{/privacy}","received_events_url":"https://api.github.com/users/apaleyes/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/EmuKit/emukit/issues/387/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":2,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/EmuKit/emukit/issues/387/timeline","performed_via_github_app":null,"state_reason":"completed"}