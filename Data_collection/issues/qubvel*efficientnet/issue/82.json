{"url":"https://api.github.com/repos/qubvel/efficientnet/issues/82","repository_url":"https://api.github.com/repos/qubvel/efficientnet","labels_url":"https://api.github.com/repos/qubvel/efficientnet/issues/82/labels{/name}","comments_url":"https://api.github.com/repos/qubvel/efficientnet/issues/82/comments","events_url":"https://api.github.com/repos/qubvel/efficientnet/issues/82/events","html_url":"https://github.com/qubvel/efficientnet/issues/82","id":523946114,"node_id":"MDU6SXNzdWU1MjM5NDYxMTQ=","number":82,"title":"transfer learning - retrain  EfficientNetB7 RTX 2070 out of memory ","user":{"login":"ntedgi","id":31243793,"node_id":"MDQ6VXNlcjMxMjQzNzkz","avatar_url":"https://avatars.githubusercontent.com/u/31243793?v=4","gravatar_id":"","url":"https://api.github.com/users/ntedgi","html_url":"https://github.com/ntedgi","followers_url":"https://api.github.com/users/ntedgi/followers","following_url":"https://api.github.com/users/ntedgi/following{/other_user}","gists_url":"https://api.github.com/users/ntedgi/gists{/gist_id}","starred_url":"https://api.github.com/users/ntedgi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ntedgi/subscriptions","organizations_url":"https://api.github.com/users/ntedgi/orgs","repos_url":"https://api.github.com/users/ntedgi/repos","events_url":"https://api.github.com/users/ntedgi/events{/privacy}","received_events_url":"https://api.github.com/users/ntedgi/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-11-17T06:53:47Z","updated_at":"2019-12-12T06:03:37Z","closed_at":"2019-12-12T06:03:37Z","author_association":"NONE","active_lock_reason":null,"body":"```\r\n\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.7\r\ntf.keras.backend.set_session(tf.Session(config=config))\r\n\r\nmodel = efn.EfficientNetB7()\r\nmodel.summary()\r\n\r\n# create new output layer\r\noutput_layer = Dense(5, activation='sigmoid', name=\"retrain_output\")(model.get_layer('top_dropout').output)\r\nnew_model = Model(model.input, output=output_layer)\r\nnew_model.summary()\r\n# lock previous weights\r\n\r\nfor i, l in enumerate(new_model.layers):\r\n    if i < 228:\r\n        l.trainable = False\r\n# lock probs weights\r\n\r\nnew_model.compile(loss='mean_squared_error', optimizer='adam')\r\n\r\nbatch_size = 5\r\nsamples_per_epoch = 30\r\nepochs = 20\r\n\r\n# generate train data\r\ntrain_datagen = ImageDataGenerator(\r\n    shear_range=0.2,\r\n    zoom_range=0.2,\r\n    horizontal_flip=True,\r\n    validation_split=0)\r\n\r\ntrain_generator = train_datagen.flow_from_directory(\r\n    train_data_input_folder,\r\n    target_size=(input_dim, input_dim),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    seed=2019,\r\n    subset='training')\r\n\r\nvalidation_generator = train_datagen.flow_from_directory(\r\n    validation_data_input_folder,\r\n    target_size=(input_dim, input_dim),\r\n    batch_size=batch_size,\r\n    class_mode='categorical',\r\n    seed=2019,\r\n    subset='validation')\r\n\r\nnew_model.fit_generator(\r\n    train_generator,\r\n    samples_per_epoch=samples_per_epoch,\r\n    epochs=epochs,\r\n    validation_steps=20,\r\n    validation_data=validation_generator,\r\n    nb_worker=24)\r\n\r\nnew_model.save(model_output_path)\r\n\r\n```\r\n\r\n> \r\n> \r\n> 2019-11-17 08:52:52.903583: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\r\n> ....\r\n> ...\r\n> 2019-11-17 08:53:24.713020: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 110 Chunks of size 27724800 totalling 2.84GiB\r\n> 2019-11-17 08:53:24.713024: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 6 Chunks of size 38814720 totalling 222.10MiB\r\n> 2019-11-17 08:53:24.713027: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 23 Chunks of size 54000128 totalling 1.16GiB\r\n> 2019-11-17 08:53:24.713031: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 1 Chunks of size 73760000 totalling 70.34MiB\r\n> 2019-11-17 08:53:24.713034: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 5.45GiB\r\n> 2019-11-17 08:53:24.713040: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: \r\n> Limit:                  5856749158\r\n> InUse:                  5848048896\r\n> MaxInUse:               5848061440\r\n> NumAllocs:                    6140\r\n> MaxAllocSize:           3259170816\r\n> \r\n> 2019-11-17 08:53:24.713214: W tensorflow/core/common_runtime/bfc_allocator.cc:271] ****************************************************************************************************\r\n> 2019-11-17 08:53:24.713232: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at cwise_ops_common.cc:70 : Resource exhausted: OOM when allocating tensor with shape[5,1344,38,38] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n> Traceback (most recent call last):\r\n>   File \"/home/naort/Desktop/deep-learning-data-preparation-tools/EfficientNet-Transfer-Learning-Boiler-Plate/model_retrain.py\", line 76, in <module>\r\n>     nb_worker=24)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1732, in fit_generator\r\n>     initial_epoch=initial_epoch)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\", line 220, in fit_generator\r\n>     reset_metrics=False)\r\n>   File \"/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\", line 1514, in train_on_batch\r\n>     outputs = self.train_function(ins)\r\n>   File \"/home/naort/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 3076, in __call__\r\n>     run_metadata=self.run_metadata)\r\n>   File \"/home/naort/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1439, in __call__\r\n>     run_metadata_ptr)\r\n>   File \"/home/naort/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 528, in __exit__\r\n>     c_api.TF_GetCode(self.status.status))\r\n> tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[5,1344,38,38] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n> \t [[{{node training/Adam/gradients/AddN_387-0-TransposeNHWCToNCHW-LayoutOptimizer}}]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> \t [[{{node Mean}}]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> \r\n> \r\n> ","closed_by":{"login":"ntedgi","id":31243793,"node_id":"MDQ6VXNlcjMxMjQzNzkz","avatar_url":"https://avatars.githubusercontent.com/u/31243793?v=4","gravatar_id":"","url":"https://api.github.com/users/ntedgi","html_url":"https://github.com/ntedgi","followers_url":"https://api.github.com/users/ntedgi/followers","following_url":"https://api.github.com/users/ntedgi/following{/other_user}","gists_url":"https://api.github.com/users/ntedgi/gists{/gist_id}","starred_url":"https://api.github.com/users/ntedgi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ntedgi/subscriptions","organizations_url":"https://api.github.com/users/ntedgi/orgs","repos_url":"https://api.github.com/users/ntedgi/repos","events_url":"https://api.github.com/users/ntedgi/events{/privacy}","received_events_url":"https://api.github.com/users/ntedgi/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/qubvel/efficientnet/issues/82/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/qubvel/efficientnet/issues/82/timeline","performed_via_github_app":null,"state_reason":"completed"}