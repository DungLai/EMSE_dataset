{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14","repository_url":"https://api.github.com/repos/wentaoyuan/pcn","labels_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14/labels{/name}","comments_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14/comments","events_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14/events","html_url":"https://github.com/wentaoyuan/pcn/issues/14","id":393608338,"node_id":"MDU6SXNzdWUzOTM2MDgzMzg=","number":14,"title":"OOM in training for shapenet","user":{"login":"mengyuest","id":8606221,"node_id":"MDQ6VXNlcjg2MDYyMjE=","avatar_url":"https://avatars.githubusercontent.com/u/8606221?v=4","gravatar_id":"","url":"https://api.github.com/users/mengyuest","html_url":"https://github.com/mengyuest","followers_url":"https://api.github.com/users/mengyuest/followers","following_url":"https://api.github.com/users/mengyuest/following{/other_user}","gists_url":"https://api.github.com/users/mengyuest/gists{/gist_id}","starred_url":"https://api.github.com/users/mengyuest/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mengyuest/subscriptions","organizations_url":"https://api.github.com/users/mengyuest/orgs","repos_url":"https://api.github.com/users/mengyuest/repos","events_url":"https://api.github.com/users/mengyuest/events{/privacy}","received_events_url":"https://api.github.com/users/mengyuest/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2018-12-21T22:33:02Z","updated_at":"2019-01-14T19:08:00Z","closed_at":"2019-01-14T19:08:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hi. I ran `python3 train.py` for shapenet car dataset on my computer with `GTX1080TI` and it raised OOM error. I am using `ubuntu16.04` and `tensorflow1.10`, with `cuda9` and `cudnn7`. Later I reduced the batchsize to 16 and it worked fine but `nvidia-smi` showed the RAM usage was `9425MiB / 11175MiB`. I wonder how to run `batchsize=32` as mentioned in the paper. Thanks for your help.\r\n\r\nFollowed is some log during training with `batchsize=32`\r\n\r\n```\r\n/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n[1221 14:15:48 @format.py:92] Found 45416 entries in data/shapenet/train.lmdb\r\n[1221 14:15:48 @parallel.py:291] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\r\n[1221 14:15:48 @format.py:92] Found 100 entries in data/shapenet/valid.lmdb\r\nWARNING:tensorflow:From /home/meng/pcn/models/pcn_cd.py:22: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nlog/pcn_cd exists. Delete? [y (or enter)/N]y\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,1029,1,16384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: gradients/folding/conv_0/conv1d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](folding/conv_0/conv1d/ExpandDims, PermConstNHWCToNCHW-LayoutOptimizer)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: add_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_492_add_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 161, in <module>\r\n    train(args)\r\n  File \"train.py\", line 97, in train\r\n    feed_dict={is_training_pl: True})\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,1029,1,16384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: gradients/folding/conv_0/conv1d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](folding/conv_0/conv1d/ExpandDims, PermConstNHWCToNCHW-LayoutOptimizer)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: add_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_492_add_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\nPrefetchDataZMQ successfully cleaned-up.\r\n```","closed_by":{"login":"wentaoyuan","id":7258482,"node_id":"MDQ6VXNlcjcyNTg0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7258482?v=4","gravatar_id":"","url":"https://api.github.com/users/wentaoyuan","html_url":"https://github.com/wentaoyuan","followers_url":"https://api.github.com/users/wentaoyuan/followers","following_url":"https://api.github.com/users/wentaoyuan/following{/other_user}","gists_url":"https://api.github.com/users/wentaoyuan/gists{/gist_id}","starred_url":"https://api.github.com/users/wentaoyuan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wentaoyuan/subscriptions","organizations_url":"https://api.github.com/users/wentaoyuan/orgs","repos_url":"https://api.github.com/users/wentaoyuan/repos","events_url":"https://api.github.com/users/wentaoyuan/events{/privacy}","received_events_url":"https://api.github.com/users/wentaoyuan/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14/timeline","performed_via_github_app":null,"state_reason":"completed"}