[{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/comments/453358602","html_url":"https://github.com/wentaoyuan/pcn/issues/14#issuecomment-453358602","issue_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14","id":453358602,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1MzM1ODYwMg==","user":{"login":"XLechter","id":41005111,"node_id":"MDQ6VXNlcjQxMDA1MTEx","avatar_url":"https://avatars.githubusercontent.com/u/41005111?v=4","gravatar_id":"","url":"https://api.github.com/users/XLechter","html_url":"https://github.com/XLechter","followers_url":"https://api.github.com/users/XLechter/followers","following_url":"https://api.github.com/users/XLechter/following{/other_user}","gists_url":"https://api.github.com/users/XLechter/gists{/gist_id}","starred_url":"https://api.github.com/users/XLechter/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/XLechter/subscriptions","organizations_url":"https://api.github.com/users/XLechter/orgs","repos_url":"https://api.github.com/users/XLechter/repos","events_url":"https://api.github.com/users/XLechter/events{/privacy}","received_events_url":"https://api.github.com/users/XLechter/received_events","type":"User","site_admin":false},"created_at":"2019-01-11T03:02:46Z","updated_at":"2019-01-11T03:02:46Z","author_association":"NONE","body":"> Hi. I ran `python3 train.py` for shapenet car dataset on my computer with `GTX1080TI` and it raised OOM error. I am using `ubuntu16.04` and `tensorflow1.10`, with `cuda9` and `cudnn7`. Later I reduced the batchsize to 16 and it worked fine but `nvidia-smi` showed the RAM usage was `9425MiB / 11175MiB`. I wonder how to run `batchsize=32` as mentioned in the paper. Thanks for your help.\r\n> \r\n> Followed is some log during training with `batchsize=32`\r\n> \r\n> ```\r\n> /usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n>   from ._conv import register_converters as _register_converters\r\n> [1221 14:15:48 @format.py:92] Found 45416 entries in data/shapenet/train.lmdb\r\n> [1221 14:15:48 @parallel.py:291] [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\r\n> [1221 14:15:48 @format.py:92] Found 100 entries in data/shapenet/valid.lmdb\r\n> WARNING:tensorflow:From /home/meng/pcn/models/pcn_cd.py:22: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\n> Instructions for updating:\r\n> keep_dims is deprecated, use keepdims instead\r\n> log/pcn_cd exists. Delete? [y (or enter)/N]y\r\n> Traceback (most recent call last):\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n>     return fn(*args)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n>     options, feed_dict, fetch_list, target_list, run_metadata)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n>     run_metadata)\r\n> tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,1029,1,16384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n> \t [[Node: gradients/folding/conv_0/conv1d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](folding/conv_0/conv1d/ExpandDims, PermConstNHWCToNCHW-LayoutOptimizer)]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> \t [[Node: add_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_492_add_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"train.py\", line 161, in <module>\r\n>     train(args)\r\n>   File \"train.py\", line 97, in train\r\n>     feed_dict={is_training_pl: True})\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n>     run_metadata_ptr)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n>     feed_dict_tensor, options, run_metadata)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n>     run_metadata)\r\n>   File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n>     raise type(e)(node_def, op, message)\r\n> tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[32,1029,1,16384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n> \t [[Node: gradients/folding/conv_0/conv1d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](folding/conv_0/conv1d/ExpandDims, PermConstNHWCToNCHW-LayoutOptimizer)]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> \t [[Node: add_2/_85 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_492_add_2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n> \r\n> PrefetchDataZMQ successfully cleaned-up.\r\n> ```\r\n\r\nI am training the model 'pcn_cd' using single GTX1080TI with batchsize=32 and there are no OOM warnings. Maybe some other settings increace your memory consumption?","reactions":{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/comments/453358602/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"XLechter","id":41005111,"node_id":"MDQ6VXNlcjQxMDA1MTEx","avatar_url":"https://avatars.githubusercontent.com/u/41005111?v=4","gravatar_id":"","url":"https://api.github.com/users/XLechter","html_url":"https://github.com/XLechter","followers_url":"https://api.github.com/users/XLechter/followers","following_url":"https://api.github.com/users/XLechter/following{/other_user}","gists_url":"https://api.github.com/users/XLechter/gists{/gist_id}","starred_url":"https://api.github.com/users/XLechter/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/XLechter/subscriptions","organizations_url":"https://api.github.com/users/XLechter/orgs","repos_url":"https://api.github.com/users/XLechter/repos","events_url":"https://api.github.com/users/XLechter/events{/privacy}","received_events_url":"https://api.github.com/users/XLechter/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/comments/454123635","html_url":"https://github.com/wentaoyuan/pcn/issues/14#issuecomment-454123635","issue_url":"https://api.github.com/repos/wentaoyuan/pcn/issues/14","id":454123635,"node_id":"MDEyOklzc3VlQ29tbWVudDQ1NDEyMzYzNQ==","user":{"login":"wentaoyuan","id":7258482,"node_id":"MDQ6VXNlcjcyNTg0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7258482?v=4","gravatar_id":"","url":"https://api.github.com/users/wentaoyuan","html_url":"https://github.com/wentaoyuan","followers_url":"https://api.github.com/users/wentaoyuan/followers","following_url":"https://api.github.com/users/wentaoyuan/following{/other_user}","gists_url":"https://api.github.com/users/wentaoyuan/gists{/gist_id}","starred_url":"https://api.github.com/users/wentaoyuan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wentaoyuan/subscriptions","organizations_url":"https://api.github.com/users/wentaoyuan/orgs","repos_url":"https://api.github.com/users/wentaoyuan/repos","events_url":"https://api.github.com/users/wentaoyuan/events{/privacy}","received_events_url":"https://api.github.com/users/wentaoyuan/received_events","type":"User","site_admin":false},"created_at":"2019-01-14T19:08:00Z","updated_at":"2019-01-14T19:08:00Z","author_association":"OWNER","body":"@mengyuest The memory consumption using batchsize=32 is 10793MiB / 11178MiB on my 1080Ti, so you need to have a completely free GPU without any process running on it. You can try reducing the batchsize and increasing the number of training steps accordingly. The results wouldn't be affected by much.","reactions":{"url":"https://api.github.com/repos/wentaoyuan/pcn/issues/comments/454123635/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"wentaoyuan","id":7258482,"node_id":"MDQ6VXNlcjcyNTg0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7258482?v=4","gravatar_id":"","url":"https://api.github.com/users/wentaoyuan","html_url":"https://github.com/wentaoyuan","followers_url":"https://api.github.com/users/wentaoyuan/followers","following_url":"https://api.github.com/users/wentaoyuan/following{/other_user}","gists_url":"https://api.github.com/users/wentaoyuan/gists{/gist_id}","starred_url":"https://api.github.com/users/wentaoyuan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wentaoyuan/subscriptions","organizations_url":"https://api.github.com/users/wentaoyuan/orgs","repos_url":"https://api.github.com/users/wentaoyuan/repos","events_url":"https://api.github.com/users/wentaoyuan/events{/privacy}","received_events_url":"https://api.github.com/users/wentaoyuan/received_events","type":"User","site_admin":false}},{"id":2072672083,"node_id":"MDExOkNsb3NlZEV2ZW50MjA3MjY3MjA4Mw==","url":"https://api.github.com/repos/wentaoyuan/pcn/issues/events/2072672083","actor":{"login":"wentaoyuan","id":7258482,"node_id":"MDQ6VXNlcjcyNTg0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7258482?v=4","gravatar_id":"","url":"https://api.github.com/users/wentaoyuan","html_url":"https://github.com/wentaoyuan","followers_url":"https://api.github.com/users/wentaoyuan/followers","following_url":"https://api.github.com/users/wentaoyuan/following{/other_user}","gists_url":"https://api.github.com/users/wentaoyuan/gists{/gist_id}","starred_url":"https://api.github.com/users/wentaoyuan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wentaoyuan/subscriptions","organizations_url":"https://api.github.com/users/wentaoyuan/orgs","repos_url":"https://api.github.com/users/wentaoyuan/repos","events_url":"https://api.github.com/users/wentaoyuan/events{/privacy}","received_events_url":"https://api.github.com/users/wentaoyuan/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2019-01-14T19:08:00Z","state_reason":null,"performed_via_github_app":null},{"id":2072672088,"node_id":"MDE0Ok1lbnRpb25lZEV2ZW50MjA3MjY3MjA4OA==","url":"https://api.github.com/repos/wentaoyuan/pcn/issues/events/2072672088","actor":{"login":"mengyuest","id":8606221,"node_id":"MDQ6VXNlcjg2MDYyMjE=","avatar_url":"https://avatars.githubusercontent.com/u/8606221?v=4","gravatar_id":"","url":"https://api.github.com/users/mengyuest","html_url":"https://github.com/mengyuest","followers_url":"https://api.github.com/users/mengyuest/followers","following_url":"https://api.github.com/users/mengyuest/following{/other_user}","gists_url":"https://api.github.com/users/mengyuest/gists{/gist_id}","starred_url":"https://api.github.com/users/mengyuest/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mengyuest/subscriptions","organizations_url":"https://api.github.com/users/mengyuest/orgs","repos_url":"https://api.github.com/users/mengyuest/repos","events_url":"https://api.github.com/users/mengyuest/events{/privacy}","received_events_url":"https://api.github.com/users/mengyuest/received_events","type":"User","site_admin":false},"event":"mentioned","commit_id":null,"commit_url":null,"created_at":"2019-01-14T19:08:00Z","performed_via_github_app":null},{"id":2072672090,"node_id":"MDE1OlN1YnNjcmliZWRFdmVudDIwNzI2NzIwOTA=","url":"https://api.github.com/repos/wentaoyuan/pcn/issues/events/2072672090","actor":{"login":"mengyuest","id":8606221,"node_id":"MDQ6VXNlcjg2MDYyMjE=","avatar_url":"https://avatars.githubusercontent.com/u/8606221?v=4","gravatar_id":"","url":"https://api.github.com/users/mengyuest","html_url":"https://github.com/mengyuest","followers_url":"https://api.github.com/users/mengyuest/followers","following_url":"https://api.github.com/users/mengyuest/following{/other_user}","gists_url":"https://api.github.com/users/mengyuest/gists{/gist_id}","starred_url":"https://api.github.com/users/mengyuest/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mengyuest/subscriptions","organizations_url":"https://api.github.com/users/mengyuest/orgs","repos_url":"https://api.github.com/users/mengyuest/repos","events_url":"https://api.github.com/users/mengyuest/events{/privacy}","received_events_url":"https://api.github.com/users/mengyuest/received_events","type":"User","site_admin":false},"event":"subscribed","commit_id":null,"commit_url":null,"created_at":"2019-01-14T19:08:00Z","performed_via_github_app":null}]