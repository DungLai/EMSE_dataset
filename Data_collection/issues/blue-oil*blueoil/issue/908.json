{"url":"https://api.github.com/repos/blue-oil/blueoil/issues/908","repository_url":"https://api.github.com/repos/blue-oil/blueoil","labels_url":"https://api.github.com/repos/blue-oil/blueoil/issues/908/labels{/name}","comments_url":"https://api.github.com/repos/blue-oil/blueoil/issues/908/comments","events_url":"https://api.github.com/repos/blue-oil/blueoil/issues/908/events","html_url":"https://github.com/blue-oil/blueoil/issues/908","id":578457993,"node_id":"MDU6SXNzdWU1Nzg0NTc5OTM=","number":908,"title":"make an inference faster in python","user":{"login":"masato0412","id":27075686,"node_id":"MDQ6VXNlcjI3MDc1Njg2","avatar_url":"https://avatars.githubusercontent.com/u/27075686?v=4","gravatar_id":"","url":"https://api.github.com/users/masato0412","html_url":"https://github.com/masato0412","followers_url":"https://api.github.com/users/masato0412/followers","following_url":"https://api.github.com/users/masato0412/following{/other_user}","gists_url":"https://api.github.com/users/masato0412/gists{/gist_id}","starred_url":"https://api.github.com/users/masato0412/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masato0412/subscriptions","organizations_url":"https://api.github.com/users/masato0412/orgs","repos_url":"https://api.github.com/users/masato0412/repos","events_url":"https://api.github.com/users/masato0412/events{/privacy}","received_events_url":"https://api.github.com/users/masato0412/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":true,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-03-10T09:52:37Z","updated_at":"2020-03-10T09:52:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"The difference between the speed of lm_fpga.elf and that of demo script in python.\r\n```\r\nlm_fpga.elf result\r\n-------------------------------------------------------------\r\nComparison: Default network test  succeeded!!!\r\n-------------------------------------------------------------\r\nTotalInitTime 42411,  sum:42.411ms\r\nTotalRunTime 85897,  sum:85.897ms\r\n..Lookup 3612,  sum:3.612ms\r\n..QuantizedConv2D 4751,9907,2638,1261,1051,937,587,325,146,207,376,1102,201,569,2021,7841,10844,10853,  sum:55.617ms\r\n....Convert Tensor 1511,1469,331,95,41,21,13,9,16,25,54,24,8,13,42,296,1502,1558,  sum:7.028ms\r\n....Sync UDMABuf Input 1406,1349,470,262,137,76,46,21,46,73,133,78,22,45,137,466,1355,1303,  sum:7.425ms\r\n....Conv2D TCA 851,6128,1560,803,799,790,491,257,42,61,94,961,123,415,1564,6122,6130,6123,  sum:33.314ms\r\n....Sync UDMABuf Output 928,929,249,79,49,31,18,21,20,29,76,19,29,77,259,932,1822,1840,  sum:7.407ms\r\n..Memcpy 1469,1462,439,99,55,30,14,18,17,32,92,14,29,95,391,1536,  sum:5.792ms\r\n..ExtractImagePatches 2012,637,82,69,25,8,39,78,  sum:2.95ms\r\n..func_ConcatOnDepth 40,3080,  sum:3.12ms\r\n..DepthToSpace 46,163,643,2560,  sum:3.412ms\r\n..QuantizedConv2D_ApplyScalingFactor 2905,2112,  sum:5.017ms\r\n..BatchNorm 1908,946,  sum:2.854ms\r\n..Add 1378,624,  sum:2.002ms\r\n```\r\n\r\n```\r\nrun.py result\r\nINFO:__main__:Benchmark avg result(sec) for 20 trials: pre_process: 0.03704105  inference: 0.09313415 post_process: 0.0644722  Total: 0.1946474\r\n```\r\n\r\nAbout 10ms difference is seen in inference.\r\nMeasure processing in `nnlib.py`\r\n```\r\n>>> flatten, 3.1120777130126953 ms\r\n>>> cast, 3.3941268920898438 ms\r\n>>> zeros, 0.5159378051757812 ms\r\n>>> inference, 85.60395240783691 ms\r\n```\r\nCaused by python processing before inference.\r\n```\r\ntensor.flatten() â†’ tensor.ravel()\r\nflatten returns a copy of the input array, flattened to one dimension (Using different memory from the original array).\r\nravel returns new view object if possible (Refers to the same memory as the original array).\r\n>>> ravel, 0.02288818359375 ms\r\n>>> np.unique(tensor.ravel() - tensor.flatten()))\r\n>>> [0.]\r\n```\r\n\r\nEliminate cast to float32 before inference then fixed to float32 when converting from PIL to numpy array in resize function.\r\n```\r\n>>>pre_process: 0.03395725\r\n```\r\npreprocess is slightly faster.\r\n\r\nSince a zero array with the same shape as output is generated each time, it overwrites the result of the previous inference.\r\n\r\nresult:\r\n```\r\nINFO:__main__:Benchmark avg result(sec) for 20 trials: pre_process: 0.03395725  inference: 0.0850629 post_process: 0.0656032  Total: 0.18462335\r\n```\r\n\r\ngot the same inference speed as lm_fpga.elf.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/blue-oil/blueoil/issues/908/reactions","total_count":3,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":3,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/blue-oil/blueoil/issues/908/timeline","performed_via_github_app":null,"state_reason":null}