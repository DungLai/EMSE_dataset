{"url":"https://api.github.com/repos/theislab/diffxpy/issues/141","repository_url":"https://api.github.com/repos/theislab/diffxpy","labels_url":"https://api.github.com/repos/theislab/diffxpy/issues/141/labels{/name}","comments_url":"https://api.github.com/repos/theislab/diffxpy/issues/141/comments","events_url":"https://api.github.com/repos/theislab/diffxpy/issues/141/events","html_url":"https://github.com/theislab/diffxpy/issues/141","id":570040972,"node_id":"MDU6SXNzdWU1NzAwNDA5NzI=","number":141,"title":"Unexpected / Uninformative Error messages with simple input data and two_sample 'wald' or 'lrt'","user":{"login":"dburkhardt","id":8322751,"node_id":"MDQ6VXNlcjgzMjI3NTE=","avatar_url":"https://avatars.githubusercontent.com/u/8322751?v=4","gravatar_id":"","url":"https://api.github.com/users/dburkhardt","html_url":"https://github.com/dburkhardt","followers_url":"https://api.github.com/users/dburkhardt/followers","following_url":"https://api.github.com/users/dburkhardt/following{/other_user}","gists_url":"https://api.github.com/users/dburkhardt/gists{/gist_id}","starred_url":"https://api.github.com/users/dburkhardt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dburkhardt/subscriptions","organizations_url":"https://api.github.com/users/dburkhardt/orgs","repos_url":"https://api.github.com/users/dburkhardt/repos","events_url":"https://api.github.com/users/dburkhardt/events{/privacy}","received_events_url":"https://api.github.com/users/dburkhardt/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-02-24T18:25:20Z","updated_at":"2020-02-25T09:26:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I came across the following errors while using Splatter simulated data.\r\n\r\n### Error 1 - Infs and NaNs\r\n\r\n```python\r\nimport numpy as np\r\nimport scanpy as sc\r\nimport diffxpy.api as de\r\n\r\ntest_data = np.array([[ 0,  6, 31,  8,  4, 24, 14, 17,  1,  6],\r\n                      [ 0,  2, 25,  1,  0, 18,  4, 31,  3,  4],\r\n                      [ 3,  4, 17, 11,  3, 22,  7, 27,  1, 21],\r\n                      [ 0,  7, 13,  4,  7, 35,  5, 22,  6,  1],\r\n                      [ 0,  5, 19,  0,  7, 24, 12, 23,  1, 15],\r\n                      [ 1,  5, 34,  0,  8, 29, 19, 28,  4,  2],\r\n                      [ 0,  9, 22,  3, 11, 32, 17, 21,  2,  7],\r\n                      [ 0,  4, 26,  4,  5, 26,  6, 25,  5, 16],\r\n                      [ 0,  6, 46,  0,  5, 96, 24, 23,  0, 12],\r\n                      [ 0,  5, 19,  2,  2, 25,  4, 27,  0,  4]])\r\n\r\n\r\ngrouping = np.array([0, 1, 0, 1, 0, 0, 1, 1, 0, 0])\r\ngene_names = np.arange(10)\r\n\r\nde.test.two_sample(test_data, gene_names=gene_names, grouping=grouping, test='wald', noise_model=\"nb\")\r\n```\r\nraises:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-86-7c3866d394b1> in <module>\r\n----> 1 de.test.two_sample(test_data, gene_names=gene_names, grouping=grouping, test='wald', noise_model=\"nb\")\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in two_sample(data, grouping, as_numeric, test, gene_names, sample_description, noise_model, size_factors, batch_size, backend, train_args, training_strategy, is_sig_zerovar, quick_scale, dtype, **kwargs)\r\n   1028         formula_loc = '~ 1 + grouping'\r\n   1029         formula_scale = '~ 1 + grouping'\r\n-> 1030         de_test = wald(\r\n   1031             data=data,\r\n   1032             factor_loc_totest=\"grouping\",\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in wald(data, factor_loc_totest, coef_to_test, formula_loc, formula_scale, as_numeric, init_a, init_b, gene_names, sample_description, dmat_loc, dmat_scale, constraints_loc, constraints_scale, noise_model, size_factors, batch_size, backend, train_args, training_strategy, quick_scale, dtype, **kwargs)\r\n    715 \r\n    716     # Fit model.\r\n--> 717     model = _fit(\r\n    718         noise_model=noise_model,\r\n    719         data=data,\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in _fit(noise_model, data, design_loc, design_scale, design_loc_names, design_scale_names, constraints_loc, constraints_scale, init_model, init_a, init_b, gene_names, size_factors, batch_size, backend, training_strategy, quick_scale, train_args, close_session, dtype)\r\n    220         raise ValueError('backend=\"%s\" not recognized.' % backend)\r\n    221 \r\n--> 222     estim = Estimator(\r\n    223         input_data=input_data,\r\n    224         init_a=init_a,\r\n\r\n~/.local/lib/python3.8/site-packages/batchglm/train/numpy/glm_nb/estimator.py in __init__(self, input_data, init_a, init_b, batch_size, quick_scale, dtype, **kwargs)\r\n     72             self._train_scale = False\r\n     73 \r\n---> 74         self.model_vars = ModelVars(\r\n     75             init_a=init_a,\r\n     76             init_b=init_b,\r\n\r\n~/.local/lib/python3.8/site-packages/batchglm/train/numpy/base_glm/vars.py in __init__(self, init_a, init_b, constraints_loc, constraints_scale, chunk_size_genes, dtype)\r\n     42 \r\n     43         init_a_clipped = self.np_clip_param(np.asarray(init_a, dtype=dtype), \"a_var\")\r\n---> 44         init_b_clipped = self.np_clip_param(np.asarray(init_b, dtype=dtype), \"b_var\")\r\n     45         self.params = dask.array.from_array(np.concatenate(\r\n     46             [\r\n\r\n/usr/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)\r\n     83 \r\n     84     \"\"\"\r\n---> 85     return array(a, dtype, copy=False, order=order)\r\n     86 \r\n     87 \r\n\r\n~/.local/lib/python3.8/site-packages/dask/array/core.py in __array__(self, dtype, **kwargs)\r\n   1312 \r\n   1313     def __array__(self, dtype=None, **kwargs):\r\n-> 1314         x = self.compute()\r\n   1315         if dtype and x.dtype != dtype:\r\n   1316             x = x.astype(dtype)\r\n\r\n~/.local/lib/python3.8/site-packages/dask/base.py in compute(self, **kwargs)\r\n    163         dask.base.compute\r\n    164         \"\"\"\r\n--> 165         (result,) = compute(self, traverse=False, **kwargs)\r\n    166         return result\r\n    167 \r\n\r\n~/.local/lib/python3.8/site-packages/dask/base.py in compute(*args, **kwargs)\r\n    434     keys = [x.__dask_keys__() for x in collections]\r\n    435     postcomputes = [x.__dask_postcompute__() for x in collections]\r\n--> 436     results = schedule(dsk, keys, **kwargs)\r\n    437     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\r\n    438 \r\n\r\n~/.local/lib/python3.8/site-packages/dask/threaded.py in get(dsk, result, cache, num_workers, pool, **kwargs)\r\n     71                 pools[thread][num_workers] = pool\r\n     72 \r\n---> 73     results = get_async(\r\n     74         pool.apply_async,\r\n     75         len(pool._pool),\r\n\r\n~/.local/lib/python3.8/site-packages/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\r\n    484                         _execute_task(task, data)  # Re-execute locally\r\n    485                     else:\r\n--> 486                         raise_exception(exc, tb)\r\n    487                 res, worker_id = loads(res_info)\r\n    488                 state[\"cache\"][key] = res\r\n\r\n~/.local/lib/python3.8/site-packages/dask/local.py in reraise(exc, tb)\r\n    314     if exc.__traceback__ is not tb:\r\n    315         raise exc.with_traceback(tb)\r\n--> 316     raise exc\r\n    317 \r\n    318 \r\n\r\n~/.local/lib/python3.8/site-packages/dask/local.py in execute_task(key, task_info, dumps, loads, get_id, pack_exception)\r\n    220     try:\r\n    221         task, data = loads(task_info)\r\n--> 222         result = _execute_task(task, data)\r\n    223         id = get_id()\r\n    224         result = dumps((result, id))\r\n\r\n~/.local/lib/python3.8/site-packages/dask/core.py in _execute_task(arg, cache, dsk)\r\n    117         func, args = arg[0], arg[1:]\r\n    118         args2 = [_execute_task(a, cache) for a in args]\r\n--> 119         return func(*args2)\r\n    120     elif not ishashable(arg):\r\n    121         return arg\r\n\r\n~/.local/lib/python3.8/site-packages/scipy/linalg/basic.py in solve_triangular(a, b, trans, lower, unit_diagonal, overwrite_b, debug, check_finite)\r\n    334 \r\n    335     a1 = _asarray_validated(a, check_finite=check_finite)\r\n--> 336     b1 = _asarray_validated(b, check_finite=check_finite)\r\n    337     if len(a1.shape) != 2 or a1.shape[0] != a1.shape[1]:\r\n    338         raise ValueError('expected square matrix')\r\n\r\n~/.local/lib/python3.8/site-packages/scipy/_lib/_util.py in _asarray_validated(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\r\n    244             raise ValueError('masked arrays are not supported')\r\n    245     toarray = np.asarray_chkfinite if check_finite else np.asarray\r\n--> 246     a = toarray(a)\r\n    247     if not objects_ok:\r\n    248         if a.dtype is np.dtype('O'):\r\n\r\n/usr/lib/python3.8/site-packages/numpy/lib/function_base.py in asarray_chkfinite(a, dtype, order)\r\n    496     a = asarray(a, dtype=dtype, order=order)\r\n    497     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():\r\n--> 498         raise ValueError(\r\n    499             \"array must not contain infs or NaNs\")\r\n    500     return a\r\n\r\nValueError: array must not contain infs or NaNs\r\n```\r\n\r\nNow looking at the `test_data`, it's clear there are no infs or NaNs in the data. I don't know `batchglm` so I can't really interpret where this issue is coming up.\r\n\r\n### Error 2: More than one scale parameter with simple data\r\n\r\nWhile hunting for a minimum reproducible example for the above error, I came across the following error that I also don't know how to interpret.\r\n\r\n```python\r\nsimple_data = np.random.normal(loc=100, size=(100,2))\r\nadata = sc.AnnData(simple_data)\r\ngrouping = np.random.choice([0,1], size=simple_data.shape[0])\r\n\r\nde.test.two_sample(adata, grouping=grouping, test='wald', noise_model=\"nb\")\r\n```\r\nraises\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-63450a98cfa2> in <module>\r\n      3 grouping = np.random.choice([0,1], size=simple_data.shape[0])\r\n      4 \r\n----> 5 de.test.two_sample(adata, grouping=grouping, test='wald', noise_model=\"nb\")\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in two_sample(data, grouping, as_numeric, test, gene_names, sample_description, noise_model, size_factors, batch_size, backend, train_args, training_strategy, is_sig_zerovar, quick_scale, dtype, **kwargs)\r\n   1028         formula_loc = '~ 1 + grouping'\r\n   1029         formula_scale = '~ 1 + grouping'\r\n-> 1030         de_test = wald(\r\n   1031             data=data,\r\n   1032             factor_loc_totest=\"grouping\",\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in wald(data, factor_loc_totest, coef_to_test, formula_loc, formula_scale, as_numeric, init_a, init_b, gene_names, sample_description, dmat_loc, dmat_scale, constraints_loc, constraints_scale, noise_model, size_factors, batch_size, backend, train_args, training_strategy, quick_scale, dtype, **kwargs)\r\n    715 \r\n    716     # Fit model.\r\n--> 717     model = _fit(\r\n    718         noise_model=noise_model,\r\n    719         data=data,\r\n\r\n~/.local/lib/python3.8/site-packages/diffxpy/testing/tests.py in _fit(noise_model, data, design_loc, design_scale, design_loc_names, design_scale_names, constraints_loc, constraints_scale, init_model, init_a, init_b, gene_names, size_factors, batch_size, backend, training_strategy, quick_scale, train_args, close_session, dtype)\r\n    220         raise ValueError('backend=\"%s\" not recognized.' % backend)\r\n    221 \r\n--> 222     estim = Estimator(\r\n    223         input_data=input_data,\r\n    224         init_a=init_a,\r\n\r\n~/.local/lib/python3.8/site-packages/batchglm/train/numpy/glm_nb/estimator.py in __init__(self, input_data, init_a, init_b, batch_size, quick_scale, dtype, **kwargs)\r\n     87             dtype=dtype\r\n     88         )\r\n---> 89         super(Estimator, self).__init__(\r\n     90             input_data=input_data,\r\n     91             model=model,\r\n\r\n~/.local/lib/python3.8/site-packages/batchglm/train/numpy/base_glm/estimator.py in __init__(self, model, input_data, dtype)\r\n     31     ):\r\n     32         if input_data.design_scale.shape[1] != 1:\r\n---> 33             raise ValueError(\"cannot model more than one scale parameter with numpy backend right now.\")\r\n     34         _EstimatorGLM.__init__(\r\n     35             self=self,\r\n\r\nValueError: cannot model more than one scale parameter with numpy backend right now.\r\n```\r\n\r\n\r\nI'm wondering if there errors are limited to the two_sample helper function, and I'll check later if I can reproduce using `test.wald`","closed_by":null,"reactions":{"url":"https://api.github.com/repos/theislab/diffxpy/issues/141/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/theislab/diffxpy/issues/141/timeline","performed_via_github_app":null,"state_reason":null}