{"url":"https://api.github.com/repos/quark0/darts/issues/36","repository_url":"https://api.github.com/repos/quark0/darts","labels_url":"https://api.github.com/repos/quark0/darts/issues/36/labels{/name}","comments_url":"https://api.github.com/repos/quark0/darts/issues/36/comments","events_url":"https://api.github.com/repos/quark0/darts/issues/36/events","html_url":"https://github.com/quark0/darts/issues/36","id":352154267,"node_id":"MDU6SXNzdWUzNTIxNTQyNjc=","number":36,"title":"Only apply softmax to alpha on operations leads to state explosion.","user":{"login":"leckie-chn","id":3284327,"node_id":"MDQ6VXNlcjMyODQzMjc=","avatar_url":"https://avatars.githubusercontent.com/u/3284327?v=4","gravatar_id":"","url":"https://api.github.com/users/leckie-chn","html_url":"https://github.com/leckie-chn","followers_url":"https://api.github.com/users/leckie-chn/followers","following_url":"https://api.github.com/users/leckie-chn/following{/other_user}","gists_url":"https://api.github.com/users/leckie-chn/gists{/gist_id}","starred_url":"https://api.github.com/users/leckie-chn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leckie-chn/subscriptions","organizations_url":"https://api.github.com/users/leckie-chn/orgs","repos_url":"https://api.github.com/users/leckie-chn/repos","events_url":"https://api.github.com/users/leckie-chn/events{/privacy}","received_events_url":"https://api.github.com/users/leckie-chn/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2018-08-20T14:13:53Z","updated_at":"2018-08-21T08:40:28Z","closed_at":"2018-08-21T08:40:28Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nI'm currently trying to reproduce your experiment in tensorflow. What I've found during the implementation & debugging is that, your current implementation of continuous relaxation (formula 2 in your paper) leads to state explosion. You just apply softmax on different operators, not different previous states, which will make the last state value much larger than the initial one.\r\n\r\nFor example, suppose `s0 = 1`, and every mixed operator is somehow a identity operator. Then `s1` will be 1, `s2=s0 + s1=2`, `s3=s0+s1+s2=4` ... finally the value of the states will grow exponentially. What's worse, the very large hidden state will feed to the next timestep/batch as initial state, which leads to infinite large state value. \r\n\r\nBut anyway, your code worked. I think the key is to shrink the state value each time it go through the gates or cells. I thought the secret sauce is shared batchnorm, but it failed. \r\n\r\nSo could you please explained for me?  \r\nThanks!","closed_by":{"login":"leckie-chn","id":3284327,"node_id":"MDQ6VXNlcjMyODQzMjc=","avatar_url":"https://avatars.githubusercontent.com/u/3284327?v=4","gravatar_id":"","url":"https://api.github.com/users/leckie-chn","html_url":"https://github.com/leckie-chn","followers_url":"https://api.github.com/users/leckie-chn/followers","following_url":"https://api.github.com/users/leckie-chn/following{/other_user}","gists_url":"https://api.github.com/users/leckie-chn/gists{/gist_id}","starred_url":"https://api.github.com/users/leckie-chn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leckie-chn/subscriptions","organizations_url":"https://api.github.com/users/leckie-chn/orgs","repos_url":"https://api.github.com/users/leckie-chn/repos","events_url":"https://api.github.com/users/leckie-chn/events{/privacy}","received_events_url":"https://api.github.com/users/leckie-chn/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/quark0/darts/issues/36/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/quark0/darts/issues/36/timeline","performed_via_github_app":null,"state_reason":"completed"}