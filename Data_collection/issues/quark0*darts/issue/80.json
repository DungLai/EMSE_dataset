{"url":"https://api.github.com/repos/quark0/darts/issues/80","repository_url":"https://api.github.com/repos/quark0/darts","labels_url":"https://api.github.com/repos/quark0/darts/issues/80/labels{/name}","comments_url":"https://api.github.com/repos/quark0/darts/issues/80/comments","events_url":"https://api.github.com/repos/quark0/darts/issues/80/events","html_url":"https://github.com/quark0/darts/issues/80","id":428537479,"node_id":"MDU6SXNzdWU0Mjg1Mzc0Nzk=","number":80,"title":"Searched architecture training time.","user":{"login":"zhaohui-yang","id":8347960,"node_id":"MDQ6VXNlcjgzNDc5NjA=","avatar_url":"https://avatars.githubusercontent.com/u/8347960?v=4","gravatar_id":"","url":"https://api.github.com/users/zhaohui-yang","html_url":"https://github.com/zhaohui-yang","followers_url":"https://api.github.com/users/zhaohui-yang/followers","following_url":"https://api.github.com/users/zhaohui-yang/following{/other_user}","gists_url":"https://api.github.com/users/zhaohui-yang/gists{/gist_id}","starred_url":"https://api.github.com/users/zhaohui-yang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhaohui-yang/subscriptions","organizations_url":"https://api.github.com/users/zhaohui-yang/orgs","repos_url":"https://api.github.com/users/zhaohui-yang/repos","events_url":"https://api.github.com/users/zhaohui-yang/events{/privacy}","received_events_url":"https://api.github.com/users/zhaohui-yang/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2019-04-03T02:45:13Z","updated_at":"2019-08-01T01:51:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Thank you for sharing this beautiful code! I use the default DARTS_V2 architecture with appriximately 3.3M parameters to train CIFAR10 dataset. However, I found that it requires about 24h to train 600 epochs on V100 GPU. I wonder if this is correct? For other architectures like resnet, only 6h is needed to train CIFAR10.\r\n\r\nEach epoch takes 2.5min for batchsize = 128 (default = 96, I enlarge it, but I think it would only speed up training time.)\r\n\r\n04/03 10:38:45 AM epoch 410 lr 5.692012e-03\r\n04/03 10:38:46 AM train 000 1.231754e-01 96.093750 100.000000\r\n04/03 10:39:03 AM train 050 1.354196e-01 97.104782 99.969368\r\n04/03 10:39:20 AM train 100 1.354751e-01 97.199875 99.976791\r\n04/03 10:39:37 AM train 150 1.358063e-01 97.138863 99.984482\r\n04/03 10:39:53 AM train 200 1.365068e-01 97.127640 99.976677\r\n04/03 10:40:11 AM train 250 1.384853e-01 97.099106 99.978218\r\n04/03 10:40:28 AM train 300 1.398857e-01 97.085236 99.971443\r\n04/03 10:40:45 AM train 350 1.402510e-01 97.057510 99.973289\r\n04/03 10:40:59 AM train_acc 97.085999\r\n04/03 10:40:59 AM valid 000 1.164408e-01 95.312500 100.000000\r\n04/03 10:41:02 AM valid 050 1.418064e-01 96.155029 99.892776\r\n04/03 10:41:03 AM valid_acc 96.089996","closed_by":null,"reactions":{"url":"https://api.github.com/repos/quark0/darts/issues/80/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/quark0/darts/issues/80/timeline","performed_via_github_app":null,"state_reason":null}