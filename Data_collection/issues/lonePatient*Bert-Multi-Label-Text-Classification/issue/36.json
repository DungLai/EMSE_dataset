{"url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36","repository_url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification","labels_url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36/labels{/name}","comments_url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36/comments","events_url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36/events","html_url":"https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/issues/36","id":533259307,"node_id":"MDU6SXNzdWU1MzMyNTkzMDc=","number":36,"title":"Weight Init from pretrained BERT error","user":{"login":"adiv5","id":22361618,"node_id":"MDQ6VXNlcjIyMzYxNjE4","avatar_url":"https://avatars.githubusercontent.com/u/22361618?v=4","gravatar_id":"","url":"https://api.github.com/users/adiv5","html_url":"https://github.com/adiv5","followers_url":"https://api.github.com/users/adiv5/followers","following_url":"https://api.github.com/users/adiv5/following{/other_user}","gists_url":"https://api.github.com/users/adiv5/gists{/gist_id}","starred_url":"https://api.github.com/users/adiv5/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adiv5/subscriptions","organizations_url":"https://api.github.com/users/adiv5/orgs","repos_url":"https://api.github.com/users/adiv5/repos","events_url":"https://api.github.com/users/adiv5/events{/privacy}","received_events_url":"https://api.github.com/users/adiv5/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-12-05T10:26:08Z","updated_at":"2020-03-03T03:31:46Z","closed_at":"2020-03-03T03:31:46Z","author_association":"NONE","active_lock_reason":null,"body":"While training the model using `run_bert.py ` following all steps as mentioned in readme, putting model files in proper location. The pretrained model weights still arent accepted.\r\n\r\nI see this error:\r\n```\r\nWeights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\r\nWeights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\r\n```\r\nThere is one more issue of tuple shape which maybe related.\r\nhere is the traceback\r\nTraceback:\r\n```\r\nTraining/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='ICD9', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=6, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='0', resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)\r\nLoading examples from cached file pybert/dataset/cached_train_examples_bert\r\nLoading features from cached file pybert/dataset/cached_train_features_256_bert\r\nsorted data by th length of input\r\nLoading examples from cached file pybert/dataset/cached_valid_examples_bert\r\nLoading features from cached file pybert/dataset/cached_valid_features_256_bert\r\ninitializing model\r\nloading configuration file pybert/pretrain/bert/base-uncased/config.json\r\nModel config {\r\n  \"attention_probs_dropout_prob\": 0.1,\r\n  \"finetuning_task\": null,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dropout_prob\": 0.1,\r\n  \"hidden_size\": 768,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 3072,\r\n  \"is_decoder\": false,\r\n  \"layer_norm_eps\": 1e-12,\r\n  \"max_position_embeddings\": 512,\r\n  \"num_attention_heads\": 12,\r\n  \"num_hidden_layers\": 12,\r\n  \"num_labels\": 19,\r\n  \"output_attentions\": false,\r\n  \"output_hidden_states\": false,\r\n  \"output_past\": true,\r\n  \"pruned_heads\": {},\r\n  \"torchscript\": false,\r\n  \"type_vocab_size\": 2,\r\n  \"use_bfloat16\": false,\r\n  \"vocab_size\": 28996\r\n}\r\n\r\nloading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin\r\nWeights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\r\nWeights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\r\ninitializing callbacks\r\n***** Running training *****\r\n  Num examples = 21479\r\n  Num Epochs = 6\r\n  Total train batch size (w. parallel, distributed & accumulation) = 8\r\n  Gradient Accumulation steps = 1\r\n  Total optimization steps = 16110\r\nWarning: There's no GPU available on this machine, training will be performed on CPU.\r\nWarning: The number of GPU's configured to use is 0, but only 0 are available on this machine.\r\nTraceback (most recent call last):\r\n  File \"run_bert.py\", line 227, in <module>\r\n    main()\r\n  File \"run_bert.py\", line 220, in main\r\n    run_train(args)\r\n  File \"run_bert.py\", line 125, in run_train\r\n    trainer.train(train_data=train_dataloader, valid_data=valid_dataloader, seed=args.seed)\r\n  File \"/home/aditya_vartak/bert_pytorch/pybert/train/trainer.py\", line 168, in train\r\n    summary(self.model,*(input_ids, segment_ids,input_mask),show_input=True)\r\n  File \"/home/aditya_vartak/bert_pytorch/pybert/common/tools.py\", line 307, in summary\r\n    model(*inputs)\r\n  File \"/home/aditya_vartak/virtualenvs/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/aditya_vartak/bert_pytorch/pybert/model/nn/bert_for_multi_label.py\", line 14, in forward\r\n    outputs = self.bert(input_ids, token_type_ids=token_type_ids,attention_mask=attention_mask, head_mask=head_mask)\r\n  File \"/home/aditya_vartak/virtualenvs/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/aditya_vartak/virtualenvs/anaconda3/envs/pytorch/lib/python3.7/site-packages/transformers/modeling_bert.py\", line 722, in forward\r\n    embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds)\r\n  File \"/home/aditya_vartak/virtualenvs/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 533, in __call__\r\n    result = hook(self, input)\r\n  File \"/home/aditya_vartak/bert_pytorch/pybert/common/tools.py\", line 269, in hook\r\n    summary[m_key][\"input_shape\"] = list(input[0].size())\r\nIndexError: tuple index out of range\r\n```\r\n\r\nWhat is the issue? How to resolve this?","closed_by":{"login":"lonePatient","id":35169745,"node_id":"MDQ6VXNlcjM1MTY5NzQ1","avatar_url":"https://avatars.githubusercontent.com/u/35169745?v=4","gravatar_id":"","url":"https://api.github.com/users/lonePatient","html_url":"https://github.com/lonePatient","followers_url":"https://api.github.com/users/lonePatient/followers","following_url":"https://api.github.com/users/lonePatient/following{/other_user}","gists_url":"https://api.github.com/users/lonePatient/gists{/gist_id}","starred_url":"https://api.github.com/users/lonePatient/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lonePatient/subscriptions","organizations_url":"https://api.github.com/users/lonePatient/orgs","repos_url":"https://api.github.com/users/lonePatient/repos","events_url":"https://api.github.com/users/lonePatient/events{/privacy}","received_events_url":"https://api.github.com/users/lonePatient/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lonePatient/Bert-Multi-Label-Text-Classification/issues/36/timeline","performed_via_github_app":null,"state_reason":"completed"}