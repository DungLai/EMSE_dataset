{"url":"https://api.github.com/repos/dbiir/UER-py/issues/338","repository_url":"https://api.github.com/repos/dbiir/UER-py","labels_url":"https://api.github.com/repos/dbiir/UER-py/issues/338/labels{/name}","comments_url":"https://api.github.com/repos/dbiir/UER-py/issues/338/comments","events_url":"https://api.github.com/repos/dbiir/UER-py/issues/338/events","html_url":"https://github.com/dbiir/UER-py/issues/338","id":1344111848,"node_id":"I_kwDOCsNQKM5QHYTo","number":338,"title":"How should I preprocess training data samples when I use HuggingFace Transformers T5?","user":{"login":"ShaneTian","id":42370681,"node_id":"MDQ6VXNlcjQyMzcwNjgx","avatar_url":"https://avatars.githubusercontent.com/u/42370681?v=4","gravatar_id":"","url":"https://api.github.com/users/ShaneTian","html_url":"https://github.com/ShaneTian","followers_url":"https://api.github.com/users/ShaneTian/followers","following_url":"https://api.github.com/users/ShaneTian/following{/other_user}","gists_url":"https://api.github.com/users/ShaneTian/gists{/gist_id}","starred_url":"https://api.github.com/users/ShaneTian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ShaneTian/subscriptions","organizations_url":"https://api.github.com/users/ShaneTian/orgs","repos_url":"https://api.github.com/users/ShaneTian/repos","events_url":"https://api.github.com/users/ShaneTian/events{/privacy}","received_events_url":"https://api.github.com/users/ShaneTian/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2022-08-19T08:18:43Z","updated_at":"2022-08-24T14:54:47Z","closed_at":"2022-08-24T14:54:47Z","author_association":"NONE","active_lock_reason":null,"body":"I want to fine-tune a T5 model for text-to-text task (e.g. translation task) using HuggingFace Transformers [uer/t5-base-chinese-cluecorpussmall](https://huggingface.co/uer/t5-base-chinese-cluecorpussmall)\r\nHow should I preprocess my training source and target sequences? I noticed that there were only MLM task in the T5 pre-training stage.\r\n- `src = [cls_token, src_token_1, src_token_2, ..., sep_token], tgt = [cls_token, tgt_token_1, tgt_token_2, ..., sep_token]`\r\n- `src = [cls_token, src_token_1, src_token_2, ..., extra0_token, sep_token], tgt = [cls_token, extra0_token, tgt_token_1, tgt_token_2, ..., sep_token]`","closed_by":{"login":"ShaneTian","id":42370681,"node_id":"MDQ6VXNlcjQyMzcwNjgx","avatar_url":"https://avatars.githubusercontent.com/u/42370681?v=4","gravatar_id":"","url":"https://api.github.com/users/ShaneTian","html_url":"https://github.com/ShaneTian","followers_url":"https://api.github.com/users/ShaneTian/followers","following_url":"https://api.github.com/users/ShaneTian/following{/other_user}","gists_url":"https://api.github.com/users/ShaneTian/gists{/gist_id}","starred_url":"https://api.github.com/users/ShaneTian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ShaneTian/subscriptions","organizations_url":"https://api.github.com/users/ShaneTian/orgs","repos_url":"https://api.github.com/users/ShaneTian/repos","events_url":"https://api.github.com/users/ShaneTian/events{/privacy}","received_events_url":"https://api.github.com/users/ShaneTian/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dbiir/UER-py/issues/338/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dbiir/UER-py/issues/338/timeline","performed_via_github_app":null,"state_reason":"completed"}