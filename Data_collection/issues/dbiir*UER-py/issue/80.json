{"url":"https://api.github.com/repos/dbiir/UER-py/issues/80","repository_url":"https://api.github.com/repos/dbiir/UER-py","labels_url":"https://api.github.com/repos/dbiir/UER-py/issues/80/labels{/name}","comments_url":"https://api.github.com/repos/dbiir/UER-py/issues/80/comments","events_url":"https://api.github.com/repos/dbiir/UER-py/issues/80/events","html_url":"https://github.com/dbiir/UER-py/issues/80","id":747636414,"node_id":"MDU6SXNzdWU3NDc2MzY0MTQ=","number":80,"title":"应用于英文除0报错","user":{"login":"wangchichi1999","id":62172616,"node_id":"MDQ6VXNlcjYyMTcyNjE2","avatar_url":"https://avatars.githubusercontent.com/u/62172616?v=4","gravatar_id":"","url":"https://api.github.com/users/wangchichi1999","html_url":"https://github.com/wangchichi1999","followers_url":"https://api.github.com/users/wangchichi1999/followers","following_url":"https://api.github.com/users/wangchichi1999/following{/other_user}","gists_url":"https://api.github.com/users/wangchichi1999/gists{/gist_id}","starred_url":"https://api.github.com/users/wangchichi1999/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangchichi1999/subscriptions","organizations_url":"https://api.github.com/users/wangchichi1999/orgs","repos_url":"https://api.github.com/users/wangchichi1999/repos","events_url":"https://api.github.com/users/wangchichi1999/events{/privacy}","received_events_url":"https://api.github.com/users/wangchichi1999/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-11-20T16:48:06Z","updated_at":"2020-11-21T04:30:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"您好！非常棒的工作，我想把他应用于英文文本分类工作\r\n我更改了预训练模型路径，词表，还有tokenizer的配置\r\n!CUDA_VISIBLE_DEVICES=0 python3 run_classifier.py --**pretrained_model_path models/google_model_en_uncased_base.bin** \\\r\n  --vocab_path **models/google_uncased_en_vocab.txt** \\\r\n  --train_path datasets/train_en.tsv --dev_path datasets/dev_en.tsv \\\r\n  --test_path datasets/test_en.tsv \\\r\n  **--tokenizer space \\**\r\n  --epochs_num 3 --batch_size 32 --encoder bert \r\n\r\n在英文二分类的实验上报了如下错误：\r\n![image](https://user-images.githubusercontent.com/62172616/99825799-7bf7f780-2b92-11eb-99c0-2de1b1af3e92.png)\r\n\r\n\r\n这是我的数据集样例：\r\n![image](https://user-images.githubusercontent.com/62172616/99826024-c37e8380-2b92-11eb-9dd6-8ee871a3102d.png)\r\n\r\n此外，我输出了pred和gold向量，发现gold正常但是pred全部为零\r\n感谢您的帮助！","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dbiir/UER-py/issues/80/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dbiir/UER-py/issues/80/timeline","performed_via_github_app":null,"state_reason":null}