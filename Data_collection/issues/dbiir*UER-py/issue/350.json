{"url":"https://api.github.com/repos/dbiir/UER-py/issues/350","repository_url":"https://api.github.com/repos/dbiir/UER-py","labels_url":"https://api.github.com/repos/dbiir/UER-py/issues/350/labels{/name}","comments_url":"https://api.github.com/repos/dbiir/UER-py/issues/350/comments","events_url":"https://api.github.com/repos/dbiir/UER-py/issues/350/events","html_url":"https://github.com/dbiir/UER-py/issues/350","id":1488308254,"node_id":"I_kwDOCsNQKM5Ytcge","number":350,"title":"Why do I encounter a sudden MLM accuracy drop during training?","user":{"login":"dr-GitHub-account","id":59919093,"node_id":"MDQ6VXNlcjU5OTE5MDkz","avatar_url":"https://avatars.githubusercontent.com/u/59919093?v=4","gravatar_id":"","url":"https://api.github.com/users/dr-GitHub-account","html_url":"https://github.com/dr-GitHub-account","followers_url":"https://api.github.com/users/dr-GitHub-account/followers","following_url":"https://api.github.com/users/dr-GitHub-account/following{/other_user}","gists_url":"https://api.github.com/users/dr-GitHub-account/gists{/gist_id}","starred_url":"https://api.github.com/users/dr-GitHub-account/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dr-GitHub-account/subscriptions","organizations_url":"https://api.github.com/users/dr-GitHub-account/orgs","repos_url":"https://api.github.com/users/dr-GitHub-account/repos","events_url":"https://api.github.com/users/dr-GitHub-account/events{/privacy}","received_events_url":"https://api.github.com/users/dr-GitHub-account/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-12-10T09:54:31Z","updated_at":"2022-12-15T13:10:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I am training a BERT-base model for Chinese. Default MLM and NSP tasks are used. I am trying to train the model for 96k steps to see if it benefits from longer training procedure. However, from step 65600 to step 65700, the MLM accuracy drops dramatically from 0.827 to 0.774 while the NSP accuracy remains high and stable. I am wondering how the drop takes place. \r\n\r\nI have around 226k sentences in the original corpus and each one is split into two parts from the middle, just like [book_review_bert.txt](https://github.com/dbiir/UER-py/blob/master/corpora/book_review_bert.txt). During data preprocess, I modified the dup_factor from 5 to 50 to ensure diversity. The actual batch_size is [16 (args.batch_size) x 2 (args.world_size) x 1 (args.accumulation_steps)].","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dbiir/UER-py/issues/350/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dbiir/UER-py/issues/350/timeline","performed_via_github_app":null,"state_reason":null}