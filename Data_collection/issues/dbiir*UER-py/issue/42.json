{"url":"https://api.github.com/repos/dbiir/UER-py/issues/42","repository_url":"https://api.github.com/repos/dbiir/UER-py","labels_url":"https://api.github.com/repos/dbiir/UER-py/issues/42/labels{/name}","comments_url":"https://api.github.com/repos/dbiir/UER-py/issues/42/comments","events_url":"https://api.github.com/repos/dbiir/UER-py/issues/42/events","html_url":"https://github.com/dbiir/UER-py/issues/42","id":603805606,"node_id":"MDU6SXNzdWU2MDM4MDU2MDY=","number":42,"title":"关于Quantitative evaluation的训练步骤问题","user":{"login":"cobraheleah","id":63284063,"node_id":"MDQ6VXNlcjYzMjg0MDYz","avatar_url":"https://avatars.githubusercontent.com/u/63284063?v=4","gravatar_id":"","url":"https://api.github.com/users/cobraheleah","html_url":"https://github.com/cobraheleah","followers_url":"https://api.github.com/users/cobraheleah/followers","following_url":"https://api.github.com/users/cobraheleah/following{/other_user}","gists_url":"https://api.github.com/users/cobraheleah/gists{/gist_id}","starred_url":"https://api.github.com/users/cobraheleah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cobraheleah/subscriptions","organizations_url":"https://api.github.com/users/cobraheleah/orgs","repos_url":"https://api.github.com/users/cobraheleah/repos","events_url":"https://api.github.com/users/cobraheleah/events{/privacy}","received_events_url":"https://api.github.com/users/cobraheleah/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-04-21T08:22:25Z","updated_at":"2020-05-13T03:10:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"您好，感谢您在预训练模型框架方面的探索研究，对于我们当前的业务有一定指导意义。\r\n在这里有几个个问题想请教一下您：\r\n1. 在Quantitative evaluation的stage1中写道“We train with batch size of 256 sequences and each sequence contains 256 tokens. We load Google's pretrained models and train upon it for 500,000 steps. The learning rate is 2e-5 and other optimizer settings are identical with Google BERT. BERT tokenizer is used.”这个步骤是用特定领域的语料，从零开始重新训练bert模型，还是加载google在wiki上面预训练好的bert然后再去训练？\r\n2.在步骤2和步骤3分别用于下游任务训练和微调的数据集要一样吗？还是说训练的数据集要远大于微调的数据集？因为我看在book_review的那个分类的demo里面，微调的train+dev+test就是对应的训练的book_review.txt数据集\r\n期待您的回复\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dbiir/UER-py/issues/42/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dbiir/UER-py/issues/42/timeline","performed_via_github_app":null,"state_reason":null}