{"url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8","repository_url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq","labels_url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8/labels{/name}","comments_url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8/comments","events_url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8/events","html_url":"https://github.com/HLTCHKUST/Mem2Seq/issues/8","id":385597313,"node_id":"MDU6SXNzdWUzODU1OTczMTM=","number":8,"title":"a problem about Mem2Seq.py","user":{"login":"bing-95","id":40850840,"node_id":"MDQ6VXNlcjQwODUwODQw","avatar_url":"https://avatars.githubusercontent.com/u/40850840?v=4","gravatar_id":"","url":"https://api.github.com/users/bing-95","html_url":"https://github.com/bing-95","followers_url":"https://api.github.com/users/bing-95/followers","following_url":"https://api.github.com/users/bing-95/following{/other_user}","gists_url":"https://api.github.com/users/bing-95/gists{/gist_id}","starred_url":"https://api.github.com/users/bing-95/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bing-95/subscriptions","organizations_url":"https://api.github.com/users/bing-95/orgs","repos_url":"https://api.github.com/users/bing-95/repos","events_url":"https://api.github.com/users/bing-95/events{/privacy}","received_events_url":"https://api.github.com/users/bing-95/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2018-11-29T07:05:13Z","updated_at":"2018-11-29T07:16:47Z","closed_at":"2018-11-29T07:16:47Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, in the Mem2Seq.py, I have a question about the GRU's hidden layer. \r\nIn the DecoderMemNN _init_ function, the code `self.gru = nn.GRU(embedding_dim, embedding_dim, dropout=dropout)` doesn't define the GRU's layer_num, so does it have a default one hidden layer?\r\nAnd in the train_batch and evaluate_batch function, the code `decoder_hidden = self.encoder(input_batches).unsqueeze(0)` also implies the GRU has one hidden layer.\r\nSo, I think, during training and evaluating, the GRU always has only one hidden layer regardless of the args['layer']. Should the code be modified? \r\n\r\nLook forward to your reply, thanks.","closed_by":{"login":"jasonwu0731","id":14951842,"node_id":"MDQ6VXNlcjE0OTUxODQy","avatar_url":"https://avatars.githubusercontent.com/u/14951842?v=4","gravatar_id":"","url":"https://api.github.com/users/jasonwu0731","html_url":"https://github.com/jasonwu0731","followers_url":"https://api.github.com/users/jasonwu0731/followers","following_url":"https://api.github.com/users/jasonwu0731/following{/other_user}","gists_url":"https://api.github.com/users/jasonwu0731/gists{/gist_id}","starred_url":"https://api.github.com/users/jasonwu0731/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jasonwu0731/subscriptions","organizations_url":"https://api.github.com/users/jasonwu0731/orgs","repos_url":"https://api.github.com/users/jasonwu0731/repos","events_url":"https://api.github.com/users/jasonwu0731/events{/privacy}","received_events_url":"https://api.github.com/users/jasonwu0731/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/HLTCHKUST/Mem2Seq/issues/8/timeline","performed_via_github_app":null,"state_reason":"completed"}