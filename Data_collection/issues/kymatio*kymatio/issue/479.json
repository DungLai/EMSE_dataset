{"url":"https://api.github.com/repos/kymatio/kymatio/issues/479","repository_url":"https://api.github.com/repos/kymatio/kymatio","labels_url":"https://api.github.com/repos/kymatio/kymatio/issues/479/labels{/name}","comments_url":"https://api.github.com/repos/kymatio/kymatio/issues/479/comments","events_url":"https://api.github.com/repos/kymatio/kymatio/issues/479/events","html_url":"https://github.com/kymatio/kymatio/issues/479","id":550989970,"node_id":"MDU6SXNzdWU1NTA5ODk5NzA=","number":479,"title":"Multi-GPU scattering2d [torch]","user":{"login":"anakin-datawalker","id":36677060,"node_id":"MDQ6VXNlcjM2Njc3MDYw","avatar_url":"https://avatars.githubusercontent.com/u/36677060?v=4","gravatar_id":"","url":"https://api.github.com/users/anakin-datawalker","html_url":"https://github.com/anakin-datawalker","followers_url":"https://api.github.com/users/anakin-datawalker/followers","following_url":"https://api.github.com/users/anakin-datawalker/following{/other_user}","gists_url":"https://api.github.com/users/anakin-datawalker/gists{/gist_id}","starred_url":"https://api.github.com/users/anakin-datawalker/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anakin-datawalker/subscriptions","organizations_url":"https://api.github.com/users/anakin-datawalker/orgs","repos_url":"https://api.github.com/users/anakin-datawalker/repos","events_url":"https://api.github.com/users/anakin-datawalker/events{/privacy}","received_events_url":"https://api.github.com/users/anakin-datawalker/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":{"url":"https://api.github.com/repos/kymatio/kymatio/milestones/6","html_url":"https://github.com/kymatio/kymatio/milestone/6","labels_url":"https://api.github.com/repos/kymatio/kymatio/milestones/6/labels","id":4908848,"node_id":"MDk6TWlsZXN0b25lNDkwODg0OA==","number":6,"title":"0.3.alpha","description":"","creator":{"login":"lostanlen","id":3943142,"node_id":"MDQ6VXNlcjM5NDMxNDI=","avatar_url":"https://avatars.githubusercontent.com/u/3943142?v=4","gravatar_id":"","url":"https://api.github.com/users/lostanlen","html_url":"https://github.com/lostanlen","followers_url":"https://api.github.com/users/lostanlen/followers","following_url":"https://api.github.com/users/lostanlen/following{/other_user}","gists_url":"https://api.github.com/users/lostanlen/gists{/gist_id}","starred_url":"https://api.github.com/users/lostanlen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lostanlen/subscriptions","organizations_url":"https://api.github.com/users/lostanlen/orgs","repos_url":"https://api.github.com/users/lostanlen/repos","events_url":"https://api.github.com/users/lostanlen/events{/privacy}","received_events_url":"https://api.github.com/users/lostanlen/received_events","type":"User","site_admin":false},"open_issues":13,"closed_issues":30,"state":"open","created_at":"2019-12-06T12:39:56Z","updated_at":"2022-07-06T15:35:01Z","due_on":null,"closed_at":null},"comments":14,"created_at":"2020-01-16T18:49:55Z","updated_at":"2021-02-11T20:06:21Z","closed_at":"2021-02-11T20:06:21Z","author_association":"COLLABORATOR","active_lock_reason":null,"body":"Hi everyone,\r\n\r\nClose #640 \r\n\r\nIn order for scattering2d torch implementation to fully leverage the fact that it inherits nn.Module and thus be parallelizable with nn.DataParallel, I believe the following lines shall be modified in kymatio-v2 branch:\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/kymatio/scattering2d/frontend/torch_frontend.py#L35\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/kymatio/scattering2d/frontend/torch_frontend.py#L43\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/kymatio/scattering2d/frontend/torch_frontend.py#L57\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/kymatio/scattering2d/frontend/torch_frontend.py#L65\r\n\r\nto respectively simply:\r\n\r\n`self.register_single_filter(phi, n)`\r\n`self.register_single_filter(v, n)`\r\n\r\nand:\r\n\r\n`phis = copy.deepcopy(self.phi)`\r\n`psis = copy.deepcopy(self.psi)`\r\n\r\nIndeed, self.phi and self.psi being dicts, at each forward pass, the replica models built by the replicate.py function of DataParallel on each GPU device all share the same underlying self.phi and self.psi dicts. If we assign the named buffers directly to those dicts (in the first 2 lines I have mentioned), then as those buffers on the other hand are replicated separately on each GPU, this means ultimately that the self.phi[c] and self.psi[j][k] will only point to one GPU while inputs will be scattered on all GPUs and as such will lead to a TypeError: Input and filter must be on the same GPU.\r\n\r\nThe problem is similar for the other 2 lines and one workaround is thus for each replica model to have its own copy of the phi and psi dicts. Another workaround would be to pass as well a buffer dict in the scattering call:\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/kymatio/scattering2d/frontend/torch_frontend.py#L125-L126\r\n\r\nand load the filters within the scattering core function (but would be less generic).\r\n\r\nProposed solution seems to work in multi-gpu by for instance slightly modifying following lines of cifar.py in examples/2d:\r\n\r\nhttps://github.com/kymatio/kymatio/blob/5bf71fd7aab1e60bb12cfaa4a2b42d722a91e893/examples/2d/cifar.py#L133-L141\r\n\r\nby:\r\n```\r\n    if use_cuda:\r\n        scattering = torch.nn.DataParallel(scattering).cuda()\r\n\r\n    model = Scattering2dCNN(K, args.classifier)\r\n\r\n    if use_cuda:\r\n        model = torch.nn.DataParallel(Scattering2dCNN(K,args.classifier)).cuda()\r\n\r\n    # DataLoaders \r\n```\r\n\r\nSeems to work most of the time with 2 GPUs, a bit more randomly with 4 GPUs where I can get sometimes a Segmentation fault (core dumped) issue, which using faulthandler and faulthandler.enable() gives the following error:\r\n\r\n```\r\nThread 0x00007f4b5c889700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/backend/torch_backend.py\", line 231 in fft\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/core/scattering2d.py\", line 23 in scattering2d\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/frontend/torch_frontend.py\", line 126 in scattering\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/frontend/torch_frontend.py\", line 20 in forward\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532 in __call__\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60 in _worker\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nCurrent thread 0x00007f4b5d08a700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/backend/torch_backend.py\", line 231 in fft\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/core/scattering2d.py\", line 23 in scattering2d\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/scattering2d/frontend/torch_frontend.py\", line 126 in scattering\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/kymatio/frontend/torch_frontend.py\", line 20 in forward\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532 in __call__\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 60 in _worker\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4b5f7fe700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 296 in wait\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 224 in _feed\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4b5ffff700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 296 in wait\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 224 in _feed\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4b80ff9700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 296 in wait\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 224 in _feed\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4b817fa700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 296 in wait\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 224 in _feed\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4b81ffb700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/selectors.py\", line 415 in select\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 920 in wait\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 414 in _poll\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/connection.py\", line 257 in poll\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/multiprocessing/queues.py\", line 104 in get\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\", line 25 in _pin_memory_loop\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 870 in run\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 926 in _bootstrap_inner\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 890 in _bootstrap\r\n\r\nThread 0x00007f4ceeacb700 (most recent call first):\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 1060 in _wait_for_tstate_lock\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/threading.py\", line 1044 in join\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 77 in parallel_apply\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 162 in parallel_apply\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\", line 152 in forward\r\n  File \"/users/data/zarka/anaconda3/envs/torch/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532 in __call__\r\n  File \"cifar.py\", line 75 in train\r\n  File \"cifar.py\", line 177 in main\r\n  File \"cifar.py\", line 184 in <module>\r\n````\r\n\r\nTested on Ubuntu 16.04 and 18.04, torch 1.4.0, torchvision 0.5.0 (got similar behaviors with 1.3.1 and 0.4.2)","closed_by":{"login":"edouardoyallon","id":4263222,"node_id":"MDQ6VXNlcjQyNjMyMjI=","avatar_url":"https://avatars.githubusercontent.com/u/4263222?v=4","gravatar_id":"","url":"https://api.github.com/users/edouardoyallon","html_url":"https://github.com/edouardoyallon","followers_url":"https://api.github.com/users/edouardoyallon/followers","following_url":"https://api.github.com/users/edouardoyallon/following{/other_user}","gists_url":"https://api.github.com/users/edouardoyallon/gists{/gist_id}","starred_url":"https://api.github.com/users/edouardoyallon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/edouardoyallon/subscriptions","organizations_url":"https://api.github.com/users/edouardoyallon/orgs","repos_url":"https://api.github.com/users/edouardoyallon/repos","events_url":"https://api.github.com/users/edouardoyallon/events{/privacy}","received_events_url":"https://api.github.com/users/edouardoyallon/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/kymatio/kymatio/issues/479/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kymatio/kymatio/issues/479/timeline","performed_via_github_app":null,"state_reason":"completed"}