{"url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185","repository_url":"https://api.github.com/repos/philipperemy/keras-tcn","labels_url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185/labels{/name}","comments_url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185/comments","events_url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185/events","html_url":"https://github.com/philipperemy/keras-tcn/issues/185","id":809181042,"node_id":"MDU6SXNzdWU4MDkxODEwNDI=","number":185,"title":"Support for tf.RaggedTensor Input","user":{"login":"mimxrt","id":53339396,"node_id":"MDQ6VXNlcjUzMzM5Mzk2","avatar_url":"https://avatars.githubusercontent.com/u/53339396?v=4","gravatar_id":"","url":"https://api.github.com/users/mimxrt","html_url":"https://github.com/mimxrt","followers_url":"https://api.github.com/users/mimxrt/followers","following_url":"https://api.github.com/users/mimxrt/following{/other_user}","gists_url":"https://api.github.com/users/mimxrt/gists{/gist_id}","starred_url":"https://api.github.com/users/mimxrt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mimxrt/subscriptions","organizations_url":"https://api.github.com/users/mimxrt/orgs","repos_url":"https://api.github.com/users/mimxrt/repos","events_url":"https://api.github.com/users/mimxrt/events{/privacy}","received_events_url":"https://api.github.com/users/mimxrt/received_events","type":"User","site_admin":false},"labels":[{"id":875498555,"node_id":"MDU6TGFiZWw4NzU0OTg1NTU=","url":"https://api.github.com/repos/philipperemy/keras-tcn/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-02-16T10:24:11Z","updated_at":"2022-09-25T15:13:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Currentrly, Keras TCN does not support [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) input. It would be very useful for speeding up training for variying length time series inputs. ~As I understand it, there is no way to batch sequences of different lengths except when using [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor)~*. See the following minimal example of the current state:\r\n\r\n**EDIT:* Masking is another option, but still...\r\n\r\n```python\r\nimport tcn\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as K\r\n\r\nbatch_size=2\r\n\r\nX = [\r\n    np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5]]),\r\n    np.array([[1, 1], [2, 2], [3, 3]]),\r\n    np.array([[1, 1]]),\r\n    np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]]),\r\n]\r\nY = np.array([6, 1, 2, 4])\r\n\r\nds_raw_X = tf.data.Dataset.from_tensor_slices(tf.ragged.constant(X, inner_shape=(2,)))\r\nds_raw_Y = tf.data.Dataset.from_tensor_slices(Y)\r\n\r\nds_raw = tf.data.Dataset.zip((ds_raw_X, ds_raw_Y))\r\n\r\nprint(\"Raw dataset:\")\r\nfor e in ds_raw.take(2):\r\n    print(f\"{(e[0].shape, e[1].shape)}\")\r\nprint()\r\n\r\nds = ds_raw.batch(batch_size)\r\n\r\nprint(\"Final dataset:\")\r\nfor e in ds.take(2):\r\n    print(f\"{(e[0].shape, e[1].shape)}\")\r\nprint()\r\n\r\nm_in = K.Input(shape=(None, 2), batch_size=batch_size)\r\nm_out = tcn.TCN(\r\n    nb_filters=32,\r\n    return_sequences=False\r\n)(m_in)\r\nm_out = K.layers.Dense(1)(m_out)\r\n\r\nmodel = K.Model([m_in], m_out)\r\nprint(model.summary(), end=\"\\n\\n\")\r\n\r\nmodel.compile(optimizer=K.optimizers.Adam(), loss=K.losses.MeanSquaredError())\r\n\r\nmodel.fit(ds, epochs=1)\r\n```\r\n\r\nOutput:\r\n```\r\nRaw dataset:\r\n(TensorShape([5, 2]), TensorShape([]))\r\n(TensorShape([3, 2]), TensorShape([]))\r\n\r\nFinal dataset:\r\n(TensorShape([2, None, 2]), TensorShape([2]))\r\n(TensorShape([2, None, 2]), TensorShape([2]))\r\n\r\nModel: \"model\"\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         [(2, None, 2)]            0         \r\n_________________________________________________________________\r\ntcn (TCN)                    (2, 32)                   23136     \r\n_________________________________________________________________\r\ndense (Dense)                (2, 1)                    33        \r\n=================================================================\r\nTotal params: 23,169\r\nTrainable params: 23,169\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nNone\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n    329     _pywrap_utils.AssertSameStructure(nest1, nest2, check_types,\r\n--> 330                                       expand_composites)\r\n    331   except (ValueError, TypeError) as e:\r\n\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, 2]), tf.int32, 1, tf.int64)\r\n\r\nSecond structure: type=Tensor str=Tensor(\"input_1:0\", shape=(2, None, 2), dtype=float32)\r\n\r\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, 2]), tf.int32, 1, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"input_1:0\", shape=(2, None, 2), dtype=float32)\" is not\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-52273b009edd> in <module>\r\n     43 model.compile(optimizer=K.optimizers.Adam(), loss=K.losses.MeanSquaredError())\r\n     44 \r\n---> 45 model.fit(ds, epochs=1)\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    817         max_queue_size=max_queue_size,\r\n    818         workers=workers,\r\n--> 819         use_multiprocessing=use_multiprocessing)\r\n    820 \r\n    821   def evaluate(self,\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\r\n    233           max_queue_size=max_queue_size,\r\n    234           workers=workers,\r\n--> 235           use_multiprocessing=use_multiprocessing)\r\n    236 \r\n    237       total_samples = _get_total_number_of_samples(training_data_adapter)\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    591         max_queue_size=max_queue_size,\r\n    592         workers=workers,\r\n--> 593         use_multiprocessing=use_multiprocessing)\r\n    594     val_adapter = None\r\n    595     if validation_data:\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\r\n    704       max_queue_size=max_queue_size,\r\n    705       workers=workers,\r\n--> 706       use_multiprocessing=use_multiprocessing)\r\n    707 \r\n    708   return adapter\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, standardize_function, **kwargs)\r\n    700 \r\n    701     if standardize_function is not None:\r\n--> 702       x = standardize_function(x)\r\n    703 \r\n    704     # Note that the dataset instance is immutable, its fine to reusing the user\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py in standardize_function(dataset)\r\n    658         model.sample_weight_mode = getattr(model, 'sample_weight_mode', None)\r\n    659 \r\n--> 660       standardize(dataset, extract_tensors_from_dataset=False)\r\n    661 \r\n    662       # Then we map using only the tensor standardization portion.\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\r\n   2381         is_dataset=is_dataset,\r\n   2382         class_weight=class_weight,\r\n-> 2383         batch_size=batch_size)\r\n   2384 \r\n   2385   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py in _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\r\n   2445     flat_expected_inputs = nest.flatten(self.inputs, expand_composites=False)\r\n   2446     for (a, b) in zip(flat_inputs, flat_expected_inputs):\r\n-> 2447       nest.assert_same_structure(a, b, expand_composites=True)\r\n   2448 \r\n   2449     if y is not None:\r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py in assert_same_structure(nest1, nest2, check_types, expand_composites)\r\n    335                   \"Entire first structure:\\n%s\\n\"\r\n    336                   \"Entire second structure:\\n%s\"\r\n--> 337                   % (str(e), str1, str2))\r\n    338 \r\n    339 \r\n\r\nValueError: The two structures don't have the same nested structure.\r\n\r\nFirst structure: type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, 2]), tf.int32, 1, tf.int64)\r\n\r\nSecond structure: type=Tensor str=Tensor(\"input_1:0\", shape=(2, None, 2), dtype=float32)\r\n\r\nMore specifically: Substructure \"type=RaggedTensorSpec str=RaggedTensorSpec(TensorShape([None, None, 2]), tf.int32, 1, tf.int64)\" is a sequence, while substructure \"type=Tensor str=Tensor(\"input_1:0\", shape=(2, None, 2), dtype=float32)\" is not\r\nEntire first structure:\r\n.\r\nEntire second structure:\r\n.\r\n```\r\n\r\nI think this is expected to fail in all cases because I did not add the `ragged=True` parameter. When adding this parameter, the error is as expected (`Layer tcn_1 does not support RaggedTensors as input`):\r\n```\r\nm_in = K.Input(shape=(None, 2), batch_size=batch_size, ragged=True) # note the added ragged=True parameter\r\nm_out = tcn.TCN(\r\n    nb_filters=32,\r\n    return_sequences=False\r\n)(m_in)\r\nm_out = K.layers.Dense(1)(m_out)\r\n\r\nmodel = K.Model([m_in], m_out)\r\nprint(model.summary(), end=\"\\n\\n\")\r\n```\r\n\r\nOutput:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-3821044ae8a6> in <module>\r\n      3     nb_filters=32,\r\n      4     return_sequences=False\r\n----> 5 )(m_in)\r\n      6 m_out = K.layers.Dense(1)(m_out)\r\n      7 \r\n\r\n~/dss/code-envs/python/tensorflow_2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\r\n    740           raise ValueError('Layer %s does not support RaggedTensors as input. '\r\n    741                            'Inputs received: %s. You can try converting your '\r\n--> 742                            'input to an uniform tensor.' % (self.name, inputs))\r\n    743 \r\n    744         graph = backend.get_graph()\r\n\r\nValueError: Layer tcn_1 does not support RaggedTensors as input. Inputs received: tf.RaggedTensor(values=Tensor(\"Placeholder:0\", shape=(None, 2), dtype=float32), row_splits=Tensor(\"Placeholder_1:0\", shape=(3,), dtype=int64)). You can try converting your input to an uniform tensor.\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/philipperemy/keras-tcn/issues/185/timeline","performed_via_github_app":null,"state_reason":null}