{"url":"https://api.github.com/repos/mravanelli/SincNet/issues/110","repository_url":"https://api.github.com/repos/mravanelli/SincNet","labels_url":"https://api.github.com/repos/mravanelli/SincNet/issues/110/labels{/name}","comments_url":"https://api.github.com/repos/mravanelli/SincNet/issues/110/comments","events_url":"https://api.github.com/repos/mravanelli/SincNet/issues/110/events","html_url":"https://github.com/mravanelli/SincNet/issues/110","id":1512667534,"node_id":"I_kwDOCF-Qwc5aKXmO","number":110,"title":"CUDA error: device-side assert triggered","user":{"login":"Raviteja-banda","id":91092148,"node_id":"MDQ6VXNlcjkxMDkyMTQ4","avatar_url":"https://avatars.githubusercontent.com/u/91092148?v=4","gravatar_id":"","url":"https://api.github.com/users/Raviteja-banda","html_url":"https://github.com/Raviteja-banda","followers_url":"https://api.github.com/users/Raviteja-banda/followers","following_url":"https://api.github.com/users/Raviteja-banda/following{/other_user}","gists_url":"https://api.github.com/users/Raviteja-banda/gists{/gist_id}","starred_url":"https://api.github.com/users/Raviteja-banda/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Raviteja-banda/subscriptions","organizations_url":"https://api.github.com/users/Raviteja-banda/orgs","repos_url":"https://api.github.com/users/Raviteja-banda/repos","events_url":"https://api.github.com/users/Raviteja-banda/events{/privacy}","received_events_url":"https://api.github.com/users/Raviteja-banda/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-12-28T10:50:19Z","updated_at":"2022-12-28T11:04:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have been trying to run the speaker_id.py file(for a customised librispeech dataset) but i was stuck at this error from the past few days. I have even checked if there's any mismatch between the number of labels and the output neurons but they are fine. I've changed the config file as below:\r\n**class_lay=251\r\nbatch_size=8\r\nN_batches=251 (Since total number of train files(excluding validation data) = 2008)**\r\n\r\n[windowing]\r\nfs=16000\r\ncw_len=200\r\ncw_shift=10\r\n\r\n[cnn]\r\ncnn_N_filt=80,60,60\r\ncnn_len_filt=251,5,5\r\ncnn_max_pool_len=3,3,3\r\ncnn_use_laynorm_inp=True\r\ncnn_use_batchnorm_inp=False\r\ncnn_use_laynorm=True,True,True\r\ncnn_use_batchnorm=False,False,False\r\ncnn_act=leaky_relu,leaky_relu,leaky_relu\r\ncnn_drop=0.0,0.0,0.0\r\n\r\n[dnn]\r\nfc_lay=2048,2048,2048\r\nfc_drop=0.0,0.0,0.0\r\nfc_use_laynorm_inp=True\r\nfc_use_batchnorm_inp=False\r\nfc_use_batchnorm=True,True,True\r\nfc_use_laynorm=False,False,False\r\nfc_act=leaky_relu,leaky_relu,leaky_relu\r\n\r\n[class]\r\nclass_lay=251\r\nclass_drop=0.0\r\nclass_use_laynorm_inp=False\r\nclass_use_batchnorm_inp=False\r\nclass_use_batchnorm=False\r\nclass_use_laynorm=False\r\nclass_act=softmax\r\n\r\nI have also changed the data paths in the config file.\r\n\r\nBelow is my stack trace of the error:\r\n\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nC:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\cuda\\Loss.cu:242: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\r\nTraceback (most recent call last):\r\n  File \"D:\\SincNet-librispeech\\speaker_id.py\", line 245, in <module>\r\n    loss.backward()\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\r\n    torch.autograd.backward(\r\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 197, in backward\r\n    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\nRuntimeError: CUDA error: device-side assert triggered\r\n\r\n\r\n\r\nAny possible solutions?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/mravanelli/SincNet/issues/110/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mravanelli/SincNet/issues/110/timeline","performed_via_github_app":null,"state_reason":null}