{"url":"https://api.github.com/repos/namisan/mt-dnn/issues/131","repository_url":"https://api.github.com/repos/namisan/mt-dnn","labels_url":"https://api.github.com/repos/namisan/mt-dnn/issues/131/labels{/name}","comments_url":"https://api.github.com/repos/namisan/mt-dnn/issues/131/comments","events_url":"https://api.github.com/repos/namisan/mt-dnn/issues/131/events","html_url":"https://github.com/namisan/mt-dnn/issues/131","id":544202359,"node_id":"MDU6SXNzdWU1NDQyMDIzNTk=","number":131,"title":"TypeError: an integer is required (got type str)","user":{"login":"antgr","id":2175768,"node_id":"MDQ6VXNlcjIxNzU3Njg=","avatar_url":"https://avatars.githubusercontent.com/u/2175768?v=4","gravatar_id":"","url":"https://api.github.com/users/antgr","html_url":"https://github.com/antgr","followers_url":"https://api.github.com/users/antgr/followers","following_url":"https://api.github.com/users/antgr/following{/other_user}","gists_url":"https://api.github.com/users/antgr/gists{/gist_id}","starred_url":"https://api.github.com/users/antgr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antgr/subscriptions","organizations_url":"https://api.github.com/users/antgr/orgs","repos_url":"https://api.github.com/users/antgr/repos","events_url":"https://api.github.com/users/antgr/events{/privacy}","received_events_url":"https://api.github.com/users/antgr/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-12-31T14:33:53Z","updated_at":"2020-01-02T14:41:42Z","closed_at":"2020-01-01T12:58:28Z","author_association":"NONE","active_lock_reason":null,"body":"```\r\npython prepro_std.py --model bert-base-uncased --root_dir ner --task_def experiments/ner/ner_task_def.yml\r\nBetter speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\nparse args\r\nmain\r\n12/31/2019 04:30:07 Task ner\r\ntask_type: TaskType.SeqenceLabeling\r\nlabel_mapper: <data_utils.vocab.Vocabulary object at 0x7f0cd3f7a950>\r\n==load data===\r\n12/31/2019 04:30:07 ner/bert_uncased/ner_train.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:10 ner/bert_uncased/ner_dev.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:10 ner/bert_uncased/ner_test.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n12/31/2019 04:30:11 Task pos\r\ntask_type: TaskType.SeqenceLabeling\r\nlabel_mapper: <data_utils.vocab.Vocabulary object at 0x7f0cd3f7a090>\r\n==load data===\r\n12/31/2019 04:30:12 ner/bert_uncased/pos_train.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:14 ner/bert_uncased/pos_dev.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:15 ner/bert_uncased/pos_test.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n12/31/2019 04:30:15 Task chunk\r\ntask_type: TaskType.SeqenceLabeling\r\nlabel_mapper: <data_utils.vocab.Vocabulary object at 0x7f0cd3f7a1d0>\r\n==load data===\r\n12/31/2019 04:30:16 ner/bert_uncased/chunk_train.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:19 ner/bert_uncased/chunk_dev.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n==load data===\r\n12/31/2019 04:30:19 ner/bert_uncased/chunk_test.json\r\n====build_data=====\r\ndata seq task type\r\n===build_data_sequence===\r\n\r\npython train.py --data_dir ner/bert_uncased/ --init_checkpoint mt_dnn_models/bert_model_base_uncased.pt --train_dataset ner --test_dataset ner --task_def experiments/ner/ner_task_def.yml\r\nBetter speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\r\nNamespace(adam_eps=1e-06, answer_att_hidden_size=128, answer_att_type='bilinear', answer_dropout_p=0.1, answer_mem_drop_p=0.1, answer_mem_type=1, answer_merge_opt=1, answer_num_turn=5, answer_opt=0, answer_rnn_type='gru', answer_sum_att_type='bilinear', answer_weight_norm_on=False, batch_size=8, batch_size_eval=8, bert_dropout_p=0.1, bert_l2norm=0.0, cuda=True, data_dir='ner/bert_uncased/', data_sort_on=False, dropout_p=0.1, dropout_w=0.0, dump_state_on=False, ema_gamma=0.995, ema_opt=0, embedding_opt=0, encoder_type=<EncoderModelType.BERT: 1>, epochs=5, fp16=False, fp16_opt_level='O1', freeze_layers=-1, global_grad_clipping=1.0, glue_format_on=False, grad_accumulation_step=1, grad_clipping=0, have_lr_scheduler=True, init_checkpoint='mt_dnn_models/bert_model_base_uncased.pt', init_ratio=1, label_size='3', learning_rate=5e-05, log_file='mt-dnn-train.log', log_per_updates=500, lr_gamma=0.5, max_seq_len=512, mem_cum_type='simple', mix_opt=0, model_ckpt='checkpoints/model_0.pt', momentum=0, mtl_opt=0, multi_gpu_on=False, multi_step_lr='10,20,30', name='farmer', optimizer='adamax', output_dir='checkpoint', ratio=0, resume=False, save_per_updates=10000, save_per_updates_on=False, scheduler_type='ms', seed=2018, task_def='experiments/ner/ner_task_def.yml', tensorboard=False, tensorboard_logdir='tensorboard_logdir', test_datasets=['ner'], train_datasets=['ner'], update_bert_opt=0, vb_dropout=True, warmup=0.1, warmup_schedule='warmup_linear', weight_decay=0)\r\n12/31/2019 04:30:48 0\r\n12/31/2019 04:30:48 Launching the MT-DNN training\r\n12/31/2019 04:30:48 Loading ner/bert_uncased/ner_train.json as task 0\r\nLoaded 14041 samples out of 14041\r\n12/31/2019 04:30:48 12\r\nLoaded 3250 samples out of 3250\r\nLoaded 3453 samples out of 3453\r\n12/31/2019 04:30:48 ####################\r\n12/31/2019 04:30:48 {'log_file': 'mt-dnn-train.log', 'tensorboard': False, 'tensorboard_logdir': 'tensorboard_logdir', 'init_checkpoint': 'mt_dnn_models/bert_model_base_uncased.pt', 'data_dir': 'ner/bert_uncased/', 'data_sort_on': False, 'name': 'farmer', 'task_def': 'experiments/ner/ner_task_def.yml', 'train_datasets': ['ner'], 'test_datasets': ['ner'], 'glue_format_on': False, 'update_bert_opt': 0, 'multi_gpu_on': False, 'mem_cum_type': 'simple', 'answer_num_turn': 5, 'answer_mem_drop_p': 0.1, 'answer_att_hidden_size': 128, 'answer_att_type': 'bilinear', 'answer_rnn_type': 'gru', 'answer_sum_att_type': 'bilinear', 'answer_merge_opt': 1, 'answer_mem_type': 1, 'answer_dropout_p': 0.1, 'answer_weight_norm_on': False, 'dump_state_on': False, 'answer_opt': [0], 'label_size': '12', 'mtl_opt': 0, 'ratio': 0, 'mix_opt': 0, 'max_seq_len': 512, 'init_ratio': 1, 'encoder_type': <EncoderModelType.BERT: 1>, 'cuda': True, 'log_per_updates': 500, 'save_per_updates': 10000, 'save_per_updates_on': False, 'epochs': 5, 'batch_size': 8, 'batch_size_eval': 8, 'optimizer': 'adamax', 'grad_clipping': 0, 'global_grad_clipping': 1.0, 'weight_decay': 0, 'learning_rate': 5e-05, 'momentum': 0, 'warmup': 0.1, 'warmup_schedule': 'warmup_linear', 'adam_eps': 1e-06, 'vb_dropout': True, 'dropout_p': 0.1, 'dropout_w': 0.0, 'bert_dropout_p': 0.1, 'model_ckpt': 'checkpoints/model_0.pt', 'resume': False, 'ema_opt': 0, 'ema_gamma': 0.995, 'have_lr_scheduler': True, 'multi_step_lr': '10,20,30', 'freeze_layers': -1, 'embedding_opt': 0, 'lr_gamma': 0.5, 'bert_l2norm': 0.0, 'scheduler_type': 'ms', 'output_dir': 'checkpoint', 'seed': 2018, 'grad_accumulation_step': 1, 'fp16': False, 'fp16_opt_level': 'O1', 'task_types': [<TaskType.SeqenceLabeling: 5>], 'tasks_dropout_p': [0.3]}\r\n12/31/2019 04:30:48 ####################\r\n12/31/2019 04:30:48 ############# Gradient Accumulation Info #############\r\n12/31/2019 04:30:48 number of step: 8780\r\n12/31/2019 04:30:48 number of grad grad_accumulation step: 1\r\n12/31/2019 04:30:48 adjusted number of step: 8780\r\n12/31/2019 04:30:48 ############# Gradient Accumulation Info #############\r\n12/31/2019 04:30:52 \r\n############# Model Arch of MT-DNN #############\r\nSANBertNetwork(\r\n  (dropout_list): ModuleList(\r\n    (0): DropoutWrapper()\r\n  )\r\n  (bert): BertModel(\r\n    (embeddings): BertEmbeddings(\r\n      (word_embeddings): Embedding(30522, 768)\r\n      (position_embeddings): Embedding(512, 768)\r\n      (token_type_embeddings): Embedding(2, 768)\r\n      (LayerNorm): BertLayerNorm()\r\n      (dropout): Dropout(p=0.1)\r\n    )\r\n    (encoder): BertEncoder(\r\n      (layer): ModuleList(\r\n        (0): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (1): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (2): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (3): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (4): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (5): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (6): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (7): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (8): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (9): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (10): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n        (11): BertLayer(\r\n          (attention): BertAttention(\r\n            (self): BertSelfAttention(\r\n              (query): Linear(in_features=768, out_features=768, bias=True)\r\n              (key): Linear(in_features=768, out_features=768, bias=True)\r\n              (value): Linear(in_features=768, out_features=768, bias=True)\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n            (output): BertSelfOutput(\r\n              (dense): Linear(in_features=768, out_features=768, bias=True)\r\n              (LayerNorm): BertLayerNorm()\r\n              (dropout): Dropout(p=0.1)\r\n            )\r\n          )\r\n          (intermediate): BertIntermediate(\r\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\r\n          )\r\n          (output): BertOutput(\r\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\r\n            (LayerNorm): BertLayerNorm()\r\n            (dropout): Dropout(p=0.1)\r\n          )\r\n        )\r\n      )\r\n    )\r\n    (pooler): BertPooler(\r\n      (dense): Linear(in_features=768, out_features=768, bias=True)\r\n      (activation): Tanh()\r\n    )\r\n  )\r\n  (scoring_list): ModuleList(\r\n    (0): Linear(in_features=768, out_features=12, bias=True)\r\n  )\r\n)\r\n\r\n12/31/2019 04:30:52 Total number of params: 109491468\r\n12/31/2019 04:30:52 At epoch 0\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 434, in <module>\r\n    main()\r\n  File \"train.py\", line 368, in main\r\n    batch_meta, batch_data = next(all_iters[task_id])\r\n  File \"/home/<user>/mt-dnn/mt_dnn/batcher.py\", line 149, in __iter__\r\n    tlab[i, : ll] = torch.LongTensor(label)\r\nTypeError: an integer is required (got type str)\r\n```","closed_by":{"login":"namisan","id":3060273,"node_id":"MDQ6VXNlcjMwNjAyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/3060273?v=4","gravatar_id":"","url":"https://api.github.com/users/namisan","html_url":"https://github.com/namisan","followers_url":"https://api.github.com/users/namisan/followers","following_url":"https://api.github.com/users/namisan/following{/other_user}","gists_url":"https://api.github.com/users/namisan/gists{/gist_id}","starred_url":"https://api.github.com/users/namisan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/namisan/subscriptions","organizations_url":"https://api.github.com/users/namisan/orgs","repos_url":"https://api.github.com/users/namisan/repos","events_url":"https://api.github.com/users/namisan/events{/privacy}","received_events_url":"https://api.github.com/users/namisan/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/namisan/mt-dnn/issues/131/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/namisan/mt-dnn/issues/131/timeline","performed_via_github_app":null,"state_reason":"completed"}