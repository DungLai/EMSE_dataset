{"url":"https://api.github.com/repos/namisan/mt-dnn/issues/125","repository_url":"https://api.github.com/repos/namisan/mt-dnn","labels_url":"https://api.github.com/repos/namisan/mt-dnn/issues/125/labels{/name}","comments_url":"https://api.github.com/repos/namisan/mt-dnn/issues/125/comments","events_url":"https://api.github.com/repos/namisan/mt-dnn/issues/125/events","html_url":"https://github.com/namisan/mt-dnn/issues/125","id":535558169,"node_id":"MDU6SXNzdWU1MzU1NTgxNjk=","number":125,"title":"modify load data method to fit billion level data, but memory leak, help!!","user":{"login":"xiangxianzhang","id":10906688,"node_id":"MDQ6VXNlcjEwOTA2Njg4","avatar_url":"https://avatars.githubusercontent.com/u/10906688?v=4","gravatar_id":"","url":"https://api.github.com/users/xiangxianzhang","html_url":"https://github.com/xiangxianzhang","followers_url":"https://api.github.com/users/xiangxianzhang/followers","following_url":"https://api.github.com/users/xiangxianzhang/following{/other_user}","gists_url":"https://api.github.com/users/xiangxianzhang/gists{/gist_id}","starred_url":"https://api.github.com/users/xiangxianzhang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xiangxianzhang/subscriptions","organizations_url":"https://api.github.com/users/xiangxianzhang/orgs","repos_url":"https://api.github.com/users/xiangxianzhang/repos","events_url":"https://api.github.com/users/xiangxianzhang/events{/privacy}","received_events_url":"https://api.github.com/users/xiangxianzhang/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"namisan","id":3060273,"node_id":"MDQ6VXNlcjMwNjAyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/3060273?v=4","gravatar_id":"","url":"https://api.github.com/users/namisan","html_url":"https://github.com/namisan","followers_url":"https://api.github.com/users/namisan/followers","following_url":"https://api.github.com/users/namisan/following{/other_user}","gists_url":"https://api.github.com/users/namisan/gists{/gist_id}","starred_url":"https://api.github.com/users/namisan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/namisan/subscriptions","organizations_url":"https://api.github.com/users/namisan/orgs","repos_url":"https://api.github.com/users/namisan/repos","events_url":"https://api.github.com/users/namisan/events{/privacy}","received_events_url":"https://api.github.com/users/namisan/received_events","type":"User","site_admin":false},"assignees":[{"login":"namisan","id":3060273,"node_id":"MDQ6VXNlcjMwNjAyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/3060273?v=4","gravatar_id":"","url":"https://api.github.com/users/namisan","html_url":"https://github.com/namisan","followers_url":"https://api.github.com/users/namisan/followers","following_url":"https://api.github.com/users/namisan/following{/other_user}","gists_url":"https://api.github.com/users/namisan/gists{/gist_id}","starred_url":"https://api.github.com/users/namisan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/namisan/subscriptions","organizations_url":"https://api.github.com/users/namisan/orgs","repos_url":"https://api.github.com/users/namisan/repos","events_url":"https://api.github.com/users/namisan/events{/privacy}","received_events_url":"https://api.github.com/users/namisan/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2019-12-10T07:57:15Z","updated_at":"2021-11-13T15:03:57Z","closed_at":"2021-11-13T15:03:57Z","author_association":"NONE","active_lock_reason":null,"body":"We have a billion pieces of data， so, source code put all data to memory is not work. i use dataloader + yield , every batch load data to memory, but gpu memory continue increase. i just test load data, not begin train. I try all clear memory methods（torch.cuda.empty_cache() ... ）, but not work.\r\nThis is my read data code\r\n```\r\nclass CustomIterableDataset(IterableDataset):\r\n    def __init__(self, task_def, task_id, batch_size=32,\r\n                 gpu=True, is_train=True, epochs=10,\r\n                 maxlen=128, dropout_w=0.005):\r\n        super(CustomIterableDataset).__init__()\r\n        ...省略n行\r\n\r\n    def _get_max_len(self, batch, key='token_id'):\r\n        tok_len = max(len(x[key]) for x in batch)\r\n        return tok_len\r\n\r\n    def __if_pair__(self, data_type):\r\n        return data_type in [DataFormat.PremiseAndOneHypothesis, DataFormat.PremiseAndMultiHypothesis]\r\n\r\n    def __random_select__(self, arr):\r\n        if self.dropout_w > 0:\r\n            return [UNK_ID if random.uniform(0, 1) < self.dropout_w else e for e in arr]\r\n        else: return arr\r\n\r\n    def patch(self, v):\r\n        v = v.cuda(non_blocking=True)\r\n        return v\r\n\r\n    def _get_batch_size(self, batch):\r\n        return len(batch)\r\n\r\n    def _prepare_model_input(self, batch_def):\r\n         #ignore\r\n        ...\r\n        return batch_data, batch_info\r\n\r\n    def _process(self, batch_def):\r\n        #省略n行....\r\n        if self.gpu:\r\n            for i, item in enumerate(batch_data):\r\n                batch_data[i] = self.patch(item)\r\n\r\n         #省略n行....\r\n        return batch_info, batch_data\r\n\r\n    def _line_mapper(self, lines):\r\n        samples = []\r\n        for line in lines:\r\n            sample = json.loads(line.strip())\r\n            sample['factor'] = 1.0\r\n            samples.append(sample)\r\n        batch_def = {\"data\": samples,\r\n                     \"task_type\": self.task_def[\"task_type\"],\r\n                     \"task_id\": self.task_id,\r\n                     \"data_type\": self.task_def[\"data_type\"],\r\n                     \"encoder_type\": self.task_def[\"encoder_type\"],\r\n        }\r\n        return self._process(batch_def)\r\n\r\n    def _dir_iter(self, file_list):\r\n        if len(file_list) == 0:\r\n            return None\r\n        #file_list = random.shuffle(file_list)\r\n        for f in file_list:\r\n            with open(f) as reader:\r\n                lines = []\r\n                for line in reader:\r\n                    if len(lines) >= self.batch_size:\r\n                        yield lines\r\n                        lines = []\r\n                        torch.cuda.empty_cache()\r\n                    lines.append(line)\r\n                yield lines\r\n\r\n    def __iter__(self):\r\n        if self.is_train:\r\n            dataset_dir = self.task_def['train_dataset_dir']\r\n        else:\r\n            dataset_dir = task_def['test_dataset_dir']\r\n        file_list = os.listdir(dataset_dir)\r\n        for i, data in enumerate(file_list):\r\n            data = os.path.join(dataset_dir, data)\r\n            file_list[i] = data\r\n\r\n        line_iter = self._dir_iter(file_list)\r\n        # Create an iterator\r\n        mapped_itr = map(self._line_mapper, line_iter)\r\n\r\n        return mapped_itr\r\n```\r\nthis  is run independently code\r\npytorch 1.2\r\n\r\n```\r\nimport os\r\nimport sys\r\nimport json\r\nimport torch\r\nimport random\r\nimport resource\r\nfrom torch.utils.data import IterableDataset\r\nfrom torch.utils.data import DataLoader\r\nfrom itertools import tee\r\n\r\nUNK_ID=100\r\nBOS_ID=101\r\n\r\nclass CustomIterableDataset(IterableDataset):\r\n    def __init__(self, task_def, task_id, batch_size=32,\r\n                 gpu=True, is_train=True, epochs=10,\r\n                 maxlen=128, dropout_w=0.005):\r\n        super(CustomIterableDataset).__init__()\r\n        self.task_def = task_def\r\n        self.task_id = task_id\r\n        self.batch_size = batch_size\r\n        self.maxlen = maxlen\r\n        self.is_train = is_train\r\n        self.epochs = 1 if not is_train else epochs\r\n        self.gpu = gpu\r\n        self.dropout_w = dropout_w\r\n        self.pairwise_size = 1\r\n\r\n\r\n    def patch(self, v):\r\n        v = v.cuda(non_blocking=True)\r\n        return v\r\n\r\n    def _dir_iter(self):\r\n        for i in range(0,3):\r\n            lines = []\r\n            reader = [i for i in range(0,1000000)]\r\n            for line in reader:\r\n                line = torch.LongTensor(self.batch_size, 10).fill_(0)\r\n                if len(lines) >= self.batch_size:\r\n                    yield lines\r\n                    lines = []\r\n                    torch.cuda.empty_cache ()\r\n                lines.append(self.patch(line))\r\n            del reader\r\n            yield lines\r\n\r\n    def __iter__(self):\r\n        mapped_itr =self._dir_iter()\r\n        return mapped_itr\r\n\r\n\r\nif __name__ == '__main__':\r\n    dataset_iter = CustomIterableDataset(task_def=None, task_id=None)\r\n    data_generator = DataLoader(dataset_iter, batch_size=None)\r\n    train_generator_list = []\r\n    train_generator_list.append(iter(data_generator))\r\n    copy_iter_list = []\r\n    for id, first_it in enumerate(train_generator_list):\r\n        first_itr, second_itr = tee(first_it)\r\n        train_generator_list[id] = first_itr\r\n        copy_iter_list.append(second_itr)\r\n    i= 0\r\n    while True:\r\n        try:\r\n            i += 1\r\n            batch_data = next(train_generator_list[0])\r\n            if i % 100 == 0:\r\n                max_mem_used = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\r\n                print(\"Mem: {:.2f} MB\".format(max_mem_used / 1024))\r\n        except StopIteration:\r\n            # end of one epoch\r\n            print('again')\r\n            _, train_generator_list[0] = tee(copy_iter_list[0])\r\n```\r\n![image](https://user-images.githubusercontent.com/10906688/70513492-b8426b80-1b6c-11ea-93b6-7940d5c4e35e.png)\r\n","closed_by":{"login":"namisan","id":3060273,"node_id":"MDQ6VXNlcjMwNjAyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/3060273?v=4","gravatar_id":"","url":"https://api.github.com/users/namisan","html_url":"https://github.com/namisan","followers_url":"https://api.github.com/users/namisan/followers","following_url":"https://api.github.com/users/namisan/following{/other_user}","gists_url":"https://api.github.com/users/namisan/gists{/gist_id}","starred_url":"https://api.github.com/users/namisan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/namisan/subscriptions","organizations_url":"https://api.github.com/users/namisan/orgs","repos_url":"https://api.github.com/users/namisan/repos","events_url":"https://api.github.com/users/namisan/events{/privacy}","received_events_url":"https://api.github.com/users/namisan/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/namisan/mt-dnn/issues/125/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/namisan/mt-dnn/issues/125/timeline","performed_via_github_app":null,"state_reason":"completed"}