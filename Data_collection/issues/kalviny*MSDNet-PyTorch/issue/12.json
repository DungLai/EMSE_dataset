{"url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12","repository_url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch","labels_url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12/labels{/name}","comments_url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12/comments","events_url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12/events","html_url":"https://github.com/kalviny/MSDNet-PyTorch/issues/12","id":562616634,"node_id":"MDU6SXNzdWU1NjI2MTY2MzQ=","number":12,"title":"Reproduce budget batch classification results for CIFAR100","user":{"login":"gdamaskinos","id":22378900,"node_id":"MDQ6VXNlcjIyMzc4OTAw","avatar_url":"https://avatars.githubusercontent.com/u/22378900?v=4","gravatar_id":"","url":"https://api.github.com/users/gdamaskinos","html_url":"https://github.com/gdamaskinos","followers_url":"https://api.github.com/users/gdamaskinos/followers","following_url":"https://api.github.com/users/gdamaskinos/following{/other_user}","gists_url":"https://api.github.com/users/gdamaskinos/gists{/gist_id}","starred_url":"https://api.github.com/users/gdamaskinos/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gdamaskinos/subscriptions","organizations_url":"https://api.github.com/users/gdamaskinos/orgs","repos_url":"https://api.github.com/users/gdamaskinos/repos","events_url":"https://api.github.com/users/gdamaskinos/events{/privacy}","received_events_url":"https://api.github.com/users/gdamaskinos/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-02-10T14:56:55Z","updated_at":"2021-10-28T18:56:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I am trying to reproduce the `MSDNet with dynamic evaluation` curve of Figure 7b of the paper (budgeted batch classification for CIFAR-100).\r\n\r\nI am training the MSDNet [as mentioned in the README file](https://github.com/kalviny/MSDNet-PyTorch/tree/b4e083cda73ce20bcee63a222dc1b57f600268d0#train-an-msdnet-block5-on-cifar-100-for-efficient-batch-computation).\r\nI then evaluate by:\r\n```python main.py --data-root /path/to/cifar100/ --data cifar100 --save /path/to/out/ --arch msdnet --batch-size 64 --epochs 300 --nBlocks 5 --stepmode lin_grow --step 1 --base 1 --nChannels 16 --evalmode dynamic --evaluate-from /path/to/out/save_models/checkpoint_299.pth.tar --use-valid -j 1 --gpu 0 ```\r\n\r\n<details><summary>Output log</summary>\r\n<p>\r\n\r\n```bash\r\nbuilding network of steps:\r\n[1, 2, 3, 4, 5] 15\r\n ********************** Block 1  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 16 outChannels 6\t\t|\r\n\r\n ********************** Block 2  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 22 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 3 inChannels 28 outChannels 6\t\t|\r\n\r\n ********************** Block 3  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 34 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 3 inChannels 40 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 2 inChannels 46 outChannels 6\t\t|\r\n|\t\tTransition layer inserted! (max), inChannels 52, outChannels 26\t|\r\n\r\n ********************** Block 4  **********************\r\n|\t\tinScales 2 outScales 2 inChannels 26 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 32 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 38 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 44 outChannels 6\t\t|\r\n\r\n ********************** Block 5  **********************\r\n|\t\tinScales 2 outScales 1 inChannels 50 outChannels 6\t\t|\r\n|\t\tTransition layer inserted! (max), inChannels 56, outChannels 28\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 28 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 34 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 40 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 46 outChannels 6\t\t|\r\n\r\n---------------------\r\nFLOPs: 6.86M, Params: 0.30M\r\n---------------------\r\nFLOPs: 14.35M, Params: 0.65M\r\n---------------------\r\nFLOPs: 27.54M, Params: 1.02M\r\n---------------------\r\nFLOPs: 41.71M, Params: 1.49M\r\n---------------------\r\nFLOPs: 58.48M, Params: 2.08M\r\nbuilding network of steps:\r\n[1, 2, 3, 4, 5] 15\r\n ********************** Block 1  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 16 outChannels 6\t\t|\r\n\r\n ********************** Block 2  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 22 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 3 inChannels 28 outChannels 6\t\t|\r\n\r\n ********************** Block 3  **********************\r\n|\t\tinScales 3 outScales 3 inChannels 34 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 3 inChannels 40 outChannels 6\t\t|\r\n\r\n|\t\tinScales 3 outScales 2 inChannels 46 outChannels 6\t\t|\r\n|\t\tTransition layer inserted! (max), inChannels 52, outChannels 26\t|\r\n\r\n ********************** Block 4  **********************\r\n|\t\tinScales 2 outScales 2 inChannels 26 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 32 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 38 outChannels 6\t\t|\r\n\r\n|\t\tinScales 2 outScales 2 inChannels 44 outChannels 6\t\t|\r\n\r\n ********************** Block 5  **********************\r\n|\t\tinScales 2 outScales 1 inChannels 50 outChannels 6\t\t|\r\n|\t\tTransition layer inserted! (max), inChannels 56, outChannels 28\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 28 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 34 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 40 outChannels 6\t\t|\r\n\r\n|\t\tinScales 1 outScales 1 inChannels 46 outChannels 6\t\t|\r\n\r\n!!!!!! Load train_set_index !!!!!!\r\n*********************\r\n/home/damaskin/MSDNet-PyTorch/adaptive_inference.py:28: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].\r\n  probs = torch.exp(torch.log(_p) * torch.range(1, args.nBlocks))\r\nvalid acc: 59.020, test acc: 61.500, test flops: 7.09M\r\n*********************\r\nvalid acc: 59.960, test acc: 62.080, test flops: 7.44M\r\n*********************\r\nvalid acc: 60.700, test acc: 62.870, test flops: 7.90M\r\n*********************\r\nvalid acc: 61.660, test acc: 63.500, test flops: 8.36M\r\n*********************\r\nvalid acc: 62.700, test acc: 64.050, test flops: 8.93M\r\n*********************\r\nvalid acc: 63.900, test acc: 64.850, test flops: 9.66M\r\n*********************\r\nvalid acc: 65.040, test acc: 65.670, test flops: 10.53M\r\n*********************\r\nvalid acc: 65.760, test acc: 66.400, test flops: 11.32M\r\n*********************\r\nvalid acc: 66.920, test acc: 67.130, test flops: 12.34M\r\n*********************\r\nvalid acc: 67.740, test acc: 67.800, test flops: 13.32M\r\n*********************\r\nvalid acc: 68.600, test acc: 68.560, test flops: 14.48M\r\n*********************\r\nvalid acc: 69.500, test acc: 69.330, test flops: 15.67M\r\n*********************\r\nvalid acc: 69.880, test acc: 69.980, test flops: 16.88M\r\n*********************\r\nvalid acc: 70.420, test acc: 70.310, test flops: 18.25M\r\n*********************\r\nvalid acc: 70.820, test acc: 70.770, test flops: 19.61M\r\n*********************\r\nvalid acc: 71.460, test acc: 71.150, test flops: 20.90M\r\n*********************\r\nvalid acc: 71.840, test acc: 71.530, test flops: 22.28M\r\n*********************\r\nvalid acc: 72.260, test acc: 71.970, test flops: 23.62M\r\n*********************\r\nvalid acc: 72.860, test acc: 72.150, test flops: 25.03M\r\n*********************\r\nvalid acc: 72.880, test acc: 72.360, test flops: 26.24M\r\n*********************\r\nvalid acc: 72.880, test acc: 72.530, test flops: 27.46M\r\n*********************\r\nvalid acc: 72.960, test acc: 72.610, test flops: 28.95M\r\n*********************\r\nvalid acc: 73.080, test acc: 72.650, test flops: 30.14M\r\n*********************\r\nvalid acc: 72.840, test acc: 72.900, test flops: 31.26M\r\n*********************\r\nvalid acc: 72.780, test acc: 72.920, test flops: 32.32M\r\n*********************\r\nvalid acc: 72.840, test acc: 73.070, test flops: 33.44M\r\n*********************\r\nvalid acc: 72.640, test acc: 73.020, test flops: 34.26M\r\n*********************\r\nvalid acc: 72.720, test acc: 73.160, test flops: 35.17M\r\n*********************\r\nvalid acc: 72.740, test acc: 72.940, test flops: 36.09M\r\n*********************\r\nvalid acc: 72.660, test acc: 73.030, test flops: 36.87M\r\n*********************\r\nvalid acc: 72.700, test acc: 72.980, test flops: 37.61M\r\n*********************\r\nvalid acc: 72.600, test acc: 72.920, test flops: 38.28M\r\n*********************\r\nvalid acc: 72.580, test acc: 72.870, test flops: 39.01M\r\n*********************\r\nvalid acc: 72.460, test acc: 72.680, test flops: 39.59M\r\n*********************\r\nvalid acc: 72.400, test acc: 72.660, test flops: 40.20M\r\n*********************\r\nvalid acc: 72.300, test acc: 72.660, test flops: 40.80M\r\n*********************\r\nvalid acc: 72.220, test acc: 72.620, test flops: 41.28M\r\n*********************\r\nvalid acc: 72.160, test acc: 72.580, test flops: 41.80M\r\n*********************\r\nvalid acc: 72.160, test acc: 72.580, test flops: 42.16M\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nThe final [test accuracy, budget] pair is  [72.58, 0.4 * 10^8] which is not consistent with the results of Figure 7b (as it should have been [~74, 0.4 * 10^8]).\r\nWhat are the parameters of the 3 MSDNets used in Figure 7b ?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/kalviny/MSDNet-PyTorch/issues/12/timeline","performed_via_github_app":null,"state_reason":null}