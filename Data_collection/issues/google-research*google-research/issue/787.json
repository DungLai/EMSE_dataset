{"url":"https://api.github.com/repos/google-research/google-research/issues/787","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/787/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/787/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/787/events","html_url":"https://github.com/google-research/google-research/issues/787","id":974407616,"node_id":"MDU6SXNzdWU5NzQ0MDc2MTY=","number":787,"title":"continuous-action environments for dual_dice","user":{"login":"cclvr","id":17332309,"node_id":"MDQ6VXNlcjE3MzMyMzA5","avatar_url":"https://avatars.githubusercontent.com/u/17332309?v=4","gravatar_id":"","url":"https://api.github.com/users/cclvr","html_url":"https://github.com/cclvr","followers_url":"https://api.github.com/users/cclvr/followers","following_url":"https://api.github.com/users/cclvr/following{/other_user}","gists_url":"https://api.github.com/users/cclvr/gists{/gist_id}","starred_url":"https://api.github.com/users/cclvr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cclvr/subscriptions","organizations_url":"https://api.github.com/users/cclvr/orgs","repos_url":"https://api.github.com/users/cclvr/repos","events_url":"https://api.github.com/users/cclvr/events{/privacy}","received_events_url":"https://api.github.com/users/cclvr/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-19T08:26:25Z","updated_at":"2021-08-19T09:03:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"@eladeban,\r\n      Recently I try to reproduce dual_dice algorithm on Reacher, but I can't get the results as you presented in the paper. The phenomena is quite strange: the estimate average step reward is quite close to the behavior policy's step reward rather than the target policy's. Since there is no source code for continuous action setting, could you help to provide more details? \r\n     i) when calculating \\nu(s',a'),  I use (1/N)*\\sum_{i=1}^{N}nu(s',\\pi(s')) and try N=1,10,100 respectively, and the results are the same. How you calculate it?\r\n    ii) For this setting, do you choose Fenchel conjugate trick or not?\r\n   iii) Is any other tips I need to know?\r\n\r\n   Look forward to your reply.\r\n   Best wishes.\r\n                       \r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/787/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/787/timeline","performed_via_github_app":null,"state_reason":null}