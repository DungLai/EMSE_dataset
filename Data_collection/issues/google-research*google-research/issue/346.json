{"url":"https://api.github.com/repos/google-research/google-research/issues/346","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/346/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/346/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/346/events","html_url":"https://github.com/google-research/google-research/issues/346","id":674413542,"node_id":"MDU6SXNzdWU2NzQ0MTM1NDI=","number":346,"title":"Question about BERT model size (transformer block number) ","user":{"login":"ZLKong","id":28882362,"node_id":"MDQ6VXNlcjI4ODgyMzYy","avatar_url":"https://avatars.githubusercontent.com/u/28882362?v=4","gravatar_id":"","url":"https://api.github.com/users/ZLKong","html_url":"https://github.com/ZLKong","followers_url":"https://api.github.com/users/ZLKong/followers","following_url":"https://api.github.com/users/ZLKong/following{/other_user}","gists_url":"https://api.github.com/users/ZLKong/gists{/gist_id}","starred_url":"https://api.github.com/users/ZLKong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ZLKong/subscriptions","organizations_url":"https://api.github.com/users/ZLKong/orgs","repos_url":"https://api.github.com/users/ZLKong/repos","events_url":"https://api.github.com/users/ZLKong/events{/privacy}","received_events_url":"https://api.github.com/users/ZLKong/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-08-06T15:44:45Z","updated_at":"2020-08-06T15:44:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nThank you for your interesting work! I have just started to learn BERT recently. I have some general questions regarding this topic.\r\n\r\n1. Why BERT has 12 blocks? Not 11 or 13 etc. ? I couldn't find any explanation.\r\n\r\n2. I want to compare the performance of BERT with different model size (transformer block number). Is it necessary to do distillation?  If I just train a BERT with 6 Layers without distillation, does the performance look bad?\r\n\r\n3. Do you have to do pretraining every time you change the layer number of  BERT? Is it possible to just remove some layers in an existing pre-trained model and finetune on tasks? \r\n\r\n\r\nThanks,\r\nZLK\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/346/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/346/timeline","performed_via_github_app":null,"state_reason":null}