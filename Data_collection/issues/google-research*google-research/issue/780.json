{"url":"https://api.github.com/repos/google-research/google-research/issues/780","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/780/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/780/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/780/events","html_url":"https://github.com/google-research/google-research/issues/780","id":970221404,"node_id":"MDU6SXNzdWU5NzAyMjE0MDQ=","number":780,"title":"Error about convert MobileBERT to TFLite ","user":{"login":"bhbruce","id":55973122,"node_id":"MDQ6VXNlcjU1OTczMTIy","avatar_url":"https://avatars.githubusercontent.com/u/55973122?v=4","gravatar_id":"","url":"https://api.github.com/users/bhbruce","html_url":"https://github.com/bhbruce","followers_url":"https://api.github.com/users/bhbruce/followers","following_url":"https://api.github.com/users/bhbruce/following{/other_user}","gists_url":"https://api.github.com/users/bhbruce/gists{/gist_id}","starred_url":"https://api.github.com/users/bhbruce/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhbruce/subscriptions","organizations_url":"https://api.github.com/users/bhbruce/orgs","repos_url":"https://api.github.com/users/bhbruce/repos","events_url":"https://api.github.com/users/bhbruce/events{/privacy}","received_events_url":"https://api.github.com/users/bhbruce/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-13T09:20:29Z","updated_at":"2021-12-04T19:16:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"@saberkun @renjie-liu \r\nI would like to convert mobilebert to tflite format.\r\nI use the quantized weight you offered in the repo with tensorflow 1.15. https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints \r\n\r\nBut the error occurs when I try to convert the model to int8 tflite model. \r\nI use the flag: `--use_post_quantization  --activation_quantization`\r\n\r\nLog:\r\n```\r\n2021-08-13 09:03:07.437436: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\r\n2021-08-13 09:03:09.818900: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2021-08-13 09:03:09.818948: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (-6520), 7317 edges (-6524), time = 1308.49194ms.\r\n2021-08-13 09:03:09.818955: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 7077 nodes (0), 7317 edges (0), time = 426.431ms.\r\nTraceback (most recent call last):\r\n  File \"run_squad_ptq.py\", line 653, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_squad_ptq.py\", line 642, in main\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0\r\n```\r\n\r\nI also try to convert the model in Tensorflow 2.3/2.4/2.6. In those versions, the model can convert successfully to tflite.\r\nHowever, a runtime error occurs during inference.\r\n```\r\nRuntimeError: tensorflow/lite/kernels/dequantize.cc:75 op_context.input->type == kTfLiteUInt8 || op_context.input->type == kTfLiteInt8 || op_context.input->type == kTfLiteInt16 || op_context.input->type == kTfLiteFloat16 was not true.Node number 2 (DEQUANTIZE) failed to prepare.\r\n ```\r\nI've referred to #325. It still does not work.\r\nI can convert model weights to integers, but whenever I want to convert activation error happened.\r\n\r\nAnother question: why the model which I converts to fp32 tflite model cannot show in Netron? Your models in tfhub can do. Are there any differences between tfhub models and https://github.com/google-research/google-research/tree/master/mobilebert#pre-trained-checkpoints? ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/780/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/780/timeline","performed_via_github_app":null,"state_reason":null}