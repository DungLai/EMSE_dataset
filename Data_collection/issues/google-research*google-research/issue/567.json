{"url":"https://api.github.com/repos/google-research/google-research/issues/567","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/567/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/567/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/567/events","html_url":"https://github.com/google-research/google-research/issues/567","id":804260112,"node_id":"MDU6SXNzdWU4MDQyNjAxMTI=","number":567,"title":"[slot_attention] Gradient instability","user":{"login":"vadimkantorov","id":1041752,"node_id":"MDQ6VXNlcjEwNDE3NTI=","avatar_url":"https://avatars.githubusercontent.com/u/1041752?v=4","gravatar_id":"","url":"https://api.github.com/users/vadimkantorov","html_url":"https://github.com/vadimkantorov","followers_url":"https://api.github.com/users/vadimkantorov/followers","following_url":"https://api.github.com/users/vadimkantorov/following{/other_user}","gists_url":"https://api.github.com/users/vadimkantorov/gists{/gist_id}","starred_url":"https://api.github.com/users/vadimkantorov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vadimkantorov/subscriptions","organizations_url":"https://api.github.com/users/vadimkantorov/orgs","repos_url":"https://api.github.com/users/vadimkantorov/repos","events_url":"https://api.github.com/users/vadimkantorov/events{/privacy}","received_events_url":"https://api.github.com/users/vadimkantorov/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-09T06:38:53Z","updated_at":"2021-03-11T10:36:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"@tkipf I'm trying slot attention with higher level features.\r\n\r\nHard K-means variant trains very fast, while the full attention variant with `slots = updates` is prone to gradient blow-up (probably because of softmax and maybe), even with warmup. \r\n\r\nIt seems that the `slots = updates` variant does not do residual update in contrast to GRU (which in return requires learned parameters). Does it make sense to first try some residual / momentum `slots = alpha * slots_prev + (1 - alpha) * updates`?\r\n\r\nAlso it turns out that temperature `sqrt(slot_size)` is too high, I've had to decrease it for softmax not converging to uniform assignment.\r\n\r\nCould you recommend what gradients / distributions should I monitor? Gradients wrt projection weights? Gradients wrt inputs?\r\n\r\nHave you tried using more than 3 iterations in training? E.g. 10 iterations? Was it still stable?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/567/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/567/timeline","performed_via_github_app":null,"state_reason":null}