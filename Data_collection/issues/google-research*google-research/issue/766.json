{"url":"https://api.github.com/repos/google-research/google-research/issues/766","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/766/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/766/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/766/events","html_url":"https://github.com/google-research/google-research/issues/766","id":948410843,"node_id":"MDU6SXNzdWU5NDg0MTA4NDM=","number":766,"title":"Question about CuBERT","user":{"login":"lveltman","id":45791426,"node_id":"MDQ6VXNlcjQ1NzkxNDI2","avatar_url":"https://avatars.githubusercontent.com/u/45791426?v=4","gravatar_id":"","url":"https://api.github.com/users/lveltman","html_url":"https://github.com/lveltman","followers_url":"https://api.github.com/users/lveltman/followers","following_url":"https://api.github.com/users/lveltman/following{/other_user}","gists_url":"https://api.github.com/users/lveltman/gists{/gist_id}","starred_url":"https://api.github.com/users/lveltman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lveltman/subscriptions","organizations_url":"https://api.github.com/users/lveltman/orgs","repos_url":"https://api.github.com/users/lveltman/repos","events_url":"https://api.github.com/users/lveltman/events{/privacy}","received_events_url":"https://api.github.com/users/lveltman/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-07-20T08:12:38Z","updated_at":"2021-07-20T08:12:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello!\r\nI have some problems with launching CuBERT. I've loaded the dataset and model for the variable misuse task using methods from bert (run_classifier.py).\r\n\r\nAnd I got the error: `\"ValueError: The initializer passed is not valid. It should be a callable with no arguments and the shape should not be provided or an instance of 'tf.keras.initializers.*' and 'shape' should be fully defined.\" `\r\n\r\nwhen trying to execute the line: \r\n`result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)`\r\n\r\nfrom this:\r\n```\r\nfrom cubert_processors import VarmisuseProcessor\r\nfrom bert.run_classifier import convert_single_example, file_based_convert_examples_to_features, file_based_input_fn_builder\r\nfrom bert.run_classifier import PaddingInputExample\r\n\r\nprocessor = VarmisuseProcessor()\r\neval_batch_size = batch_size\r\neval_examples = processor.get_test_examples(data_dir)\r\nnum_actual_eval_examples = len(eval_examples)\r\n\r\nlabel_list = processor.get_labels()\r\nconvert_single_example(0, eval_examples[1], label_list, max_seq_length, full_tokenizer)\r\n\r\nif is_use_tpu:\r\n    while len(eval_examples) % eval_batch_size != 0:\r\n        eval_examples.append(PaddingInputExample())\r\n\r\neval_file = os.path.join(output_dir, \"eval.tf_record\")\r\nfile_based_convert_examples_to_features(\r\n    eval_examples, label_list, max_seq_length, full_tokenizer, eval_file)\r\n\r\ntf.logging.info(\"***** Running evaluation *****\")\r\ntf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\r\n                len(eval_examples), num_actual_eval_examples,\r\n                len(eval_examples) - num_actual_eval_examples)\r\ntf.logging.info(\"  Batch size = %d\", eval_batch_size)\r\n\r\n# This tells the estimator to run through the entire set.\r\neval_steps = None\r\n# However, if running eval on the TPU, you will need to specify the\r\n# number of steps.\r\nif is_use_tpu:\r\n    assert len(eval_examples) % eval_batch_size == 0\r\n    eval_steps = int(len(eval_examples) // eval_batch_size)\r\n\r\neval_drop_remainder = True if is_use_tpu else False\r\neval_input_fn = file_based_input_fn_builder(\r\n    input_file=eval_file,\r\n    seq_length=max_seq_length,\r\n    is_training=False,\r\n    drop_remainder=eval_drop_remainder)\r\n\r\n# evaluate all checkpoints; you can use the checkpoint with the best dev accuarcy\r\nsteps_and_files = []\r\nmodel_dir = saved_model_path\r\nfilenames = tf.gfile.ListDirectory(model_dir)\r\nfor filename in filenames:\r\n    if filename.endswith(\".index\"):\r\n        ckpt_name = filename[:-6]\r\n        cur_filename = os.path.join(model_dir, ckpt_name)\r\n        global_step = int(cur_filename.split(\"-\")[-1])\r\n        tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\r\n        steps_and_files.append([global_step, cur_filename])\r\nsteps_and_files = sorted(steps_and_files, key=lambda x: x[0])\r\noutput_eval_file = os.path.join(output_dir, \"eval_results_albert_zh.txt\")\r\nprint(\"output_eval_file:\", output_eval_file)\r\ntf.logging.info(\"output_eval_file:\" + output_eval_file)\r\nwith tf.gfile.GFile(output_eval_file, \"w\") as writer:\r\n    for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):\r\n        result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, checkpoint_path=filename)\r\n\r\n        tf.logging.info(\"***** Eval results %s *****\" % (filename))\r\n        writer.write(\"***** Eval results %s *****\\n\" % (filename))\r\n        for key in sorted(result.keys()):\r\n            tf.logging.info(\"  %s = %s\", key, str(result[key]))\r\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\r\n```\r\n\r\nCould someone tell me what I'm doing wrong?\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/766/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/766/timeline","performed_via_github_app":null,"state_reason":null}