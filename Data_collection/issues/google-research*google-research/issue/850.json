{"url":"https://api.github.com/repos/google-research/google-research/issues/850","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/850/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/850/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/850/events","html_url":"https://github.com/google-research/google-research/issues/850","id":1030397842,"node_id":"I_kwDOCQmIhc49ap-S","number":850,"title":"Order of Training","user":{"login":"wenjunli-0","id":29220984,"node_id":"MDQ6VXNlcjI5MjIwOTg0","avatar_url":"https://avatars.githubusercontent.com/u/29220984?v=4","gravatar_id":"","url":"https://api.github.com/users/wenjunli-0","html_url":"https://github.com/wenjunli-0","followers_url":"https://api.github.com/users/wenjunli-0/followers","following_url":"https://api.github.com/users/wenjunli-0/following{/other_user}","gists_url":"https://api.github.com/users/wenjunli-0/gists{/gist_id}","starred_url":"https://api.github.com/users/wenjunli-0/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wenjunli-0/subscriptions","organizations_url":"https://api.github.com/users/wenjunli-0/orgs","repos_url":"https://api.github.com/users/wenjunli-0/repos","events_url":"https://api.github.com/users/wenjunli-0/events{/privacy}","received_events_url":"https://api.github.com/users/wenjunli-0/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-19T14:41:58Z","updated_at":"2021-10-19T14:41:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Thanks for your awesome work! \r\n\r\nI am confused about the order of training, which is described in the first paragraph in Part 4 of your paper as this \"In order to approximate regret, we use the difference between the payoffs of two agents acting under the same environment conditions. Assume we are given a fixed environment with parameters fixed policy for the protagonist agent, and we then train a second antagonist agent, Ï€A, to optimality in this environment. Then, the difference between the reward obtained by the antagonist, and the protagonist.\"\r\n\r\nSo, the env_adversary will first generate an env based on the fixed policy of the Protagonist, then the Antagonist will be trained to optimality on this env. After training the Antagonist, we will compute the reward obtained by the Antagonist (already been trained) and Protagonist (not yet been trained) on this env. However, in your implementation, I could not find the order of training follows this way. What I find is that in the run() function you run the three agents in order, and the Antagonist is not first being trained (https://github.com/google-research/google-research/blob/5a4d95be1ca8ca4335d42f5a0326d79262c6992e/social_rl/adversarial_env/adversarial_driver.py#L118). \r\n\r\nCould you please explain this a bit to me? Really appreciate it.\r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/850/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/850/timeline","performed_via_github_app":null,"state_reason":null}