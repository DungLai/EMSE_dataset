{"url":"https://api.github.com/repos/google-research/google-research/issues/734","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/734/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/734/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/734/events","html_url":"https://github.com/google-research/google-research/issues/734","id":919885265,"node_id":"MDU6SXNzdWU5MTk4ODUyNjU=","number":734,"title":"CuBERT fine-tuning parameters","user":{"login":"wenting-zhao","id":8762524,"node_id":"MDQ6VXNlcjg3NjI1MjQ=","avatar_url":"https://avatars.githubusercontent.com/u/8762524?v=4","gravatar_id":"","url":"https://api.github.com/users/wenting-zhao","html_url":"https://github.com/wenting-zhao","followers_url":"https://api.github.com/users/wenting-zhao/followers","following_url":"https://api.github.com/users/wenting-zhao/following{/other_user}","gists_url":"https://api.github.com/users/wenting-zhao/gists{/gist_id}","starred_url":"https://api.github.com/users/wenting-zhao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wenting-zhao/subscriptions","organizations_url":"https://api.github.com/users/wenting-zhao/orgs","repos_url":"https://api.github.com/users/wenting-zhao/repos","events_url":"https://api.github.com/users/wenting-zhao/events{/privacy}","received_events_url":"https://api.github.com/users/wenting-zhao/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-06-13T21:29:44Z","updated_at":"2021-06-13T21:29:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there,\r\n\r\nThank you for the amazing work and for making the fine-tuning datasets online.  I was wondering if it would be possible to make cubert's fine-tuning parameters available to us?  Looking at the paper, it said,\r\n\r\n> We pre-train CuBERT with the default configuration of the BERT Large model, one model per example length (128, 256, 512, and 1,024 subword tokens) with batch sizes of 8,192, 4,096, 2,048, and 1,024 respectively, and the default BERT learning rate of 1 × 10−4. Fine-tuned models also used the same batch sizes as for pre-training, and BERT’s default learning rate (5 × 10−5). For both, we gradually warm up the learning rate for the first 10 % of examples, which is BERT’s default value.\r\n\r\nWhat do you mean by with batch sizes 8,192, 4,096, 2,048, and 1,024?  This seems not to match what I found on this page https://github.com/google-research/bert/blob/master/README.md.  Also, the paper said the evaluation is done on either V100 or P100.  Could you also specify the memory limit on those GPUs?\r\n\r\nFor the exception dataset, I can only achieve an accuracy of 75%, which is 4% away from what was reported, so I would greatly appreciate any help, as we find this work very interesting!  Thank you in advance!","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/734/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/734/timeline","performed_via_github_app":null,"state_reason":null}