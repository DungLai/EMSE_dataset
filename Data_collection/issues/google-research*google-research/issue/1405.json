{"url":"https://api.github.com/repos/google-research/google-research/issues/1405","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1405/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1405/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1405/events","html_url":"https://github.com/google-research/google-research/issues/1405","id":1470407045,"node_id":"I_kwDOCQmIhc5XpKGF","number":1405,"title":"prepend_prompt function for UL2 paper","user":{"login":"conceptofmind","id":25208228,"node_id":"MDQ6VXNlcjI1MjA4MjI4","avatar_url":"https://avatars.githubusercontent.com/u/25208228?v=4","gravatar_id":"","url":"https://api.github.com/users/conceptofmind","html_url":"https://github.com/conceptofmind","followers_url":"https://api.github.com/users/conceptofmind/followers","following_url":"https://api.github.com/users/conceptofmind/following{/other_user}","gists_url":"https://api.github.com/users/conceptofmind/gists{/gist_id}","starred_url":"https://api.github.com/users/conceptofmind/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/conceptofmind/subscriptions","organizations_url":"https://api.github.com/users/conceptofmind/orgs","repos_url":"https://api.github.com/users/conceptofmind/repos","events_url":"https://api.github.com/users/conceptofmind/events{/privacy}","received_events_url":"https://api.github.com/users/conceptofmind/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-12-01T00:22:09Z","updated_at":"2022-12-04T05:10:31Z","closed_at":"2022-12-04T05:10:31Z","author_association":"NONE","active_lock_reason":null,"body":"Hi @vanzytay,\r\n\r\nIs there any information on how the prepend_prompt function is defined in the UL2 paper?\r\n\r\nThank you,\r\n\r\nEnrico\r\n\r\n```python\r\ndataset = tfds.load('wikipedia/20220620.en', split='train', shuffle_files=True)\r\n\r\n# def prepend_prompt(ds, output_features, prompt_mode, mode):\r\n\r\n    \r\ndef ul2_objective(dataset: tf.data.Dataset,\r\n    sequence_length: seqio.preprocessors.SequenceLengthType, \r\n    output_features: seqio.preprocessors.OutputFeaturesType, \r\n    use_prefix_lm_task: bool = False,\r\n    rates: Optional[Sequence[float]] = None,\r\n    mean_noise_span_lengths: Sequence[float] = (3.0,),\r\n    noise_densities: Sequence[float] = (0.15,), \r\n    shard_ds: bool = True, \r\n    optional_task_prefixes: Optional[Sequence[str]] = None, \r\n    input_feature_key: str = \"inputs\", \r\n    merge_examples_to_reduce_padding: bool = True, \r\n    reserved_for_packing: bool = None, \r\n    seed: int = 7) -> tf.data.Dataset:\r\n    \r\n    \"\"\"\r\n    UL2-like pre-training objectives. This preprocessor amounts to calling the ‘span_corruption‘ function several times with different values of ‘noise_density‘ and ‘mean_noise_span_length‘. \r\n    We either shard or copy the dataset, then apply each function to each shard. Add S-denoising (prefixLM) using use_prefix_lm_task. \r\n    \r\n    Args: \r\n    dataset: A tf.data.Dataset with dictionaries containing the key ‘input_feature_key‘. \r\n    sequence_length: dict mapping of feature key to int length for that feature. \r\n    output_features: mapping of keys to features. use_prefix_lm_task: <bool> If True, include PrefixLM in the task mix. \r\n    rates: <Optional<List<float>> List of rates per task. If None, tasks are sampled uniformly. \r\n    mean_noise_span_lengths: List of mean number of tokens per masked span per example. \r\n    noise_densities: List of what fraction of the tokens to mask. \r\n    shard_ds: <bool> If True, shard dataset per objective. \r\n    optional_task_prefixes: <Optional<list<str>> Strings to prepend for each orruption scheme. \r\n    NOTE: If including prefixLM task, it must be the last prefix. \r\n    input_feature_key: which feature to use from the dataset as the input text tokens. \r\n    merge_examples_to_reduce_padding: if True, combines multiple input examples to reduce padding. reserved_for_packing: if specified, reduces the desired inputs length by the specified amount to enable multiple examples to be packed together downstream. \r\n    seed: tf.int64 for controlling the random choice of spans. Returns: a dataset \r\n    \"\"\"\r\n\r\n    if optional_task_prefixes: # Ensure each task has a prefix. \r\n        num_tasks = len(noise_densities) + int(use_prefix_lm_task) \r\n        valid_number_of_prefixes = num_tasks == len(optional_task_prefixes) \r\n        if not valid_number_of_prefixes: \r\n            raise ValueError(\"Number of task prefixes must match number of tasks.\") \r\n    inputs_length = sequence_length[input_feature_key] \r\n    input_lengths, targets_lengths = [], [] \r\n    sequence_lengths = {x: y for x, y in sequence_length.items()} \r\n    if reserved_for_packing: \r\n        inputs_length -= reserved_for_packing \r\n        for x, y in sequence_length.items(): \r\n            sequence_lengths[x] = y - reserved_for_packing \r\n    hyperparams = list(zip(mean_noise_span_lengths, noise_densities)) \r\n    for mean_noise_span_length, noise_density in hyperparams: \r\n        input_length, targets_length = t5.data.preprocessors.random_spans_helper(\r\n            extra_tokens_per_span_inputs=1, \r\n            extra_tokens_per_span_targets=1, \r\n            inputs_length=inputs_length, \r\n            mean_noise_span_length=mean_noise_span_length, \r\n            noise_density=noise_density) \r\n        input_lengths.append(input_length) \r\n        targets_lengths.append(targets_length)\r\n\r\n        if sequence_length[\"targets\"] < targets_length: \r\n            upper_bound = max(targets_lengths) \r\n            raise ValueError(f\"Targets length {sequence_length['targets']} is too small for the given noise_density and mean_noise_span_length. Please increase the targets length to at least {upper_bound}.\")\r\n            #raise ValueError(\"f’Expected max targets length for span corruption ({upper_bound}) is ’ f’greater than configured targets length ’ f\"({sequence_length[’targets’]})\")\r\n\r\n    ds = dataset \r\n    ds = t5.data.preprocessors.select_random_chunk(\r\n        ds, \r\n        output_features=output_features, \r\n        feature_key=\"targets\", \r\n        max_length=65536) \r\n    if merge_examples_to_reduce_padding: \r\n        ds = t5.data.preprocessors.reduce_concat_tokens(\r\n            ds, \r\n            feature_key=\"targets\", \r\n            batch_size=128) \r\n    num_shards = len(input_lengths) + int(use_prefix_lm_task) \r\n    if shard_ds: \r\n        ds_shards = [ds.shard(num_shards, i) for i in range(num_shards)] \r\n    else: \r\n         ds_shards = [ds for _ in range(num_shards)] \r\n    processed_ds = [] \r\n    hyperparams = zip(input_lengths, hyperparams, range(num_shards)) \r\n    for input_length, (noise_span_length, noise_density), i in hyperparams: \r\n        ds = ds_shards[i] \r\n        ds = t5.data.preprocessors.split_tokens(\r\n            ds,\r\n            feature_key=\"targets\", \r\n            min_tokens_per_segment=None, \r\n            max_tokens_per_segment=input_length) \r\n        ds = t5.data.preprocessors.denoise(\r\n            ds, \r\n            output_features, \r\n            inputs_fn=t5.data.preprocessors.noise_span_to_unique_sentinel, \r\n            targets_fn=t5.data.preprocessors.nonnoise_span_to_unique_sentinel, \r\n            noise_density=noise_density, \r\n            noise_mask_fn=functools.partial(\r\n                t5.data.preprocessors.random_spans_noise_mask, \r\n                mean_noise_span_length=noise_span_length), \r\n                input_feature_key=input_feature_key) \r\n        if optional_task_prefixes: \r\n            ds = prepend_prompt(\r\n                ds, \r\n                output_features, \r\n                prompt_mode=optional_task_prefixes[i], \r\n                mode=optional_task_prefixes[i]) \r\n        processed_ds.append(ds) \r\n    if use_prefix_lm_task: \r\n        ds = ds_shards[-1] \r\n        ds = t5.data.preprocessors.prefix_lm(ds, sequence_lengths, output_features) \r\n        if optional_task_prefixes: \r\n            ds = prepend_prompt(\r\n                ds, \r\n                output_features, \r\n                prompt_mode=optional_task_prefixes[-1], \r\n                mode=optional_task_prefixes[-1]) \r\n        processed_ds.append(ds) \r\n    ds = tf.data.experimental.sample_from_datasets(processed_ds, rates, seed) \r\n    return ds\r\n\r\nsequence_length = {\r\n    \"inputs\": 512,\r\n    \"targets\": 512,\r\n}\r\n\r\noutput_features = {\r\n    \"inputs\":\r\n        seqio.Feature(\r\n            vocabulary=t5.data.get_default_vocabulary(), add_eos=False),\r\n    \"targets\":\r\n        seqio.Feature(\r\n            vocabulary=t5.data.get_default_vocabulary(), add_eos=False)\r\n}\r\n\r\nul2_data = ul2_objective(\r\n    dataset,\r\n    sequence_length, \r\n    output_features, \r\n    use_prefix_lm_task=False,\r\n    rates=None,\r\n    mean_noise_span_lengths=(3.0,),\r\n    noise_densities=(0.15,), \r\n    shard_ds=True, \r\n    optional_task_prefixes=None, \r\n    input_feature_key=\"text\", \r\n    merge_examples_to_reduce_padding=True, \r\n    reserved_for_packing=None, \r\n    seed=7)\r\n```","closed_by":{"login":"conceptofmind","id":25208228,"node_id":"MDQ6VXNlcjI1MjA4MjI4","avatar_url":"https://avatars.githubusercontent.com/u/25208228?v=4","gravatar_id":"","url":"https://api.github.com/users/conceptofmind","html_url":"https://github.com/conceptofmind","followers_url":"https://api.github.com/users/conceptofmind/followers","following_url":"https://api.github.com/users/conceptofmind/following{/other_user}","gists_url":"https://api.github.com/users/conceptofmind/gists{/gist_id}","starred_url":"https://api.github.com/users/conceptofmind/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/conceptofmind/subscriptions","organizations_url":"https://api.github.com/users/conceptofmind/orgs","repos_url":"https://api.github.com/users/conceptofmind/repos","events_url":"https://api.github.com/users/conceptofmind/events{/privacy}","received_events_url":"https://api.github.com/users/conceptofmind/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1405/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1405/timeline","performed_via_github_app":null,"state_reason":"completed"}