{"url":"https://api.github.com/repos/google-research/google-research/issues/1188","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1188/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1188/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1188/events","html_url":"https://github.com/google-research/google-research/issues/1188","id":1289727703,"node_id":"I_kwDOCQmIhc5M367X","number":1188,"title":"It takes too long to compile the train and eval process when reproducing muNet on 8 A100 GPUs","user":{"login":"gouchangjiang","id":7204483,"node_id":"MDQ6VXNlcjcyMDQ0ODM=","avatar_url":"https://avatars.githubusercontent.com/u/7204483?v=4","gravatar_id":"","url":"https://api.github.com/users/gouchangjiang","html_url":"https://github.com/gouchangjiang","followers_url":"https://api.github.com/users/gouchangjiang/followers","following_url":"https://api.github.com/users/gouchangjiang/following{/other_user}","gists_url":"https://api.github.com/users/gouchangjiang/gists{/gist_id}","starred_url":"https://api.github.com/users/gouchangjiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gouchangjiang/subscriptions","organizations_url":"https://api.github.com/users/gouchangjiang/orgs","repos_url":"https://api.github.com/users/gouchangjiang/repos","events_url":"https://api.github.com/users/gouchangjiang/events{/privacy}","received_events_url":"https://api.github.com/users/gouchangjiang/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-06-30T07:39:36Z","updated_at":"2022-06-30T07:39:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there,\r\n\r\nI am reproducing the muNet on 8 A100 GPUs. Compared to running it on Colab TPUv2 8 cores,  it takes too long to compile each child model. XLA also reminds me that it takes too long and suggests that there may be a bug. So, an [issue](https://github.com/google/jax/issues/11271) is also posted on the JAX repository, but so far no one respond. Maybe it's better to post it here. To be clear, I will add more main info here.\r\n\r\n- The script I am using is [munet](https://colab.research.google.com/github/google-research/google-research/blob/master/muNet/muNet.ipynb), the experiment running is the smallest one, 'ViT tiny 3 layers / characters benchmarkâ€™. \r\n- Packages are installed in the way specified by munet script.\r\n- The Cuda version is 11.4, then cudnn version is 8.2, python version is 3.9\r\n- it only happens when running on 8 gpus, running it on one gpu is fine\r\n- I am running it with Slurm, so maybe slurm does not give enough process to Python? The command is srun --partition=xxx --gres=gpu:8 -N1 -n1 --ntasks-per-node=1 --cpus-per-task=32 python munet_30June.py. I give 32 cores to this task. It should be fine.\r\n\r\nAny hint is welcome. Thanks in advance.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1188/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1188/timeline","performed_via_github_app":null,"state_reason":null}