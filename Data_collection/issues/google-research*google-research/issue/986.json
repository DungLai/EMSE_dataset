{"url":"https://api.github.com/repos/google-research/google-research/issues/986","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/986/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/986/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/986/events","html_url":"https://github.com/google-research/google-research/issues/986","id":1128455369,"node_id":"I_kwDOCQmIhc5DQtzJ","number":986,"title":"[Scaling Efficiently: Insights from Pre-training and Finetuning Transformers] Some checkpoints seem to be incomplete or incorrectly uploaded","user":{"login":"patrickvonplaten","id":23423619,"node_id":"MDQ6VXNlcjIzNDIzNjE5","avatar_url":"https://avatars.githubusercontent.com/u/23423619?v=4","gravatar_id":"","url":"https://api.github.com/users/patrickvonplaten","html_url":"https://github.com/patrickvonplaten","followers_url":"https://api.github.com/users/patrickvonplaten/followers","following_url":"https://api.github.com/users/patrickvonplaten/following{/other_user}","gists_url":"https://api.github.com/users/patrickvonplaten/gists{/gist_id}","starred_url":"https://api.github.com/users/patrickvonplaten/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/patrickvonplaten/subscriptions","organizations_url":"https://api.github.com/users/patrickvonplaten/orgs","repos_url":"https://api.github.com/users/patrickvonplaten/repos","events_url":"https://api.github.com/users/patrickvonplaten/events{/privacy}","received_events_url":"https://api.github.com/users/patrickvonplaten/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2022-02-09T12:06:00Z","updated_at":"2022-02-16T12:49:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey @vanzytay,\r\n\r\nThanks a lot for uploading all of your newest T5 checkpoints to GCP. \r\nIt's really amazing that you guys are taking the time to make the weights available to the community. Especially, the smaller versions `{small, mini, tiny}` are extremely useful IMO.\r\n\r\nI've started an automatic conversion of these checkpoints to Transformers (see: https://huggingface.co/NewT5/) to make them even more available. The idea is to convert them to all frameworks we offer: PyTorch, Tensorflow, and JAX and give them a bit more visibility - hope this is ok for you! Before releasing them we would move them to the Google org on the Hub.\r\n\r\nThe conversion is going well so far - however I've noticed the that the following checkpoints seem to be incomplete or missing:\r\n1. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l16_law_03-21-23-05 has no checkpoints\r\n2. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_lg_h8_l32_law_03-21-23-06\r\n3. - https://console.cloud.google.com/storage/browser/scenic-bucket/scaling_explorer/scaling_explorer/bi_v1_3B_l28_law_04-04-11-05 \r\n\r\nFor 1. there are no checkpoints at all.\r\nFor 2. & 3. it seems like only the checkpoints up to step 306000 are available - should I just take those for the conversion or are the 524288 ones available somewhere?\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/986/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":2,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/986/timeline","performed_via_github_app":null,"state_reason":null}