{"url":"https://api.github.com/repos/google-research/google-research/issues/771","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/771/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/771/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/771/events","html_url":"https://github.com/google-research/google-research/issues/771","id":954271442,"node_id":"MDU6SXNzdWU5NTQyNzE0NDI=","number":771,"title":"Negative loss values for adaptive loss","user":{"login":"rahulvs94","id":34693127,"node_id":"MDQ6VXNlcjM0NjkzMTI3","avatar_url":"https://avatars.githubusercontent.com/u/34693127?v=4","gravatar_id":"","url":"https://api.github.com/users/rahulvs94","html_url":"https://github.com/rahulvs94","followers_url":"https://api.github.com/users/rahulvs94/followers","following_url":"https://api.github.com/users/rahulvs94/following{/other_user}","gists_url":"https://api.github.com/users/rahulvs94/gists{/gist_id}","starred_url":"https://api.github.com/users/rahulvs94/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rahulvs94/subscriptions","organizations_url":"https://api.github.com/users/rahulvs94/orgs","repos_url":"https://api.github.com/users/rahulvs94/repos","events_url":"https://api.github.com/users/rahulvs94/events{/privacy}","received_events_url":"https://api.github.com/users/rahulvs94/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-07-27T21:13:35Z","updated_at":"2021-10-07T08:54:51Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello, I have used adaptive loss implementation on a neural network, however after training a model long enough, I am getting negative loss values. Any help/suggestion would be highly appreciated! Please let me know if you need additional info\r\n\r\n*Model definition -* \r\n\r\n####### define model\r\nbest_hyperparameter_space = {\"gru_up\": 64, \r\n                             \"up_dropout\": 0.2,\r\n                             \"learning_rate\": 0.004,\r\n                             \"batch_size\": 1024}\r\n\r\ndef many_to_one_model(params):\r\n  input_1 = tf.keras.Input(shape =(1, 53), name='input_1')\r\n\r\n  input_2 = tf.keras.Input(shape=(1, 19), name='input_2') # this one is \r\n  \r\n  input_3 = tf.keras.Input(shape=(1, 130), \r\n                                   name='input_3')\r\n  input_3_flatten = Flatten()(input_3)\r\n  input_3_flatten = RepeatVector(1)(input_3_flatten)\r\n  \r\n  concat_outputs = Concatenate()([input_1, \r\n                                  input_2, \r\n                                  input_3_flatten])\r\n\r\n  output_1 = GRU(units=int(params['gru_up']), \r\n                              kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                              activation='relu')(concat_outputs)\r\n  output_1 = Dropout(rate=float(params['up_dropout']))(output_1)\r\n  output_1 = Dense(units=1, \r\n                               activation='linear', \r\n                               name='output_1')(output_1)\r\n\r\n  model = tf.keras.models.Model(inputs=[input_1, \r\n                                        input_2, \r\n                                        input_3], \r\n                                outputs=[output_1],\r\n                                name='many_to_one_model')\r\n\r\n  return model\r\n\r\n\r\n*Model summary -*\r\n\r\nModel: \"many_to_one_model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_3 (InputLayer)            [(None, 1, 130)]     0                                            \r\n__________________________________________________________________________________________________\r\nflatten_5 (Flatten)             (None, 130)          0           input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ninput_1 (InputLayer)            [(None, 1, 53)]      0                                            \r\n__________________________________________________________________________________________________\r\ninput_2 (InputLayer)            [(None, 1, 19)]      0                                            \r\n__________________________________________________________________________________________________\r\nrepeat_vector_5 (RepeatVector)  (None, 1, 130)       0           flatten_5[0][0]                  \r\n__________________________________________________________________________________________________\r\nconcatenate_5 (Concatenate)     (None, 1, 202)       0           input_1[0][0]                    \r\n                                                                 input_2[0][0]                    \r\n                                                                 repeat_vector_5[0][0]            \r\n__________________________________________________________________________________________________\r\ngru_5 (GRU)                     (None, 64)           51456       concatenate_5[0][0]              \r\n__________________________________________________________________________________________________\r\ndropout_5 (Dropout)             (None, 64)           0           gru_5[0][0]                      \r\n__________________________________________________________________________________________________\r\noutput_1 (Dense)                (None, 1)            65          dropout_5[0][0]                  \r\n==================================================================================================\r\nTotal params: 51,521\r\nTrainable params: 51,521\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n\r\n\r\n*Adaptive loss implementation -*\r\n\r\n####### Create the initial model object\r\nmodel = many_to_one_model(best_hyperparameter_space)\r\n\r\n####### Define robust loss function\r\nadaptive_lossfun = robust_loss.adaptive.AdaptiveLossFunction(num_channels=1, float_dtype=np.float32)\r\n\r\nvariables = (list(model.trainable_variables) + list(adaptive_lossfun.trainable_variables))\r\n\r\n####### Get dynamic param for learning rate\r\noptimizer_call = getattr(tf.keras.optimizers, \"Adam\")         # Can update to meet needs\r\noptimizer = optimizer_call(learning_rate=best_hyperparameter_space[\"learning_rate\"], amsgrad=True)\r\n\r\nmlflow_callback = LambdaCallback()\r\nfor epoch in range(750):\r\n  def lossfun():\r\n    ####### Stealthily unsqueeze to an (n,1) matrix, and then compute the loss.\r\n    ####### A matrix with this shape corresponds to a loss where there's one shape\r\n    ####### and scale parameter per dimension (and there's only one dimension for\r\n    ####### this data).\r\n    aa = y_train_up - model([train_cat_ip, train_num_ip, ex_train_num_ip])\r\n    mean_calc = tf.reduce_mean(adaptive_lossfun(aa))\r\n    return mean_calc\r\n\r\n  optimizer.minimize(lossfun, variables)\r\n\r\n  loss = lossfun()\r\n  alpha = adaptive_lossfun.alpha()[0, 0]\r\n  scale = adaptive_lossfun.scale()[0, 0]\r\n  print('{:<4}: loss={:+0.5f}  alpha={:0.5f}  scale={:0.5f}'.format(epoch, loss, alpha, scale))\r\n  mlflow_callback.on_batch_end(epoch, mlflow.log_metrics({\"loss\":loss.numpy(), \r\n                                                          \"alpha\":alpha.numpy(), \r\n                                                          \"scale\":scale.numpy()}, epoch))\r\n\r\n\r\n*Loss, alpha and scale vs epochs graph -*\r\n\r\n![image](https://user-images.githubusercontent.com/34693127/127228166-a490381b-1d3c-4603-b0cd-50488976f26c.png)\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/771/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/771/timeline","performed_via_github_app":null,"state_reason":null}