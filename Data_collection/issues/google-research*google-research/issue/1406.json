{"url":"https://api.github.com/repos/google-research/google-research/issues/1406","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1406/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1406/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1406/events","html_url":"https://github.com/google-research/google-research/issues/1406","id":1470502463,"node_id":"I_kwDOCQmIhc5XphY_","number":1406,"title":"Simplified Causal/Unidirectional Fast Attention","user":{"login":"jackd","id":659115,"node_id":"MDQ6VXNlcjY1OTExNQ==","avatar_url":"https://avatars.githubusercontent.com/u/659115?v=4","gravatar_id":"","url":"https://api.github.com/users/jackd","html_url":"https://github.com/jackd","followers_url":"https://api.github.com/users/jackd/followers","following_url":"https://api.github.com/users/jackd/following{/other_user}","gists_url":"https://api.github.com/users/jackd/gists{/gist_id}","starred_url":"https://api.github.com/users/jackd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackd/subscriptions","organizations_url":"https://api.github.com/users/jackd/orgs","repos_url":"https://api.github.com/users/jackd/repos","events_url":"https://api.github.com/users/jackd/events{/privacy}","received_events_url":"https://api.github.com/users/jackd/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-12-01T01:24:48Z","updated_at":"2022-12-01T01:24:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I've been playing around with the [causal attention performer implementation](https://github.com/google-research/google-research/blob/master/performer/fast_attention/tensorflow/fast_attention.py) and believe I've found a much simpler representation that leads to more straight-forward implementation (no explicit loops/scans or custom gradients). Performance is comparable, though compilation/tracing time is orders of magnitude faster. I'd be happy to put a PR in with changes, but I realize these types of improvements aren't necessarily a high priority so thought I'd check here first to see if there's demand for it here first.\r\n\r\nTL;DR: the implementations below, compared to the tensorflow implementation in this repo:\r\n- is way less code (~5 lines of logic for numerator/denominator, compared to ~50)\r\n- jit-compiles faster (thanks to no explicit python loops)\r\n- gives the same results in roughly the same time using basic dimensions\r\n\r\nImplementations/benchmarks/tests are available [here](https://github.com/jackd/simple-fast-attention). Relevant parts included below and a notebook is [here](https://colab.research.google.com/drive/1Fk7LWy87LzZ1usDDVhllIYxUNpoZelTh?usp=sharing).\r\n\r\n## Implementations\r\n\r\n```python\r\ndef causal_numerator(qs: tf.Tensor, ks: tf.Tensor, vs: tf.Tensor):\r\n    \"\"\"Computes not-normalized FAVOR causal attention A_{masked}V.\r\n\r\n    Args:\r\n      qs: query_prime tensor of the shape [L,B,H,M].\r\n      ks: key_prime tensor of the shape [L,B,H,M].\r\n      vs: value tensor of the shape [L,B,H,D].\r\n\r\n    Returns:\r\n      Not-normalized FAVOR causal attention A_{masked}V.\r\n    \"\"\"\r\n    # rhs = tf.einsum('lbhm,lbhd->lbhdm', ks, vs)\r\n    rhs = tf.expand_dims(ks, axis=-2) * tf.expand_dims(vs, axis=-1)  # [L,B,H,D,M]\r\n    rhs = tf.cumsum(rhs, axis=0)\r\n    # return tf.einsum('lbhm,lbhdm->lbhd', qs, rhs)\r\n    return tf.linalg.matvec(rhs, qs)\r\n```\r\n\r\n```python\r\ndef causal_denominator(qs, ks):\r\n    \"\"\"Computes FAVOR normalizer in causal attention.\r\n\r\n    Args:\r\n      qs: query_prime tensor of the shape [L,B,H,M].\r\n      ks: key_prime tensor of the shape [L,B,H,M].\r\n\r\n    Returns:\r\n      FAVOR normalizer in causal attention.\r\n    \"\"\"\r\n    rhs = tf.cumsum(ks, axis=0)\r\n    return tf.einsum(\"lbhm,lbhm->lbh\", qs, rhs)\r\n```\r\n\r\n## Theory\r\n\r\nThe task we consider is to compute the noncausal numerator $N$, where\r\n\r\n$N = \\left[(Q K^T) \\circ L\\right] V$\r\n\r\nwhere $Q$, $K$ and $V$ are the query, key and value matrices used in fast attention, $L$ is a lower triangular matrix with values of $1$ on and below the diagonal and $\\circ$ is the _Hadamard product_ (elementwise product). Noting that $Q$ and $K$ are low-rank (that's the whole point of performers/FAVOR), we can use the following handy dandy property of Hadamard products ([Property 1](http://pi.math.cornell.edu/~ajt/presentations/HadamardProduct.pdf)):\r\n\r\n$\\left[A \\circ \\sum_j \\mathbf{u}_j \\mathbf{Pv}_j^T\\right]x = \\sum_j D(\\mathbf{u}_j) A D(\\mathbf{v}_j) \\mathbf{x}$\r\n\r\nwhere $D(\\mathbf{z})$ is the diagonal matrix with diagonal values $\\mathbf{z}$. This means we can express our fast causal attention output as\r\n\r\n$N = \\sum_m D(\\mathbf{q}_m) L D(\\mathbf{k}_m) V$\r\n\r\nwhere $\\mathbf{q}_m$ and $\\mathbf{k}_m$ are the $m^\\text{th}$ columns of Q and K respectively.\r\n\r\nNote it is neither efficient nor necessary to compute any of the new matrices above. $D(\\mathbf{k}_m) Z$ is just the scaling of rows of $Z$ by $\\mathbf{k}_m$, while $L Z$ is the cumulative sum of $Z$ on the leading dimension. This results in a significantly simpler tensorflow implementation without the need to implement custom gradients or use python loops.\r\n\r\n## Benchmarks\r\n\r\nResults using google-benchmark are given below. `v0` is the original, `v1` is the one discussed above. `warmup_time` is the time for the first run, which I'm using as a proxy for compilation time. Results were generated on a fairly old laptop with an Nvidia 1050Ti. Script to generate available [here](https://github.com/jackd/simple-fast-attention/blob/main/gbenchmark.py).\r\n\r\n```txt\r\n--------------------------------------------------------------\r\nBenchmark                    Time             CPU   Iterations\r\n--------------------------------------------------------------\r\nv0_forward-cpu         5403096 ns       364764 ns         1000\r\nv1_forward-cpu         5419832 ns       365650 ns         1000\r\nv0_backward-cpu         268558 ns       238634 ns         2896\r\nv1_backward-cpu         267089 ns       235842 ns         2937\r\nv0_forward-gpu          288531 ns       241580 ns         2874\r\nv1_forward-gpu          285695 ns       238078 ns         2908\r\nv0_backward-gpu         268220 ns       237309 ns         2869\r\nv1_backward-gpu         268324 ns       240429 ns         2751\r\nv0_forward-cpu-jit      299143 ns       271613 ns         2516\r\nv1_forward-cpu-jit      291873 ns       269618 ns         2538\r\nv0_backward-cpu-jit     303150 ns       275359 ns         2483\r\nv1_backward-cpu-jit     303948 ns       276806 ns         2482\r\nv0_forward-gpu-jit      278147 ns       277842 ns         2450\r\nv1_forward-gpu-jit      276128 ns       275956 ns         2523\r\nv0_backward-gpu-jit     256809 ns       256798 ns         2706\r\nv1_backward-gpu-jit     252543 ns       252537 ns         2769\r\n\r\nWarmup time for v0_forward-cpu: 6.56445574760437\r\nWarmup time for v1_forward-cpu: 0.1015627384185791\r\nWarmup time for v0_backward-cpu: 22.0670325756073\r\nWarmup time for v1_backward-cpu: 0.08140373229980469\r\nWarmup time for v0_forward-gpu: 6.233572244644165\r\nWarmup time for v1_forward-gpu: 0.028412342071533203\r\nWarmup time for v0_backward-gpu: 22.226712226867676\r\nWarmup time for v1_backward-gpu: 0.051419734954833984\r\nWarmup time for v0_forward-cpu-jit: 6.481787443161011\r\nWarmup time for v1_forward-cpu-jit: 0.05790424346923828\r\nWarmup time for v0_backward-cpu-jit: 24.72081184387207\r\nWarmup time for v1_backward-cpu-jit: 0.09151363372802734\r\nWarmup time for v0_forward-gpu-jit: 8.328083515167236\r\nWarmup time for v1_forward-gpu-jit: 0.08592033386230469\r\nWarmup time for v0_backward-gpu-jit: 24.7033634185791\r\nWarmup time for v1_backward-gpu-jit: 0.12377095222473145\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1406/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1406/timeline","performed_via_github_app":null,"state_reason":null}