{"url":"https://api.github.com/repos/google-research/google-research/issues/9","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/9/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/9/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/9/events","html_url":"https://github.com/google-research/google-research/issues/9","id":394970040,"node_id":"MDU6SXNzdWUzOTQ5NzAwNDA=","number":9,"title":"in ./graph_embedding/watch_your_step Reason for scaling up positive loss by the number of nodes, significance of attention and why no validation split?","user":{"login":"karthik63","id":18640459,"node_id":"MDQ6VXNlcjE4NjQwNDU5","avatar_url":"https://avatars.githubusercontent.com/u/18640459?v=4","gravatar_id":"","url":"https://api.github.com/users/karthik63","html_url":"https://github.com/karthik63","followers_url":"https://api.github.com/users/karthik63/followers","following_url":"https://api.github.com/users/karthik63/following{/other_user}","gists_url":"https://api.github.com/users/karthik63/gists{/gist_id}","starred_url":"https://api.github.com/users/karthik63/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/karthik63/subscriptions","organizations_url":"https://api.github.com/users/karthik63/orgs","repos_url":"https://api.github.com/users/karthik63/repos","events_url":"https://api.github.com/users/karthik63/events{/privacy}","received_events_url":"https://api.github.com/users/karthik63/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2018-12-31T11:54:58Z","updated_at":"2019-09-06T19:03:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"[Line no 226, graph_attention_learning.py, Watch Your Step] return tf.transpose(d_sum) * **GetNumNodes()** * 80, feed_dict\r\nWhy is an arbitrary scaling by the number of nodes done? I am not sure if it is reported in the Watch Your Step paper.\r\nWhen I remove the scaling, there is some decrease in the results for most of the datasets\r\nResults and Ablation Study:\r\nPPI(without scaling,learn attention) - 90.84\r\nPPI(with scaling,learn attention) - 91.8\r\nPPI(uniform attention,scaling) - 91.7\r\n\r\nca-HepTh(without scaling,learn attention) - 93.14\r\nca-HepTh(with scaling,learn attention) - 93.8\r\nca-HepTh(uniform attention,scaling) - 93.9\r\n\r\nWiki-Vote(without scaling,learn attention) - 94.3\r\nWiki-Vote(with scaling,learn attention) - 93.7\r\nWiki-Vote(uniform attention,scaling) - 94.0\r\n\r\nConfigs:\r\nEmbedding dimension:128\r\nShare Embeddings: False\r\nTransition_powers: 5\r\nLoss: nlgl\r\ncontext_regularizer: 0.1\r\nlearnable attention - softmax over 5 hops\r\nuniform attention - Equal attention of earch of 5 hops(0.2 for each hop)\r\n\r\nA couple of more questions:\r\n1) Why is a validation set(validation positive edges, negative edges) not chosen for stopping? I know that Learning Edge Representations via Low-Rank Asymmetric Projections, other baselines and many of the graph embedding literature doesn't use it for link prediction, but I feel that stopping based on validation from ROC-AUC scores is more appropriate than stopping by best train ROC-AUC.\r\n\r\n2.a) I see that attention makes only little contribution to an increase in the performance. Uniform attention works well. For example, in PPI, the paper reports that the attention learned favors the first hop. But not learning any attention(or in other words, uniform attention) also performs equally well. This is true even for Soc-Facebook, Wiki-Vote, ca-HepTh, ca-AstroPh. \r\n\r\n2.b)\r\nWhy is the stopping criteria in line 441\r\n\"if i - 100 > eval_metrics['i at best train']:\r\n      LogMsg('Reached peak a while ago. Terminating...')\r\n      break\"\r\n\r\nbased on training error? Shouldn't stopping criteria be always based on validation error?\r\n\r\n2.c) in line 340 \"eval_metrics['test auc at best train'] = float(test_auc)\"\r\n\r\nWhy log and report test AUC at best train? We always report metrics and save model that gives the best performance on the validation set. If we modify the code to report the AUC / precision scores at least validation error then there is little to no difference between having weights and trainable attention.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/9/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/9/timeline","performed_via_github_app":null,"state_reason":null}