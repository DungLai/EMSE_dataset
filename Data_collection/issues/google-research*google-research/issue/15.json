{"url":"https://api.github.com/repos/google-research/google-research/issues/15","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/15/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/15/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/15/events","html_url":"https://github.com/google-research/google-research/issues/15","id":415742490,"node_id":"MDU6SXNzdWU0MTU3NDI0OTA=","number":15,"title":"[state_of_sparsity] - Knowledge transfer and reconstitution","user":{"login":"iandanforth","id":446062,"node_id":"MDQ6VXNlcjQ0NjA2Mg==","avatar_url":"https://avatars.githubusercontent.com/u/446062?v=4","gravatar_id":"","url":"https://api.github.com/users/iandanforth","html_url":"https://github.com/iandanforth","followers_url":"https://api.github.com/users/iandanforth/followers","following_url":"https://api.github.com/users/iandanforth/following{/other_user}","gists_url":"https://api.github.com/users/iandanforth/gists{/gist_id}","starred_url":"https://api.github.com/users/iandanforth/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iandanforth/subscriptions","organizations_url":"https://api.github.com/users/iandanforth/orgs","repos_url":"https://api.github.com/users/iandanforth/repos","events_url":"https://api.github.com/users/iandanforth/events{/privacy}","received_events_url":"https://api.github.com/users/iandanforth/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-02-28T17:38:06Z","updated_at":"2019-03-03T13:46:44Z","closed_at":"2019-03-02T05:24:57Z","author_association":"NONE","active_lock_reason":null,"body":"@sarahooker I really enjoyed the paper. I'd like to engage in a little speculation and I hope you'll indulge me.\r\n\r\n### Knowledge transfer during iterative sparsification\r\n\r\nThe lottery ticket result surprises me. I think you should be able to retrain to much closer to the same accuracy given the same initialization and a sparse mask. However I speculate that the magnitude pruning method induces knowledge transfer which prevents this.\r\n\r\nBecause the sparsity inducing mask changes during the iterative process you're dealing with some number of subnets. If they were fully disjoint you would transfer knowledge using one as a teacher and the other as the student. However in the iterative process you have a gradual knowledge transfer. Which means that the representations (and ultimate accuracy of the sparsified network) are no longer a function of sparse initial weights + training, but the full initial weights and the sparsification procedure. \r\n\r\nIf this is the case I suspect that if you do a single-step sparsification at the end of training and use *that* sparse mask along with the same initial weights (lottery ticket) you should see much closer accuracies. \r\n\r\n(Iterative pruning is still a *better* way to do pruning of course.)\r\n\r\n### Knowledge reconstitution\r\n\r\nI'm curious how much work has been done in the area of densifying sparse nets. For example, can you perfectly reverse the accurracy loss curves by increasing sparsity and retraining? Does it work better if you do this in one step (go from 90% sparsity to 70% sparsity by initializing a lot of random weights) or iteratively (90->85->80->75->70)\r\n\r\nUltimately the question is do you think a sparse bottleneck + densification + retraining procedure can produce a highly efficient and compressed version of finetuning?","closed_by":{"login":"dbieber","id":892765,"node_id":"MDQ6VXNlcjg5Mjc2NQ==","avatar_url":"https://avatars.githubusercontent.com/u/892765?v=4","gravatar_id":"","url":"https://api.github.com/users/dbieber","html_url":"https://github.com/dbieber","followers_url":"https://api.github.com/users/dbieber/followers","following_url":"https://api.github.com/users/dbieber/following{/other_user}","gists_url":"https://api.github.com/users/dbieber/gists{/gist_id}","starred_url":"https://api.github.com/users/dbieber/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dbieber/subscriptions","organizations_url":"https://api.github.com/users/dbieber/orgs","repos_url":"https://api.github.com/users/dbieber/repos","events_url":"https://api.github.com/users/dbieber/events{/privacy}","received_events_url":"https://api.github.com/users/dbieber/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/15/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/15/timeline","performed_via_github_app":null,"state_reason":"completed"}