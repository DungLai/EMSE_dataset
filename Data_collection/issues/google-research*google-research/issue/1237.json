{"url":"https://api.github.com/repos/google-research/google-research/issues/1237","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1237/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1237/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1237/events","html_url":"https://github.com/google-research/google-research/issues/1237","id":1339390291,"node_id":"I_kwDOCQmIhc5P1XlT","number":1237,"title":"[SCANN] support for large (out-of-memory) dataset","user":{"login":"WenqiJiang","id":42897680,"node_id":"MDQ6VXNlcjQyODk3Njgw","avatar_url":"https://avatars.githubusercontent.com/u/42897680?v=4","gravatar_id":"","url":"https://api.github.com/users/WenqiJiang","html_url":"https://github.com/WenqiJiang","followers_url":"https://api.github.com/users/WenqiJiang/followers","following_url":"https://api.github.com/users/WenqiJiang/following{/other_user}","gists_url":"https://api.github.com/users/WenqiJiang/gists{/gist_id}","starred_url":"https://api.github.com/users/WenqiJiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/WenqiJiang/subscriptions","organizations_url":"https://api.github.com/users/WenqiJiang/orgs","repos_url":"https://api.github.com/users/WenqiJiang/repos","events_url":"https://api.github.com/users/WenqiJiang/events{/privacy}","received_events_url":"https://api.github.com/users/WenqiJiang/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-08-15T19:22:36Z","updated_at":"2022-08-15T19:22:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I wonder if SCANN supports large dataset in general. For example, to train the sift1B (1 billion x 128 dim) dataset on a small machine with 64 GB of memory, I need to memmap the dataset and pass it to the scann's builder. However, seems SCANN only supports to load the dataset fully in memory before it can start training. \r\n\r\nIs there any way to train large dataset given constraint memory (like faiss)? Or is SCANN designed for rather small datasets only?  \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/data/scann_test/scann_sift1B.py\", line 68, in <module>\r\n    searcher = scann.scann_ops_pybind.builder(xb, topK, \"squared_l2\").tree(\r\n  File \"/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_builder.py\", line 243, in build\r\n    return self.builder_lambda(self.db, config, self.training_threads, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py\", line 78, in builder_lambda\r\n    return create_searcher(db, config, training_threads, **kwargs)\r\n  File \"/home/ubuntu/anaconda3/envs/scann/lib/python3.9/site-packages/scann/scann_ops/py/scann_ops_pybind.py\", line 86, in create_searcher\r\n    scann_pybind.ScannNumpy(db, scann_config, training_threads))\r\nTypeError: __init__(): incompatible constructor arguments. The following argument types are supported:\r\n    1. scann_pybind.ScannNumpy(arg0: str, arg1: str)\r\n    2. scann_pybind.ScannNumpy(arg0: numpy.ndarray[numpy.float32], arg1: str, arg2: int)\r\n\r\nInvoked with: memmap([[2.9440201e-20, 1.8216880e-44, 2.8302559e+26, ..., 3.4590261e-11,\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1237/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1237/timeline","performed_via_github_app":null,"state_reason":null}