{"url":"https://api.github.com/repos/google-research/google-research/issues/325","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/325/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/325/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/325/events","html_url":"https://github.com/google-research/google-research/issues/325","id":657260107,"node_id":"MDU6SXNzdWU2NTcyNjAxMDc=","number":325,"title":"Question about exporting an integer-only MobileBERT to TF-Lite format.","user":{"login":"nadongguri","id":23691318,"node_id":"MDQ6VXNlcjIzNjkxMzE4","avatar_url":"https://avatars.githubusercontent.com/u/23691318?v=4","gravatar_id":"","url":"https://api.github.com/users/nadongguri","html_url":"https://github.com/nadongguri","followers_url":"https://api.github.com/users/nadongguri/followers","following_url":"https://api.github.com/users/nadongguri/following{/other_user}","gists_url":"https://api.github.com/users/nadongguri/gists{/gist_id}","starred_url":"https://api.github.com/users/nadongguri/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nadongguri/subscriptions","organizations_url":"https://api.github.com/users/nadongguri/orgs","repos_url":"https://api.github.com/users/nadongguri/repos","events_url":"https://api.github.com/users/nadongguri/events{/privacy}","received_events_url":"https://api.github.com/users/nadongguri/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2020-07-15T10:58:32Z","updated_at":"2021-12-04T19:18:09Z","closed_at":"2020-12-28T04:43:51Z","author_association":"NONE","active_lock_reason":null,"body":"\r\nHi, I'm trying to export a mobilebert model to tflite format.\r\n\r\n**Environment**\r\nDocker (tensorflow/tensorflow:1.15.0-gpu-py3) image\r\nV100 16GB\r\n\r\nAs guided in README.md., I followed \"Run Quantization-aware-training with Squad\" then \"Export an integer-only MobileBERT to TF-Lite format.\" However, I got an error while converting to quantized tflite model.\r\n\r\n> 2020-07-15 10:26:10.934857: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize\r\n2020-07-15 10:26:10.934903: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4461 nodes (-1120), 4701 edges (-1124), time = 779.203ms.\r\n2020-07-15 10:26:10.934931: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 4461 nodes (0), 4701 edges (0), time = 374.792ms.\r\nTraceback (most recent call last):\r\n  File \"run_squad.py\", line 1517, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\r\n    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\r\n    _run_main(main, args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"run_squad.py\", line 1508, in main\r\n    tflite_model = converter.convert()\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 993, in convert\r\n    inference_output_type)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/lite.py\", line 239, in _calibrate_quantize_model\r\n    inference_output_type, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/calibrator.py\", line 78, in calibrate_and_quantize\r\n    np.dtype(output_type.as_numpy_dtype()).num, allow_float)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\", line 115, in QuantizeModel\r\n    return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)\r\nRuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0\r\n\r\nI used pre-trained weights (uncased_L-24_H-128_B-512_A-4_F-4_OPT) that mentioned in README.md.\r\nIs it required to distillation process before quantization-aware-training? \r\n\r\n\r\nRegards,\r\nDongjin.\r\n","closed_by":{"login":"nadongguri","id":23691318,"node_id":"MDQ6VXNlcjIzNjkxMzE4","avatar_url":"https://avatars.githubusercontent.com/u/23691318?v=4","gravatar_id":"","url":"https://api.github.com/users/nadongguri","html_url":"https://github.com/nadongguri","followers_url":"https://api.github.com/users/nadongguri/followers","following_url":"https://api.github.com/users/nadongguri/following{/other_user}","gists_url":"https://api.github.com/users/nadongguri/gists{/gist_id}","starred_url":"https://api.github.com/users/nadongguri/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nadongguri/subscriptions","organizations_url":"https://api.github.com/users/nadongguri/orgs","repos_url":"https://api.github.com/users/nadongguri/repos","events_url":"https://api.github.com/users/nadongguri/events{/privacy}","received_events_url":"https://api.github.com/users/nadongguri/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/325/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/325/timeline","performed_via_github_app":null,"state_reason":"completed"}