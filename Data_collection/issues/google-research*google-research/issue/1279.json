{"url":"https://api.github.com/repos/google-research/google-research/issues/1279","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1279/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1279/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1279/events","html_url":"https://github.com/google-research/google-research/issues/1279","id":1373518214,"node_id":"I_kwDOCQmIhc5R3jmG","number":1279,"title":"[Poem] Clarification for Video Alignment","user":{"login":"YQZ530","id":35812742,"node_id":"MDQ6VXNlcjM1ODEyNzQy","avatar_url":"https://avatars.githubusercontent.com/u/35812742?v=4","gravatar_id":"","url":"https://api.github.com/users/YQZ530","html_url":"https://github.com/YQZ530","followers_url":"https://api.github.com/users/YQZ530/followers","following_url":"https://api.github.com/users/YQZ530/following{/other_user}","gists_url":"https://api.github.com/users/YQZ530/gists{/gist_id}","starred_url":"https://api.github.com/users/YQZ530/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/YQZ530/subscriptions","organizations_url":"https://api.github.com/users/YQZ530/orgs","repos_url":"https://api.github.com/users/YQZ530/repos","events_url":"https://api.github.com/users/YQZ530/events{/privacy}","received_events_url":"https://api.github.com/users/YQZ530/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-09-14T20:01:57Z","updated_at":"2022-09-14T20:02:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\nI have question about the description in the sectioni 4.3 of the paper\r\n\"In all the following experiments in this section, we compute our Pr-VIPE embeddings on single video frames and use the negative logarithm of the matching probability (7) as the distance between two frames. Then we apply temporal averaging within an atrous kernel of size 7 and rate 3 around the two center frames and use this averaged distance as the frame matching distance. Given the matching distance, we use standard dynamic time warping (DTW) algorithm to align two action sequences by minimizing the sum of frame matching distances.\"\r\n\r\n\r\nWhy we need matching distance as input for DTW to align two video sequences? Do you mean the distance function used in DTW is different from the default( L2 distance), and we should use matching distance function you describe above?\r\n \r\n\r\n\r\nAnd also in section 4.3.2 \r\n\"We measure the alignment quality of our embeddings quantitatively using Kendallâ€™s Tau [12], which reflects how well an embedding model can be applied to align unseen sequences if we use nearest neighbor in the embedding space to match frames for video pairs.\"\r\n\r\nAs far as I understand, you use KNN to align two video sequences, which is somehow different from 4.3 description.\r\n\r\n\r\nThank you!\r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1279/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1279/timeline","performed_via_github_app":null,"state_reason":null}