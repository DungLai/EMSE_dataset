{"url":"https://api.github.com/repos/google-research/google-research/issues/384","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/384/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/384/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/384/events","html_url":"https://github.com/google-research/google-research/issues/384","id":694468877,"node_id":"MDU6SXNzdWU2OTQ0Njg4Nzc=","number":384,"title":"[mobilebert] Tensor had NaN values Error when using quantization-aware training ","user":{"login":"zaidalyafeai","id":15667714,"node_id":"MDQ6VXNlcjE1NjY3NzE0","avatar_url":"https://avatars.githubusercontent.com/u/15667714?v=4","gravatar_id":"","url":"https://api.github.com/users/zaidalyafeai","html_url":"https://github.com/zaidalyafeai","followers_url":"https://api.github.com/users/zaidalyafeai/followers","following_url":"https://api.github.com/users/zaidalyafeai/following{/other_user}","gists_url":"https://api.github.com/users/zaidalyafeai/gists{/gist_id}","starred_url":"https://api.github.com/users/zaidalyafeai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zaidalyafeai/subscriptions","organizations_url":"https://api.github.com/users/zaidalyafeai/orgs","repos_url":"https://api.github.com/users/zaidalyafeai/repos","events_url":"https://api.github.com/users/zaidalyafeai/events{/privacy}","received_events_url":"https://api.github.com/users/zaidalyafeai/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2020-09-06T17:55:11Z","updated_at":"2020-12-22T10:40:17Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey, I am trying to retrain mobilebert. I was able to run the pretraing scrips with no problems. However, when fine-tuning on squad using the flag `--use_quantized_training=true`. I get the following error \r\n\r\n```\r\nERROR:tensorflow:Error recorded from training_loop: From /job:worker/replica:0/task:0:\r\nGradient for bert/encoder/layer_9/attention/output/dense/bias:0 is NaN : Tensor had NaN values\r\n```\r\nHere is the script I use for fine-tuning \r\n\r\n```\r\npython3 run_squad.py \\\r\n\t--bert_config_file=config/uncased_L-24_H-128_B-512_A-4_F-4_OPT_QAT.json \\\r\n\t--data_dir=${DATA_DIR} \\\r\n\t--do_train \\\r\n\t--doc_stride=128 \\\r\n\t--init_checkpoint={INIT_CKPT}/model.ckpt-1000.index \\\r\n\t--learning_rate=4e-05 \\\r\n\t--predict_file=dev.json \\\r\n\t--do_lower_case \\\r\n  \t--do_predict \\\r\n\t--max_answer_length=30 \\\r\n\t--max_query_length=64 \\\r\n\t--max_seq_length=128\\\r\n\t--n_best_size=20 \\\r\n\t--num_train_epochs=1 \\\r\n\t--output_dir=${OUTPUT_DIR} \\\r\n\t--train_batch_size=32 \\\r\n\t--train_file=train.json \\\r\n\t--use_tpu \\\r\n\t--tpu_name=${TPU_NAME} \\\r\n\t--vocab_file=../bert/en-vocab.txt \\\r\n\t--warmup_proportion=0.1 \\\r\n\t--verbose_logging=True \\\r\n\t--use_quantized_training=true\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/384/timeline","performed_via_github_app":null,"state_reason":null}