{"url":"https://api.github.com/repos/google-research/google-research/issues/1187","repository_url":"https://api.github.com/repos/google-research/google-research","labels_url":"https://api.github.com/repos/google-research/google-research/issues/1187/labels{/name}","comments_url":"https://api.github.com/repos/google-research/google-research/issues/1187/comments","events_url":"https://api.github.com/repos/google-research/google-research/issues/1187/events","html_url":"https://github.com/google-research/google-research/issues/1187","id":1289500966,"node_id":"I_kwDOCQmIhc5M3Dkm","number":1187,"title":"OOM when jax.jit is used to compile and execute mu2Net train_step function","user":{"login":"adamantboy","id":20296728,"node_id":"MDQ6VXNlcjIwMjk2NzI4","avatar_url":"https://avatars.githubusercontent.com/u/20296728?v=4","gravatar_id":"","url":"https://api.github.com/users/adamantboy","html_url":"https://github.com/adamantboy","followers_url":"https://api.github.com/users/adamantboy/followers","following_url":"https://api.github.com/users/adamantboy/following{/other_user}","gists_url":"https://api.github.com/users/adamantboy/gists{/gist_id}","starred_url":"https://api.github.com/users/adamantboy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adamantboy/subscriptions","organizations_url":"https://api.github.com/users/adamantboy/orgs","repos_url":"https://api.github.com/users/adamantboy/repos","events_url":"https://api.github.com/users/adamantboy/events{/privacy}","received_events_url":"https://api.github.com/users/adamantboy/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2022-06-30T02:34:05Z","updated_at":"2022-10-11T07:48:16Z","closed_at":"2022-07-18T09:43:18Z","author_association":"NONE","active_lock_reason":null,"body":"@agesmundo, hope to receive some reply, thanks.\r\n- Duplicate issues: [#788](https://github.com/google/jax/issues/788) and [#4528](https://github.com/google/jax/issues/4528) are not suitable for this case.\r\n-  How to reproduce the bug:\r\n[1] Just run [mu2Net](https://github.com/google-research/google-research/blob/master/muNet/mu2Net.ipynb) on 8gpus A100, use BENCHMARK = 'ViT large / Chars benchmark'\r\n[2] OOM error will occur when train_step function compiled by jax.jit is executed.\r\n[3] The A100 have sufficent 80GiB memory per gpu, i use 8gpus. My cpu has 256g memory and 112+ cores.\r\n[4] I can't understand why the executable needs to preallocate 114.44GiB temp allocation, though the seed ViT model is just 300M.\r\n[5]It's useless to set any env variable about [jax memory allocation](https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html  )\r\n\r\n- Model hyperparameters:\r\n```python\r\ndef get_exp_config_large(benchmark_string_id):\r\n  exp_config = ConfigDict()\r\n  exp_config.experiment_name = EXPERIMENT_NAME\r\n  exp_config.experiments_root_dir = EXPERIMENTS_ROOT_DIR\r\n  # Cap to 1/10th of imagenet train set size to have similar ratio of exps reported in:\r\n  # https://arxiv.org/abs/2106.10270\r\n  exp_config.num_train_examples_between_validations_max = 128_116\r\n  exp_config.num_validations_per_path_training = 4\r\n  exp_config.num_validation_examples_max = 10_000\r\n  # Fit HBM memory: TPUv4 megacore=64, TPUv3=32.\r\n  exp_config.batch_size = 64\r\n  exp_config.num_task_iters = 1\r\n  # Assuming TPUv4 32 cores * 4 generations.\r\n  exp_config.num_samples_per_task = 32 * 4\r\n  exp_config.mutate_adapters = False\r\n  exp_config.force_finetune_components = ['encoder_norm']\r\n  # Population policy params:\r\n  exp_config.policy_class = 'PPDecay'\r\n  exp_config.policy_kwargs = {}\r\n  # Scorer params:\r\n  exp_config.scorer_class = 'ScorerDecay'\r\n  exp_config.scorer_kwargs = dict(\r\n      base=1.0,\r\n      num_params=303_303_682,  # Params in L/16\r\n      )\r\n  # Seed models params:\r\n  exp_config.load_rand_init = False\r\n  exp_config.load_vit_checkpoint = True\r\n  exp_config.load_vit_checkpoint_query = 'name==\"L/16\" and ds==\"i21k\" and aug==\"medium2\" and wd==0.03 and sd==0.1'\r\n  exp_config.load_experiment = False\r\n  exp_config.load_experiment_dir = ''\r\n  set_continue_configs(exp_config)\r\n\r\n  # Hyperparameters:\r\n  max_num_layers = get_max_num_layers(exp_config.load_vit_checkpoint_query)\r\n  exp_config.models_default_hparams = {\r\n      '_mu_': 0.2,\r\n      'num_classes': 1,\r\n      'adapter_layers': '',\r\n      'num_layers': max_num_layers,\r\n      'adapter_dim': 16,\r\n      'opt_lr': 0.01,\r\n      'opt_lr_schedule': 'cosine',\r\n      'opt_lr_warmup_ratio': 0.05,\r\n      'opt_momentum': 0.9,\r\n      'opt_nesterov': False,\r\n      'ds_image_size': 384,\r\n      'ds_crop': True,\r\n      'ds_area_range_min': 0.05,\r\n      'ds_aspect_ratio_range_min': 0.75,\r\n      'ds_flip_left_right': True,\r\n      'ds_brightness_delta': 0.0,\r\n      'ds_contrast_delta': 0.0,\r\n      'ds_saturation_delta': 0.0,\r\n      'ds_hue_delta': 0.0,\r\n  }\r\n```\r\n\r\n- Core code:\r\n```python\r\n@partial(jax.jit, static_argnames=['model', 'optimizer'], donate_argnums=[0, 2])\r\ndef train_step(params, fixed_params, opt_state, images, labels, model, optimizer):\r\n  def loss_fn(params, fixed_params, images, labels):\r\n    logits = model.apply({'params': format_params(params, fixed_params)},\r\n                         images, train=USE_DROPOUT)\r\n    labels = jax.nn.one_hot(labels, logits.shape[-1])\r\n    return -jnp.mean(jnp.sum(labels * nn.log_softmax(logits), axis=-1))\r\n  grads = jax.grad(loss_fn)(params, fixed_params, images, labels)\r\n  updates, opt_state = optimizer.update(grads, opt_state, params=params)\r\n  params = optax.apply_updates(params, updates)\r\n  return params, opt_state\r\n\r\ndef train_loop(paths, ds_train, ds_validation, devices, exp_config):\r\n  global LOOP_START\r\n  timing = {'start_time': time.time(),\r\n            'start_time_loop': LOOP_START}\r\n  task = paths[0].task\r\n  # The following values should be shared by all paths in this generation batch.\r\n  for path in paths:\r\n    assert task == path.task\r\n    assert paths[0].hparams['ds_image_size'] == path.hparams['ds_image_size']\r\n\r\n  gc.collect()\r\n\r\n  # Compile.\r\n  compile_train_batches_arr = jax.device_put_replicated(\r\n      get_sample_batch(\r\n        paths[0].hparams['ds_image_size'],\r\n        task.train_batch_size),\r\n      devices)\r\n  compile_eval_batches_arr = jax.device_put_replicated(\r\n      get_sample_batch(\r\n          paths[0].hparams['ds_image_size'],\r\n          task.validation_batch_size),\r\n      devices)\r\n\r\n  for p_id, path in enumerate(paths):\r\n    if VERBOSE:\r\n      print('Parent')\r\n      print(prp(path.parent))\r\n      print(prp(path))\r\n    path.device_id = p_id % len(devices)\r\n    path.device = devices[path.device_id]\r\n    print(\"path:\", p_id, \"device:\", path.device)\r\n    path.optimizer = path.get_optimizer()\r\n    path.optimizer_init_fn = jax.jit(path.optimizer.init, device=path.device)\r\n    path.best_params_local = None\r\n    path.best_opt_state_local = None\r\n    path.best_quality = None\r\n    path.best_score = path.parent.score() if path.task is path.parent.task else -np.inf\r\n    path.evals = []\r\n\r\n    # Launch parallel compilation of eval and train step functions.\r\n    params_local = path.get_trainable_params()\r\n    check_is_local(params_local)\r\n    path.compile_params_device = jax.device_put(params_local, path.device)\r\n    path.compile_fixed_params_device = jax.device_put(\r\n        path.get_fixed_params(),\r\n        path.device)\r\n    path.compile_train = Thread(\r\n        target=train_step,\r\n        args=(path.compile_params_device,\r\n              path.compile_fixed_params_device,\r\n              path.optimizer_init_fn(params_local),\r\n              compile_train_batches_arr['image'][path.device_id],\r\n              compile_train_batches_arr['label'][path.device_id],\r\n              path.model,\r\n              path.optimizer))\r\n    path.compile_eval = Thread(\r\n        target=eval_step,\r\n        args=(format_params(\r\n                  path.compile_params_device,\r\n                  path.compile_fixed_params_device),\r\n              compile_eval_batches_arr['image'][path.device_id],\r\n              compile_eval_batches_arr['label'][path.device_id],\r\n              path.model))\r\n    path.compile_eval.start()\r\n\r\n  for path in paths:\r\n    path.compile_eval.join()\r\n    del path.compile_eval\r\n    timing['end_compile_eval'] = time.time()\r\n    path.compile_train.start()\r\n  del compile_eval_batches_arr\r\n\r\n  for path in paths:\r\n    path.compile_train.join()\r\n    del path.compile_train\r\n    del path.compile_params_device\r\n    del path.compile_fixed_params_device\r\n    timing['end_compile'] = time.time()\r\n  del compile_train_batches_arr\r\n\r\n  gc.collect()\r\n\r\n  # Parameter transfer.\r\n  for path in paths:\r\n    path.params_device = jax.device_put(\r\n        path.get_trainable_params(),\r\n        path.device)\r\n    path.fixed_params_device = jax.device_put(\r\n        path.get_fixed_params(),\r\n        path.device)\r\n    path.opt_state_device = path.optimizer_init_fn(path.params_device)\r\n    # Set opt state.\r\n    for c in path.components:\r\n      if c.is_trainable():\r\n        assert c.name in path.opt_state_device[1][0].trace.keys()\r\n        if c.opt_state is not None:\r\n          path.opt_state_device = (\r\n              path.opt_state_device[0],\r\n              (optax.TraceState(\r\n                  trace=path.opt_state_device[1][0].trace.copy(\r\n                      {c.name: jax.device_put(c.opt_state,\r\n                                              path.device)})),\r\n               path.opt_state_device[1][1]\r\n               )\r\n          )\r\n    check_is_on_device(path.opt_state_device, path.device)\r\n\r\n  iter_ds_validation = iter(ds_validation)\r\n  # TRAIN\r\n  for t_step, train_batch in zip(\r\n      range(exp_config.num_validations_per_path_training\r\n            * task.num_train_batches_between_validations),\r\n      ds_train,\r\n  ):\r\n    train_batch_arr = jax.device_put_replicated(train_batch, devices)\r\n    for p_id, path in enumerate(paths):\r\n      if t_step == 0:\r\n        timing['end_prep'] = time.time()\r\n        t_step_0_time = time.time()\r\n      train_step_start = time.time()\r\n      path.params_device, path.opt_state_device = train_step(\r\n          path.params_device,\r\n          path.fixed_params_device,\r\n          path.opt_state_device,\r\n          train_batch_arr['image'][path.device_id],\r\n          train_batch_arr['label'][path.device_id],\r\n          path.model,\r\n          path.optimizer)\r\n      if t_step == 0 and time.time() - t_step_0_time > 1:\r\n        print(f'WARNING: First train step took: {time.time()-t_step_0_time:.2f} s')\r\n    del train_batch, train_batch_arr\r\n\r\n    # EVAL\r\n    # ...\r\n```\r\n\r\n- Full error messages/tracebacks:\r\n```bash\r\nException in thread Thread-14:\r\nTraceback (most recent call last):\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py\", line 890, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n    self.run()\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/threading.py\", line 870, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/traceback_util.py\", line 162, in reraise_with_filtered_traceback\r\n    return fun(*args, **kwargs)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/api.py\", line 476, in cache_miss\r\n    donated_invars=donated_invars, inline=inline, keep_unused=keep_unused)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py\", line 1765, in bind\r\n    return call_bind(self, fun, *args, **params)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py\", line 1781, in call_bind\r\n    outs = top_trace.process_call(primitive, fun_, tracers, params)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/core.py\", line 678, in process_call\r\n    return primitive.impl(f, *tracers, **params)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/dispatch.py\", line 185, in _xla_call_impl\r\n    return compiled_fun(*args)\r\n  File \"/mnt/lustre/liujun1/.conda/envs/muNet/lib/python3.7/site-packages/jax/_src/dispatch.py\", line 615, in _execute_compiled\r\n    out_bufs_flat = compiled.execute(input_bufs_flat)\r\njax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 122875791936 bytes.\r\nBufferAssignment OOM Debugging.\r\nBufferAssignment stats:\r\n             parameter allocation:    1.43GiB\r\n              constant allocation:         8B\r\n        maybe_live_out allocation:  581.19MiB\r\n     preallocated temp allocation:  114.44GiB\r\n  preallocated temp fragmentation:  146.50MiB (0.13%)\r\n                 total allocation:  115.86GiB\r\n              total fragmentation:  146.53MiB (0.12%)\r\nPeak buffers:\r\n        Buffer 1:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n        Buffer 2:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n        Buffer 3:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n        Buffer 4:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n        Buffer 5:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n        Buffer 6:\r\n                Size: 1.27GiB\r\n                XLA Label: custom-call\r\n                Shape: f32[64,16,577,577]\r\n                ==========================\r\n\r\n```\r\n\r\n- environment:\r\npython: 3.7.13\r\njax: 0.3.14/0.3.13\r\njaxlib: 0.3.14+cuda11.cudnn82/0.3.10+cuda11.cudnn82\r\n\r\n- python packages:\r\nabsl-py                      1.1.0\r\naqtp                         0.0.7\r\nastunparse                   1.6.3\r\ncachetools                   5.2.0\r\ncertifi                      2022.6.15\r\ncharset-normalizer           2.0.12\r\nchex                         0.1.3\r\ncloudpickle                  2.1.0\r\nclu                          0.0.3\r\ncolorama                     0.4.5\r\ncommonmark                   0.9.1\r\ncontextlib2                  21.6.0\r\ncycler                       0.11.0\r\ndacite                       1.6.0\r\ndecorator                    5.1.1\r\ndill                         0.3.5.1\r\ndm-tree                      0.1.7\r\neinops                       0.3.0\r\netils                        0.6.0\r\nflatbuffers                  1.12\r\nflax                         0.5.2\r\nflaxformer                   0.4.2\r\nfonttools                    4.33.3\r\ngast                         0.4.0\r\ngoogle-auth                  2.8.0\r\ngoogle-auth-oauthlib         0.4.6\r\ngoogle-pasta                 0.2.0\r\ngoogleapis-common-protos     1.56.3\r\ngrpcio                       1.47.0\r\nh5py                         3.7.0\r\nidna                         3.3\r\nimportlib-metadata           4.12.0\r\nimportlib-resources          5.8.0\r\njax                          0.3.14\r\njaxlib                       0.3.14+cuda11.cudnn82\r\nkeras                        2.9.0\r\nKeras-Preprocessing          1.1.2\r\nkiwisolver                   1.4.3\r\nlibclang                     14.0.1\r\nMarkdown                     3.3.7\r\nmatplotlib                   3.5.2\r\nml-collections               0.1.1\r\nmsgpack                      1.0.4\r\nnumpy                        1.21.6\r\noauthlib                     3.2.0\r\nopt-einsum                   3.3.0\r\noptax                        0.1.2\r\npackaging                    21.3\r\npandas                       1.3.5\r\nPillow                       9.1.1\r\npip                          21.2.2\r\npromise                      2.3\r\nprotobuf                     3.19.4\r\npyasn1                       0.4.8\r\npyasn1-modules               0.2.8\r\nPygments                     2.12.0\r\npyparsing                    3.0.9\r\npython-dateutil              2.8.2\r\npytz                         2022.1\r\nPyYAML                       6.0\r\nrequests                     2.28.0\r\nrequests-oauthlib            1.3.1\r\nrich                         11.2.0\r\nrsa                          4.8\r\nscipy                        1.7.3\r\nsetuptools                   61.2.0\r\nsix                          1.16.0\r\ntensorboard                  2.9.1\r\ntensorboard-data-server      0.6.1\r\ntensorboard-plugin-wit       1.8.1\r\ntensorflow-addons            0.17.1\r\ntensorflow-cpu               2.9.1\r\ntensorflow-datasets          4.6.0\r\ntensorflow-estimator         2.9.0\r\ntensorflow-hub               0.12.0\r\ntensorflow-io-gcs-filesystem 0.26.0\r\ntensorflow-metadata          1.9.0\r\ntensorflow-probability       0.17.0\r\ntensorflow-text              2.9.0\r\ntermcolor                    1.1.0\r\ntoml                         0.10.2\r\ntoolz                        0.11.2\r\ntqdm                         4.64.0\r\ntypeguard                    2.13.3\r\ntyping_extensions            4.2.0\r\nurllib3                      1.26.9\r\nWerkzeug                     2.1.2\r\nwheel                        0.37.1\r\nwrapt                        1.14.1\r\nzipp                         3.8.0","closed_by":{"login":"adamantboy","id":20296728,"node_id":"MDQ6VXNlcjIwMjk2NzI4","avatar_url":"https://avatars.githubusercontent.com/u/20296728?v=4","gravatar_id":"","url":"https://api.github.com/users/adamantboy","html_url":"https://github.com/adamantboy","followers_url":"https://api.github.com/users/adamantboy/followers","following_url":"https://api.github.com/users/adamantboy/following{/other_user}","gists_url":"https://api.github.com/users/adamantboy/gists{/gist_id}","starred_url":"https://api.github.com/users/adamantboy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adamantboy/subscriptions","organizations_url":"https://api.github.com/users/adamantboy/orgs","repos_url":"https://api.github.com/users/adamantboy/repos","events_url":"https://api.github.com/users/adamantboy/events{/privacy}","received_events_url":"https://api.github.com/users/adamantboy/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/1187/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/google-research/google-research/issues/1187/timeline","performed_via_github_app":null,"state_reason":"completed"}