[{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/752028943","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-752028943","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":752028943,"node_id":"MDEyOklzc3VlQ29tbWVudDc1MjAyODk0Mw==","user":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"created_at":"2020-12-29T10:35:18Z","updated_at":"2020-12-29T10:35:18Z","author_association":"NONE","body":"any help, thanks.","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/752028943/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/752321029","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-752321029","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":752321029,"node_id":"MDEyOklzc3VlQ29tbWVudDc1MjMyMTAyOQ==","user":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"created_at":"2020-12-30T04:09:44Z","updated_at":"2020-12-30T04:09:44Z","author_association":"NONE","body":"Currently the problem is:\r\n\r\nWhile inferring, the first token can be generated correctly, but it starts generating repeatedly from the second step. So the output looks like \"PAD A A A A A A A...\". (A is expected; PAD is the start token of sentence of decoder.)\r\n\r\nAfter the first token has been generated, I remove the PAD-token, then the second token can be correct, but afterwards it repeats again. The output looks like \"A B C B C B C B C...\" or \"A B B B B B B B B...\" (A B C are all expected)","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/752321029/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/766352496","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-766352496","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":766352496,"node_id":"MDEyOklzc3VlQ29tbWVudDc2NjM1MjQ5Ng==","user":{"login":"johnght","id":10274958,"node_id":"MDQ6VXNlcjEwMjc0OTU4","avatar_url":"https://avatars.githubusercontent.com/u/10274958?v=4","gravatar_id":"","url":"https://api.github.com/users/johnght","html_url":"https://github.com/johnght","followers_url":"https://api.github.com/users/johnght/followers","following_url":"https://api.github.com/users/johnght/following{/other_user}","gists_url":"https://api.github.com/users/johnght/gists{/gist_id}","starred_url":"https://api.github.com/users/johnght/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnght/subscriptions","organizations_url":"https://api.github.com/users/johnght/orgs","repos_url":"https://api.github.com/users/johnght/repos","events_url":"https://api.github.com/users/johnght/events{/privacy}","received_events_url":"https://api.github.com/users/johnght/received_events","type":"User","site_admin":false},"created_at":"2021-01-24T13:59:07Z","updated_at":"2021-01-24T14:56:27Z","author_association":"NONE","body":"Just another fast attention user. Haven't studied your model.\r\nFrom my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.\r\nIn my model, the validation loss is higher than the training loss. Hence, overfitting.\r\nIs this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.\r\nps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/766352496/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"johnght","id":10274958,"node_id":"MDQ6VXNlcjEwMjc0OTU4","avatar_url":"https://avatars.githubusercontent.com/u/10274958?v=4","gravatar_id":"","url":"https://api.github.com/users/johnght","html_url":"https://github.com/johnght","followers_url":"https://api.github.com/users/johnght/followers","following_url":"https://api.github.com/users/johnght/following{/other_user}","gists_url":"https://api.github.com/users/johnght/gists{/gist_id}","starred_url":"https://api.github.com/users/johnght/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnght/subscriptions","organizations_url":"https://api.github.com/users/johnght/orgs","repos_url":"https://api.github.com/users/johnght/repos","events_url":"https://api.github.com/users/johnght/events{/privacy}","received_events_url":"https://api.github.com/users/johnght/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767360685","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-767360685","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":767360685,"node_id":"MDEyOklzc3VlQ29tbWVudDc2NzM2MDY4NQ==","user":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"created_at":"2021-01-26T07:35:18Z","updated_at":"2021-01-26T07:35:18Z","author_association":"NONE","body":"> Just another fast attention user. Haven't studied your model.\r\n> From my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.\r\n> In my model, the validation loss is higher than the training loss. Hence, overfitting.\r\n> Is this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.\r\n> ps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?\r\n\r\nThanks, I just tried it and indeed didn't delve into the implementation of fast-attention, sorry. \r\n\r\nOut of confidence in the author, I didn't think it was an simple overfitting problem due to the invalid dropout, but a complex one.\r\n\r\nI will read the code in detail. \r\n\r\nThanks again.","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767360685/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false}},{"id":4249294927,"node_id":"MDExOkNsb3NlZEV2ZW50NDI0OTI5NDkyNw==","url":"https://api.github.com/repos/google-research/google-research/issues/events/4249294927","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2021-01-26T07:35:18Z","state_reason":null,"performed_via_github_app":null},{"id":4249379751,"node_id":"MDEzOlJlb3BlbmVkRXZlbnQ0MjQ5Mzc5NzUx","url":"https://api.github.com/repos/google-research/google-research/issues/events/4249379751","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"event":"reopened","commit_id":null,"commit_url":null,"created_at":"2021-01-26T08:03:19Z","state_reason":null,"performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767376456","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-767376456","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":767376456,"node_id":"MDEyOklzc3VlQ29tbWVudDc2NzM3NjQ1Ng==","user":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"created_at":"2021-01-26T08:11:31Z","updated_at":"2021-01-26T09:18:07Z","author_association":"NONE","body":"> Just another fast attention user. Haven't studied your model.\r\n> From my observation, the attention_dropout and the training are not yet supported in the build and call function correspondingly.\r\n> In my model, the validation loss is higher than the training loss. Hence, overfitting.\r\n> Is this the reason behind your 'train well predict bad' scenario? If yes, place a separate Dropout layer after this fast Attention layer.\r\n> ps: tensorflow supports embedded dropout badly. e.g. LSTM(dropout=...) memory leakage since 2.0 not fixed until recently >2.3?\r\n\r\nI check the code and it's true for \"the attention_dropout and the training are not yet supported in the build\".\r\n\r\nFor \"If yes, place a separate Dropout layer after this fast Attention layer.\", each layer has been wrapped by the PrePostProcessingWrapper layer, which include the layer norm and dropout:\r\n```\r\n# code in DecodeStack/build()\r\n...\r\n        for _ in range(params[\"num_hidden_layers\"]):\r\n            if not params[\"use_fast_attn\"]:\r\n                self_attention_layer = attention_layer.SelfAttention(\r\n                    params[\"hidden_size\"], params[\"num_heads\"],\r\n                    params[\"attention_dropout\"])\r\n                enc_dec_attention_layer = attention_layer.Attention(\r\n                    params[\"hidden_size\"], params[\"num_heads\"],\r\n                    params[\"attention_dropout\"])\r\n            else:\r\n                self_attention_layer = fast_attention.SelfAttention(\r\n                    params[\"hidden_size\"], params[\"num_heads\"],\r\n                    params[\"attention_dropout\"],\r\n                    kernel_transformation=fast_attention.softmax_kernel_transformation,\r\n                    causal=True,\r\n                    projection_matrix_type=1,\r\n                    nb_random_features=nb_random_features,\r\n                )\r\n                enc_dec_attention_layer = fast_attention.Attention(\r\n                    params[\"hidden_size\"], params[\"num_heads\"],\r\n                    params[\"attention_dropout\"],\r\n                    kernel_transformation=fast_attention.softmax_kernel_transformation,\r\n                    causal=False,\r\n                    projection_matrix_type=1,\r\n                    nb_random_features=nb_random_features,\r\n                )\r\n            feed_forward_network = ffn_layer.FeedForwardNetwork(\r\n                params[\"hidden_size\"], params[\"filter_size\"], params[\"relu_dropout\"])\r\n            # ==================wrap each layer===================\r\n            self.layers.append([\r\n                PrePostProcessingWrapper(self_attention_layer, params),  # x->LNorm(x)->{layer(x)=y}->Dropout(y)->y+x\r\n                PrePostProcessingWrapper(enc_dec_attention_layer, params),\r\n                PrePostProcessingWrapper(feed_forward_network, params)\r\n            ])\r\n...\r\n# And the PrePostProcessingWrapper\r\nclass PrePostProcessingWrapper(tf.keras.layers.Layer):\r\n    \"\"\"Wrapper class that applies layer pre-processing and post-processing.\"\"\"\r\n    ...\r\n    def call(self, x, *args, **kwargs):\r\n        \"\"\"Calls wrapped layer with same parameters.\"\"\"\r\n        # Preprocessing: apply layer normalization\r\n        training = kwargs[\"training\"]\r\n\r\n        # axis=-1\r\n        y = self.layer_norm(x)  #  ============using pre layer norm==============\r\n\r\n        # Get layer output\r\n        y = self.layer(y, *args, **kwargs)\r\n\r\n        # Postprocessing: apply dropout and residual connection\r\n        if training:\r\n            y = tf.nn.dropout(y, rate=self.postprocess_dropout)  # ========using dropout when training=========\r\n        return x + y  #\r\n```\r\n\r\nI will try to add dropout in the favor_attention.","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767376456/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767974729","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-767974729","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":767974729,"node_id":"MDEyOklzc3VlQ29tbWVudDc2Nzk3NDcyOQ==","user":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false},"created_at":"2021-01-27T02:42:19Z","updated_at":"2021-01-31T02:54:30Z","author_association":"NONE","body":"I train the transformer model with the built-in attention. The predictions are good with results like \"A B C D E F G...\".\r\n\r\nThen I almost directly replace the built-in attention with the fast one(TF performer). Without any re-training, the predictions become bad with results containing repetive words like \"A B B B...\"\r\n\r\nMaybe the TF-version performer has a bug. I'll keep going through the code.","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/767974729/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"2020zyc","id":7539692,"node_id":"MDQ6VXNlcjc1Mzk2OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7539692?v=4","gravatar_id":"","url":"https://api.github.com/users/2020zyc","html_url":"https://github.com/2020zyc","followers_url":"https://api.github.com/users/2020zyc/followers","following_url":"https://api.github.com/users/2020zyc/following{/other_user}","gists_url":"https://api.github.com/users/2020zyc/gists{/gist_id}","starred_url":"https://api.github.com/users/2020zyc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2020zyc/subscriptions","organizations_url":"https://api.github.com/users/2020zyc/orgs","repos_url":"https://api.github.com/users/2020zyc/repos","events_url":"https://api.github.com/users/2020zyc/events{/privacy}","received_events_url":"https://api.github.com/users/2020zyc/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/773105020","html_url":"https://github.com/google-research/google-research/issues/508#issuecomment-773105020","issue_url":"https://api.github.com/repos/google-research/google-research/issues/508","id":773105020,"node_id":"MDEyOklzc3VlQ29tbWVudDc3MzEwNTAyMA==","user":{"login":"johnght","id":10274958,"node_id":"MDQ6VXNlcjEwMjc0OTU4","avatar_url":"https://avatars.githubusercontent.com/u/10274958?v=4","gravatar_id":"","url":"https://api.github.com/users/johnght","html_url":"https://github.com/johnght","followers_url":"https://api.github.com/users/johnght/followers","following_url":"https://api.github.com/users/johnght/following{/other_user}","gists_url":"https://api.github.com/users/johnght/gists{/gist_id}","starred_url":"https://api.github.com/users/johnght/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnght/subscriptions","organizations_url":"https://api.github.com/users/johnght/orgs","repos_url":"https://api.github.com/users/johnght/repos","events_url":"https://api.github.com/users/johnght/events{/privacy}","received_events_url":"https://api.github.com/users/johnght/received_events","type":"User","site_admin":false},"created_at":"2021-02-04T07:47:27Z","updated_at":"2021-02-04T07:47:27Z","author_association":"NONE","body":"Your inference output seems stuttering in the early stage. Then it must be related to the inference cycle. Say the proper use of the 'cache' parameter in the function call() because, unlike ordinary RNN with self-attention, fast attention relies on manual relaying of internal states. If it's still not working, then there maybe programming bugs in the inference logic.","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/773105020/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"johnght","id":10274958,"node_id":"MDQ6VXNlcjEwMjc0OTU4","avatar_url":"https://avatars.githubusercontent.com/u/10274958?v=4","gravatar_id":"","url":"https://api.github.com/users/johnght","html_url":"https://github.com/johnght","followers_url":"https://api.github.com/users/johnght/followers","following_url":"https://api.github.com/users/johnght/following{/other_user}","gists_url":"https://api.github.com/users/johnght/gists{/gist_id}","starred_url":"https://api.github.com/users/johnght/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnght/subscriptions","organizations_url":"https://api.github.com/users/johnght/orgs","repos_url":"https://api.github.com/users/johnght/repos","events_url":"https://api.github.com/users/johnght/events{/privacy}","received_events_url":"https://api.github.com/users/johnght/received_events","type":"User","site_admin":false}}]