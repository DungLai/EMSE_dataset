[{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/612576537","html_url":"https://github.com/google-research/google-research/issues/243#issuecomment-612576537","issue_url":"https://api.github.com/repos/google-research/google-research/issues/243","id":612576537,"node_id":"MDEyOklzc3VlQ29tbWVudDYxMjU3NjUzNw==","user":{"login":"ptuls","id":1995167,"node_id":"MDQ6VXNlcjE5OTUxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/1995167?v=4","gravatar_id":"","url":"https://api.github.com/users/ptuls","html_url":"https://github.com/ptuls","followers_url":"https://api.github.com/users/ptuls/followers","following_url":"https://api.github.com/users/ptuls/following{/other_user}","gists_url":"https://api.github.com/users/ptuls/gists{/gist_id}","starred_url":"https://api.github.com/users/ptuls/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ptuls/subscriptions","organizations_url":"https://api.github.com/users/ptuls/orgs","repos_url":"https://api.github.com/users/ptuls/repos","events_url":"https://api.github.com/users/ptuls/events{/privacy}","received_events_url":"https://api.github.com/users/ptuls/received_events","type":"User","site_admin":false},"created_at":"2020-04-12T07:35:11Z","updated_at":"2020-04-12T07:35:11Z","author_association":"NONE","body":"Closing, made pull request #244 ","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/612576537/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"ptuls","id":1995167,"node_id":"MDQ6VXNlcjE5OTUxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/1995167?v=4","gravatar_id":"","url":"https://api.github.com/users/ptuls","html_url":"https://github.com/ptuls","followers_url":"https://api.github.com/users/ptuls/followers","following_url":"https://api.github.com/users/ptuls/following{/other_user}","gists_url":"https://api.github.com/users/ptuls/gists{/gist_id}","starred_url":"https://api.github.com/users/ptuls/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ptuls/subscriptions","organizations_url":"https://api.github.com/users/ptuls/orgs","repos_url":"https://api.github.com/users/ptuls/repos","events_url":"https://api.github.com/users/ptuls/events{/privacy}","received_events_url":"https://api.github.com/users/ptuls/received_events","type":"User","site_admin":false}},{"id":3224848506,"node_id":"MDExOkNsb3NlZEV2ZW50MzIyNDg0ODUwNg==","url":"https://api.github.com/repos/google-research/google-research/issues/events/3224848506","actor":{"login":"ptuls","id":1995167,"node_id":"MDQ6VXNlcjE5OTUxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/1995167?v=4","gravatar_id":"","url":"https://api.github.com/users/ptuls","html_url":"https://github.com/ptuls","followers_url":"https://api.github.com/users/ptuls/followers","following_url":"https://api.github.com/users/ptuls/following{/other_user}","gists_url":"https://api.github.com/users/ptuls/gists{/gist_id}","starred_url":"https://api.github.com/users/ptuls/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ptuls/subscriptions","organizations_url":"https://api.github.com/users/ptuls/orgs","repos_url":"https://api.github.com/users/ptuls/repos","events_url":"https://api.github.com/users/ptuls/events{/privacy}","received_events_url":"https://api.github.com/users/ptuls/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2020-04-12T07:35:15Z","state_reason":null,"performed_via_github_app":null},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/950155383","html_url":"https://github.com/google-research/google-research/issues/243#issuecomment-950155383","issue_url":"https://api.github.com/repos/google-research/google-research/issues/243","id":950155383,"node_id":"IC_kwDOCQmIhc44ojh3","user":{"login":"julanchen","id":92902081,"node_id":"U_kgDOBYmSwQ","avatar_url":"https://avatars.githubusercontent.com/u/92902081?v=4","gravatar_id":"","url":"https://api.github.com/users/julanchen","html_url":"https://github.com/julanchen","followers_url":"https://api.github.com/users/julanchen/followers","following_url":"https://api.github.com/users/julanchen/following{/other_user}","gists_url":"https://api.github.com/users/julanchen/gists{/gist_id}","starred_url":"https://api.github.com/users/julanchen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julanchen/subscriptions","organizations_url":"https://api.github.com/users/julanchen/orgs","repos_url":"https://api.github.com/users/julanchen/repos","events_url":"https://api.github.com/users/julanchen/events{/privacy}","received_events_url":"https://api.github.com/users/julanchen/received_events","type":"User","site_admin":false},"created_at":"2021-10-23T13:47:26Z","updated_at":"2021-10-23T13:47:26Z","author_association":"NONE","body":"> First of, I'd like to thank the authors for their interesting work! I'm not sure how to make a pull request to this repository, so I'll leave my thoughts here regarding TabNet training.\r\n> \r\n> **Problem**: TabNet training is slow because the parsing of the CSV is performed on the entire file each time, instead of performed over a single batch each time (see [line](https://github.com/google-research/google-research/blob/432ad195830f3722e2d2bc6dfe5f39991c384806/tabnet/data_helper_covertype.py#L126)). This results in overhead in CPU processing, reducing the utilization of the GPU.\r\n> \r\n> **Solution**: In order to improve the training speed, we set the batching operation before the mapping operation so that the mapping operation only operates on a smaller set of data each iteration as follows:\r\n> \r\n> ```\r\n>     dataset = dataset.batch(batch_size, drop_remainder=True)\r\n>     dataset = dataset.map(parse_csv, num_parallel_calls=n_parallel)\r\n> \r\n>     # Repeat after shuffling, to prevent separate epochs from blending together.\r\n>     dataset = dataset.repeat(num_epochs)\r\n>     return dataset\r\n> ```\r\n> \r\n> In terms of GPU utilization on an RTX2080Ti, I am getting about 70% utilization compared to 28% previously. More could potentially be done but this is a start.\r\n> \r\n> **Caveat**: The unfortunate side-effect of this is that the last small batch would be dropped. Fortunately this has not impacted accuracy on a test set.\r\n> \r\n> Padding was tried, but I think I am not getting the shapes right, so I've used this as a solution in the interim.\r\n> \r\n> **Testing**: Shown here are the test accuracy and test loss when tested on the CoverType dataset. When tested on the CoverType dataset, the accuracy reached on the test set is 96.23% for the final model.\r\n> \r\n> ![image](https://user-images.githubusercontent.com/1995167/79059722-bcb89580-7cc0-11ea-9bb6-3087f2a58fdf.png)\r\n> \r\n> ![image](https://user-images.githubusercontent.com/1995167/79059768-2f297580-7cc1-11ea-9027-2acf825d0eb3.png)\r\n\r\nHI! I met the same problem. The TabNet Training is slow coping with [38*130000] data. Did you process the raw data or do sth. special to increase the speed? How you get it? Highly appreciated for any information and suggestions\r\n","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/950155383/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"julanchen","id":92902081,"node_id":"U_kgDOBYmSwQ","avatar_url":"https://avatars.githubusercontent.com/u/92902081?v=4","gravatar_id":"","url":"https://api.github.com/users/julanchen","html_url":"https://github.com/julanchen","followers_url":"https://api.github.com/users/julanchen/followers","following_url":"https://api.github.com/users/julanchen/following{/other_user}","gists_url":"https://api.github.com/users/julanchen/gists{/gist_id}","starred_url":"https://api.github.com/users/julanchen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/julanchen/subscriptions","organizations_url":"https://api.github.com/users/julanchen/orgs","repos_url":"https://api.github.com/users/julanchen/repos","events_url":"https://api.github.com/users/julanchen/events{/privacy}","received_events_url":"https://api.github.com/users/julanchen/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/955628191","html_url":"https://github.com/google-research/google-research/issues/243#issuecomment-955628191","issue_url":"https://api.github.com/repos/google-research/google-research/issues/243","id":955628191,"node_id":"IC_kwDOCQmIhc449bqf","user":{"login":"yhkll","id":41032255,"node_id":"MDQ6VXNlcjQxMDMyMjU1","avatar_url":"https://avatars.githubusercontent.com/u/41032255?v=4","gravatar_id":"","url":"https://api.github.com/users/yhkll","html_url":"https://github.com/yhkll","followers_url":"https://api.github.com/users/yhkll/followers","following_url":"https://api.github.com/users/yhkll/following{/other_user}","gists_url":"https://api.github.com/users/yhkll/gists{/gist_id}","starred_url":"https://api.github.com/users/yhkll/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yhkll/subscriptions","organizations_url":"https://api.github.com/users/yhkll/orgs","repos_url":"https://api.github.com/users/yhkll/repos","events_url":"https://api.github.com/users/yhkll/events{/privacy}","received_events_url":"https://api.github.com/users/yhkll/received_events","type":"User","site_admin":false},"created_at":"2021-10-31T03:02:04Z","updated_at":"2021-10-31T03:02:04Z","author_association":"NONE","body":"> > First of, I'd like to thank the authors for their interesting work! I'm not sure how to make a pull request to this repository, so I'll leave my thoughts here regarding TabNet training.\r\n> > **Problem**: TabNet training is slow because the parsing of the CSV is performed on the entire file each time, instead of performed over a single batch each time (see [line](https://github.com/google-research/google-research/blob/432ad195830f3722e2d2bc6dfe5f39991c384806/tabnet/data_helper_covertype.py#L126)). This results in overhead in CPU processing, reducing the utilization of the GPU.\r\n> > **Solution**: In order to improve the training speed, we set the batching operation before the mapping operation so that the mapping operation only operates on a smaller set of data each iteration as follows:\r\n> > ```\r\n> >     dataset = dataset.batch(batch_size, drop_remainder=True)\r\n> >     dataset = dataset.map(parse_csv, num_parallel_calls=n_parallel)\r\n> > \r\n> >     # Repeat after shuffling, to prevent separate epochs from blending together.\r\n> >     dataset = dataset.repeat(num_epochs)\r\n> >     return dataset\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > In terms of GPU utilization on an RTX2080Ti, I am getting about 70% utilization compared to 28% previously. More could potentially be done but this is a start.\r\n> > **Caveat**: The unfortunate side-effect of this is that the last small batch would be dropped. Fortunately this has not impacted accuracy on a test set.\r\n> > Padding was tried, but I think I am not getting the shapes right, so I've used this as a solution in the interim.\r\n> > **Testing**: Shown here are the test accuracy and test loss when tested on the CoverType dataset. When tested on the CoverType dataset, the accuracy reached on the test set is 96.23% for the final model.\r\n> > ![image](https://user-images.githubusercontent.com/1995167/79059722-bcb89580-7cc0-11ea-9bb6-3087f2a58fdf.png)\r\n> > ![image](https://user-images.githubusercontent.com/1995167/79059768-2f297580-7cc1-11ea-9027-2acf825d0eb3.png)\r\n> \r\n> HI! I met the same problem. The TabNet Training is slow coping with [38*130000] data. Did you process the raw data or do sth. special to increase the speed? How you get it? Highly appreciated for any information and suggestions\r\n\r\nHello,as the README.md said,I use `python -m test_experiment_covertype.py` for simple test,but its result is obviously wrong,The traing loss and val accuracy are irregular,just because it's  in the low-resource environment?How can I fix this problem?\r\n![KH HVLC(7N@1KB$A@Y%FBDF](https://user-images.githubusercontent.com/41032255/139565137-ac318333-3c6e-4833-84a3-33320e4357b0.png)","reactions":{"url":"https://api.github.com/repos/google-research/google-research/issues/comments/955628191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"yhkll","id":41032255,"node_id":"MDQ6VXNlcjQxMDMyMjU1","avatar_url":"https://avatars.githubusercontent.com/u/41032255?v=4","gravatar_id":"","url":"https://api.github.com/users/yhkll","html_url":"https://github.com/yhkll","followers_url":"https://api.github.com/users/yhkll/followers","following_url":"https://api.github.com/users/yhkll/following{/other_user}","gists_url":"https://api.github.com/users/yhkll/gists{/gist_id}","starred_url":"https://api.github.com/users/yhkll/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yhkll/subscriptions","organizations_url":"https://api.github.com/users/yhkll/orgs","repos_url":"https://api.github.com/users/yhkll/repos","events_url":"https://api.github.com/users/yhkll/events{/privacy}","received_events_url":"https://api.github.com/users/yhkll/received_events","type":"User","site_admin":false}}]