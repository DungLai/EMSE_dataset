{"url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19","repository_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019","labels_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19/labels{/name}","comments_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19/comments","events_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19/events","html_url":"https://github.com/tbepler/protein-sequence-embedding-iclr2019/issues/19","id":568073402,"node_id":"MDU6SXNzdWU1NjgwNzM0MDI=","number":19,"title":"Reproduce performance problems","user":{"login":"DaHaiHuha","id":38548370,"node_id":"MDQ6VXNlcjM4NTQ4Mzcw","avatar_url":"https://avatars.githubusercontent.com/u/38548370?v=4","gravatar_id":"","url":"https://api.github.com/users/DaHaiHuha","html_url":"https://github.com/DaHaiHuha","followers_url":"https://api.github.com/users/DaHaiHuha/followers","following_url":"https://api.github.com/users/DaHaiHuha/following{/other_user}","gists_url":"https://api.github.com/users/DaHaiHuha/gists{/gist_id}","starred_url":"https://api.github.com/users/DaHaiHuha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DaHaiHuha/subscriptions","organizations_url":"https://api.github.com/users/DaHaiHuha/orgs","repos_url":"https://api.github.com/users/DaHaiHuha/repos","events_url":"https://api.github.com/users/DaHaiHuha/events{/privacy}","received_events_url":"https://api.github.com/users/DaHaiHuha/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2020-02-20T06:07:59Z","updated_at":"2020-11-17T07:55:28Z","closed_at":"2020-11-17T07:55:28Z","author_association":"NONE","active_lock_reason":null,"body":"Great work and thanks for releasing the codes + dataset + pre-trained model.\r\n\r\nBut I still have some questions about the training procedure, could you kindly spare some time to review the process? Iâ€™ve used the codes in Github repo to train the model for several times but failed to reproduce the result (the gap is about 5%), I was wondering that there are some differences between how your released model is trained and how I trained the models.\r\n\r\nThe training details are as follows:\r\n```bash\r\npython train_similarity_and_contact.py \\\r\n    --rnn-type lstm \\\r\n    --embedding-dim 100 \\\r\n    --input-dim 512 \\\r\n    --rnn-dim 512 \\\r\n    --hidden-dim 50 \\\r\n    --width 7 \\\r\n    --num-layers 3 \\\r\n    --dropout 0 \\\r\n    --epoch-scale 5 \\\r\n    --epoch-size 100000 \\\r\n    --num-epochs 100 \\\r\n    --similarity-batch-size 64 \\\r\n    --contact-batch-size 10 \\\r\n    --weight-decay 0 \\\r\n    --lr 0.001 \\\r\n    --tau 0.5 \\\r\n    --lambda 0.1 \\\r\n    --augment 0.05 \\\r\n    --lm /embedding/data/raw/bepler/pretrained_models/pfam_lm_lstm2x1024_tied_mb64.sav \\\r\n    --output /embedding/v-dache/save_logs/train_lambda0.1_augment0.05.txt \\\r\n    --save-prefix /embedding/v-dache/save_logs/train_lambda0.1_augment0.05 \\\r\n    --device -2\r\n```\r\nHere are the questions I would like to know the answers, could you kindly answer them?\r\nIs the released LM same as the one you used for training?\r\nShall I modify the code to reproduce the result?\r\nI noticed that when loading the samples for SCOP task, the number is 22408 but after resampled to match CMAP task, the number is only about 10% left, that is 2241. So I was wondering whether the resampled dataset matters.\r\nIs the released model obtained by searching some hyperparameters? If yes, how does it be done?\r\n \r\nBesides, I revised the source code a little bit and submitted a PR to Github: https://github.com/tbepler/protein-sequence-embedding-iclr2019/pull/18/commits/dc75f65c1734e7b696825fdefbb4bdc64385d6ae\r\nWill this lead to a performance drop?\r\n \r\nThe evaluation of the models are as follows:\r\n![image](https://user-images.githubusercontent.com/38548370/74905790-18b21c80-53ea-11ea-93aa-6991f09935a2.png)\r\nResults from eval_similarity.py\r\n\r\n![image](https://user-images.githubusercontent.com/38548370/74905801-1ea7fd80-53ea-11ea-8486-deefdd8598a0.png)\r\nResults from eval_similarity.py & eval_secstr.py \r\n\r\n![image](https://user-images.githubusercontent.com/38548370/74905823-2e274680-53ea-11ea-8f12-41f13207d03f.png)\r\nResults from eval_contact_scop.py\r\n\r\n![image](https://user-images.githubusercontent.com/38548370/74905813-29629280-53ea-11ea-9d8f-f59cad0db6a1.png)\r\nResults from eval_transmembrane.py\r\n \r\nAny suggestions will be appreciated! ","closed_by":{"login":"tbepler","id":5326126,"node_id":"MDQ6VXNlcjUzMjYxMjY=","avatar_url":"https://avatars.githubusercontent.com/u/5326126?v=4","gravatar_id":"","url":"https://api.github.com/users/tbepler","html_url":"https://github.com/tbepler","followers_url":"https://api.github.com/users/tbepler/followers","following_url":"https://api.github.com/users/tbepler/following{/other_user}","gists_url":"https://api.github.com/users/tbepler/gists{/gist_id}","starred_url":"https://api.github.com/users/tbepler/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbepler/subscriptions","organizations_url":"https://api.github.com/users/tbepler/orgs","repos_url":"https://api.github.com/users/tbepler/repos","events_url":"https://api.github.com/users/tbepler/events{/privacy}","received_events_url":"https://api.github.com/users/tbepler/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/19/timeline","performed_via_github_app":null,"state_reason":"completed"}