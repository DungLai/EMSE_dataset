{"url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3","repository_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019","labels_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3/labels{/name}","comments_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3/comments","events_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3/events","html_url":"https://github.com/tbepler/protein-sequence-embedding-iclr2019/issues/3","id":424770301,"node_id":"MDU6SXNzdWU0MjQ3NzAzMDE=","number":3,"title":"Question about sequence length","user":{"login":"kezhai","id":28676177,"node_id":"MDQ6VXNlcjI4Njc2MTc3","avatar_url":"https://avatars.githubusercontent.com/u/28676177?v=4","gravatar_id":"","url":"https://api.github.com/users/kezhai","html_url":"https://github.com/kezhai","followers_url":"https://api.github.com/users/kezhai/followers","following_url":"https://api.github.com/users/kezhai/following{/other_user}","gists_url":"https://api.github.com/users/kezhai/gists{/gist_id}","starred_url":"https://api.github.com/users/kezhai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kezhai/subscriptions","organizations_url":"https://api.github.com/users/kezhai/orgs","repos_url":"https://api.github.com/users/kezhai/repos","events_url":"https://api.github.com/users/kezhai/events{/privacy}","received_events_url":"https://api.github.com/users/kezhai/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-03-25T08:07:24Z","updated_at":"2019-03-27T20:43:29Z","closed_at":"2019-03-27T20:43:29Z","author_association":"NONE","active_lock_reason":null,"body":"HI,\r\n\r\nI am a little bit worried about the capacity of Bi-LSTM. As it is shown at Table 4, the maximum sequence length is 1,664. Is that mean your pre-trained LSTM model need to load all 1663 amino acid to predict the last one? How does that sequence perform? Do you have any algorithm to avoid the long sequence length that may encounter?\r\n\r\nThanks in advance, ","closed_by":{"login":"tbepler","id":5326126,"node_id":"MDQ6VXNlcjUzMjYxMjY=","avatar_url":"https://avatars.githubusercontent.com/u/5326126?v=4","gravatar_id":"","url":"https://api.github.com/users/tbepler","html_url":"https://github.com/tbepler","followers_url":"https://api.github.com/users/tbepler/followers","following_url":"https://api.github.com/users/tbepler/following{/other_user}","gists_url":"https://api.github.com/users/tbepler/gists{/gist_id}","starred_url":"https://api.github.com/users/tbepler/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbepler/subscriptions","organizations_url":"https://api.github.com/users/tbepler/orgs","repos_url":"https://api.github.com/users/tbepler/repos","events_url":"https://api.github.com/users/tbepler/events{/privacy}","received_events_url":"https://api.github.com/users/tbepler/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/tbepler/protein-sequence-embedding-iclr2019/issues/3/timeline","performed_via_github_app":null,"state_reason":"completed"}