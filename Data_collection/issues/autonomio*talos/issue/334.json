{"url":"https://api.github.com/repos/autonomio/talos/issues/334","repository_url":"https://api.github.com/repos/autonomio/talos","labels_url":"https://api.github.com/repos/autonomio/talos/issues/334/labels{/name}","comments_url":"https://api.github.com/repos/autonomio/talos/issues/334/comments","events_url":"https://api.github.com/repos/autonomio/talos/issues/334/events","html_url":"https://github.com/autonomio/talos/issues/334","id":460091318,"node_id":"MDU6SXNzdWU0NjAwOTEzMTg=","number":334,"title":"type error first neuron","user":{"login":"elizabellatran","id":20696567,"node_id":"MDQ6VXNlcjIwNjk2NTY3","avatar_url":"https://avatars.githubusercontent.com/u/20696567?v=4","gravatar_id":"","url":"https://api.github.com/users/elizabellatran","html_url":"https://github.com/elizabellatran","followers_url":"https://api.github.com/users/elizabellatran/followers","following_url":"https://api.github.com/users/elizabellatran/following{/other_user}","gists_url":"https://api.github.com/users/elizabellatran/gists{/gist_id}","starred_url":"https://api.github.com/users/elizabellatran/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elizabellatran/subscriptions","organizations_url":"https://api.github.com/users/elizabellatran/orgs","repos_url":"https://api.github.com/users/elizabellatran/repos","events_url":"https://api.github.com/users/elizabellatran/events{/privacy}","received_events_url":"https://api.github.com/users/elizabellatran/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-06-24T20:54:07Z","updated_at":"2019-06-24T23:58:11Z","closed_at":"2019-06-24T23:58:11Z","author_association":"NONE","active_lock_reason":null,"body":"I saw on troubleshooting page is says that when the keyerror is \"first_neuron\" the parameters for the first layer needs to be called \"first_neuron.\"\r\n\r\nBut I currently do have my parameters as first neuron for my first layer. Is there something I'm missing? Or if anyone can see an error and point it out I would really appreciate it! \r\n\r\nError message, parameters, and model:\r\n\r\n\r\n```KeyError Traceback (most recent call last)\r\n\r\n<ipython-input-162-fa3c1c7694dd> in <module>\r\n      1 h = ta.Scan(x, y, params = params,\r\n----> 2        model= bayes_opt_model)\r\n\r\n~/.local/lib/python3.6/site-packages/talos/scan/Scan.py in __init__(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, seed, clear_session, disable_progress_bar, print_params, debug)\r\n    170         # input parameters section ends\r\n    171 \r\n--> 172         self.runtime()\r\n    173 \r\n    174     def runtime(self):\r\n\r\n~/.local/lib/python3.6/site-packages/talos/scan/Scan.py in runtime(self)\r\n    175 \r\n    176         from .scan_run import scan_run\r\n--> 177         self = scan_run(self)\r\n\r\n~/.local/lib/python3.6/site-packages/talos/scan/scan_run.py in scan_run(self)\r\n      7 \r\n      8     from .scan_prepare import scan_prepare\r\n----> 9     self = scan_prepare(self)\r\n     10 \r\n     11     # initiate the progress bar\r\n\r\n~/.local/lib/python3.6/site-packages/talos/scan/scan_prepare.py in scan_prepare(self)\r\n     30                                    round_limit=self.round_limit,\r\n     31                                    time_limit=self.time_limit,\r\n---> 32                                    boolean_limit=self.boolean_limit\r\n     33                                    )\r\n     34 \r\n\r\n~/.local/lib/python3.6/site-packages/talos/parameters/ParamSpace.py in __init__(self, params, param_keys, random_method, fraction_limit, round_limit, time_limit, boolean_limit)\r\n     33 \r\n     34         # create list of list from the params dictionary\r\n---> 35         self._params_temp = [list(self.p[key]) for key in self.param_keys]\r\n     36 \r\n     37         # establish max dimensions\r\n\r\n~/.local/lib/python3.6/site-packages/talos/parameters/ParamSpace.py in <listcomp>(.0)\r\n     33 \r\n     34         # create list of list from the params dictionary\r\n---> 35         self._params_temp = [list(self.p[key]) for key in self.param_keys]\r\n     36 \r\n     37         # establish max dimensions\r\nKeyError: first_neuron ```\r\n\r\n`params= {\r\n     \"activation_type\": [\"linear\", \"elu\", \"LeakyReLU\", \"ReLU\", \"sigmoid\", \"tanh\"],\r\n     \"hidden_layers\": random.choice(range(1, 6, 1)),\r\n     \"first_neuron\": random.choice((range(1, 11, 1))), \r\n     \"hidden_neurons\":  random.choice((range(1, 11, 1))),\r\n      \"is_batch_norm\": [True],\r\n      \"max_epochs\":  [50,100, 250, 500,1000,2500,5000,10000,25000],\r\n      \"mini_batch_size\":[128],\r\n      \"optimizer_function\": [Adam],\r\n     \"learning_rate\": [np.array(round(np.random.uniform(0.00001, 0.1), 10))]\r\n}`\r\n\r\n```def bayes_opt_model(inputs, outputs, params, batchnorm = False, winit = 'xavier'):\r\n    activation_type  = random.choice(params['activation_type'])\r\n    hidden_layer_number = random.choice(params['hidden_layers'])\r\n    \r\n    def weight_initi(winit):\r\n        if winit == 'xavier' :\r\n            if activation_type == 'sigmoid' or activation_type == 'linear':\r\n                factor = 1 # Xavier weight initialization factor\r\n            elif activation_type== 'relu' or activation_type == 'LeakyReLU':\r\n                factor = 2\r\n            elif activation_type == 'tanh':\r\n                factor = 16\r\n            return factor\r\n    factor = weight_initi(winit)\r\n\r\n    def initializer_pick(curr_winit):\r\n        if curr_winit == 'xavier':\r\n            initialization =  initializers.glorot_uniform(factor)\r\n        else:\r\n            if activation == 'LeakyReLU':\r\n                initialization = initializers.he_normal(factor)\r\n            elif activation == 'relu' or activation == 'elu':\r\n                initialization = initializers.he_normal(factor)\r\n            else:\r\n                initialization = initializers.he_normal(factor)\r\n        return initialization\r\n\r\n    initialization = initializer_pick(winit)\r\n    #####################\r\n    # Begin Model       #\r\n    ####################\r\n    model = Sequential()\r\n    model.add(Dense(params['first_neuron'], input_dim = inputs.shape[1],activation = activation_type, kernel_initializer = initialization ))\r\n    if batchnorm == True:\r\n        model.add(BatchNormalization())\r\n\r\n\r\n    ############################\r\n    # Remaining hidden layers  #\r\n    ############################\r\n    for i in range(hidden_layer_number -1):\r\n        model.add(Dense(units = params['hidden_neurons'], activation=activation_type, kernel_initializer = initialization))\r\n        if batchnorm == True:\r\n            model.add(BatchNormalization())    \r\n            ############# ##\r\n    model.add(Dense(units = params['hidden_neurons'], activation = activation_type, kernel_initializer = initialization))\r\n    model.compile(optimizer = params['optimizer'](lr_normalizer(params['learning_rate'], params['optimizer_function'])), loss=params['losses'])\r\n\r\n    ###############\r\n    # Model summary#\r\n    ################\r\n    print(model.summary())\r\n    out = model.fit(inputs, outputs, batch_size=params['batch_size'], epochs=params['epochs'],verbose= 1)\r\n\r\n    #modify the output model\r\n    return out, model```\r\n","closed_by":{"login":"elizabellatran","id":20696567,"node_id":"MDQ6VXNlcjIwNjk2NTY3","avatar_url":"https://avatars.githubusercontent.com/u/20696567?v=4","gravatar_id":"","url":"https://api.github.com/users/elizabellatran","html_url":"https://github.com/elizabellatran","followers_url":"https://api.github.com/users/elizabellatran/followers","following_url":"https://api.github.com/users/elizabellatran/following{/other_user}","gists_url":"https://api.github.com/users/elizabellatran/gists{/gist_id}","starred_url":"https://api.github.com/users/elizabellatran/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/elizabellatran/subscriptions","organizations_url":"https://api.github.com/users/elizabellatran/orgs","repos_url":"https://api.github.com/users/elizabellatran/repos","events_url":"https://api.github.com/users/elizabellatran/events{/privacy}","received_events_url":"https://api.github.com/users/elizabellatran/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/autonomio/talos/issues/334/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/autonomio/talos/issues/334/timeline","performed_via_github_app":null,"state_reason":"completed"}