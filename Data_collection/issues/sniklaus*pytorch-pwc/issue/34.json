{"url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34","repository_url":"https://api.github.com/repos/sniklaus/pytorch-pwc","labels_url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34/labels{/name}","comments_url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34/comments","events_url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34/events","html_url":"https://github.com/sniklaus/pytorch-pwc/issues/34","id":560952549,"node_id":"MDU6SXNzdWU1NjA5NTI1NDk=","number":34,"title":"Convert pwc_net.caffemodel to network-default.pytorch","user":{"login":"lhao0301","id":10356897,"node_id":"MDQ6VXNlcjEwMzU2ODk3","avatar_url":"https://avatars.githubusercontent.com/u/10356897?v=4","gravatar_id":"","url":"https://api.github.com/users/lhao0301","html_url":"https://github.com/lhao0301","followers_url":"https://api.github.com/users/lhao0301/followers","following_url":"https://api.github.com/users/lhao0301/following{/other_user}","gists_url":"https://api.github.com/users/lhao0301/gists{/gist_id}","starred_url":"https://api.github.com/users/lhao0301/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lhao0301/subscriptions","organizations_url":"https://api.github.com/users/lhao0301/orgs","repos_url":"https://api.github.com/users/lhao0301/repos","events_url":"https://api.github.com/users/lhao0301/events{/privacy}","received_events_url":"https://api.github.com/users/lhao0301/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-02-06T11:30:24Z","updated_at":"2020-03-20T17:08:13Z","closed_at":"2020-02-10T04:20:58Z","author_association":"NONE","active_lock_reason":null,"body":"Thanks for your wonderful code!\r\n\r\nI try to convert the official `pwc_net.caffemodel` to pytorch. And I find a interesing problem, the `bias` of the `FeatureUpsample` layers within all the `decoder` in this repo are not consistent with official model. \r\n\r\nI compare my pytorch weights (converted  from official caffemodel) with `network-default.pytorch` in this  repo as follows. \r\n\r\n```\r\ndata1 = torch.load('network-default.pytorch')  \r\ndata2 = torch.load('my-network.pytorch')  \r\nfor key in data1.keys():\r\n    print(key, torch.max(data1[key] - data2[key]), torch.min(data1[key] - data2[key])\r\n\r\nmoduleExtractor.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleOne.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleOne.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleOne.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleOne.4.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleTwo.4.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleThr.4.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFou.4.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleFiv.4.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.2.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.2.bias tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.4.weight tensor(0.) tensor(0.)\r\nmoduleExtractor.moduleSix.4.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleUpflow.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleUpflow.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleUpfeat.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleUpfeat.bias tensor(0.0873) tensor(0.0774)\r\nmoduleTwo.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleTwo.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleTwo.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleUpflow.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleUpflow.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleUpfeat.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleUpfeat.bias tensor(-0.1421) tensor(-0.1514)\r\nmoduleThr.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleThr.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleThr.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleUpflow.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleUpflow.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleUpfeat.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleUpfeat.bias tensor(0.1554) tensor(-0.1341)\r\nmoduleFou.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleFou.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleFou.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleUpflow.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleUpflow.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleUpfeat.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleUpfeat.bias tensor(0.0935) tensor(-0.1689)\r\nmoduleFiv.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleFiv.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleFiv.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleOne.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleOne.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleTwo.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleTwo.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleThr.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleThr.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleFou.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleFou.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleFiv.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleFiv.0.bias tensor(0.) tensor(0.)\r\nmoduleSix.moduleSix.0.weight tensor(0.) tensor(0.)\r\nmoduleSix.moduleSix.0.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.0.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.0.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.2.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.2.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.4.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.4.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.6.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.6.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.8.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.8.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.10.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.10.bias tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.12.weight tensor(0.) tensor(0.)\r\nmoduleRefiner.moduleMain.12.bias tensor(0.) tensor(0.)\r\n```\r\n\r\nAll the parameters are actually same but the `bias` in `FeatureUpsample` as aforementioned. And I find the `bias` in my converted pytorch model are `0` and in the `network-default.pytorch` they are `not 0`. I look into the official trianing [prototxt file](https://github.com/NVlabs/PWC-Net/blob/master/Caffe/model/train.prototxt) and find the `lr_mult` for all `deconvolution layers` are `0` and so does the initializer(`constant` means `0` also by default). **So the real value of `bias` should be `0`.**\r\n\r\nI also check the `sintel.pytorch` in an older version of this repo. I compare `sintel.pytorch` with `network-default.pytorch` and find the same phenomenon, all the parameters are actually same but the `bias` in `FeatureUpsample`. There may be some randomness during your conversion within the `FeatureUpsample` layers. I guess the `bias` are initialized differently everytime.\r\n\r\n@sniklaus ","closed_by":{"login":"sniklaus","id":1238034,"node_id":"MDQ6VXNlcjEyMzgwMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/1238034?v=4","gravatar_id":"","url":"https://api.github.com/users/sniklaus","html_url":"https://github.com/sniklaus","followers_url":"https://api.github.com/users/sniklaus/followers","following_url":"https://api.github.com/users/sniklaus/following{/other_user}","gists_url":"https://api.github.com/users/sniklaus/gists{/gist_id}","starred_url":"https://api.github.com/users/sniklaus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sniklaus/subscriptions","organizations_url":"https://api.github.com/users/sniklaus/orgs","repos_url":"https://api.github.com/users/sniklaus/repos","events_url":"https://api.github.com/users/sniklaus/events{/privacy}","received_events_url":"https://api.github.com/users/sniklaus/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/sniklaus/pytorch-pwc/issues/34/timeline","performed_via_github_app":null,"state_reason":"completed"}