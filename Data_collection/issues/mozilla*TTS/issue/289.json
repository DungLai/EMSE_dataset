{"url":"https://api.github.com/repos/mozilla/TTS/issues/289","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/289/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/289/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/289/events","html_url":"https://github.com/mozilla/TTS/issues/289","id":498789403,"node_id":"MDU6SXNzdWU0OTg3ODk0MDM=","number":289,"title":"Unexpected bus error encountered in worker","user":{"login":"vcjob","id":51916323,"node_id":"MDQ6VXNlcjUxOTE2MzIz","avatar_url":"https://avatars.githubusercontent.com/u/51916323?v=4","gravatar_id":"","url":"https://api.github.com/users/vcjob","html_url":"https://github.com/vcjob","followers_url":"https://api.github.com/users/vcjob/followers","following_url":"https://api.github.com/users/vcjob/following{/other_user}","gists_url":"https://api.github.com/users/vcjob/gists{/gist_id}","starred_url":"https://api.github.com/users/vcjob/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vcjob/subscriptions","organizations_url":"https://api.github.com/users/vcjob/orgs","repos_url":"https://api.github.com/users/vcjob/repos","events_url":"https://api.github.com/users/vcjob/events{/privacy}","received_events_url":"https://api.github.com/users/vcjob/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2019-09-26T10:07:42Z","updated_at":"2020-04-23T14:14:24Z","closed_at":"2019-09-26T11:46:17Z","author_association":"NONE","active_lock_reason":null,"body":"Hello everyone.\r\nWhen training Tacotron2 from dev branch on V100 16gb single GPU, near the end of the second epoch I've got error:\r\n   | > Step:598/770  GlobalStep:1370  PostnetLoss:0.03293  DecoderLoss:0.01078  StopLoss:0.17731  AlignScore:0.0933  GradNorm:0.32636  GradNormST:0.20265  AvgTextLen:140.4  AvgSpecLen:646.5  StepTime:2.75  LoaderTime:0.02  LR:0.000100\r\nERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\r\n ! Run is kept in /ssd/ya/outputs/V100-September-26-2019_08+54AM-53d658f\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 682, in <module>\r\n    main(args)\r\n  File \"train.py\", line 587, in main\r\n    ap, global_step, epoch)\r\n  File \"train.py\", line 165, in train\r\n    text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/models/tacotron2.py\", line 53, in forward\r\n    encoder_outputs, mel_specs, mask)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/layers/tacotron2.py\", line 250, in forward\r\n    mel_output, stop_token, attention_weights = self.decode(memory)\r\n  File \"/opt/tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/layers/tacotron2.py\", line 232, in decode\r\n    stop_token = self.stopnet(stopnet_input.detach())\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/container.py\", line 92, in forward\r\n    input = module(input)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 547, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/modules/dropout.py\", line 54, in forward\r\n    return F.dropout(input, self.p, self.training, self.inplace)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/nn/functional.py\", line 806, in dropout\r\n    else _VF.dropout(input, p, training))\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 32284) is killed by signal: Bus error.\r\n(tts) root@server_ip:/opt/tts/TTS#\r\n______________\r\nWhen try second time - once again on the near end of the 2nd epoch another error:\r\n   | > Step:638/770  GlobalStep:1410  PostnetLoss:0.04727  DecoderLoss:0.01074  StopLoss:0.15123  AlignScore:0.0929  GradNorm:0.29388  GradNormST:0.20302  AvgTextLen:148.7  AvgSpecLen:689.9  StepTime:3.27  LoaderTime:0.03  LR:0.000100\r\n ! Run is kept in /ssd/ya/outputs/V100-September-26-2019_10+02AM-53d658f\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 682, in <module>\r\n    main(args)\r\n  File \"train.py\", line 587, in main\r\n    ap, global_step, epoch)\r\n  File \"train.py\", line 113, in train\r\n    for num_iter, data in enumerate(data_loader):\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/utils/data/dataloader.py\", line 819, in __next__\r\n    return self._process_data(data)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/utils/data/dataloader.py\", line 846, in _process_data\r\n    data.reraise()\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/_utils.py\", line 369, in reraise\r\n    raise self.exc_type(msg)\r\nRuntimeError: Caught RuntimeError in DataLoader worker process 1.\r\nOriginal Traceback (most recent call last):\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/opt/tts/lib/python3.6/site-packages/torch-1.2.0-py3.6-linux-x86_64.egg/torch/utils/data/_utils/fetch.py\", line 47, in fetch\r\n    return self.collate_fn(data)\r\n  File \"/opt/tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/datasets/TTSDataset.py\", line 221, in collate_fn\r\n    linear = torch.FloatTensor(linear).contiguous()\r\nRuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 99187200 bytes. Error code 12 (Cannot allocate memory)\r\n\r\n\r\nWhat can be the reason? The files train.py and others, except for some of utils/text python scripts are unchanged. \r\nBatch size is 32. number of workers = 4.","closed_by":{"login":"erogol","id":1402048,"node_id":"MDQ6VXNlcjE0MDIwNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1402048?v=4","gravatar_id":"","url":"https://api.github.com/users/erogol","html_url":"https://github.com/erogol","followers_url":"https://api.github.com/users/erogol/followers","following_url":"https://api.github.com/users/erogol/following{/other_user}","gists_url":"https://api.github.com/users/erogol/gists{/gist_id}","starred_url":"https://api.github.com/users/erogol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/erogol/subscriptions","organizations_url":"https://api.github.com/users/erogol/orgs","repos_url":"https://api.github.com/users/erogol/repos","events_url":"https://api.github.com/users/erogol/events{/privacy}","received_events_url":"https://api.github.com/users/erogol/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/289/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/289/timeline","performed_via_github_app":null,"state_reason":"completed"}