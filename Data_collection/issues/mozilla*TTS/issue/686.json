{"url":"https://api.github.com/repos/mozilla/TTS/issues/686","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/686/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/686/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/686/events","html_url":"https://github.com/mozilla/TTS/issues/686","id":833757462,"node_id":"MDU6SXNzdWU4MzM3NTc0NjI=","number":686,"title":"AssertionError while converting Tacotron2 Pytorch model to TF","user":{"login":"khalilRhouma","id":35801846,"node_id":"MDQ6VXNlcjM1ODAxODQ2","avatar_url":"https://avatars.githubusercontent.com/u/35801846?v=4","gravatar_id":"","url":"https://api.github.com/users/khalilRhouma","html_url":"https://github.com/khalilRhouma","followers_url":"https://api.github.com/users/khalilRhouma/followers","following_url":"https://api.github.com/users/khalilRhouma/following{/other_user}","gists_url":"https://api.github.com/users/khalilRhouma/gists{/gist_id}","starred_url":"https://api.github.com/users/khalilRhouma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/khalilRhouma/subscriptions","organizations_url":"https://api.github.com/users/khalilRhouma/orgs","repos_url":"https://api.github.com/users/khalilRhouma/repos","events_url":"https://api.github.com/users/khalilRhouma/events{/privacy}","received_events_url":"https://api.github.com/users/khalilRhouma/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-17T13:12:40Z","updated_at":"2021-03-30T07:37:17Z","closed_at":"2021-03-30T07:37:17Z","author_association":"NONE","active_lock_reason":null,"body":"I am trying to improve inference response time, I read from your comments that converting Pytorch model to TF will improve TTS inference, so when I tried to run  `convert_tacotron2_torch_to_tf.py` on Tacotron2 checkpoint I got this AssertionError  and I didn't get it where is the problem exactly.  This is the log below :\r\n```\r\n  2021-03-16 16:29:00.636264: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1Traceback (most recent call last):\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/site-packages/TTS/bin/convert_tacotron2_torch_to_tf.py\", line 40, in <module>\r\n    c = load_config(config_path)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/site-packages/TTS/utils/io.py\", line 41, in load_config\r\n    ext = os.path.splitext(config_path)[1]\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/posixpath.py\", line 122, in splitext\r\n    p = os.fspath(p)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n(aaj) khalil@arabic-assistant:~/arabic-assistant$ python -m TTS.bin.convert_tacotron2_torch_to_tf.py --config_path /home/khalil/arabic-assistant/app/experiment_demo/tts_config.json --torch_model_path app/checkpoint/400-arabic-buckwalter-aug_text_input-Tacatron_ddc-November-21-2020.pth.tar --output_path app/checkpoint/tts_tac_model_tf.pkl\r\n2021-03-16 16:29:30.037874: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\r\n > Using model: Tacotron2\r\n2021-03-16 16:29:33.191075: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\r\n2021-03-16 16:29:33.952304: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-03-16 16:29:33.952415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: arabic-assistant\r\n2021-03-16 16:29:33.952431: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: arabic-assistant\r\n2021-03-16 16:29:33.952635: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.102.4\r\n2021-03-16 16:29:33.952674: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.102.4\r\n2021-03-16 16:29:33.952683: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.102.4\r\n2021-03-16 16:29:33.953211: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-03-16 16:29:33.961595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199995000 Hz\r\n2021-03-16 16:29:33.962212: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55d33cb75e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2021-03-16 16:29:33.962245: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n(1, None, 80)\r\n(1, None, 80)\r\n[('embedding/embeddings:0', 'embedding.weight'),\r\n ('encoder/lstm/forward_lstm/lstm_cell_1/kernel:0',\r\n  'encoder.lstm.weight_ih_l0'),\r\n ('encoder/lstm/forward_lstm/lstm_cell_1/recurrent_kernel:0',\r\n  'encoder.lstm.weight_hh_l0'),\r\n ('encoder/lstm/backward_lstm/lstm_cell_2/kernel:0',\r\n  'encoder.lstm.weight_ih_l0_reverse'),\r\n ('encoder/lstm/backward_lstm/lstm_cell_2/recurrent_kernel:0',\r\n  'encoder.lstm.weight_hh_l0_reverse'),\r\n ('encoder/lstm/forward_lstm/lstm_cell_1/bias:0',\r\n  ('encoder.lstm.bias_ih_l0', 'encoder.lstm.bias_hh_l0')),\r\n ('encoder/lstm/backward_lstm/lstm_cell_2/bias:0',\r\n  ('encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse')),\r\n ('attention/v/kernel:0', 'decoder.attention.v.linear_layer.weight'),\r\n ('decoder/linear_projection/kernel:0',\r\n  'decoder.linear_projection.linear_layer.weight'),\r\n ('decoder/stopnet/kernel:0', 'decoder.stopnet.1.linear_layer.weight'),\r\n ('encoder/convolutions_0/convolution1d/kernel:0',\r\n  'encoder.convolutions.0.convolution1d.weight'),\r\n ('encoder/convolutions_0/convolution1d/bias:0',\r\n  'encoder.convolutions.0.convolution1d.bias'),\r\n ('encoder/convolutions_0/batch_normalization/gamma:0',\r\n  'encoder.convolutions.0.batch_normalization.weight'),\r\n ('encoder/convolutions_0/batch_normalization/beta:0',\r\n  'encoder.convolutions.0.batch_normalization.bias'),\r\n ('encoder/convolutions_1/convolution1d/kernel:0',\r\n  'encoder.convolutions.1.convolution1d.weight'),\r\n ('encoder/convolutions_1/convolution1d/bias:0',\r\n  'encoder.convolutions.1.convolution1d.bias'),\r\n ('encoder/convolutions_1/batch_normalization/gamma:0',\r\n  'encoder.convolutions.1.batch_normalization.weight'),\r\n ('encoder/convolutions_1/batch_normalization/beta:0',\r\n  'encoder.convolutions.1.batch_normalization.bias'),\r\n ('encoder/convolutions_2/convolution1d/kernel:0',\r\n  'encoder.convolutions.2.convolution1d.weight'),\r\n ('encoder/convolutions_2/convolution1d/bias:0',\r\n  'encoder.convolutions.2.convolution1d.bias'),\r\n ('encoder/convolutions_2/batch_normalization/gamma:0',\r\n  'encoder.convolutions.2.batch_normalization.weight'),\r\n ('encoder/convolutions_2/batch_normalization/beta:0',\r\n  'encoder.convolutions.2.batch_normalization.bias'),\r\n ('encoder/convolutions_0/batch_normalization/moving_mean:0',\r\n  'encoder.convolutions.0.batch_normalization.running_mean'),\r\n ('encoder/convolutions_0/batch_normalization/moving_variance:0',\r\n  'encoder.convolutions.0.batch_normalization.running_var'),\r\n ('encoder/convolutions_1/batch_normalization/moving_mean:0',\r\n  'encoder.convolutions.1.batch_normalization.running_mean'),\r\n ('encoder/convolutions_1/batch_normalization/moving_variance:0',\r\n  'encoder.convolutions.1.batch_normalization.running_var'),\r\n ('encoder/convolutions_2/batch_normalization/moving_mean:0',\r\n  'encoder.convolutions.2.batch_normalization.running_mean'),\r\n ('encoder/convolutions_2/batch_normalization/moving_variance:0',\r\n  'encoder.convolutions.2.batch_normalization.running_var'),\r\n ('decoder/while/prenet/linear_layer_0/linear_layer/kernel:0',\r\n  'decoder.prenet.linear_layers.0.linear_layer.weight'),\r\n ('decoder/while/prenet/linear_layer_1/linear_layer/kernel:0',\r\n  'decoder.prenet.linear_layers.1.linear_layer.weight'),\r\n ('decoder/while/attention_rnn/kernel:0', 'decoder.attention_rnn.weight_ih'),\r\n ('decoder/while/attention_rnn/recurrent_kernel:0',\r\n  'decoder.attention_rnn.weight_hh'),\r\n ('decoder/while/attention_rnn/bias:0', 'decoder.attention_rnn.bias_ih'),\r\n ('decoder/while/attention/query_layer/linear_layer/kernel:0',\r\n  'decoder.attention.query_layer.linear_layer.weight'),\r\n ('decoder/attention/inputs_layer/linear_layer/kernel:0',\r\n  'decoder.attention.inputs_layer.linear_layer.weight'),\r\n ('decoder/while/attention/v/linear_layer/kernel:0',\r\n  'decoder.attention.v.linear_layer.weight'),\r\n ('decoder/while/attention/v/linear_layer/bias:0',\r\n  'decoder.attention.v.linear_layer.bias'),\r\n ('decoder/while/attention/location_layer/location_conv1d/kernel:0',\r\n  'decoder.attention.location_layer.location_conv1d.weight'),\r\n ('decoder/while/attention/location_layer/location_dense/kernel:0',\r\n  'decoder.attention.location_layer.location_dense.linear_layer.weight'),\r\n ('decoder/while/decoder_rnn/kernel:0', 'decoder.decoder_rnn.weight_ih'),\r\n ('decoder/while/decoder_rnn/recurrent_kernel:0',\r\n  'decoder.decoder_rnn.weight_hh'),\r\n ('decoder/while/decoder_rnn/bias:0', 'decoder.decoder_rnn.bias_ih'),\r\n ('decoder/while/linear_projection/linear_layer/kernel:0',\r\n  'decoder.linear_projection.linear_layer.weight'),\r\n ('decoder/while/linear_projection/linear_layer/bias:0',\r\n  'decoder.linear_projection.linear_layer.bias'),\r\n ('decoder/while/stopnet/linear_layer/kernel:0',\r\n  'decoder.stopnet.1.linear_layer.weight'),\r\n ('decoder/while/stopnet/linear_layer/bias:0',\r\n  'decoder.stopnet.1.linear_layer.bias'),\r\n ('postnet/convolutions_0/convolution1d/kernel:0',\r\n  'postnet.convolutions.0.convolution1d.weight'),\r\n ('postnet/convolutions_0/convolution1d/bias:0',\r\n  'postnet.convolutions.0.convolution1d.bias'),\r\n ('postnet/convolutions_0/batch_normalization/gamma:0',\r\n  'postnet.convolutions.0.batch_normalization.weight'),\r\n ('postnet/convolutions_0/batch_normalization/beta:0',\r\n  'postnet.convolutions.0.batch_normalization.bias'),\r\n ('postnet/convolutions_1/convolution1d/kernel:0',\r\n  'postnet.convolutions.1.convolution1d.weight'),\r\n ('postnet/convolutions_1/convolution1d/bias:0',\r\n  'postnet.convolutions.1.convolution1d.bias'),\r\n ('postnet/convolutions_1/batch_normalization/gamma:0',\r\n  'postnet.convolutions.1.batch_normalization.weight'),\r\n ('postnet/convolutions_1/batch_normalization/beta:0',\r\n  'postnet.convolutions.1.batch_normalization.bias'),\r\n ('postnet/convolutions_2/convolution1d/kernel:0',\r\n  'postnet.convolutions.2.convolution1d.weight'),\r\n ('postnet/convolutions_2/convolution1d/bias:0',\r\n  'postnet.convolutions.2.convolution1d.bias'),\r\n ('postnet/convolutions_2/batch_normalization/gamma:0',\r\n  'postnet.convolutions.2.batch_normalization.weight'),\r\n ('postnet/convolutions_2/batch_normalization/beta:0',\r\n  'postnet.convolutions.2.batch_normalization.bias'),\r\n ('postnet/convolutions_3/convolution1d/kernel:0',\r\n  'postnet.convolutions.3.convolution1d.weight'),\r\n ('postnet/convolutions_3/convolution1d/bias:0',\r\n  'postnet.convolutions.3.convolution1d.bias'),\r\n ('postnet/convolutions_3/batch_normalization/gamma:0',\r\n  'postnet.convolutions.3.batch_normalization.weight'),\r\n ('postnet/convolutions_3/batch_normalization/beta:0',\r\n  'postnet.convolutions.3.batch_normalization.bias'),\r\n ('postnet/convolutions_4/convolution1d/kernel:0',\r\n  'postnet.convolutions.4.convolution1d.weight'),\r\n ('postnet/convolutions_4/convolution1d/bias:0',\r\n  'postnet.convolutions.4.convolution1d.bias'),\r\n ('postnet/convolutions_4/batch_normalization/gamma:0',\r\n  'postnet.convolutions.4.batch_normalization.weight'),\r\n ('postnet/convolutions_4/batch_normalization/beta:0',\r\n  'postnet.convolutions.4.batch_normalization.bias'),\r\n ('postnet/convolutions_0/batch_normalization/moving_mean:0',\r\n  'postnet.convolutions.0.batch_normalization.running_mean'),\r\n ('postnet/convolutions_0/batch_normalization/moving_variance:0',\r\n  'postnet.convolutions.0.batch_normalization.running_var'),\r\n ('postnet/convolutions_1/batch_normalization/moving_mean:0',\r\n  'postnet.convolutions.1.batch_normalization.running_mean'),\r\n ('postnet/convolutions_1/batch_normalization/moving_variance:0',\r\n  'postnet.convolutions.1.batch_normalization.running_var'),\r\n ('postnet/convolutions_2/batch_normalization/moving_mean:0',\r\n  'postnet.convolutions.2.batch_normalization.running_mean'),\r\n ('postnet/convolutions_2/batch_normalization/moving_variance:0',\r\n  'postnet.convolutions.2.batch_normalization.running_var'),\r\n ('postnet/convolutions_3/batch_normalization/moving_mean:0',\r\n  'postnet.convolutions.3.batch_normalization.running_mean'),\r\n ('postnet/convolutions_3/batch_normalization/moving_variance:0',\r\n  'postnet.convolutions.3.batch_normalization.running_var'),\r\n ('postnet/convolutions_4/batch_normalization/moving_mean:0',\r\n  'postnet.convolutions.4.batch_normalization.running_mean'),\r\n ('postnet/convolutions_4/batch_normalization/moving_variance:0',\r\n  'postnet.convolutions.4.batch_normalization.running_var')]\r\n['embedding.weight',\r\n 'encoder.convolutions.0.batch_normalization.num_batches_tracked',\r\n 'encoder.convolutions.1.batch_normalization.num_batches_tracked',\r\n 'encoder.convolutions.2.batch_normalization.num_batches_tracked',\r\n 'encoder.lstm.weight_ih_l0',\r\n 'encoder.lstm.weight_hh_l0',\r\n 'encoder.lstm.bias_ih_l0',\r\n 'encoder.lstm.bias_hh_l0',\r\n 'encoder.lstm.weight_ih_l0_reverse',\r\n 'encoder.lstm.weight_hh_l0_reverse',\r\n 'encoder.lstm.bias_ih_l0_reverse',\r\n 'encoder.lstm.bias_hh_l0_reverse',\r\n 'decoder.attention_rnn.bias_hh',\r\n 'decoder.decoder_rnn.bias_hh',\r\n 'postnet.convolutions.0.batch_normalization.num_batches_tracked',\r\n 'postnet.convolutions.1.batch_normalization.num_batches_tracked',\r\n 'postnet.convolutions.2.batch_normalization.num_batches_tracked',\r\n 'postnet.convolutions.3.batch_normalization.num_batches_tracked',\r\n 'postnet.convolutions.4.batch_normalization.num_batches_tracked',\r\n 'coarse_decoder.prenet.linear_layers.0.linear_layer.weight',\r\n 'coarse_decoder.prenet.linear_layers.1.linear_layer.weight',\r\n 'coarse_decoder.attention_rnn.weight_ih',\r\n 'coarse_decoder.attention_rnn.weight_hh',\r\n 'coarse_decoder.attention_rnn.bias_ih',\r\n 'coarse_decoder.attention_rnn.bias_hh',\r\n 'coarse_decoder.attention.query_layer.linear_layer.weight',\r\n 'coarse_decoder.attention.inputs_layer.linear_layer.weight',\r\n 'coarse_decoder.attention.v.linear_layer.weight',\r\n 'coarse_decoder.attention.v.linear_layer.bias',\r\n 'coarse_decoder.attention.location_layer.location_conv1d.weight',\r\n 'coarse_decoder.attention.location_layer.location_dense.linear_layer.weight',\r\n 'coarse_decoder.decoder_rnn.weight_ih',\r\n 'coarse_decoder.decoder_rnn.weight_hh',\r\n 'coarse_decoder.decoder_rnn.bias_ih',\r\n 'coarse_decoder.decoder_rnn.bias_hh',\r\n 'coarse_decoder.linear_projection.linear_layer.weight',\r\n 'coarse_decoder.linear_projection.linear_layer.bias',\r\n 'coarse_decoder.stopnet.1.linear_layer.weight',\r\n 'coarse_decoder.stopnet.1.linear_layer.bias']\r\n > Passing weights from Torch to TF ...\r\n | > embedding/embeddings:0 <-- embedding.weight\r\n | > encoder/convolutions_0/convolution1d/kernel:0 <-- encoder.convolutions.0.convolution1d.weight\r\n | > encoder/convolutions_0/convolution1d/bias:0 <-- encoder.convolutions.0.convolution1d.bias\r\n | > encoder/convolutions_0/batch_normalization/gamma:0 <-- encoder.convolutions.0.batch_normalization.weight\r\n | > encoder/convolutions_0/batch_normalization/beta:0 <-- encoder.convolutions.0.batch_normalization.bias\r\n | > encoder/convolutions_1/convolution1d/kernel:0 <-- encoder.convolutions.1.convolution1d.weight\r\n | > encoder/convolutions_1/convolution1d/bias:0 <-- encoder.convolutions.1.convolution1d.bias\r\n | > encoder/convolutions_1/batch_normalization/gamma:0 <-- encoder.convolutions.1.batch_normalization.weight\r\n | > encoder/convolutions_1/batch_normalization/beta:0 <-- encoder.convolutions.1.batch_normalization.bias\r\n | > encoder/convolutions_2/convolution1d/kernel:0 <-- encoder.convolutions.2.convolution1d.weight\r\n | > encoder/convolutions_2/convolution1d/bias:0 <-- encoder.convolutions.2.convolution1d.bias\r\n | > encoder/convolutions_2/batch_normalization/gamma:0 <-- encoder.convolutions.2.batch_normalization.weight\r\n | > encoder/convolutions_2/batch_normalization/beta:0 <-- encoder.convolutions.2.batch_normalization.bias\r\n | > encoder/lstm/forward_lstm/lstm_cell_1/kernel:0 <-- encoder.lstm.weight_ih_l0\r\n | > encoder/lstm/forward_lstm/lstm_cell_1/recurrent_kernel:0 <-- encoder.lstm.weight_hh_l0\r\n | > encoder/lstm/forward_lstm/lstm_cell_1/bias:0 <-- ('encoder.lstm.bias_ih_l0', 'encoder.lstm.bias_hh_l0')\r\n | > encoder/lstm/backward_lstm/lstm_cell_2/kernel:0 <-- encoder.lstm.weight_ih_l0_reverse\r\n | > encoder/lstm/backward_lstm/lstm_cell_2/recurrent_kernel:0 <-- encoder.lstm.weight_hh_l0_reverse\r\n | > encoder/lstm/backward_lstm/lstm_cell_2/bias:0 <-- ('encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse')\r\n | > encoder/convolutions_0/batch_normalization/moving_mean:0 <-- encoder.convolutions.0.batch_normalization.running_mean\r\n | > encoder/convolutions_0/batch_normalization/moving_variance:0 <-- encoder.convolutions.0.batch_normalization.running_var\r\n | > encoder/convolutions_1/batch_normalization/moving_mean:0 <-- encoder.convolutions.1.batch_normalization.running_mean\r\n | > encoder/convolutions_1/batch_normalization/moving_variance:0 <-- encoder.convolutions.1.batch_normalization.running_var\r\n | > encoder/convolutions_2/batch_normalization/moving_mean:0 <-- encoder.convolutions.2.batch_normalization.running_mean\r\n | > encoder/convolutions_2/batch_normalization/moving_variance:0 <-- encoder.convolutions.2.batch_normalization.running_var\r\n | > decoder/while/prenet/linear_layer_0/linear_layer/kernel:0 <-- decoder.prenet.linear_layers.0.linear_layer.weight\r\n | > decoder/while/prenet/linear_layer_1/linear_layer/kernel:0 <-- decoder.prenet.linear_layers.1.linear_layer.weight\r\n | > decoder/while/attention_rnn/kernel:0 <-- decoder.attention_rnn.weight_ih\r\n | > decoder/while/attention_rnn/recurrent_kernel:0 <-- decoder.attention_rnn.weight_hh\r\n | > decoder/while/attention_rnn/bias:0 <-- decoder.attention_rnn.bias_ih\r\nTraceback (most recent call last):\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/runpy.py\", line 183, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/runpy.py\", line 109, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/site-packages/TTS/bin/convert_tacotron2_torch_to_tf.py\", line 121, in <module>\r\n    tf_vars = transfer_weights_torch_to_tf(tf_vars, dict(var_map), state_dict)\r\n  File \"/home/khalil/anaconda3/envs/aaj/lib/python3.6/site-packages/TTS/tts/tf/utils/convert_torch_to_tf_utils.py\", line 70, in transfer_weights_torch_to_tf\r\n    assert len(bias_vectors) == 2\r\nAssertionError\r\n```\r\nI searched for this issue here and on your discussion forum and didn't find anything related.\r\nAny ideas about this problems? \r\nThank you\r\n","closed_by":{"login":"khalilRhouma","id":35801846,"node_id":"MDQ6VXNlcjM1ODAxODQ2","avatar_url":"https://avatars.githubusercontent.com/u/35801846?v=4","gravatar_id":"","url":"https://api.github.com/users/khalilRhouma","html_url":"https://github.com/khalilRhouma","followers_url":"https://api.github.com/users/khalilRhouma/followers","following_url":"https://api.github.com/users/khalilRhouma/following{/other_user}","gists_url":"https://api.github.com/users/khalilRhouma/gists{/gist_id}","starred_url":"https://api.github.com/users/khalilRhouma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/khalilRhouma/subscriptions","organizations_url":"https://api.github.com/users/khalilRhouma/orgs","repos_url":"https://api.github.com/users/khalilRhouma/repos","events_url":"https://api.github.com/users/khalilRhouma/events{/privacy}","received_events_url":"https://api.github.com/users/khalilRhouma/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/686/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/686/timeline","performed_via_github_app":null,"state_reason":"completed"}