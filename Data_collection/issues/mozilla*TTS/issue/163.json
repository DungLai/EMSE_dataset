{"url":"https://api.github.com/repos/mozilla/TTS/issues/163","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/163/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/163/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/163/events","html_url":"https://github.com/mozilla/TTS/issues/163","id":435587507,"node_id":"MDU6SXNzdWU0MzU1ODc1MDc=","number":163,"title":"Batch synthesis experience","user":{"login":"PPGGG","id":11624203,"node_id":"MDQ6VXNlcjExNjI0MjAz","avatar_url":"https://avatars.githubusercontent.com/u/11624203?v=4","gravatar_id":"","url":"https://api.github.com/users/PPGGG","html_url":"https://github.com/PPGGG","followers_url":"https://api.github.com/users/PPGGG/followers","following_url":"https://api.github.com/users/PPGGG/following{/other_user}","gists_url":"https://api.github.com/users/PPGGG/gists{/gist_id}","starred_url":"https://api.github.com/users/PPGGG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PPGGG/subscriptions","organizations_url":"https://api.github.com/users/PPGGG/orgs","repos_url":"https://api.github.com/users/PPGGG/repos","events_url":"https://api.github.com/users/PPGGG/events{/privacy}","received_events_url":"https://api.github.com/users/PPGGG/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":25,"created_at":"2019-04-22T03:39:42Z","updated_at":"2019-06-03T09:14:17Z","closed_at":"2019-06-03T09:14:16Z","author_association":"NONE","active_lock_reason":null,"body":"I have noticed that the main obstacle in generating audio in batch is to remove the extra generation parts, like if both long sentence and short sentence are inside a batch, the short sentence audio generation would large probably generate extra vibration wav. But if we could intercept the real part of each sentence, then we could solve the problem.\r\n\r\nHere's my solution.\r\n\r\nFirst, we have to enable the model batch interference capability. Cause we would input batch of sentence to the model, therefore the main change is in `layers/tacotron.py`, which is to add `.min()` after `stop_token` and `attention`. From \r\n\r\n```\r\nif memory is not None:\r\n    if t >= T_decoder:\r\n        break\r\nelse:\r\n    if t > inputs.shape[1] / 4 and (stop_token > 0.6 or\r\n                                    attention[:, -1].item() > 0.6):\r\n        break\r\n    elif t > self.max_decoder_steps:\r\n        print(\"   | > Decoder stopped with 'max_decoder_steps\")\r\n        break\r\n```\r\n\r\nTo \r\n```\r\nif memory is not None:\r\n    if t >= T_decoder:\r\n        break\r\nelse:\r\n    if t > inputs.shape[1] / 4 and (stop_token.min() > 0.6 or\r\n                                    attention[:, -1].min() > 0.6):\r\n        break\r\n    elif t > self.max_decoder_steps:\r\n        print(\"   | > Decoder stopped with 'max_decoder_steps\")\r\n        break\r\n```\r\n\r\nSecond, we have to get the real generation parts of each sentence, i noticed that the relationship between `linear dimension` and `stop token dimension`  of each sentence is nearly `linear_dim == 2*stop_dim`. Therefore, we could get the nearly result using below `_get_output_lengths` function.\r\n\r\n```\r\ndef get_linear_target_index(self, stop_token):\r\n\r\n        index_pools = (torch.gt(stop_token, 0.6) == 1).view(-1)\r\n        index_pools = np.where(index_pools.data.cpu().numpy() == 1)[0]\r\n        for order, num in enumerate(index_pools):\r\n            if len(index_pools) == 1 : return index_pools[0] * 2\r\n            elif index_pools[-1] - index_pools[0] + 1 == len(index_pools):\r\n                return index_pools[-2] * 2\r\n            elif index_pools[order] + 1 == index_pools[order+1]:\r\n                continue\r\n            else:\r\n                return num*2\r\n\r\ndef _get_output_lengths(self, stop_tokens):\r\n\r\n    output_lengths = [self.get_linear_target_index(stop_token) for stop_token in stop_tokens]\r\n    return output_lengths\r\n```\r\n\r\nFinally, after knowing the real `linear spectrogram dimension`, we could now achieve batch synthesis using `TTS`, without losing any audio quality. I have tested yet that batch synthesis would have a great speed improvement in long sentence, which is split into several parts as a list then feed to the model. While in short sentence, batch synthesis may not effect a lot, which is much the same as a whole sentence as input.","closed_by":{"login":"erogol","id":1402048,"node_id":"MDQ6VXNlcjE0MDIwNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1402048?v=4","gravatar_id":"","url":"https://api.github.com/users/erogol","html_url":"https://github.com/erogol","followers_url":"https://api.github.com/users/erogol/followers","following_url":"https://api.github.com/users/erogol/following{/other_user}","gists_url":"https://api.github.com/users/erogol/gists{/gist_id}","starred_url":"https://api.github.com/users/erogol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/erogol/subscriptions","organizations_url":"https://api.github.com/users/erogol/orgs","repos_url":"https://api.github.com/users/erogol/repos","events_url":"https://api.github.com/users/erogol/events{/privacy}","received_events_url":"https://api.github.com/users/erogol/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/163/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/163/timeline","performed_via_github_app":null,"state_reason":"completed"}