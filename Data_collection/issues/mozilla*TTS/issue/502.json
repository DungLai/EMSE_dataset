{"url":"https://api.github.com/repos/mozilla/TTS/issues/502","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/502/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/502/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/502/events","html_url":"https://github.com/mozilla/TTS/issues/502","id":681565932,"node_id":"MDU6SXNzdWU2ODE1NjU5MzI=","number":502,"title":"RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 9.59 GiB already allocated; 4.25 MiB free; 9.78 GiB reserved in total by PyTorch)","user":{"login":"andrenatal","id":973388,"node_id":"MDQ6VXNlcjk3MzM4OA==","avatar_url":"https://avatars.githubusercontent.com/u/973388?v=4","gravatar_id":"","url":"https://api.github.com/users/andrenatal","html_url":"https://github.com/andrenatal","followers_url":"https://api.github.com/users/andrenatal/followers","following_url":"https://api.github.com/users/andrenatal/following{/other_user}","gists_url":"https://api.github.com/users/andrenatal/gists{/gist_id}","starred_url":"https://api.github.com/users/andrenatal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andrenatal/subscriptions","organizations_url":"https://api.github.com/users/andrenatal/orgs","repos_url":"https://api.github.com/users/andrenatal/repos","events_url":"https://api.github.com/users/andrenatal/events{/privacy}","received_events_url":"https://api.github.com/users/andrenatal/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2020-08-19T05:49:49Z","updated_at":"2020-08-26T05:52:31Z","closed_at":"2020-08-26T05:48:54Z","author_association":"MEMBER","active_lock_reason":null,"body":"\r\nI keep getting this error when training on different servers using a custom dataset. I got it when using: \r\n\r\n- `distributed_tts.py` on 4,3 and 2 RTX 2080TI \r\n- `train_tts.py` on 1 RTX 2080TI \r\n- `distributed_tts.py` on 1 GTX 1070\r\n- `train_tts.py` on 1 GTX 1070\r\n\r\nI managed to train in a different server on 1 GTX 1080 until completion.\r\n\r\nBelow is the stack. When I'm using `distributed_tts.py`, I get the error but the script does not exit and the other process remain running on the GPUs:\r\n\r\n```\r\n\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 208...  On   | 00000000:19:00.0 Off |                  N/A |\r\n| 28%   27C    P8    22W / 250W |     12MiB / 11019MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  GeForce RTX 208...  On   | 00000000:1A:00.0 Off |                  N/A |\r\n| 58%   88C    P2    71W / 250W |  10915MiB / 11019MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   2  GeForce RTX 208...  On   | 00000000:67:00.0 Off |                  N/A |\r\n| 52%   84C    P2   102W / 250W |  10991MiB / 11019MiB |    100%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   3  GeForce RTX 208...  On   | 00000000:68:00.0 Off |                  N/A |\r\n| 31%   47C    P8    25W / 250W |     12MiB / 11018MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n\r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n|    1     27800      C   python3                                    10903MiB |\r\n|    2     27801      C   python3                                    10979MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\n\r\n```\r\n\r\n\r\n > TRAINING (2020-08-19 05:42:26)\r\n/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/distributed/di\r\nstributed_c10d.py:125: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp\r\ninstead\r\n  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\r\n\r\n   --> STEP: 10/13 -- GLOBAL_STEP: 25\r\n     | > decoder_loss: 11.49612  (11.66776)\r\n     | > postnet_loss: 12.56934  (12.94062)\r\n     | > stopnet_loss: 0.68627  (0.70399)\r\n     | > decoder_coarse_loss: 11.49253  (11.67952)\r\n     | > decoder_ddc_loss: 0.00083  (0.00142)\r\n     | > ga_loss: 0.02466  (0.04069)\r\n     | > loss: 35.61561  (36.34546)\r\n     | > align_error: 0.99221  (0.98766)\r\n     | > avg_spec_length: 856.9\r\n     | > avg_text_length: 127.1\r\n     | > step_time: 2.0197\r\n     | > loader_time: 0.02\r\n     | > current_lr: 0.0001\r\nTraceback (most recent call last):\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 714, in <module>\r\n    main(args)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 626, in main\r\n    global_step, epoch, amp, speaker_mapping)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 171, in train\r\n    text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/mod\r\nules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/models/tacotron2.py\", line 136, in forward\r\n    decoder_outputs_backward, alignments_backward = self._coarse_decoder_pass(mel_specs, encoder_outputs, alignments,\r\n input_mask)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/models/tacotron_abstract.py\", line 156, in _coarse_decoder_pas\r\ns\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/layers/tacotron2.py\", line 326, in forward\r\n    decoder_output, attention_weights, stop_token = self.decode(memory)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/layers/tacotron2.py\", line 282, in decode\r\n    decoder_rnn_input, (self.decoder_hidden, self.decoder_cell))\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/rnn.py\", line 969, in forward\r\n    self.bias_ih, self.bias_hh,\r\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 3; 10.76 GiB total capacity; 9.57 GiB already allocated; 2.94 MiB free; 9.77 GiB reserved in total by PyTorch)\r\n ! Run is kept in /media/disk2/corpora/perola/outputs_treino/perola-August-19-2020_05+40AM-3424181\r\nTraceback (most recent call last):\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 714, in <module>\r\n    main(args)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 626, in main\r\n    global_step, epoch, amp, speaker_mapping)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/bin/train_tts.py\", line 171, in train\r\n    text_input, text_lengths, mel_input, mel_lengths, speaker_ids=speaker_ids, speaker_embeddings=speaker_embeddings)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/models/tacotron2.py\", line 136, in forward\r\n    decoder_outputs_backward, alignments_backward = self._coarse_decoder_pass(mel_specs, encoder_outputs, alignments, input_mask)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/models/tacotron_abstract.py\", line 156, in _coarse_decoder_pass\r\n    encoder_outputs.detach(), mel_specs, input_mask)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/layers/tacotron2.py\", line 326, in forward\r\n    decoder_output, attention_weights, stop_token = self.decode(memory)\r\n  File \"/media/disk2/engines/TTS/mozilla_voice_tts/tts/layers/tacotron2.py\", line 293, in decode\r\n    stop_token = self.stopnet(stopnet_input.detach())\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/container.py\", line 117, in forward\r\n    input = module(input)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/module.py\", line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/modules/dropout.py\", line 58, in forward\r\n    return F.dropout(input, self.p, self.training, self.inplace)\r\n  File \"/media/disk2/engines/TTS/tts_venv/lib/python3.6/site-packages/torch-1.6.0-py3.6-linux-x86_64.egg/torch/nn/functional.py\", line 973, in dropout\r\n    else _VF.dropout(input, p, training))\r\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.76 GiB total capacity; 9.68 GiB already allocated; 4.25 MiB free; 9.77 GiB reserved in total by PyTorch)\r\n```\r\n\r\n\r\n\r\n","closed_by":{"login":"andrenatal","id":973388,"node_id":"MDQ6VXNlcjk3MzM4OA==","avatar_url":"https://avatars.githubusercontent.com/u/973388?v=4","gravatar_id":"","url":"https://api.github.com/users/andrenatal","html_url":"https://github.com/andrenatal","followers_url":"https://api.github.com/users/andrenatal/followers","following_url":"https://api.github.com/users/andrenatal/following{/other_user}","gists_url":"https://api.github.com/users/andrenatal/gists{/gist_id}","starred_url":"https://api.github.com/users/andrenatal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andrenatal/subscriptions","organizations_url":"https://api.github.com/users/andrenatal/orgs","repos_url":"https://api.github.com/users/andrenatal/repos","events_url":"https://api.github.com/users/andrenatal/events{/privacy}","received_events_url":"https://api.github.com/users/andrenatal/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/502/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/502/timeline","performed_via_github_app":null,"state_reason":"completed"}