{"url":"https://api.github.com/repos/mozilla/TTS/issues/329","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/329/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/329/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/329/events","html_url":"https://github.com/mozilla/TTS/issues/329","id":542779354,"node_id":"MDU6SXNzdWU1NDI3NzkzNTQ=","number":329,"title":"high stop loss on LJSpeech dataset with TTS","user":{"login":"PPGGG","id":11624203,"node_id":"MDQ6VXNlcjExNjI0MjAz","avatar_url":"https://avatars.githubusercontent.com/u/11624203?v=4","gravatar_id":"","url":"https://api.github.com/users/PPGGG","html_url":"https://github.com/PPGGG","followers_url":"https://api.github.com/users/PPGGG/followers","following_url":"https://api.github.com/users/PPGGG/following{/other_user}","gists_url":"https://api.github.com/users/PPGGG/gists{/gist_id}","starred_url":"https://api.github.com/users/PPGGG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PPGGG/subscriptions","organizations_url":"https://api.github.com/users/PPGGG/orgs","repos_url":"https://api.github.com/users/PPGGG/repos","events_url":"https://api.github.com/users/PPGGG/events{/privacy}","received_events_url":"https://api.github.com/users/PPGGG/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-12-27T07:27:47Z","updated_at":"2019-12-31T16:04:12Z","closed_at":"2019-12-30T12:41:10Z","author_association":"NONE","active_lock_reason":null,"body":"I am trying Tacotron2 based on LJSpeech dataset and try to reproduced the result that `erogol` did.\r\nBut both two experiments did not work well, both of two seem to have high stop loss.\r\n\r\nThe first experiment main config is below \r\n```\r\n{\r\n\"github_branch\":\"* master\",\r\n        \"run_name\": \"tacotron2_test11\",\r\n        \"run_description\": \"using forward attention, with original prenet, loss masking,separate stopnet, sigmoid. Compare this with 4817. Pytorch DPP\",\r\n    \r\n        \"audio\":{\r\n            // Audio processing parameters\r\n            \"num_mels\": 80,         // size of the mel spec frame. \r\n            \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\r\n            \"sample_rate\": 22050,   // DATASET-RELATED: wav sample-rate. If different than the original data, it is resampled.\r\n            \"frame_length_ms\": 50,  // stft window length in ms.\r\n            \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\r\n            \"preemphasis\": 0.98,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n            \"min_level_db\": -100,   // normalization range\r\n            \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\r\n            \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\r\n            \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\r\n            // Normalization parameters\r\n            \"signal_norm\": true,    // normalize the spec values in range [0, 1]\r\n            \"symmetric_norm\": false, // move normalization to range [-1, 1]\r\n            \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n            \"clip_norm\": true,      // clip normalized values into the range.\r\n            \"mel_fmin\": 0.0,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n            \"mel_fmax\": 8000.0,        // maximum freq level for mel-spec. Tune for dataset!!\r\n            \"do_trim_silence\": true  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\r\n        },\r\n    \r\n        \"distributed\":{\r\n            \"backend\": \"nccl\",\r\n            \"url\": \"tcp:\\/\\/localhost:54321\"\r\n        },\r\n    \r\n        \"reinit_layers\": [],\r\n    \r\n        \"model\": \"Tacotron2\",          // one of the model in models/    \r\n        \"grad_clip\": 1,                // upper limit for gradients for clipping.\r\n        \"epochs\": 1000,                // total number of epochs to train.\r\n        \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n        \"lr_decay\": false,             // if true, Noam learning rate decaying is applied through training.\r\n        \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\r\n        \"memory_size\": 5,              // ONLY TACOTRON - memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \r\n        \"attention_norm\": \"sigmoid\",   // softmax or sigmoid. Suggested to use softmax for Tacotron2 and sigmoid for Tacotron.\r\n        \"prenet_type\": \"original\",     // \"original\" or \"bn\".\r\n        \"prenet_dropout\": true,        // enable/disable dropout at prenet.\r\n        \"use_forward_attn\": true,      // enable/disable forward attention. In general, it aligns faster.\r\n        \"forward_attn_mask\": false,    // Apply forward attention mask af inference to prevent bad modes. Try it if your model does not align well.\r\n        \"transition_agent\": true,     // enable/disable transition agent of forward attention.\r\n        \"location_attn\": false,        // enable_disable location sensitive attention. It is enabled for TACOTRON by default.\r\n        \"loss_masking\": true,          // enable / disable loss masking against the sequence padding.\r\n        \"enable_eos_bos_chars\": true, // enable/disable beginning of sentence and end of sentence chars.\r\n        \"stopnet\": true,               // Train stopnet predicting the end of synthesis. \r\n        \"separate_stopnet\": true,      // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\r\n        \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \r\n\r\n        \"windowing\": false,             // Enables attention windowing. Used only in eval mode.\r\n        \"forward_attn_masking\": false,  // Enable forward attention masking which improves attention stability. Use it if network does not work as you like when it is off.        \r\n        \r\n        \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\r\n        \"eval_batch_size\":16,   \r\n        \"r\": 1,                 // Number of frames to predict for step.\r\n\t\"gradual_training\": null,\r\n        \"wd\": 0.000001,         // Weight decay weight.\r\n        \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n        \"save_step\": 1000,      // Number of training steps expected to save traning stats and checkpoints.\r\n        \"print_step\": 10,       // Number of steps to log traning on console.\r\n        \"batch_group_size\": 0,  //Number of batches to shuffle after bucketing.\r\n    \r\n        \"run_eval\": true,\r\n        \"test_delay_epochs\": 5,  //Until attention is aligned, testing only wastes computation time.\r\n        \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\r\n        //\"data_path\": \"/home/jovyan/zengruihong-volume-1/peter/tts_data/LJSpeech-1.1/\",  // DATASET-RELATED: can overwritten from command argument\r\n        //\"meta_file_train\": \"metadata_train.csv\",      // DATASET-RELATED: metafile for training dataloader.\r\n        //\"meta_file_val\": \"metadata_val.csv\",    // DATASET-RELATED: metafile for evaluation dataloader.\r\n        //\"dataset\": \"ljspeech\",      // DATASET-RELATED: one of TTS.dataset.preprocessors depending on your target dataset. Use \"tts_cache\" for pre-computed dataset by extract_features.py\r\n        \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\r\n        \"max_seq_len\": 300,     // DATASET-RELATED: maximum text length\r\n        \"output_path\": \"taco_output/LJ_tacotron2_output/\",      // DATASET-RELATED: output path for all training outputs.\r\n        \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n        \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\r\n        \"phoneme_cache_path\": \"mozilla_us_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\r\n        \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\r\n        \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\r\n        \"text_cleaner\": \"phoneme_cleaners\",\r\n        \"use_speaker_embedding\": false, // whether to use additional embeddings for separate speakers\r\n\t\"use_gst\": false,\r\n\t\"datasets\":   // List of datasets. They all merged and they get different speaker_ids.\r\n        [\r\n            {\r\n                \"name\": \"ljspeech\",\r\n                \"path\": \"/tts_data/LJSpeech-1.1/\",\r\n                \"meta_file_train\": \"metadata_train.csv\",\r\n                \"meta_file_val\": \"metadata_val.csv\"\r\n            }\r\n        ]\r\n\r\n\r\n    }\r\n```\r\nThe results are below\r\n![Screenshot 2019-12-27 at 3 18 30 PM](https://user-images.githubusercontent.com/11624203/71506680-e8a03e80-28bc-11ea-94a2-0c120f43544e.png)\r\n![Screenshot 2019-12-27 at 3 18 26 PM](https://user-images.githubusercontent.com/11624203/71506682-e8a03e80-28bc-11ea-8c58-d4e6e2da0ae3.png)\r\n\r\nThe second experiment main config is below \r\n```\r\n{\r\n\"github_branch\":\"* master\",\r\n        \"run_name\": \"tacotron2_test12\",\r\n        \"run_description\": \"using forward attention, with original prenet, loss masking,separate stopnet, sigmoid. Compare this with 4817. Pytorch DPP\",\r\n    \r\n        \"audio\":{\r\n            // Audio processing parameters\r\n            \"num_mels\": 80,         // size of the mel spec frame. \r\n            \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\r\n            \"sample_rate\": 22050,   // DATASET-RELATED: wav sample-rate. If different than the original data, it is resampled.\r\n            \"frame_length_ms\": 50,  // stft window length in ms.\r\n            \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\r\n            \"preemphasis\": 0.98,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n            \"min_level_db\": -100,   // normalization range\r\n            \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\r\n            \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\r\n            \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\r\n            // Normalization parameters\r\n            \"signal_norm\": true,    // normalize the spec values in range [0, 1]\r\n            \"symmetric_norm\": false, // move normalization to range [-1, 1]\r\n            \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n            \"clip_norm\": true,      // clip normalized values into the range.\r\n            \"mel_fmin\": 0.0,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n            \"mel_fmax\": 8000.0,        // maximum freq level for mel-spec. Tune for dataset!!\r\n            \"do_trim_silence\": true  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\r\n        },\r\n    \r\n        \"distributed\":{\r\n            \"backend\": \"nccl\",\r\n            \"url\": \"tcp:\\/\\/localhost:54321\"\r\n        },\r\n    \r\n        \"reinit_layers\": [],\r\n    \r\n        \"model\": \"Tacotron2\",          // one of the model in models/    \r\n        \"grad_clip\": 1,                // upper limit for gradients for clipping.\r\n        \"epochs\": 1000,                // total number of epochs to train.\r\n        \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n        \"lr_decay\": false,             // if true, Noam learning rate decaying is applied through training.\r\n        \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\r\n        \"memory_size\": 5,              // ONLY TACOTRON - memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \r\n        \"attention_norm\": \"softmax\",   // softmax or sigmoid. Suggested to use softmax for Tacotron2 and sigmoid for Tacotron.\r\n        \"prenet_type\": \"original\",     // \"original\" or \"bn\".\r\n        \"prenet_dropout\": true,        // enable/disable dropout at prenet.\r\n        \"use_forward_attn\": true,      // enable/disable forward attention. In general, it aligns faster.\r\n        \"forward_attn_mask\": true,    // Apply forward attention mask af inference to prevent bad modes. Try it if your model does not align well.\r\n        \"transition_agent\": false,     // enable/disable transition agent of forward attention.\r\n        \"location_attn\": false,        // enable_disable location sensitive attention. It is enabled for TACOTRON by default.\r\n        \"loss_masking\": true,          // enable / disable loss masking against the sequence padding.\r\n        \"enable_eos_bos_chars\": false, // enable/disable beginning of sentence and end of sentence chars.\r\n        \"stopnet\": true,               // Train stopnet predicting the end of synthesis. \r\n        \"separate_stopnet\": true,      // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\r\n        \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \r\n\r\n        \"windowing\": false,             // Enables attention windowing. Used only in eval mode.\r\n        \"forward_attn_masking\": false,  // Enable forward attention masking which improves attention stability. Use it if network does not work as you like when it is off.        \r\n        \r\n        \"batch_size\": 80,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\r\n        \"eval_batch_size\":16,   \r\n        \"r\": 1,                 // Number of frames to predict for step.\r\n\t\"gradual_training\": null,\r\n        \"wd\": 0.000001,         // Weight decay weight.\r\n        \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n        \"save_step\": 1000,      // Number of training steps expected to save traning stats and checkpoints.\r\n        \"print_step\": 10,       // Number of steps to log traning on console.\r\n        \"batch_group_size\": 0,  //Number of batches to shuffle after bucketing.\r\n    \r\n        \"run_eval\": true,\r\n        \"test_delay_epochs\": 5,  //Until attention is aligned, testing only wastes computation time.\r\n        \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\r\n        //\"data_path\": \"/home/jovyan/zengruihong-volume-1/peter/tts_data/LJSpeech-1.1/\",  // DATASET-RELATED: can overwritten from command argument\r\n        //\"meta_file_train\": \"metadata_train.csv\",      // DATASET-RELATED: metafile for training dataloader.\r\n        //\"meta_file_val\": \"metadata_val.csv\",    // DATASET-RELATED: metafile for evaluation dataloader.\r\n        //\"dataset\": \"ljspeech\",      // DATASET-RELATED: one of TTS.dataset.preprocessors depending on your target dataset. Use \"tts_cache\" for pre-computed dataset by extract_features.py\r\n        \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\r\n        \"max_seq_len\": 300,     // DATASET-RELATED: maximum text length\r\n        \"output_path\": \"taco_output/LJ_tacotron2_output/\",      // DATASET-RELATED: output path for all training outputs.\r\n        \"num_loader_workers\": 3,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n        \"num_val_loader_workers\": 3,    // number of evaluation data loader processes.\r\n        \"phoneme_cache_path\": \"mozilla_us_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\r\n        \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\r\n        \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\r\n        \"text_cleaner\": \"phoneme_cleaners\",\r\n        \"use_speaker_embedding\": false, // whether to use additional embeddings for separate speakers\r\n\t\"use_gst\": false,\r\n\t\"datasets\":   // List of datasets. They all merged and they get different speaker_ids.\r\n        [\r\n            {\r\n                \"name\": \"ljspeech\",\r\n                \"path\": \"/home/jovyan/data1/peter/tts_data/LJSpeech-1.1/\",\r\n                \"meta_file_train\": \"metadata_train.csv\",\r\n                \"meta_file_val\": \"metadata_val.csv\"\r\n            }\r\n        ]\r\n    }\r\n```\r\n![Screenshot 2019-12-27 at 3 26 23 PM](https://user-images.githubusercontent.com/11624203/71506786-4cc30280-28bd-11ea-91e2-7d6647e7b47a.png)\r\n![Screenshot 2019-12-27 at 3 26 19 PM](https://user-images.githubusercontent.com/11624203/71506787-4d5b9900-28bd-11ea-9f57-2c81f33342b5.png)\r\n\r\nBoth two experiments have been trained almost 10 days.","closed_by":{"login":"erogol","id":1402048,"node_id":"MDQ6VXNlcjE0MDIwNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1402048?v=4","gravatar_id":"","url":"https://api.github.com/users/erogol","html_url":"https://github.com/erogol","followers_url":"https://api.github.com/users/erogol/followers","following_url":"https://api.github.com/users/erogol/following{/other_user}","gists_url":"https://api.github.com/users/erogol/gists{/gist_id}","starred_url":"https://api.github.com/users/erogol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/erogol/subscriptions","organizations_url":"https://api.github.com/users/erogol/orgs","repos_url":"https://api.github.com/users/erogol/repos","events_url":"https://api.github.com/users/erogol/events{/privacy}","received_events_url":"https://api.github.com/users/erogol/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/329/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/329/timeline","performed_via_github_app":null,"state_reason":"completed"}