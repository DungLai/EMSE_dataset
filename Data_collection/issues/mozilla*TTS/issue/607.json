{"url":"https://api.github.com/repos/mozilla/TTS/issues/607","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/607/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/607/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/607/events","html_url":"https://github.com/mozilla/TTS/issues/607","id":774815595,"node_id":"MDU6SXNzdWU3NzQ4MTU1OTU=","number":607,"title":"Vocoder can't adapt to TTS Model","user":{"login":"Wangzhen-kris","id":37279265,"node_id":"MDQ6VXNlcjM3Mjc5MjY1","avatar_url":"https://avatars.githubusercontent.com/u/37279265?v=4","gravatar_id":"","url":"https://api.github.com/users/Wangzhen-kris","html_url":"https://github.com/Wangzhen-kris","followers_url":"https://api.github.com/users/Wangzhen-kris/followers","following_url":"https://api.github.com/users/Wangzhen-kris/following{/other_user}","gists_url":"https://api.github.com/users/Wangzhen-kris/gists{/gist_id}","starred_url":"https://api.github.com/users/Wangzhen-kris/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Wangzhen-kris/subscriptions","organizations_url":"https://api.github.com/users/Wangzhen-kris/orgs","repos_url":"https://api.github.com/users/Wangzhen-kris/repos","events_url":"https://api.github.com/users/Wangzhen-kris/events{/privacy}","received_events_url":"https://api.github.com/users/Wangzhen-kris/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":17,"created_at":"2020-12-26T03:32:36Z","updated_at":"2021-01-25T10:15:27Z","closed_at":"2021-01-25T10:15:27Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\nI tried to train taco2 and vocoder separately and I can't synthesize effective voices with the pre-trained vocoder. I'm very strange that use griffin lim, the synthesized voice seems to be pretty good.\r\nHere synthesized voice with WaveGrad and GL:\r\nhttps://drive.google.com/file/d/1fAf7SGdnWfODQsCgVYBFd2l1g0ZWy1fA/view?usp=sharing\r\nhttps://drive.google.com/file/d/1AVXXRXarlZQ2pmYmg-pI-oUNu9Nx2IHP/view?usp=sharing\r\n\r\nHere config.json of Taco2:\r\n`\r\n{\r\n    \"model\": \"Tacotron2\",\r\n    \"run_name\": \"ayq\",\r\n    \"run_description\": \"tacotron2 with DDC and differential spectral loss.\",\r\n\r\n    // AUDIO PARAMETERS\r\n    \"audio\":{\r\n        // stft parameters\r\n        \"fft_size\": 1024,         // number of stft frequency levels. Size of the linear spectogram frame.\r\n        \"win_length\": 1024,      // stft window length in ms.\r\n        \"hop_length\": 256,       // stft window hop-lengh in ms.\r\n        \"frame_length_ms\": null, // stft window length in ms.If null, 'win_length' is used.\r\n        \"frame_shift_ms\": null,  // stft window hop-lengh in ms. If null, 'hop_length' is used.\r\n\r\n        // Audio processing parameters\r\n        \"sample_rate\": 22050,   // DATASET-RELATED: wav sample-rate.\r\n        \"preemphasis\": 0.0,     // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n        \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\r\n\r\n        // Silence trimming\r\n        \"do_trim_silence\": false,// enable trimming of slience of audio as you load it. LJspeech (true), TWEB (false), Nancy (true)\r\n        \"trim_db\": 60,          // threshold for timming silence. Set this according to your dataset.\r\n\r\n        // Griffin-Lim\r\n        \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\r\n        \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\r\n\r\n        // MelSpectrogram parameters\r\n        \"num_mels\": 80,         // size of the mel spec frame.\r\n        \"mel_fmin\": 50.0,        // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n        \"mel_fmax\": 7600.0,     // maximum freq level for mel-spec. Tune for dataset!!\r\n        \"spec_gain\": 1,\r\n\r\n        // Normalization parameters\r\n        \"signal_norm\": true,    // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise range normalization defined by the other params.\r\n        \"min_level_db\": -100,   // lower bound for normalization\r\n        \"symmetric_norm\": true, // move normalization to range [-1, 1]\r\n        \"max_norm\": 4.0,        // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n        \"clip_norm\": true,      // clip normalized values into the range.\r\n        \"stats_path\": null    // DO NOT USE WITH MULTI_SPEAKER MODEL. scaler stats file computed by 'compute_statistics.py'. If it is defined, mean-std based notmalization is used and other normalization params are ignored\r\n    },\r\n\r\n    // VOCABULARY PARAMETERS\r\n    // if custom character set is not defined,\r\n    // default set in symbols.py is used\r\n    // \"characters\":{\r\n    //     \"pad\": \"_\",\r\n    //     \"eos\": \"~\",\r\n    //     \"bos\": \"^\",\r\n    //     \"characters\": \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!'(),-.:;? \",\r\n    //     \"punctuations\":\"!'(),-.:;? \",\r\n    //     \"phonemes\":\"iyɨʉɯuɪʏʊeøɘəɵɤoɛœɜɞʌɔæɐaɶɑɒᵻʘɓǀɗǃʄǂɠǁʛpbtdʈɖcɟkɡqɢʔɴŋɲɳnɱmʙrʀⱱɾɽɸβfvθðszʃʒʂʐçʝxɣχʁħʕhɦɬɮʋɹɻjɰlɭʎʟˈˌːˑʍwɥʜʢʡɕʑɺɧɚ˞ɫ\"\r\n    // },\r\n\r\n    // DISTRIBUTED TRAINING\r\n    \"distributed\":{\r\n        \"backend\": \"nccl\",\r\n        \"url\": \"tcp:\\/\\/localhost:54321\"\r\n    },\r\n\r\n    \"reinit_layers\": [],    // give a list of layer names to restore from the given checkpoint. If not defined, it reloads all heuristically matching layers.\r\n\r\n    // TRAINING\r\n    \"batch_size\": 32,       // Batch size for training. Lower values than 32 might cause hard to learn attention. It is overwritten by 'gradual_training'.\r\n    \"eval_batch_size\":16,\r\n    \"r\": 7,                 // Number of decoder frames to predict per iteration. Set the initial values if gradual training is enabled.\r\n    \"gradual_training\": null,   //[[0, 7, 64], [1, 5, 64], [50000, 3, 32], [130000, 2, 32], [290000, 1, 32]], //set gradual training steps [first_step, r, batch_size]. If it is null, gradual training is disabled. For Tacotron, you might need to reduce the 'batch_size' as you proceeed.\r\n    \"mixed_precision\": false,     // level of optimization with NVIDIA's apex feature for automatic mixed FP16/FP32 precision (AMP), NOTE: currently only O1 is supported, and use \"O1\" to activate.\r\n\r\n    // LOSS SETTINGS\r\n    \"loss_masking\": true,       // enable / disable loss masking against the sequence padding.\r\n    \"decoder_loss_alpha\": 0.5,  // original decoder loss weight. If > 0, it is enabled\r\n    \"postnet_loss_alpha\": 0.25, // original postnet loss weight. If > 0, it is enabled\r\n    \"postnet_diff_spec_alpha\": 0.25,     // differential spectral loss weight. If > 0, it is enabled\r\n    \"decoder_diff_spec_alpha\": 0.25,     // differential spectral loss weight. If > 0, it is enabled\r\n    \"decoder_ssim_alpha\": 0.5,     // decoder ssim loss weight. If > 0, it is enabled\r\n    \"postnet_ssim_alpha\": 0.25,     // postnet ssim loss weight. If > 0, it is enabled\r\n    \"ga_alpha\": 5.0,           // weight for guided attention loss. If > 0, guided attention is enabled.\r\n    \"stopnet_pos_weight\": 15.0, // pos class weight for stopnet loss since there are way more negative samples than positive samples.\r\n\r\n\r\n    // VALIDATION\r\n    \"run_eval\": true,\r\n    \"test_delay_epochs\": 10,  //Until attention is aligned, testing only wastes computation time.\r\n    \"test_sentences_file\": null,  // set a file to load sentences to be used for testing. If it is null then we use default english sentences.\r\n\r\n    // OPTIMIZER\r\n    \"noam_schedule\": false,        // use noam warmup and lr schedule.\r\n    \"grad_clip\": 1.0,              // upper limit for gradients for clipping.\r\n    \"epochs\": 100000,                // total number of epochs to train.\r\n    \"lr\": 0.0001,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n    \"wd\": 0.000001,                // Weight decay weight.\r\n    \"warmup_steps\": 4000,          // Noam decay steps to increase the learning rate from 0 to \"lr\"\r\n    \"seq_len_norm\": false,         // Normalize eash sample loss with its length to alleviate imbalanced datasets. Use it if your dataset is small or has skewed distribution of sequence lengths.\r\n\r\n    // TACOTRON PRENET\r\n    \"memory_size\": -1,             // ONLY TACOTRON - size of the memory queue used fro storing last decoder predictions for auto-regression. If < 0, memory queue is disabled and decoder only uses the last prediction frame.\r\n    \"prenet_type\": \"original\",     // \"original\" or \"bn\".\r\n    \"prenet_dropout\": false,       // enable/disable dropout at prenet.\r\n\r\n    // TACOTRON ATTENTION\r\n    \"attention_type\": \"original\",  // 'original' or 'graves'\r\n    \"attention_heads\": 4,          // number of attention heads (only for 'graves')\r\n    \"attention_norm\": \"sigmoid\",   // softmax or sigmoid.\r\n    \"windowing\": false,            // Enables attention windowing. Used only in eval mode.\r\n    \"use_forward_attn\": false,     // if it uses forward attention. In general, it aligns faster.\r\n    \"forward_attn_mask\": false,    // Additional masking forcing monotonicity only in eval mode.\r\n    \"transition_agent\": false,     // enable/disable transition agent of forward attention.\r\n    \"location_attn\": true,         // enable_disable location sensitive attention. It is enabled for TACOTRON by default.\r\n    \"bidirectional_decoder\": false,  // use https://arxiv.org/abs/1907.09006. Use it, if attention does not work well with your dataset.\r\n    \"double_decoder_consistency\": true,  // use DDC explained here https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency-draft/\r\n    \"ddc_r\": 7,                           // reduction rate for coarse decoder.\r\n\r\n    // STOPNET\r\n    \"stopnet\": true,               // Train stopnet predicting the end of synthesis.\r\n    \"separate_stopnet\": true,      // Train stopnet seperately if 'stopnet==true'. It prevents stopnet loss to influence the rest of the model. It causes a better model, but it trains SLOWER.\r\n\r\n    // TENSORBOARD and LOGGING\r\n    \"print_step\": 25,       // Number of steps to log training on console.\r\n    \"tb_plot_step\": 100,    // Number of steps to plot TB training figures.\r\n    \"print_eval\": false,     // If True, it prints intermediate loss values in evalulation.\r\n    \"save_step\": 10000,      // Number of training steps expected to save traninpg stats and checkpoints.\r\n    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n    \"tb_model_param_stats\": false,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging.\r\n\r\n    // DATA LOADING\r\n    \"text_cleaner\": \"basic_cleaners\",\r\n    \"enable_eos_bos_chars\": false, // enable/disable beginning of sentence and end of sentence chars.\r\n    \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\r\n    \"batch_group_size\": 4,  //Number of batches to shuffle after bucketing.\r\n    \"min_seq_len\": 6,       // DATASET-RELATED: minimum text length to use in training\r\n    \"max_seq_len\": 220,     // DATASET-RELATED: maximum text length\r\n\r\n    // PATHS\r\n    \"output_path\": \"/data1/mozillawz/result_aqy\",\r\n\r\n    // PHONEMES\r\n    \"phoneme_cache_path\": \"/data1/mozillawz/mozilla_pinyin/Phoneme_cache/\",  // phoneme computation is slow, therefore, it caches results in the given folder.\r\n    \"use_phonemes\": false,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\r\n    \"phoneme_language\": \"en-us\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\r\n\r\n    // MULTI-SPEAKER and GST\r\n    \"use_speaker_embedding\": true,      // use speaker embedding to enable multi-speaker learning.\r\n    \"use_gst\": false,       \t\t\t    // use global style tokens\r\n    \"use_external_speaker_embedding_file\": false, // if true, forces the model to use external embedding per sample instead of nn.embeddings, that is, it supports external embeddings such as those used at: https://arxiv.org/abs /1806.04558\r\n    \"external_speaker_embedding_file\": \"../../speakers-vctk-en.json\", // if not null and use_external_speaker_embedding_file is true, it is used to load a specific embedding file and thus uses these embeddings instead of nn.embeddings, that is, it supports external embeddings such as those used at: https://arxiv.org/abs /1806.04558\r\n    \"gst\":\t{\t\t\t                // gst parameter if gst is enabled\r\n        \"gst_style_input\": null,        // Condition the style input either on a\r\n                                        // -> wave file [path to wave] or\r\n                                        // -> dictionary using the style tokens {'token1': 'value', 'token2': 'value'} example {\"0\": 0.15, \"1\": 0.15, \"5\": -0.15}\r\n                                        // with the dictionary being len(dict) <= len(gst_style_tokens).\r\n        \"gst_embedding_dim\": 512,\r\n        \"gst_num_heads\": 4,\r\n        \"gst_style_tokens\": 10,\r\n        \"gst_use_speaker_embedding\": false\r\n\t},\r\n\r\n    // DATASETS\r\n    \"datasets\":   // List of datasets. They all merged and they get different speaker_ids.\r\n        [\r\n            {\r\n                \"name\": \"own\",\r\n                \"path\": \"/data1/mozillawz/\",\r\n                \"meta_file_train\": \"train_aqy.txt\", // for vtck if list, ignore speakers id in list for train, its useful for test cloning with new speakers\r\n                \"meta_file_val\": \"val_aqy.txt\"\r\n            }\r\n        ]\r\n}\r\n`\r\n\r\nHere vocoder_config.json of WaveGrad:\r\n`\r\n{\r\n    \"run_name\": \"wavegrad-aqy\",\r\n    \"run_description\": \"wavegrad aqy\",\r\n\r\n    \"audio\":{\r\n        \"fft_size\": 1024,         // number of stft frequency levels. Size of the linear spectogram frame.\r\n        \"win_length\": 1024,      // stft window length in ms.\r\n        \"hop_length\": 256,       // stft window hop-lengh in ms.\r\n        \"frame_length_ms\": null, // stft window length in ms.If null, 'win_length' is used.\r\n        \"frame_shift_ms\": null,  // stft window hop-lengh in ms. If null, 'hop_length' is used.\r\n\r\n        // Audio processing parameters\r\n        \"sample_rate\": 22050,   // DATASET-RELATED: wav sample-rate. If different than the original data, it is resampled.\r\n        \"preemphasis\": 0.0,     // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n        \"ref_level_db\": 0,     // reference level db, theoretically 20db is the sound of air.\r\n\r\n        // Silence trimming\r\n        \"do_trim_silence\": false,// enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\r\n        \"trim_db\": 60,          // threshold for timming silence. Set this according to your dataset.\r\n\r\n        // MelSpectrogram parameters\r\n        \"num_mels\": 80,         // size of the mel spec frame.\r\n        \"mel_fmin\": 50.0,        // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n        \"mel_fmax\": 7600.0,     // maximum freq level for mel-spec. Tune for dataset!!\r\n        \"spec_gain\": 1.0,         // scaler value appplied after log transform of spectrogram.\r\n\r\n        // Normalization parameters\r\n        \"signal_norm\": true,    // normalize spec values. Mean-Var normalization if 'stats_path' is defined otherwise range normalization defined by the other params.\r\n        \"min_level_db\": -100,   // lower bound for normalization\r\n        \"symmetric_norm\": true, // move normalization to range [-1, 1]\r\n        \"max_norm\": 4.0,        // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n        \"clip_norm\": true,      // clip normalized values into the range.\r\n        \"stats_path\": \"\"    // DO NOT USE WITH MULTI_SPEAKER MODEL. scaler stats file computed by 'compute_statistics.py'. If it is defined, mean-std based notmalization is used and other normalization params are ignored\r\n    },\r\n\r\n    // DISTRIBUTED TRAINING\r\n    \"mixed_precision\": true,     // enable torch mixed precision training (true, false)\r\n    \"distributed\":{\r\n        \"backend\": \"nccl\",\r\n        \"url\": \"tcp:\\/\\/localhost:54322\"\r\n    },\r\n\r\n    \"target_loss\": \"avg_wavegrad_loss\",  // loss value to pick the best model to save after each epoch\r\n\r\n    // MODEL PARAMETERS\r\n    \"generator_model\": \"wavegrad\",\r\n    \"model_params\":{\r\n        \"use_weight_norm\": true,\r\n        \"y_conv_channels\":32,\r\n        \"x_conv_channels\":768,\r\n        \"ublock_out_channels\": [512, 512, 256, 128, 128],\r\n        \"dblock_out_channels\": [128, 128, 256, 512],\r\n        \"upsample_factors\": [4, 4, 4, 2, 2],\r\n        \"upsample_dilations\": [\r\n            [1, 2, 1, 2],\r\n            [1, 2, 1, 2],\r\n            [1, 2, 4, 8],\r\n            [1, 2, 4, 8],\r\n            [1, 2, 4, 8]]\r\n    },\r\n\r\n    // DATASET\r\n    \"data_path\": \"/data1/wangzhen/data_aishell/AISHELL_and_Originbeat/\",  // root data path. It finds all wav files recursively from there.\r\n    \"feature_path\": null,   // if you use precomputed features\r\n    \"seq_len\": 6144,        // 24 * hop_length\r\n    \"pad_short\": 0,      // additional padding for short wavs\r\n    \"conv_pad\": 0,          // additional padding against convolutions applied to spectrograms\r\n    \"use_noise_augment\": false,     // add noise to the audio signal for augmentation\r\n    \"use_cache\": false,      // use in memory cache to keep the computed features. This might cause OOM.\r\n\r\n    \"reinit_layers\": [],    // give a list of layer names to restore from the given checkpoint. If not defined, it reloads all heuristically matching layers.\r\n\r\n    // TRAINING\r\n    \"batch_size\": 64,      // Batch size for training.\r\n\r\n    // NOISE SCHEDULE PARAMS - Only effective at training time.\r\n    \"train_noise_schedule\":{\r\n        \"min_val\": 1e-6,\r\n        \"max_val\": 1e-2,\r\n        \"num_steps\": 1000\r\n    },\r\n    \"test_noise_schedule\":{\r\n        \"min_val\": 1e-6,\r\n        \"max_val\": 1e-2,\r\n        \"num_steps\": 50\r\n    },\r\n\r\n    // VALIDATION\r\n    \"run_eval\": true,       // enable/disable evaluation run\r\n\r\n    // OPTIMIZER\r\n    \"epochs\": 1000000,                // total number of epochs to train.\r\n    \"clip_grad\": 1.0,                 // Generator gradient clipping threshold. Apply gradient clipping if > 0\r\n    \"lr_scheduler\": \"MultiStepLR\",  // one of the schedulers from https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\r\n    \"lr_scheduler_params\": {\r\n        \"gamma\": 0.5,\r\n        \"milestones\": [100000, 200000, 300000, 400000, 500000, 600000]\r\n    },\r\n    \"lr\": 1e-4,                  // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n\r\n    // TENSORBOARD and LOGGING\r\n    \"print_step\": 50,       // Number of steps to log traning on console.\r\n    \"print_eval\": false,     // If True, it prints loss values for each step in eval run.\r\n    \"save_step\": 5000,      // Number of training steps expected to plot training stats on TB and save model checkpoints.\r\n    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n    \"tb_model_param_stats\": true,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging.\r\n\r\n    // DATA LOADING\r\n    \"num_loader_workers\": 4,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\r\n    \"eval_split_size\": 256,\r\n\r\n    // PATHS\r\n    \"output_path\": \"/data1/mozillawz/wavegrad_models/\"\r\n}\r\n`\r\n","closed_by":{"login":"Wangzhen-kris","id":37279265,"node_id":"MDQ6VXNlcjM3Mjc5MjY1","avatar_url":"https://avatars.githubusercontent.com/u/37279265?v=4","gravatar_id":"","url":"https://api.github.com/users/Wangzhen-kris","html_url":"https://github.com/Wangzhen-kris","followers_url":"https://api.github.com/users/Wangzhen-kris/followers","following_url":"https://api.github.com/users/Wangzhen-kris/following{/other_user}","gists_url":"https://api.github.com/users/Wangzhen-kris/gists{/gist_id}","starred_url":"https://api.github.com/users/Wangzhen-kris/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Wangzhen-kris/subscriptions","organizations_url":"https://api.github.com/users/Wangzhen-kris/orgs","repos_url":"https://api.github.com/users/Wangzhen-kris/repos","events_url":"https://api.github.com/users/Wangzhen-kris/events{/privacy}","received_events_url":"https://api.github.com/users/Wangzhen-kris/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/607/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/607/timeline","performed_via_github_app":null,"state_reason":"completed"}