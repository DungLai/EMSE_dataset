{"url":"https://api.github.com/repos/mozilla/TTS/issues/143","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/143/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/143/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/143/events","html_url":"https://github.com/mozilla/TTS/issues/143","id":427245634,"node_id":"MDU6SXNzdWU0MjcyNDU2MzQ=","number":143,"title":"Try to convert TTS to onnx format ","user":{"login":"PPGGG","id":11624203,"node_id":"MDQ6VXNlcjExNjI0MjAz","avatar_url":"https://avatars.githubusercontent.com/u/11624203?v=4","gravatar_id":"","url":"https://api.github.com/users/PPGGG","html_url":"https://github.com/PPGGG","followers_url":"https://api.github.com/users/PPGGG/followers","following_url":"https://api.github.com/users/PPGGG/following{/other_user}","gists_url":"https://api.github.com/users/PPGGG/gists{/gist_id}","starred_url":"https://api.github.com/users/PPGGG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PPGGG/subscriptions","organizations_url":"https://api.github.com/users/PPGGG/orgs","repos_url":"https://api.github.com/users/PPGGG/repos","events_url":"https://api.github.com/users/PPGGG/events{/privacy}","received_events_url":"https://api.github.com/users/PPGGG/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-03-30T05:28:04Z","updated_at":"2019-05-02T04:32:00Z","closed_at":"2019-04-05T14:12:51Z","author_association":"NONE","active_lock_reason":null,"body":"I am trying to deploy `TTS` using `onnx` and `Tensorflow serving`.  Like\r\n\r\n```\r\nmodel_path = 'best_model.pth.tar'\r\nmodel_config = 'config.json'\r\n\r\nuse_cuda = False\r\n\r\nconfig = load_config(model_config)\r\nap = AudioProcessor(**config.audio)\r\n\r\ninput_adapter = lambda sen: text_to_sequence(sen, [config.text_cleaner])\r\ninput_size = len(symbols)\r\n\r\ntest_model = Tacotron(input_size, config.embedding_size, ap.num_freq, ap.num_mels, config.r)\r\nif use_cuda:\r\n    cp = torch.load(model_path)\r\nelse:\r\n    cp = torch.load(model_path, map_location=lambda storage, loc: storage)\r\ntest_model.load_state_dict(cp['model'])\r\nif use_cuda:\r\n    test_model.cuda()\r\ntest_model.eval()\r\n\r\ndummpy_input = torch.ones(1, MAX_LEN, dtype=torch.long)\r\n\r\ntorch.onnx.export(test_model, args=dummpy_input, f='test.onnx', verbose=True)\r\n```\r\n\r\nIt shown me an error like \r\n\r\n```\r\n/home/data/peter/TTS/layers/tacotron.py:209: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  if x.size(-1) == self.in_features:\r\n/home/data/peter/TTS/layers/tacotron.py:219: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\r\n  assert x.size(1) == self.conv_bank_features * len(self.conv1d_banks)\r\n/home/data/peter/TTS/layers/tacotron.py:226: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator add_. This might cause the trace to be incorrect, because all other views that also reference this data will not not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\r\n  x += inputs\r\n/home/data/peter/TTS/layers/tacotron.py:363: TracerWarning: new_zeros is a legacy constructor and is not supported in the JIT.\r\n  initial_memory = self.memory_init(inputs.data.new_zeros(B).long())\r\n/home/data/peter/TTS/layers/tacotron.py:366: TracerWarning: new_zeros is a legacy constructor and is not supported in the JIT.\r\n  attention_rnn_hidden = self.attention_rnn_init(inputs.data.new_zeros(B).long())\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-46-fae7353f3a28> in <module>()\r\n----> 1 torch.onnx.export(test_model, args=dummpy_input, f='test.onnx', verbose=True)\r\n      2 \r\n      3 # model = test_model\r\n      4 # # text_input = torch.from_numpy(seq).unsqueeze(0).long()\r\n      5 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/__init__.py in export(*args, **kwargs)\r\n     25 def export(*args, **kwargs):\r\n     26     from torch.onnx import utils\r\n---> 27     return utils.export(*args, **kwargs)\r\n     28 \r\n     29 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in export(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type)\r\n    102             operator_export_type = OperatorExportTypes.ONNX\r\n    103     _export(model, args, f, export_params, verbose, training, input_names, output_names,\r\n--> 104             operator_export_type=operator_export_type)\r\n    105 \r\n    106 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _export(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, propagate)\r\n    279                                                training, input_names,\r\n    280                                                output_names, operator_export_type,\r\n--> 281                                                example_outputs, propagate)\r\n    282 \r\n    283     # TODO: Don't allocate a in-memory string for the protobuf\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _model_to_graph(model, args, f, verbose, training, input_names, output_names, operator_export_type, example_outputs, propagate)\r\n    222             raise RuntimeError('\\'forward\\' method must be a script method')\r\n    223     else:\r\n--> 224         graph, torch_out = _trace_and_get_graph_from_model(model, args, training)\r\n    225         params = list(_unique_state_dict(model).values())\r\n    226 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/onnx/utils.py in _trace_and_get_graph_from_model(model, args, training)\r\n    190     # training mode was.)\r\n    191     with set_training(model, training):\r\n--> 192         trace, torch_out = torch.jit.get_trace_graph(model, args, _force_outplace=True)\r\n    193 \r\n    194     if orig_state_dict_keys != _unique_state_dict(model).keys():\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py in get_trace_graph(f, args, kwargs, _force_outplace)\r\n    195     if not isinstance(args, tuple):\r\n    196         args = (args,)\r\n--> 197     return LegacyTracedModule(f, _force_outplace)(*args, **kwargs)\r\n    198 \r\n    199 \r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n--> 489             result = self.forward(*input, **kwargs)\r\n    490         for hook in self._forward_hooks.values():\r\n    491             hook_result = hook(self, input, result)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/jit/__init__.py in forward(self, *args)\r\n    250         try:\r\n    251             trace_inputs = _unflatten(all_trace_inputs[:len(in_vars)], in_desc)\r\n--> 252             out = self.inner(*trace_inputs)\r\n    253             out_vars, _ = _flatten(out)\r\n    254             torch._C._tracer_exit(tuple(out_vars))\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    485             hook(self, input)\r\n    486         if torch._C._get_tracing_state():\r\n--> 487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n    489             result = self.forward(*input, **kwargs)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    475         tracing_state._traced_module_stack.append(self)\r\n    476         try:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         finally:\r\n    479             tracing_state.pop_scope()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/models/tacotron.py in forward(self, characters, mel_specs, mask)\r\n     37         # batch x time x dim*r\r\n     38         mel_outputs, alignments, stop_tokens = self.decoder(\r\n---> 39             encoder_outputs, mel_specs, mask)\r\n     40         # Reshape\r\n     41         # batch x time x dim\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    485             hook(self, input)\r\n    486         if torch._C._get_tracing_state():\r\n--> 487             result = self._slow_forward(*input, **kwargs)\r\n    488         else:\r\n    489             result = self.forward(*input, **kwargs)\r\n\r\n/data/zeng_ruihong/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py in _slow_forward(self, *input, **kwargs)\r\n    475         tracing_state._traced_module_stack.append(self)\r\n    476         try:\r\n--> 477             result = self.forward(*input, **kwargs)\r\n    478         finally:\r\n    479             tracing_state.pop_scope()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in forward(self, inputs, memory, mask)\r\n    404         t = 0\r\n    405         memory_input, attention_rnn_hidden, decoder_rnn_hiddens,\\\r\n--> 406             current_context_vec, attention, attention_cum = self._init_states(inputs)\r\n    407         while True:\r\n    408             if t > 0:\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in _init_states(self, inputs)\r\n    367         decoder_rnn_hiddens = [\r\n    368             self.decoder_rnn_inits(inputs.data.new_tensor([idx]*B).long())\r\n--> 369             for idx in range(len(self.decoder_rnns))\r\n    370         ]\r\n    371         current_context_vec = inputs.data.new(B, self.in_features).zero_()\r\n\r\n/data/zeng_ruihong/tts_test_hupo_v3/TTS/layers/tacotron.py in <listcomp>(.0)\r\n    367         decoder_rnn_hiddens = [\r\n    368             self.decoder_rnn_inits(inputs.data.new_tensor([idx]*B).long())\r\n--> 369             for idx in range(len(self.decoder_rnns))\r\n    370         ]\r\n    371         current_context_vec = inputs.data.new(B, self.in_features).zero_()\r\n\r\nTypeError: mul(): argument 'other' (position 1) must be Tensor, not list\r\n```\r\n\r\nHope someone could give me a hint. Thanks a lot.","closed_by":{"login":"PPGGG","id":11624203,"node_id":"MDQ6VXNlcjExNjI0MjAz","avatar_url":"https://avatars.githubusercontent.com/u/11624203?v=4","gravatar_id":"","url":"https://api.github.com/users/PPGGG","html_url":"https://github.com/PPGGG","followers_url":"https://api.github.com/users/PPGGG/followers","following_url":"https://api.github.com/users/PPGGG/following{/other_user}","gists_url":"https://api.github.com/users/PPGGG/gists{/gist_id}","starred_url":"https://api.github.com/users/PPGGG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PPGGG/subscriptions","organizations_url":"https://api.github.com/users/PPGGG/orgs","repos_url":"https://api.github.com/users/PPGGG/repos","events_url":"https://api.github.com/users/PPGGG/events{/privacy}","received_events_url":"https://api.github.com/users/PPGGG/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/143/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/143/timeline","performed_via_github_app":null,"state_reason":"completed"}