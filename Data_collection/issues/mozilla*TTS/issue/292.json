{"url":"https://api.github.com/repos/mozilla/TTS/issues/292","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/292/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/292/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/292/events","html_url":"https://github.com/mozilla/TTS/issues/292","id":500101086,"node_id":"MDU6SXNzdWU1MDAxMDEwODY=","number":292,"title":"CUDA OOM while indeed there is plenty of memory left!","user":{"login":"vcjob","id":51916323,"node_id":"MDQ6VXNlcjUxOTE2MzIz","avatar_url":"https://avatars.githubusercontent.com/u/51916323?v=4","gravatar_id":"","url":"https://api.github.com/users/vcjob","html_url":"https://github.com/vcjob","followers_url":"https://api.github.com/users/vcjob/followers","following_url":"https://api.github.com/users/vcjob/following{/other_user}","gists_url":"https://api.github.com/users/vcjob/gists{/gist_id}","starred_url":"https://api.github.com/users/vcjob/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vcjob/subscriptions","organizations_url":"https://api.github.com/users/vcjob/orgs","repos_url":"https://api.github.com/users/vcjob/repos","events_url":"https://api.github.com/users/vcjob/events{/privacy}","received_events_url":"https://api.github.com/users/vcjob/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-09-30T07:27:46Z","updated_at":"2019-09-30T08:44:34Z","closed_at":"2019-09-30T08:44:34Z","author_association":"NONE","active_lock_reason":null,"body":"Hello everyone!\r\n\r\nI try to train Tacotron2 Model. First tried with dev branch, now with master. Closely to end of the 1st epoch I got error:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 647, in <module>\r\n    main(args)\r\n  File \"train.py\", line 554, in main\r\n    ap, global_step, epoch)\r\n  File \"train.py\", line 153, in train\r\n    text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/models/tacotron2.py\", line 54, in forward\r\n    mel_outputs_postnet = self.postnet(mel_outputs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/layers/tacotron2.py\", line 45, in forward\r\n    x = layer(x)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/TTS-0.0.1+53d658f-py3.6.egg/TTS/layers/tacotron2.py\", line 27, in forward\r\n    output = self.net(x)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 92, in forward\r\n    input = module(input)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/opt/mozilla-tts/lib/python3.6/site-packages/torch/nn/modules/activation.py\", line 292, in forward\r\n    return torch.tanh(input)\r\nRuntimeError: CUDA out of memory. Tried to allocate 51.62 MiB (GPU 0; 10.73 GiB total capacity; 6.36 GiB already allocated; 7.25 MiB free; 137.09 MiB cached)\r\n\r\nThe thing is, no other processes use GPU, except for the only one python3 with training. Server has no GUI, just console. \r\nOnly 6.36 gb were used, and python3 could not allocate 51.65 Mb extra? (Its RTX2080ti with 10.9 GB).\r\nWhat could it be? Driver issues? \r\nCUDA 10.0. NVIDIA_SMI 410.93\r\nThank you!","closed_by":{"login":"vcjob","id":51916323,"node_id":"MDQ6VXNlcjUxOTE2MzIz","avatar_url":"https://avatars.githubusercontent.com/u/51916323?v=4","gravatar_id":"","url":"https://api.github.com/users/vcjob","html_url":"https://github.com/vcjob","followers_url":"https://api.github.com/users/vcjob/followers","following_url":"https://api.github.com/users/vcjob/following{/other_user}","gists_url":"https://api.github.com/users/vcjob/gists{/gist_id}","starred_url":"https://api.github.com/users/vcjob/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vcjob/subscriptions","organizations_url":"https://api.github.com/users/vcjob/orgs","repos_url":"https://api.github.com/users/vcjob/repos","events_url":"https://api.github.com/users/vcjob/events{/privacy}","received_events_url":"https://api.github.com/users/vcjob/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/292/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/292/timeline","performed_via_github_app":null,"state_reason":"completed"}