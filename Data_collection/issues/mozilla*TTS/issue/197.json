{"url":"https://api.github.com/repos/mozilla/TTS/issues/197","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/197/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/197/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/197/events","html_url":"https://github.com/mozilla/TTS/issues/197","id":444577029,"node_id":"MDU6SXNzdWU0NDQ1NzcwMjk=","number":197,"title":"Inference at fp16","user":{"login":"mrgloom","id":4003908,"node_id":"MDQ6VXNlcjQwMDM5MDg=","avatar_url":"https://avatars.githubusercontent.com/u/4003908?v=4","gravatar_id":"","url":"https://api.github.com/users/mrgloom","html_url":"https://github.com/mrgloom","followers_url":"https://api.github.com/users/mrgloom/followers","following_url":"https://api.github.com/users/mrgloom/following{/other_user}","gists_url":"https://api.github.com/users/mrgloom/gists{/gist_id}","starred_url":"https://api.github.com/users/mrgloom/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mrgloom/subscriptions","organizations_url":"https://api.github.com/users/mrgloom/orgs","repos_url":"https://api.github.com/users/mrgloom/repos","events_url":"https://api.github.com/users/mrgloom/events{/privacy}","received_events_url":"https://api.github.com/users/mrgloom/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-05-15T18:15:52Z","updated_at":"2019-07-11T13:42:17Z","closed_at":"2019-07-11T13:41:27Z","author_association":"NONE","active_lock_reason":null,"body":"Is it possible to use fp16 during tacotron2 inference?\r\n\r\nI have tried to load model like:\r\n\r\n```\r\ndef load_tocotron_2_model():\r\n    from utils.text.symbols import symbols, phonemes\r\n    from models.tacotron2 import Tacotron2\r\n\r\n    n_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols) # 'use_phonemes': True\r\n\r\n    model = Tacotron2(num_chars=n_chars, r=CONFIG.r, attn_win=CONFIG.windowing, attn_norm=CONFIG.attention_norm,\r\n                      prenet_type=CONFIG.prenet_type, forward_attn=CONFIG.use_forward_attn,\r\n                      trans_agent=CONFIG.transition_agent)\r\n\r\n    if use_cuda:\r\n        cp = torch.load(MODEL_PATH)\r\n    else:\r\n        cp = torch.load(MODEL_PATH, map_location='cpu')\r\n\r\n    model.load_state_dict(cp['model'])\r\n\r\n    if use_cuda:\r\n        model.cuda()\r\n\r\n    model.eval()\r\n\r\n    if use_fp16:\r\n        model.half()\r\n\r\n    return model\r\n```\r\n\r\nBut get error:\r\n\r\n```\r\nRuntimeError: _th_index_select is not implemented for type torch.HalfTensor\r\n\r\n  File \"/Users/my_user/external_projects/text-to-speech/mozila-tts-wavernn/TTS/models/tacotron2.py\", line 43, in inference\r\n    embedded_inputs = self.embedding(text).transpose(1, 2)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/modules/sparse.py\", line 118, in forward\r\n    self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n  File \"/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\", line 1454, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\n```\r\n\r\nDoes it mean that `nn.Embedding` layer is not supported for fp16 precision ?\r\nhttps://github.com/mozilla/TTS/blob/dev-tacotron2/models/tacotron2.py#L43\r\nhttps://github.com/mozilla/TTS/blob/dev-tacotron2/models/tacotron2.py#L16\r\n\r\nOn `python -c \"import torch; print(torch.__version__)\"\r\n1.1.0.dev20190514` I get\r\n`RuntimeError: _th_index_select not supported on CPUType for Half`\r\n\r\nAlso what is the reason that `chars_var.long()` is used here?\r\nhttps://github.com/mozilla/TTS/blob/dev-tacotron2/utils/synthesis.py#L43\r\nLooks like it's int64\r\nhttps://pytorch.org/docs/stable/tensors.html#torch.Tensor.long\r\n\r\nIf `nn.Embedding` layer is not supported is it possible to do inference on part of graph in fp16?","closed_by":{"login":"erogol","id":1402048,"node_id":"MDQ6VXNlcjE0MDIwNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1402048?v=4","gravatar_id":"","url":"https://api.github.com/users/erogol","html_url":"https://github.com/erogol","followers_url":"https://api.github.com/users/erogol/followers","following_url":"https://api.github.com/users/erogol/following{/other_user}","gists_url":"https://api.github.com/users/erogol/gists{/gist_id}","starred_url":"https://api.github.com/users/erogol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/erogol/subscriptions","organizations_url":"https://api.github.com/users/erogol/orgs","repos_url":"https://api.github.com/users/erogol/repos","events_url":"https://api.github.com/users/erogol/events{/privacy}","received_events_url":"https://api.github.com/users/erogol/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/197/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/197/timeline","performed_via_github_app":null,"state_reason":"completed"}