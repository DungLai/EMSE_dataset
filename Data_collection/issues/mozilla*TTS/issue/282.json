{"url":"https://api.github.com/repos/mozilla/TTS/issues/282","repository_url":"https://api.github.com/repos/mozilla/TTS","labels_url":"https://api.github.com/repos/mozilla/TTS/issues/282/labels{/name}","comments_url":"https://api.github.com/repos/mozilla/TTS/issues/282/comments","events_url":"https://api.github.com/repos/mozilla/TTS/issues/282/events","html_url":"https://github.com/mozilla/TTS/issues/282","id":493898003,"node_id":"MDU6SXNzdWU0OTM4OTgwMDM=","number":282,"title":"Warning: NaN or Inf found in input tensor","user":{"login":"duman","id":22964050,"node_id":"MDQ6VXNlcjIyOTY0MDUw","avatar_url":"https://avatars.githubusercontent.com/u/22964050?v=4","gravatar_id":"","url":"https://api.github.com/users/duman","html_url":"https://github.com/duman","followers_url":"https://api.github.com/users/duman/followers","following_url":"https://api.github.com/users/duman/following{/other_user}","gists_url":"https://api.github.com/users/duman/gists{/gist_id}","starred_url":"https://api.github.com/users/duman/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/duman/subscriptions","organizations_url":"https://api.github.com/users/duman/orgs","repos_url":"https://api.github.com/users/duman/repos","events_url":"https://api.github.com/users/duman/events{/privacy}","received_events_url":"https://api.github.com/users/duman/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-09-16T07:54:55Z","updated_at":"2019-09-16T10:31:35Z","closed_at":"2019-09-16T10:25:12Z","author_association":"NONE","active_lock_reason":null,"body":"I've recorded my own voice, and tried to train it. I'm using a GTX1080Ti and a 1080 over distribute.py\r\n\r\nLogs looks like the below:\r\n```\r\ntnt@GpuServer:~/tts-turkish-gpu/TTS$ python3 distribute.py --config_path config.json\r\n > Git Hash: 0179c74\r\n > Experiment folder: /home/tnt/tts-turkish-gpu/TURKISH/model_output/ljspeech-September-16-2019_10+43AM-0179c74\r\n['train.py', '--restore_path=', '--config_path=config.json', '--group_id=group_2019_09_16-104338', '--data_path=', '--output_path=/home/tnt/tts-turkish-gpu/TURKISH/model_output/ljspeech-September-16-2019_10+43AM-0179c74', '--rank=0']\r\n['train.py', '--restore_path=', '--config_path=config.json', '--group_id=group_2019_09_16-104338', '--data_path=', '--output_path=/home/tnt/tts-turkish-gpu/TURKISH/model_output/ljspeech-September-16-2019_10+43AM-0179c74', '--rank=1']\r\n > Using CUDA:  True\r\n > Number of GPUs:  2\r\n > Setting up Audio Processor...\r\n | > bits:None\r\n | > sample_rate:22050\r\n | > num_mels:80\r\n | > min_level_db:-100\r\n | > frame_shift_ms:12.5\r\n | > frame_length_ms:50\r\n | > ref_level_db:20\r\n | > num_freq:1025\r\n | > power:1.5\r\n | > preemphasis:0.98\r\n | > griffin_lim_iters:60\r\n | > signal_norm:True\r\n | > symmetric_norm:False\r\n | > mel_fmin:0.0\r\n | > mel_fmax:8000.0\r\n | > max_norm:1.0\r\n | > clip_norm:True\r\n | > do_trim_silence:True\r\n | > n_fft:2048\r\n | > hop_length:275\r\n | > win_length:1102\r\n > Using model: Tacotron2\r\n | > Num output units : 1025\r\n\r\n > Model has 28151842 parameters\r\n\r\n > DataLoader initialization\r\n | > Data path: /home/tnt/tts-turkish-gpu/TURKISH\r\n | > Use phonemes: True\r\n   | > phoneme language: tr\r\n | > Cached dataset: False\r\n | > Number of instances : 16\r\n | > Max length sequence: 221\r\n | > Min length sequence: 52\r\n | > Avg length sequence: 109.4375\r\n | > Num. instances discarded by max-min seq limits: 2\r\n | > Batch group size: 128.\r\n\r\n > Epoch 0/1000\r\n/home/tnt/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\r\n  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\r\n/home/tnt/.local/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py:100: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\r\n  warnings.warn(\"torch.distributed.reduce_op is deprecated, please use \"\r\n   | > EPOCH END -- GlobalStep:1  AvgTotalLoss:1.42835  AvgPostnetLoss:0.63763  AvgDecoderLoss:0.09636  AvgStopLoss:0.69435  EpochTime:13.14  AvgStepTime:13.14\r\nWarning: NaN or Inf found in input tensor.\r\nWarning: NaN or Inf found in input tensor.\r\n\r\n > Validation\r\n   | > TotalLoss: 0.93946   PostnetLoss: 0.11113   DecoderLoss:0.11158  StopLoss: 0.71675  \r\n | > Training Loss: 0.63763   Validation Loss: 0.11018\r\n\r\n > BEST MODEL (0.11018) : /home/tnt/tts-turkish-gpu/TURKISH/model_output/ljspeech-September-16-2019_10+43AM-0179c74/best_model.pth.tar\r\n\r\n > Epoch 1/1000\r\n```\r\n\r\nAlso after a certain while a new error (I guess) pops up.\r\n\r\n```\r\n Epoch 6/1000\r\n   | > EPOCH END -- GlobalStep:7  AvgTotalLoss:1.22286  AvgPostnetLoss:0.47923  AvgDecoderLoss:0.03675  AvgStopLoss:0.70688  EpochTime:2.40  AvgStepTime:2.40\r\nWarning: NaN or Inf found in input tensor.\r\nWarning: NaN or Inf found in input tensor.\r\n\r\n > Validation\r\n   | > TotalLoss: 0.90788   PostnetLoss: 0.09696   DecoderLoss:0.09326  StopLoss: 0.71766  \r\n | > Synthesizing test sentences\r\n   | > Decoder stopped with 'max_decoder_steps\r\n   | > Decoder stopped with 'max_decoder_steps\r\n   | > Decoder stopped with 'max_decoder_steps\r\n   | > Decoder stopped with 'max_decoder_steps\r\n | > Training Loss: 0.47923   Validation Loss: 0.09463\r\n```\r\n\r\nMy question is why would \"NaN or Inf found in input tensor.\" and \"Decoder stopped with 'max_decoder_steps\" happen? All of the files are recorded by me, created in the exact format of LJSpeech for Turkish instead. They were stereo 44,100 Hz, I converted them to mono 22,050 Hz and started training like that.\r\n\r\nThis is my config file:\r\n\r\n```\r\n{\r\n\"github_branch\":\"dev-tacotron2\",\r\n\"restore_path\":\"/home/tnt/tts-turkish-gpu/TURKISH/model_output/best_model.pth.tar\",\r\n    \"run_name\": \"ljspeech\",\r\n    \"run_description\": \"finetune 4241 for align with architectural changes\",\r\n\r\n    \"audio\":{\r\n        // Audio processing parameters\r\n        \"num_mels\": 80,         // size of the mel spec frame. \r\n        \"num_freq\": 1025,       // number of stft frequency levels. Size of the linear spectogram frame.\r\n        \"sample_rate\": 22050,   // wav sample-rate. If different than the original data, it is resampled.\r\n        \"frame_length_ms\": 50,  // stft window length in ms.\r\n        \"frame_shift_ms\": 12.5, // stft window hop-lengh in ms.\r\n        \"preemphasis\": 0.98,    // pre-emphasis to reduce spec noise and make it more structured. If 0.0, no -pre-emphasis.\r\n        \"min_level_db\": -100,   // normalization range\r\n        \"ref_level_db\": 20,     // reference level db, theoretically 20db is the sound of air.\r\n        \"power\": 1.5,           // value to sharpen wav signals after GL algorithm.\r\n        \"griffin_lim_iters\": 60,// #griffin-lim iterations. 30-60 is a good range. Larger the value, slower the generation.\r\n        // Normalization parameters\r\n        \"signal_norm\": true,    // normalize the spec values in range [0, 1]\r\n        \"symmetric_norm\": false, // move normalization to range [-1, 1]\r\n        \"max_norm\": 1,          // scale normalization to range [-max_norm, max_norm] or [0, max_norm]\r\n        \"clip_norm\": true,      // clip normalized values into the range.\r\n        \"mel_fmin\": 0.0,         // minimum freq level for mel-spec. ~50 for male and ~95 for female voices. Tune for dataset!!\r\n        \"mel_fmax\": 8000.0,        // maximum freq level for mel-spec. Tune for dataset!!\r\n        \"do_trim_silence\": true  // enable trimming of slience of audio as you load it. LJspeech (false), TWEB (false), Nancy (true)\r\n    },\r\n\r\n    \"distributed\":{\r\n        \"backend\": \"nccl\",\r\n        \"url\": \"tcp:\\/\\/localhost:54321\"\r\n    },\r\n\r\n    \"reinit_layers\": [],  //set which layers to be reinitialized in finetunning. Only used if --restore_model is provided.\r\n\r\n    \"model\": \"Tacotron2\",   // one of the model in models/    \r\n    \"grad_clip\": 1,      // upper limit for gradients for clipping.\r\n    \"epochs\": 1000,         // total number of epochs to train.\r\n    \"lr\": 0.0001,            // Initial learning rate. If Noam decay is active, maximum learning rate.\r\n    \"lr_decay\": false,      // if true, Noam learning rate decaying is applied through training.\r\n    \"warmup_steps\": 4000,   // Noam decay steps to increase the learning rate from 0 to \"lr\"\r\n    \"windowing\": false,      // Enables attention windowing. Used only in eval mode.\r\n    \"memory_size\": 5,       //  ONLY TACOTRON - memory queue size used to queue network predictions to feed autoregressive connection. Useful if r < 5. \r\n    \"attention_norm\": \"softmax\",   // softmax or sigmoid. Suggested to use softmax for Tacotron2 and sigmoid for Tacotron.\r\n    \"prenet_type\": \"bn\",    // ONLY TACOTRON2 - \"original\" or \"bn\".\r\n    \"use_forward_attn\": true,    // ONLY TACOTRON2 - if it uses forward attention. In general, it aligns faster.\r\n    \"transition_agent\": false,    // ONLY TACOTRON2 - enable/disable transition agent of forward attention.\r\n    \"loss_masking\": false,       // enable / disable loss masking against the sequence padding.\r\n    \"enable_eos_bos_chars\": true, // enable/disable beginning of sentence and end of sentence chars.\r\n\r\n    \"batch_size\": 16,       // Batch size for training. Lower values than 32 might cause hard to learn attention.\r\n    \"eval_batch_size\":16,   \r\n    \"r\": 1,                 // Number of frames to predict for step.\r\n    \"wd\": 0.000001,         // Weight decay weight.\r\n    \"checkpoint\": true,     // If true, it saves checkpoints per \"save_step\"\r\n    \"save_step\": 1000,      // Number of training steps expected to save traning stats and checkpoints.\r\n    \"print_step\": 100,      // Number of steps to log traning on console.\r\n    \"tb_model_param_stats\": true,     // true, plots param stats per layer on tensorboard. Might be memory consuming, but good for debugging. \r\n    \"batch_group_size\": 8,  // Number of batches to shuffle after bucketing.\r\n\r\n    \"run_eval\": true,\r\n    \"test_delay_epochs\": 2,  //Until attention is aligned, testing only wastes computation time.\r\n    \"data_path\": \"/home/tnt/tts-turkish-gpu/TURKISH\",  // DATASET-RELATED: can overwritten from command argument\r\n    \"meta_file_train\": \"metadata_train.csv\",      // DATASET-RELATED: metafile for training dataloader.\r\n    \"meta_file_val\": \"metadata_val.csv\",    // DATASET-RELATED: metafile for evaluation dataloader.\r\n    \"dataset\": \"ljspeech\",      // DATASET-RELATED: one of TTS.dataset.preprocessors depending on your target dataset. Use \"tts_cache\" for pre-computed dataset by extract_features.py\r\n    \"min_seq_len\": 0,       // DATASET-RELATED: minimum text length to use in training\r\n    \"max_seq_len\": 150,     // DATASET-RELATED: maximum text length\r\n    \"output_path\": \"/home/tnt/tts-turkish-gpu/TURKISH/model_output/\",      // DATASET-RELATED: output path for all training outputs.\r\n    \"num_loader_workers\": 8,        // number of training data loader processes. Don't set it too big. 4-8 are good values.\r\n    \"num_val_loader_workers\": 4,    // number of evaluation data loader processes.\r\n    \"phoneme_cache_path\": \"ljspeech_phonemes\",  // phoneme computation is slow, therefore, it caches results in the given folder.\r\n    \"use_phonemes\": true,           // use phonemes instead of raw characters. It is suggested for better pronounciation.\r\n    \"phoneme_language\": \"tr\",     // depending on your target language, pick one from  https://github.com/bootphon/phonemizer#languages\r\n    \"text_cleaner\": \"phoneme_cleaners\"\r\n}\r\n```\r\n\r\nPlease do let me know what am I doing wrong here or which logs (if there any) should I look for? Again, thanks for creating this project.","closed_by":{"login":"erogol","id":1402048,"node_id":"MDQ6VXNlcjE0MDIwNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1402048?v=4","gravatar_id":"","url":"https://api.github.com/users/erogol","html_url":"https://github.com/erogol","followers_url":"https://api.github.com/users/erogol/followers","following_url":"https://api.github.com/users/erogol/following{/other_user}","gists_url":"https://api.github.com/users/erogol/gists{/gist_id}","starred_url":"https://api.github.com/users/erogol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/erogol/subscriptions","organizations_url":"https://api.github.com/users/erogol/orgs","repos_url":"https://api.github.com/users/erogol/repos","events_url":"https://api.github.com/users/erogol/events{/privacy}","received_events_url":"https://api.github.com/users/erogol/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/mozilla/TTS/issues/282/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/mozilla/TTS/issues/282/timeline","performed_via_github_app":null,"state_reason":"completed"}