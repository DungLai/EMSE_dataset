{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/996","id":516041754,"node_id":"MDU6SXNzdWU1MTYwNDE3NTQ=","number":996,"title":"How does weight tying work between src_embed and tgt_embed in NMTModel?","user":{"login":"zeeshansayyed","id":543495,"node_id":"MDQ6VXNlcjU0MzQ5NQ==","avatar_url":"https://avatars.githubusercontent.com/u/543495?v=4","gravatar_id":"","url":"https://api.github.com/users/zeeshansayyed","html_url":"https://github.com/zeeshansayyed","followers_url":"https://api.github.com/users/zeeshansayyed/followers","following_url":"https://api.github.com/users/zeeshansayyed/following{/other_user}","gists_url":"https://api.github.com/users/zeeshansayyed/gists{/gist_id}","starred_url":"https://api.github.com/users/zeeshansayyed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zeeshansayyed/subscriptions","organizations_url":"https://api.github.com/users/zeeshansayyed/orgs","repos_url":"https://api.github.com/users/zeeshansayyed/repos","events_url":"https://api.github.com/users/zeeshansayyed/events{/privacy}","received_events_url":"https://api.github.com/users/zeeshansayyed/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-11-01T10:48:20Z","updated_at":"2019-11-08T15:44:47Z","closed_at":"2019-11-02T00:37:30Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, I have a question regarding weight tying in the encoder and decoder embedding matrix of the NMTModel. Consider [these lines](https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/model/translation.py#L84-L130) of the NMTModel class.\r\n\r\nAs has been described in Issue [#7785](https://github.com/apache/incubator-mxnet/issues/7785) of mxnet repo and [this section](https://d2l.ai/chapter_deep-learning-computation/parameters.html#tied-parameters) in d2l.ai, we usually do it by calling the `params` argument of the layer new layer and pass the params of the layer from which we want to tie.\r\n\r\nBut in the NMTModel class referenced above, we are simply assigning the references in line 98 and 101 as `self.src_embed = src_embed` and `self.tgt_embed = self.src_embed`.\r\n\r\n_Is this also a valid way of tying weights?_\r\n\r\nNote: I have also opened an [issue](https://github.com/apache/incubator-mxnet/issues/16684) in the mxnet repo since I didn't know which was the right place to ask. I will close the one which is not needed.\r\n\r\n","closed_by":{"login":"leezu","id":946903,"node_id":"MDQ6VXNlcjk0NjkwMw==","avatar_url":"https://avatars.githubusercontent.com/u/946903?v=4","gravatar_id":"","url":"https://api.github.com/users/leezu","html_url":"https://github.com/leezu","followers_url":"https://api.github.com/users/leezu/followers","following_url":"https://api.github.com/users/leezu/following{/other_user}","gists_url":"https://api.github.com/users/leezu/gists{/gist_id}","starred_url":"https://api.github.com/users/leezu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leezu/subscriptions","organizations_url":"https://api.github.com/users/leezu/orgs","repos_url":"https://api.github.com/users/leezu/repos","events_url":"https://api.github.com/users/leezu/events{/privacy}","received_events_url":"https://api.github.com/users/leezu/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/996/timeline","performed_via_github_app":null,"state_reason":"completed"}