{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/654","id":429439075,"node_id":"MDU6SXNzdWU0Mjk0MzkwNzU=","number":654,"title":"[Discussion] Roadmap","user":{"login":"szha","id":2626883,"node_id":"MDQ6VXNlcjI2MjY4ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2626883?v=4","gravatar_id":"","url":"https://api.github.com/users/szha","html_url":"https://github.com/szha","followers_url":"https://api.github.com/users/szha/followers","following_url":"https://api.github.com/users/szha/following{/other_user}","gists_url":"https://api.github.com/users/szha/gists{/gist_id}","starred_url":"https://api.github.com/users/szha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/szha/subscriptions","organizations_url":"https://api.github.com/users/szha/orgs","repos_url":"https://api.github.com/users/szha/repos","events_url":"https://api.github.com/users/szha/events{/privacy}","received_events_url":"https://api.github.com/users/szha/received_events","type":"User","site_admin":false},"labels":[{"id":890393504,"node_id":"MDU6TGFiZWw4OTAzOTM1MDQ=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/help%20wanted","name":"help wanted","color":"b0f22e","default":true,"description":"Extra attention is needed"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":26,"created_at":"2019-04-04T19:12:38Z","updated_at":"2020-07-19T21:46:38Z","closed_at":"2020-07-19T21:46:38Z","author_association":"MEMBER","active_lock_reason":null,"body":"Hi,\r\n\r\nLet's start a discussion here about the roadmap towards 0.10 and 1.0. We are looking for:\r\n- New features that are useful to your research\r\n- Improvements and patches to existing features\r\n\r\nIf you have any item that you'd like to propose to have in the roadmap, please do:\r\n- Create (or locate existing) issue for the item, note the issue number.\r\n- Comment in this issue: 1) the above issue number, 2) one sentence of what the item is about and why it's useful to you.\r\n- Indication on whether you'd be willing to help out on the item.\r\n\r\n## Features\r\n\r\nThe following features have been proposed to include in GluonNLP 0.7.0 (subject to change)\r\n\r\n### Models\r\n#### Language modeling  \r\n- [ ] ALBERT  https://github.com/dmlc/gluon-nlp/issues/955\r\n- [x] SciBERT pre-trained models (#650)\r\n- [x] Release GluonNLP's pre-trained models (#642)\r\n- [ ] Korean BERT https://github.com/dmlc/gluon-nlp/issues/939\r\n- [x] GPT/GPT-2 training from scratch (#592) [OpenGPT-2](https://medium.com/@vanya_cohen/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc)\r\n- [ ] [SRU](https://arxiv.org/abs/1709.02755) \r\n- [ ] [ERNIE](https://arxiv.org/abs/1905.07129)\r\n- [ ] XLNet training from scratch \r\n- [ ] Transformer-XL training from scratch \r\n- [ ] ALBERT\r\n- [ ] XLM-Roberta\r\n\r\n\r\n#### Word Embedding \r\n- [ ] [Poincaré GloVE Embedding](https://arxiv.org/abs/1810.06546)\r\n- [ ] [ICLR2019] Adaptive Input Representations for Neural Language Modeling \r\n- [ ] [NIPS2017] Poincaré Embeddings for Learning Hierarchical Representations\r\n\r\n#### NER\r\n- [x] BERT for NER (#593 #612)\r\n\r\n#### Memory networks and transformer\r\n- [ ] Augmenting Self-attention with Persistent Memory\r\n- [ ] [Large Memory Layers with Product Keys](https://arxiv.org/pdf/1907.05242.pdf)\r\n- [ ] Adaptive Attention Span in Transformers (https://www.aclweb.org/anthology/P19-1032)\r\n- [ ] Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting (https://arxiv.org/pdf/1907.00235.pdf)\r\n- [ ] related: Hybrid computing using a neural network with dynamic external memory\r\n- [ ] related: Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\r\n\r\n\r\n#### Visualization \r\n- [ ] Visualizing and Measuring the Geometry of BERT\r\n- [ ] Understanding Black-box Predictions via Influence Functions\r\n- [ ] [NeurIPS2019] Visualizing and Measuring the Geometry of BERT (https://arxiv.org/pdf/1906.02715.pdf)\r\n- [ ] [Arxiv2019] Visualizing and Understanding the Effectiveness of BERT (https://arxiv.org/pdf/1908.05620.pdf)\r\n- [ ] [Arxiv2019] What Does BERT Look At? An Analysis of BERT’s Attention (https://arxiv.org/pdf/1906.04341.pdf)\r\n- [ ] related: Bertviz: https://github.com/jessevig/bertviz\r\n\r\n#### Quantization\r\n- [ ] [ICLR2019] Learning Recurrent Binary/Ternary Weights (https://openreview.net/pdf?id=HkNGYjR9FX)\r\n- [ ] [JMLR2018] Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations (http://www.jmlr.org/papers/volume18/16-456/16-456.pdf)\r\n\r\n\r\n\r\n#### Multi-Task/Transfer Learning \r\n- [ ] MT-DNN and multi-task learning (#633)\r\n- [ ] [ICML2019] Parameter-Efficient Transfer Learning for NLP (http://proceedings.mlr.press/v97/houlsby19a.html)\r\n- [ ] [ACL2019] MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension (https://arxiv.org/pdf/1905.13453.pdf)\r\n\r\n#### Machine translation \r\n- [ ] Cross-lingual BERT (#657)\r\n- [ ] [MASS](https://github.com/microsoft/MASS)\r\n- [ ] [ACL2016] Incorporating Copying Mechanism in Sequence-to-Sequence Learning (https://arxiv.org/pdf/1603.06393.pdf)\r\n- [ ] [EMNLP2018] Understanding Back-Translation at Scale (https://aclweb.org/anthology/D18-1045)\r\n- [ ] [ICLR2019] Universal Transformers (https://arxiv.org/pdf/1807.03819.pdf)\r\n- [ ] [ICLR2019] Pay Less Attention With Lightweight and Dynamic Convolutions (https://arxiv.org/pdf/1901.10430.pdf)\r\n- [ ] Document MT: [EMNLP2018] Document-Level Neural Machine Translation with Hierarchical Attention Networks (https://www.aclweb.org/anthology/D18-1325)\r\n\r\n#### Tokenization\r\n- [ ] Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates\r\n\r\n#### Text classification\r\n- [ ] Text-matching models (#616)\r\n- [ ] Region Embedding (#437)\r\n- [ ] Heirarchical Attention Models for Document (HAN)\r\n\r\n#### Topic modeling\r\n- [ ] LDA\r\n\r\n#### Knowledge distillation \r\n-  [ ] [EMNLP2019] Patient Knowledge Distillation for BERT Model Compressions (https://arxiv.org/pdf/1908.09355.pdf)\r\n- [ ] [ICLR2020 Submission] TinyBERT\r\n\r\n\r\n\r\n### APIs\r\n- [ ] fp16 trainer API (#674)\r\n- [x] BERT export API (#659, replaces static BERT scripts)\r\n- [x] LAMB optimizer (#677)\r\n- [x] GLUE datasets (#673)\r\n- [x] Vocab special token registration (#572)\r\n- [x] Make Vocab unknown token index configurable (#393)\r\n- [ ] ROUGE metric (#561)\r\n- [ ] subword-nmt BPE support (#374)\r\n- [ ] vocab look-up with case-insensitive backoff (#367)\r\n- [ ] configurable experiment management \r\n- [ ] natural question dataset\r\n\r\n\r\n### Scripts\r\n- [ ] ELMo feature extraction script (#595)\r\n- [ ] Deep Biaffine Attention Dependency Parser follow-up (#427)\r\n\r\n### Documentation\r\n- [x] Add documentation for return types of datasets and transform functions (#405)\r\n\r\n### Demos\r\n- [ ] Demo application for NER\r\n- [ ] Demo application for text classification\r\n- [ ] [Demo application for question answering](https://demo.allennlp.org/reading-comprehension)\r\n- [ ] Demo application for dependency parsing \r\n- [ ] Demo application for machine translation   \r\n- [ ] Demo application for text generation \r\n\r\ncc @dmlc/gluon-nlp-team \r\n\r\n## Related Projects\r\n- [QA: QANet + BiDAF](https://github.com/dmlc/gluon-nlp/projects/7)\r\n- [NER: BiLSTM + CRF](https://github.com/dmlc/gluon-nlp/projects/2)\r\n- [BERT](https://github.com/dmlc/gluon-nlp/projects/6)\r\n\r\n","closed_by":{"login":"szha","id":2626883,"node_id":"MDQ6VXNlcjI2MjY4ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2626883?v=4","gravatar_id":"","url":"https://api.github.com/users/szha","html_url":"https://github.com/szha","followers_url":"https://api.github.com/users/szha/followers","following_url":"https://api.github.com/users/szha/following{/other_user}","gists_url":"https://api.github.com/users/szha/gists{/gist_id}","starred_url":"https://api.github.com/users/szha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/szha/subscriptions","organizations_url":"https://api.github.com/users/szha/orgs","repos_url":"https://api.github.com/users/szha/repos","events_url":"https://api.github.com/users/szha/events{/privacy}","received_events_url":"https://api.github.com/users/szha/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/654/timeline","performed_via_github_app":null,"state_reason":"completed"}