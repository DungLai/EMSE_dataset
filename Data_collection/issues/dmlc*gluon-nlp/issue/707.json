{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/707","id":444267710,"node_id":"MDU6SXNzdWU0NDQyNjc3MTA=","number":707,"title":"Better multiprocessing dataloader support for large datasets","user":{"login":"eric-haibin-lin","id":5545640,"node_id":"MDQ6VXNlcjU1NDU2NDA=","avatar_url":"https://avatars.githubusercontent.com/u/5545640?v=4","gravatar_id":"","url":"https://api.github.com/users/eric-haibin-lin","html_url":"https://github.com/eric-haibin-lin","followers_url":"https://api.github.com/users/eric-haibin-lin/followers","following_url":"https://api.github.com/users/eric-haibin-lin/following{/other_user}","gists_url":"https://api.github.com/users/eric-haibin-lin/gists{/gist_id}","starred_url":"https://api.github.com/users/eric-haibin-lin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eric-haibin-lin/subscriptions","organizations_url":"https://api.github.com/users/eric-haibin-lin/orgs","repos_url":"https://api.github.com/users/eric-haibin-lin/repos","events_url":"https://api.github.com/users/eric-haibin-lin/events{/privacy}","received_events_url":"https://api.github.com/users/eric-haibin-lin/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"},{"id":963101581,"node_id":"MDU6TGFiZWw5NjMxMDE1ODE=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/discussion","name":"discussion","color":"c5def5","default":false,"description":""}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-05-15T06:56:53Z","updated_at":"2020-10-15T20:16:22Z","closed_at":"2020-10-15T20:16:22Z","author_association":"MEMBER","active_lock_reason":null,"body":"## problem statement\r\nI have a large corpus for training, and ideally I don't want to load them all at once due to limited CPU memory. Currently, the `DatasetStream` API serves this purpose by returning one Dataset at a time. \r\n\r\nHowever, the `DatasetStream.__iter__` API limits the data pipeline performance if the data preprocessing is complicated and computational heavy. For instance, for BERT training, the raw dataset is split into multiple small chunks. and for each of these chunks, we \r\n- read file in memory, perform tokenization, vocabulary lookup\r\n- group sentences into documents according to document boundaries\r\n- for each sentence in a document, generated random next sentence pairs and masks for language modeling\r\n- prepare labels for next sentence prediction and masked language model\r\nThen we have a dataset for samples suitable for BERT training.\r\n\r\nNote that all the steps above happens before a dataset is formed (i.e. `transform()` cannot be used here). Performing all these steps with a single python process is slow. \r\n\r\nNow, the `DatasetStream.__iter__` API returns one dataset at a time, which is a problem if it is used in combination with `PrefetcherIter(worker_type='process')`, as the process-based prefetcher does not allow creation of child processes. \r\n\r\n## walk-arounds\r\n\r\n1. Use `PrefetcherIter(worker_type='thread')`. It causes the training loop to slowdown because of the time sharing between the dataset creation thread and the thread for training the model. \r\n2. Use `PrefetcherIter(worker_type='process_no_daemon')`. It launches a process without daemon mode, where children processes can be created. This solves the speed issue, but at the cost of letting users maintain all these processes. In another word, users have call process.join() before exiting the program, otherwise the program hangs. \r\n\r\n## better solutions?\r\n\r\nI think GluonNLP needs to provide a data loader for a `SimpleDatasetStream` that supports multiprocessing for dataset creation. I picked `SimpleDatasetStream` because it has a complete view of what data url to load, where `DatasetStream` does not provide that visibility. \r\n\r\nFor implementation, the dataloading would happen in hierarchy using two worker pools. The first worker pool is responsible for loading and forming the dataset. The formed dataset is then passed to the second worker pool, where workers generate data batches that user wants. \r\n\r\n@leezu @szha \r\n","closed_by":{"login":"eric-haibin-lin","id":5545640,"node_id":"MDQ6VXNlcjU1NDU2NDA=","avatar_url":"https://avatars.githubusercontent.com/u/5545640?v=4","gravatar_id":"","url":"https://api.github.com/users/eric-haibin-lin","html_url":"https://github.com/eric-haibin-lin","followers_url":"https://api.github.com/users/eric-haibin-lin/followers","following_url":"https://api.github.com/users/eric-haibin-lin/following{/other_user}","gists_url":"https://api.github.com/users/eric-haibin-lin/gists{/gist_id}","starred_url":"https://api.github.com/users/eric-haibin-lin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eric-haibin-lin/subscriptions","organizations_url":"https://api.github.com/users/eric-haibin-lin/orgs","repos_url":"https://api.github.com/users/eric-haibin-lin/repos","events_url":"https://api.github.com/users/eric-haibin-lin/events{/privacy}","received_events_url":"https://api.github.com/users/eric-haibin-lin/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/707/timeline","performed_via_github_app":null,"state_reason":"completed"}