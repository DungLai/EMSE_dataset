{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1039","id":534666460,"node_id":"MDU6SXNzdWU1MzQ2NjY0NjA=","number":1039,"title":"multilingual bert requested","user":{"login":"leolikescoding","id":3581832,"node_id":"MDQ6VXNlcjM1ODE4MzI=","avatar_url":"https://avatars.githubusercontent.com/u/3581832?v=4","gravatar_id":"","url":"https://api.github.com/users/leolikescoding","html_url":"https://github.com/leolikescoding","followers_url":"https://api.github.com/users/leolikescoding/followers","following_url":"https://api.github.com/users/leolikescoding/following{/other_user}","gists_url":"https://api.github.com/users/leolikescoding/gists{/gist_id}","starred_url":"https://api.github.com/users/leolikescoding/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leolikescoding/subscriptions","organizations_url":"https://api.github.com/users/leolikescoding/orgs","repos_url":"https://api.github.com/users/leolikescoding/repos","events_url":"https://api.github.com/users/leolikescoding/events{/privacy}","received_events_url":"https://api.github.com/users/leolikescoding/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-12-09T03:21:14Z","updated_at":"2020-06-01T17:09:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Description\r\n bert model trained with multilingual corpus was released by google.\r\n Basically corpus with different-top used languages are prepared and multilingual word-tokens are \r\nprecessed with BPE(Byte pair encoding) . Below features are requested.\r\n1.multilingual corpus preparing code.\r\n2.multilingual word-tokens precessed of BPE with code.\r\n3.pre-training code of bert with multilingual corpus \r\n4.fine-tuning with multilingual corpus \r\n\r\n## References\r\ngoogle-research use 102 languages to train bert. \r\n\r\n'''\r\nData Source and Sampling\r\nThe languages chosen were the top 100 languages with the largest Wikipedias. The entire Wikipedia dump for each language (excluding user and talk pages) was taken as the training data for each language\r\n\r\nHowever, the size of the Wikipedia for a given language varies greatly, and therefore low-resource languages may be \"under-represented\" in terms of the neural network model (under the assumption that languages are \"competing\" for limited model capacity to some extent). At the same time, we also don't want to overfit the model by performing thousands of epochs over a tiny Wikipedia for a particular language.\r\n\r\nTo balance these two factors, we performed exponentially smoothed weighting of the data during pre-training data creation (and WordPiece vocab creation). In other words, let's say that the probability of a language is P(L), e.g., P(English) = 0.21 means that after concatenating all of the Wikipedias together, 21% of our data is English. We exponentiate each probability by some factor S and then re-normalize, and sample from that distribution. In our case we use S=0.7. So, high-resource languages like English will be under-sampled, and low-resource languages like Icelandic will be over-sampled. E.g., in the original distribution English would be sampled 1000x more than Icelandic, but after smoothing it's only sampled 100x more.\r\n'''\r\n\r\nhttps://github.com/google-research/bert/blob/master/multilingual.md\r\nhttps://github.com/google-research/bert \r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1039/timeline","performed_via_github_app":null,"state_reason":null}