{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1081","id":542309889,"node_id":"MDU6SXNzdWU1NDIzMDk4ODk=","number":1081,"title":"[website] GluonNLP Website Rework","user":{"login":"eric-haibin-lin","id":5545640,"node_id":"MDQ6VXNlcjU1NDU2NDA=","avatar_url":"https://avatars.githubusercontent.com/u/5545640?v=4","gravatar_id":"","url":"https://api.github.com/users/eric-haibin-lin","html_url":"https://github.com/eric-haibin-lin","followers_url":"https://api.github.com/users/eric-haibin-lin/followers","following_url":"https://api.github.com/users/eric-haibin-lin/following{/other_user}","gists_url":"https://api.github.com/users/eric-haibin-lin/gists{/gist_id}","starred_url":"https://api.github.com/users/eric-haibin-lin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eric-haibin-lin/subscriptions","organizations_url":"https://api.github.com/users/eric-haibin-lin/orgs","repos_url":"https://api.github.com/users/eric-haibin-lin/repos","events_url":"https://api.github.com/users/eric-haibin-lin/events{/privacy}","received_events_url":"https://api.github.com/users/eric-haibin-lin/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"},{"id":1440115736,"node_id":"MDU6TGFiZWwxNDQwMTE1NzM2","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/documentation","name":"documentation","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2019-12-25T10:01:16Z","updated_at":"2020-02-10T07:22:09Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Goal: better feature/model discoverability for the GluonNLP website\r\n\r\n\r\n### Side Navigation Bar\r\n\r\n* Installation \r\n* Models \r\n* Tutorials \r\n* Demos (in the future)\r\n* API Documentations\r\n* Community \r\n\r\n\r\n\r\n### Tutorials\r\n\r\nTable Of Content:  \r\n\r\n* Datasets and Vocabularies\r\n  * Data Loading APIs (was http://gluon-nlp.mxnet.io/api/notes/data_api.html)\r\n  * Vocabulary APIs (was https://gluon-nlp.mxnet.io/api/notes/vocab_emb.html)\r\n* Representation Learning\r\n    * Using Pre-trained Word Embedding\r\n    * Word Embedding Training and Evaluation\r\n    * Extracting Sentence Features with Pre-trained ELMo\r\n    * Fine-tuning Pre-trained BERT Model\r\n* Language Modeling\r\n    * Using Pre-trained Language Model (2nd part of https://gluon-nlp.mxnet.io/examples/language_model/language_model.html)\r\n    * Train your own LSTM based Language Model (1st part of https://gluon-nlp.mxnet.io/examples/language_model/language_model.html)\r\n* Machine Translation\r\n    * Training GNMT on IWSLT 2015 Dataset (was: https://gluon-nlp.mxnet.io/examples/machine_translation/gnmt.html)\r\n    * Using Pre-trained Transformer (was: https://gluon-nlp.mxnet.io/examples/machine_translation/transformer.html)\r\n* Sentiment Analysis\r\n    * Fine-tuning LSTM-based Language Model (was https://gluon-nlp.mxnet.io/examples/sentiment_analysis/sentiment_analysis.html)\r\n    * Training Structured Self-attentive Sentence Embedding (was: https://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html)\r\n* Question Answering (to be added later)\r\n    * Inference with Pre-trained BERT-QA (from AMLC notebook (https://github.com/eric-haibin-lin/AMLC19-GluonNLP/), to be added later)\r\n* Text Generation \r\n    * Inference with Beam Search Sampler and Sequence Sampler \r\n* Deployment (in the future)\r\n    * Export Models to JSON for Inference (in the future)\r\n    * Inference with Quantized Models (in the future)\r\n    * Inference with TVM (in the future)\r\n\r\nMain changes for tutorials\r\n\r\n* Tutorials are grouped by topics. \r\n* In general, in each topic we should cover both model training and inference with pre-trained models.\r\n* Moved data loading and embedding APIs notes to tutorials \r\n\r\n### Model Zoo (involves splitting/moving scripts, to be worked on later)\r\n\r\nTable Of Content: \r\n\r\n* Representation Learning\r\n    * *Word Embedding*\r\n    * *BERT* (Pre-training)\r\n* *Language Modeling*: AWD-RNN, Cache LM, GPT-2, Transformer-XL\r\n* *Machine Translation*: GNMT, Transformer\r\n* Sentiment Analysis and Text Classification\r\n    * TextCNN from sentiment analysis (http://gluon-nlp.mxnet.io/model_zoo/sentiment_analysis/index.html)\r\n    * FastText from text classification (http://gluon-nlp.mxnet.io/model_zoo/text_classification/index.html)\r\n* *Question Answering*: BERT, QANet (in the future), BiDAF (in the future)\r\n* Natural Language Inference: Decomposable Attention, BERT\r\n* Text Generation: AWD-RNN, GPT-2\r\n* Named Entity Recognition: BERT \r\n* Intent Classification and Slot Labeling: BERT \r\n* Dependency Parsing: Biaffine \r\n* Model Gallery: Links to all trained models\r\n\r\nMain changes:\r\n\r\n* We group models by tasks. \r\n    * sentiment analysis (http://gluon-nlp.mxnet.io/model_zoo/sentiment_analysis/index.html) and text classification (http://gluon-nlp.mxnet.io/model_zoo/text_classification/index.html) are merged to one category\r\n* If a task involves lots of model architectures and becomes too heavy, we break it down into two\r\n    *  For example, if we train both GNMT and Transformer on 20+ languages, we’ll create 2 separate pages under the machine translation task, to avoid over-loaded contents.\r\n    * BERT is split into several tasks: Representational learning, NER, IC/SL, NLI and QA\r\n* Apart from listing pre-trained models in the task-specific page (e.g. showing GPT-2 in Language Model), we add a model gallery which indexes all available pre-trained models. In general, if a model takes more than 30 minutes to train, including the model in model gallery is encouraged to save users’ time. \r\n* Each page should include the link for the training script, command for training and reported accuracy. Reference format: https://gluon-cv.mxnet.io/model_zoo/classification.html#vgg \r\n\r\n\r\n\r\n\r\n### Community\r\n\r\n* Contribution Guideline (the first half of https://gluon-nlp.mxnet.io/community/contribute.html)\r\n* Git HowTos (was: https://gluon-nlp.mxnet.io/community/contribute.html#git-workflow-howtos) \r\n* Release Checklist\r\n* User Gallery (see https://spacy.io/universe and https://allennlp.org/gallery, we should add icons for github and paper links)\r\n    * Dynamic Key-Value Memory Networks for Knowledge Tracing (http://papers.www2017.com.au.s3-website-ap-southeast-2.amazonaws.com/proceedings/p765.pdf). \r\n        https://github.com/jennyzhang0215/DKVMN\r\n    * STAR-GCN: Stacked and Reconstructed Graph Convolutional Networks for Recommender Systems\r\n        https://github.com/jennyzhang0215/STAR-GCN (https://github.com/jennyzhang0215/STAR-GCN)\r\n    * https://arxiv.org/pdf/1906.04287.pdf\r\n        Chinese Embedding via Stroke and Glyph Information:A Dual-channel View\r\n    * https://arxiv.org/pdf/1905.09899.pdf\r\n        Blockwise Adaptivity: Faster Training and Better Generalization in Deep Learning\r\n    * Unlearn Dataset Bias in Natural Language Inference by Fitting the Residual\r\n        https://arxiv.org/pdf/1908.10763.pdf\r\n    * Non-Parallel Text Style Transfer via Iterative Matching and Translation (http://zhijing-jin.com/files/papers/Iterative.pdf)\r\n        http://zhijing-jin.com/files/papers/Iterative.pdf\r\n    * SELF-ATTENTION NETWORKS FOR CONNECTIONIST TEMPORALCLASSIFICATION IN SPEECH RECOGNITION\r\n        https://arxiv.org/pdf/1901.10055.pdf (https://arxiv.org/pdf/1901.10055.pdf)\r\n    * Contextual Phonetic Pretraining for End-to-endUtterance-level Language and Speaker Recognition\r\n        https://arxiv.org/pdf/1907.00457.pdf\r\n    * https://github.com/imgarylai/bert-embedding\r\n        Token level embeddings from BERT model on mxnet and gluonnlp \r\n    * https://github.com/elitcloud/elit\r\n        *E*mory *L*anguage *I*nformation *T*oolkit or *E*volution of *L*anguage and *I*nformation *T*echnology\r\n    * Adversarial Representation Learning for Text-to-Image Matching\r\n        https://arxiv.org/pdf/1908.10534.pdf\r\n\r\n\r\n### installation\r\n\r\n* Better installation experience with selection boxes: https://gluon-cv.mxnet.io/install.html \r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1081/timeline","performed_via_github_app":null,"state_reason":null}