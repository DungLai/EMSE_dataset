{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1203","id":598465277,"node_id":"MDU6SXNzdWU1OTg0NjUyNzc=","number":1203,"title":"Difference between embeddings Gluon and Huggingface","user":{"login":"evah88","id":53484865,"node_id":"MDQ6VXNlcjUzNDg0ODY1","avatar_url":"https://avatars.githubusercontent.com/u/53484865?v=4","gravatar_id":"","url":"https://api.github.com/users/evah88","html_url":"https://github.com/evah88","followers_url":"https://api.github.com/users/evah88/followers","following_url":"https://api.github.com/users/evah88/following{/other_user}","gists_url":"https://api.github.com/users/evah88/gists{/gist_id}","starred_url":"https://api.github.com/users/evah88/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/evah88/subscriptions","organizations_url":"https://api.github.com/users/evah88/orgs","repos_url":"https://api.github.com/users/evah88/repos","events_url":"https://api.github.com/users/evah88/events{/privacy}","received_events_url":"https://api.github.com/users/evah88/received_events","type":"User","site_admin":false},"labels":[{"id":890393501,"node_id":"MDU6TGFiZWw4OTAzOTM1MDE=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":15,"created_at":"2020-04-12T11:31:41Z","updated_at":"2021-02-01T20:33:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We have a BERT model that we trained from scratch on a proprietary dataset using Huggingface. I'm trying to port it to the GluonNLP version of BERT and roughly followed the conversion script. Specifically, we found the matching parameter names and then copied the model weights. The output of the converted gluon model is different from our original huggingface model so I'm trying to debug.\r\n\r\nTo simplify I calculated the embeddings of the sentence \"Hello, my dog is cute\" using the pretrained BERT models from GluonNLP and Huggingface and the encodings are different as well.\r\n\r\nCode to calculate GluonNLP embeddings:\r\n\r\n```\r\nimport gluonnlp as nlp; import mxnet as mx;\r\nmodel, vocab = nlp.model.get_model('bert_12_768_12', dataset_name='book_corpus_wiki_en_uncased', use_classifier=False, use_decoder=False);\r\ntokenizer = nlp.data.BERTTokenizer(vocab, lower=True);\r\ntransform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=512, pair=False, pad=False);\r\nsample = transform(['Hello, my dog is cute']);\r\nwords, valid_len, segments = mx.nd.array([sample[0]]), mx.nd.array([sample[1]]), mx.nd.array([sample[2]]);\r\nseq_encoding, cls_encoding = model(words, segments, valid_len);\r\n```\r\n\r\nCode to calculate Huggingface embeddings:\r\n```\r\nfrom transformers import BertModel, BertTokenizer\r\nimport torch\r\n\r\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\nmodel = BertModel.from_pretrained('bert-base-uncased')\r\n\r\ninput_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\r\n%timeit outputs = model(input_ids)\r\noutputs = model(input_ids)\r\n\r\nlast_hidden_states = outputs[0] \r\n```\r\nThe result is that seq_encoding and last_hidden_states are very different. Any suggestions on what we're missing?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1203/timeline","performed_via_github_app":null,"state_reason":null}