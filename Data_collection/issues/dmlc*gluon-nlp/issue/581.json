{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/581","id":407492916,"node_id":"MDU6SXNzdWU0MDc0OTI5MTY=","number":581,"title":"FP16 Support for LSTM","user":{"login":"whamza15","id":21658044,"node_id":"MDQ6VXNlcjIxNjU4MDQ0","avatar_url":"https://avatars.githubusercontent.com/u/21658044?v=4","gravatar_id":"","url":"https://api.github.com/users/whamza15","html_url":"https://github.com/whamza15","followers_url":"https://api.github.com/users/whamza15/followers","following_url":"https://api.github.com/users/whamza15/following{/other_user}","gists_url":"https://api.github.com/users/whamza15/gists{/gist_id}","starred_url":"https://api.github.com/users/whamza15/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/whamza15/subscriptions","organizations_url":"https://api.github.com/users/whamza15/orgs","repos_url":"https://api.github.com/users/whamza15/repos","events_url":"https://api.github.com/users/whamza15/events{/privacy}","received_events_url":"https://api.github.com/users/whamza15/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-02-07T00:26:23Z","updated_at":"2019-02-07T07:30:46Z","closed_at":"2019-02-07T07:30:46Z","author_association":"NONE","active_lock_reason":null,"body":"Trying to get LSTM layer work within FP16 setup. Small code below reproduces the problem. Code works fine for fp32 but does only turn in the training loop then crashes. \r\nforward pass is ok. It is only when you do backward() once, it crashes in the second turn in the loop.\r\n``` python\r\nimport mxnet as mx\r\nimport mxnet.autograd as ag\r\nfrom mxnet.gluon.loss import SoftmaxCrossEntropyLoss\r\n\r\n\r\nclass Net(mx.gluon.Block):\r\n    def __init__(self, num_tags, hidden=100, **kwargs):\r\n        super(Net,self).__init__(**kwargs)\r\n\r\n        self.lstm = mx.gluon.rnn.LSTM(hidden_size=hidden, bidirectional=False, layout=\"NTC\")\r\n        self.output_layer = mx.gluon.nn.Dense(num_tags, flatten=False)\r\n\r\n\r\n    def forward(self, batch):\r\n        states = self.lstm.begin_state(batch.shape[0],\r\n                                       ctx=batch.context,\r\n                                       dtype=batch.dtype)\r\n        output, _ = self.lstm(batch, states)\r\n        sequence_level = output.max(axis=1)\r\n        logits = self.output_layer(sequence_level)\r\n        return logits\r\n\r\ndtype = \"float16\"\r\n\r\nemb_size = 128\r\nlongest = 24\r\nbatch_size = 17\r\nnum_tags = 10\r\nctx = mx.gpu(0)\r\n\r\nmodel = Net(num_tags)\r\nloss = SoftmaxCrossEntropyLoss()\r\n\r\nmodel.initialize(init=mx.init.Xavier(), ctx=ctx)\r\n\r\ntrainer = mx.gluon.Trainer(model.collect_params(), 'sgd', optimizer_params=dict(\r\n    learning_rate=0.1,\r\n    multi_precision=True\r\n))\r\n\r\nmodel.cast(dtype)\r\n\r\n\r\nfor _ in range(10):\r\n\r\n    batch = mx.nd.random_normal(0,1, shape=(batch_size, longest, emb_size), ctx=ctx)\r\n    gold = mx.nd.cast(mx.nd.random_uniform(0,num_tags,shape=(batch_size,), ctx=ctx), dtype=\"int32\")\r\n    batch = batch.astype(dtype, copy=False)\r\n\r\n    with ag.record():\r\n        logits = model(batch)\r\n        l = loss(logits,gold).mean()\r\n\r\n        l.backward()\r\n\r\n    trainer.step(1)\r\n\r\n    print(\"loss='{}'\".format(l.asscalar()))\r\n```\r\nThe error I am getting is:\r\n```\r\nloss='2.462890625'\r\nTraceback (most recent call last):\r\n  File \"config/test_float16.py\", line 62, in <module>\r\n    print(\"loss='{}'\".format(l.asscalar()))\r\n  File \".../lib/python3.4/site-packages/mxnet/ndarray/ndarray.py\", line 1990, in asscalar\r\n    return self.asnumpy()[0]\r\n  File \".../lib/python3.4/site-packages/mxnet/ndarray/ndarray.py\", line 1972, in asnumpy\r\n    ctypes.c_size_t(data.size)))\r\n  File \".../lib/python3.4/site-packages/mxnet/base.py\", line 251, in check_call\r\n    raise MXNetError(py_str(_LIB.MXGetLastError()))\r\nmxnet.base.MXNetError: [00:23:12] .../src/include/mxnet/././tensor_blob.h:203: Check failed: mshadow::DataType<DType>::kFlag == type_flag_ TBlob.get_with_shape: data type do not match specified type.Expected: 0 v.s. given 2\r\n[bt] (0) libmxnet.so(dmlc::StackTrace()+0x3d) [0x7fd19318be6d]\r\n[bt] (1) libmxnet.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x1a) [0x7fd19318c15a]\r\n[bt] (2) libmxnet.so(mshadow::Tensor<mshadow::gpu, 3, mshadow::half::half_t> mxnet::TBlob::get<mshadow::gpu, 3, mshadow::half::half_t>(mshadow::Stream<mshadow::gpu>*) const+0x15b) [0x7fd1956dd8bb]\r\n[bt] (3) libmxnet.so(mxnet::op::CuDNNRNNOp<mshadow::half::half_t>::Backward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x101c) [0x7fd1957ea88c]\r\n[bt] (4) libmxnet.so(mxnet::op::OperatorState::Backward(mxnet::OpContext const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, std::vector<mxnet::TBlob, std::allocator<mxnet::TBlob> > const&)+0x72d) [0x7fd19348785d]\r\n[bt] (5) libmxnet.so(mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext, mxnet::engine::CallbackOnComplete)#3}::operator()(mxnet::RunContext, mxnet::engine::CallbackOnComplete) const+0x35d) [0x7fd1932fe65d]\r\n[bt] (6) libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::imperative::PushOperator(mxnet::OpStatePtr const&, nnvm::Op const*, nnvm::NodeAttrs const&, mxnet::Context const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::Resource, std::allocator<mxnet::Resource> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<mxnet::NDArray*, std::allocator<mxnet::NDArray*> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&, std::vector<mxnet::OpReqType, std::allocator<mxnet::OpReqType> > const&, mxnet::DispatchMode)::{lambda(mxnet::RunContext)#4}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x12) [0x7fd1932fe8f2]\r\n[bt] (7) libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x48) [0x7fd1932794c8]\r\n[bt] (8) libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x32) [0x7fd1932794b2]\r\n[bt] (9) libmxnet.so(std::_Function_handler<void (mxnet::RunContext), mxnet::engine::ThreadedEngine::BulkAppend(std::function<void (mxnet::RunContext)>, mxnet::Context, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&, std::vector<mxnet::engine::Var*, std::allocator<mxnet::engine::Var*> > const&)::{lambda(mxnet::RunContext)#1}>::_M_invoke(std::_Any_data const&, mxnet::RunContext)+0x32) [0x7fd1932794b2]\r\n\r\n```\r\n","closed_by":{"login":"szha","id":2626883,"node_id":"MDQ6VXNlcjI2MjY4ODM=","avatar_url":"https://avatars.githubusercontent.com/u/2626883?v=4","gravatar_id":"","url":"https://api.github.com/users/szha","html_url":"https://github.com/szha","followers_url":"https://api.github.com/users/szha/followers","following_url":"https://api.github.com/users/szha/following{/other_user}","gists_url":"https://api.github.com/users/szha/gists{/gist_id}","starred_url":"https://api.github.com/users/szha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/szha/subscriptions","organizations_url":"https://api.github.com/users/szha/orgs","repos_url":"https://api.github.com/users/szha/repos","events_url":"https://api.github.com/users/szha/events{/privacy}","received_events_url":"https://api.github.com/users/szha/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/581/timeline","performed_via_github_app":null,"state_reason":"completed"}