{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/992","id":513616463,"node_id":"MDU6SXNzdWU1MTM2MTY0NjM=","number":992,"title":"[Inconsistency] Non-deterministic behavior of pre-trained BERT within mx.autograd.record() after calling uuid.uuid1()","user":{"login":"AaronYALai","id":14147661,"node_id":"MDQ6VXNlcjE0MTQ3NjYx","avatar_url":"https://avatars.githubusercontent.com/u/14147661?v=4","gravatar_id":"","url":"https://api.github.com/users/AaronYALai","html_url":"https://github.com/AaronYALai","followers_url":"https://api.github.com/users/AaronYALai/followers","following_url":"https://api.github.com/users/AaronYALai/following{/other_user}","gists_url":"https://api.github.com/users/AaronYALai/gists{/gist_id}","starred_url":"https://api.github.com/users/AaronYALai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AaronYALai/subscriptions","organizations_url":"https://api.github.com/users/AaronYALai/orgs","repos_url":"https://api.github.com/users/AaronYALai/repos","events_url":"https://api.github.com/users/AaronYALai/events{/privacy}","received_events_url":"https://api.github.com/users/AaronYALai/received_events","type":"User","site_admin":false},"labels":[{"id":890393501,"node_id":"MDU6TGFiZWw4OTAzOTM1MDE=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2019-10-28T23:46:58Z","updated_at":"2019-11-13T02:23:40Z","closed_at":"2019-11-13T02:23:40Z","author_association":"NONE","active_lock_reason":null,"body":"Sample snippets to reproduce the inconsistency issue:\r\n\r\n```\r\nimport random\r\nimport mxnet as mx\r\nimport numpy as np\r\n\r\nimport gluonnlp as nlp\r\nimport uuid\r\n\r\n# mx.__version__ '1.5.0' (mxnet-cu100mkl)\r\n# np.__version__ '1.17.3'\r\n# nlp.__version__ '0.8.1'\r\n# environment: source activate mxnet_p36 (Amazon Deep Learning AMI)\r\n# machine: EC2 p3.8xl\r\n\r\n\r\ndef run_bert(context=[mx.gpu()]):\r\n    seed = 0\r\n    mx.random.seed(seed)\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n\r\n    # Execute this line makes BERT non-deterministic inside mx.autograd.record()\r\n    # uid = uuid.uuid1()\r\n\r\n    bert_model, bert_vocab = nlp.model.get_model(\r\n        name='bert_12_768_12',\r\n        dataset_name='book_corpus_wiki_en_uncased',\r\n        pretrained=True,\r\n        ctx=context,\r\n        use_pooler=True,\r\n        use_decoder=False,\r\n        use_classifier=False,\r\n        dropout=0.1,\r\n        embed_dropout=0.1)\r\n\r\n    # sample inputs\r\n    ti = mx.nd.array([[2, 22100, 2080, 2629, 2072, 2475, 2912, 7685, 4160, 2078,\r\n                       2629, 4430, 2581, 6895,  3501, 2546, 2629, 2480, 17299, 3],\r\n                      [2, 2064, 1045, 2689, 2000, 2151, 12485, 1029, 2835, 1999,\r\n                       6421, 2575, 2683, 12521, 18139, 2575, 2581, 16068, 2475, 3]],\r\n                     dtype=int, ctx=context[0])\r\n    tt = mx.nd.zeros((2, 20), dtype=int, ctx=context[0])\r\n    vl = mx.nd.array([20, 20], ctx=context[0])\r\n\r\n    # BERT forward, outside and inside autograd.record()\r\n    out1 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]\r\n\r\n    with mx.autograd.record():\r\n        # Inconsistent if call \"uuid.uuid1()\"\r\n        out2 = [a.sum().asscalar() for a in bert_model(ti, tt, vl)]\r\n\r\n    print('Outside autograd:', out1)\r\n    print('Inside autograd:', out2)\r\n\r\n\r\nif __name__ == '__main__':\r\n    run_bert()\r\n```\r\n\r\nOnly when un-commenting and executing \"uuid.uuid1()\", the \"out2\" will have different values when you run the script again and again.\r\n\r\nSuspect the it messed up with internal states used by gluonnlp BERT implementation.\r\n\r\nThanks in advance for taking care of this issue!","closed_by":{"login":"sxjscience","id":5178350,"node_id":"MDQ6VXNlcjUxNzgzNTA=","avatar_url":"https://avatars.githubusercontent.com/u/5178350?v=4","gravatar_id":"","url":"https://api.github.com/users/sxjscience","html_url":"https://github.com/sxjscience","followers_url":"https://api.github.com/users/sxjscience/followers","following_url":"https://api.github.com/users/sxjscience/following{/other_user}","gists_url":"https://api.github.com/users/sxjscience/gists{/gist_id}","starred_url":"https://api.github.com/users/sxjscience/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sxjscience/subscriptions","organizations_url":"https://api.github.com/users/sxjscience/orgs","repos_url":"https://api.github.com/users/sxjscience/repos","events_url":"https://api.github.com/users/sxjscience/events{/privacy}","received_events_url":"https://api.github.com/users/sxjscience/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/992/timeline","performed_via_github_app":null,"state_reason":"completed"}