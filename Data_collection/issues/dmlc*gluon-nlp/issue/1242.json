{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1242","id":636000634,"node_id":"MDU6SXNzdWU2MzYwMDA2MzQ=","number":1242,"title":"[Numpy Refactor] Tokenizers wishlist","user":{"login":"sxjscience","id":5178350,"node_id":"MDQ6VXNlcjUxNzgzNTA=","avatar_url":"https://avatars.githubusercontent.com/u/5178350?v=4","gravatar_id":"","url":"https://api.github.com/users/sxjscience","html_url":"https://github.com/sxjscience","followers_url":"https://api.github.com/users/sxjscience/followers","following_url":"https://api.github.com/users/sxjscience/following{/other_user}","gists_url":"https://api.github.com/users/sxjscience/gists{/gist_id}","starred_url":"https://api.github.com/users/sxjscience/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sxjscience/subscriptions","organizations_url":"https://api.github.com/users/sxjscience/orgs","repos_url":"https://api.github.com/users/sxjscience/repos","events_url":"https://api.github.com/users/sxjscience/events{/privacy}","received_events_url":"https://api.github.com/users/sxjscience/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"},{"id":1689031381,"node_id":"MDU6TGFiZWwxNjg5MDMxMzgx","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/numpyrefactor","name":"numpyrefactor","color":"2f38ed","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-06-10T07:25:57Z","updated_at":"2020-06-10T16:13:11Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"We revised the implementation of tokenizers in the new version of GluonNLP.\r\n\r\nBasically we have integrated the following tokenizers:\r\n\r\n- whitespace\r\n- spacy\r\n- jieba\r\n- SentencePiece\r\n- YTTM\r\n- HuggingFaceBPE\r\n- HuggingFaceByteBPE\r\n- HuggingFaceWordPiece\r\n\r\nFor all tokenizers, we support the following methods:\r\n- Encode into a list of integers: `encode('hello world!', int)`\r\n- Encode into a list of string tokens: `encode('hello world!', str)`\r\n- Encode into a list of integers + offsets: `encode_with_offsets('hello world!', int)`\r\n- Encode into a list of string tokens + offsets: `encode_with_offsets('hello world!', str)`\r\n\r\nTo give an example, we load the tokenizer in ALBERT, which is a SentencePieceTokenizer and illustrate these functionalities:\r\n\r\n```python\r\nIn [1]: from gluonnlp.models.albert import get_pretrained_albert                \r\n\r\nIn [2]: cfg, tokenizer, _,_ = get_pretrained_albert()                           \r\n\r\nIn [3]: tokenizer                                                               \r\nOut[3]: \r\nSentencepieceTokenizer(\r\n   model_path = /Users/xjshi/.mxnet/models/nlp/google_albert_base_v2/spm-65999e5d.model\r\n   do_lower = True, nbest = 0, alpha = 0.0\r\n   vocab = Vocab(size=30000, unk_token=\"<unk>\", pad_token=\"<pad>\", cls_token=\"[CLS]\", sep_token=\"[SEP]\", mask_token=\"[MASK]\")\r\n)\r\n\r\nIn [4]: tokenizer.encode('hello world!', int)                                   \r\nOut[4]: [10975, 126, 187]\r\n\r\nIn [5]: tokenizer.encode('hello world!', str)                                   \r\nOut[5]: ['▁hello', '▁world', '!']\r\n\r\nIn [6]: tokenizer.encode_with_offsets('hello world!', str)                      \r\nOut[6]: (['▁hello', '▁world', '!'], [(0, 5), (5, 11), (11, 12)])\r\n\r\nIn [7]: tokenizer.encode_with_offsets('hello world!', int)                      \r\nOut[7]: ([10975, 126, 187], [(0, 5), (5, 11), (11, 12)])\r\n\r\n```\r\n\r\nHowever, there are a lot of other commonly used tokenizers. We can consider to integrate:\r\n\r\n- [ ] BlingFire: https://github.com/microsoft/BlingFire\r\n- [ ] Mecab: A commonly used tokenizer for Japanese: https://taku910.github.io/mecab/","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1242/timeline","performed_via_github_app":null,"state_reason":null}