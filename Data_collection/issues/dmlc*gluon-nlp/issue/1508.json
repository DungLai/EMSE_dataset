{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1508","id":795798784,"node_id":"MDU6SXNzdWU3OTU3OTg3ODQ=","number":1508,"title":"Have problom in BERT pre-training: how to training on multiple GPUs","user":{"login":"yangshuo0323","id":76250793,"node_id":"MDQ6VXNlcjc2MjUwNzkz","avatar_url":"https://avatars.githubusercontent.com/u/76250793?v=4","gravatar_id":"","url":"https://api.github.com/users/yangshuo0323","html_url":"https://github.com/yangshuo0323","followers_url":"https://api.github.com/users/yangshuo0323/followers","following_url":"https://api.github.com/users/yangshuo0323/following{/other_user}","gists_url":"https://api.github.com/users/yangshuo0323/gists{/gist_id}","starred_url":"https://api.github.com/users/yangshuo0323/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yangshuo0323/subscriptions","organizations_url":"https://api.github.com/users/yangshuo0323/orgs","repos_url":"https://api.github.com/users/yangshuo0323/repos","events_url":"https://api.github.com/users/yangshuo0323/events{/privacy}","received_events_url":"https://api.github.com/users/yangshuo0323/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2021-01-28T08:48:34Z","updated_at":"2021-02-01T08:11:17Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Description\r\n- I want to train BERT model on GPU, but have some problems. My configuration:\r\n  * Software environment: Python: 3.7.7,  Cuda: 10.2\r\n  * Install MXNet: `pip install mxnet-cu102`  , verion is 1.7.0\r\n  * Download Model script:  `https://github.com/dmlc/gluon-nlp`, which branch is 2.0.\r\n- Run script `gluon-nlp/scripts/bert/run_pretraining.py`:\r\n  * Reference the instruction: `https://nlp.gluon.ai/model_zoo/bert/index.html#bert-model-zoo`\r\n  * And download DataSet alse in above web.\r\n  ```\r\n    $  mpirun -np 8 -H localhost:8 -mca pml ob1 -mca btl ^openib \\\r\n         -mca btl_tcp_if_exclude docker0,lo --map-by ppr:4:socket \\\r\n         --mca plm_rsh_agent 'ssh -q -o StrictHostKeyChecking=no' \\\r\n         -x NCCL_MIN_NRINGS=8 -x NCCL_DEBUG=INFO -x HOROVOD_HIERARCHICAL_ALLREDUCE=1 \\\r\n         -x MXNET_SAFE_ACCUMULATION=1 --tag-output \\\r\n    python run_pretraining.py --verbose --model=\"bert_12_768_12\" --warmup_ratio=1 --comm_backend=\"horovod\" \\\r\n\t\t--accumulate=1 --max_seq_length=128 --raw --max_predictions_per_seq=20 --log_interval=1 --ckpt_interval=1000 \\\r\n\t\t--no_compute_acc --data=/home/yangshuo/mxnet/Dataset/pre-train-datasets/enwiki-feb-doc-split/*.train \\\r\n\t\t--num_steps=1000 --total_batch_size=128 --dtype=\"float16\"\r\n  ```\r\n- Result error: \r\n  \r\n![image](https://user-images.githubusercontent.com/76250793/106083385-633e2c00-6157-11eb-9059-da6a0092570c.png)\r\n\r\n## Seek help:\r\n  I have read the guidance, but still don't known how to  running. \r\n  Please help me, or can I have correct instruction or suggestion ? thanks.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1508/timeline","performed_via_github_app":null,"state_reason":null}