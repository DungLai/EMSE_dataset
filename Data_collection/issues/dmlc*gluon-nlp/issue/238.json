{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/238","id":346377149,"node_id":"MDU6SXNzdWUzNDYzNzcxNDk=","number":238,"title":"Batch-dependent behavior in SoftmaxCEMaskedLoss?","user":{"login":"JulianSlzr","id":4734836,"node_id":"MDQ6VXNlcjQ3MzQ4MzY=","avatar_url":"https://avatars.githubusercontent.com/u/4734836?v=4","gravatar_id":"","url":"https://api.github.com/users/JulianSlzr","html_url":"https://github.com/JulianSlzr","followers_url":"https://api.github.com/users/JulianSlzr/followers","following_url":"https://api.github.com/users/JulianSlzr/following{/other_user}","gists_url":"https://api.github.com/users/JulianSlzr/gists{/gist_id}","starred_url":"https://api.github.com/users/JulianSlzr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JulianSlzr/subscriptions","organizations_url":"https://api.github.com/users/JulianSlzr/orgs","repos_url":"https://api.github.com/users/JulianSlzr/repos","events_url":"https://api.github.com/users/JulianSlzr/events{/privacy}","received_events_url":"https://api.github.com/users/JulianSlzr/received_events","type":"User","site_admin":false},"labels":[{"id":963101581,"node_id":"MDU6TGFiZWw5NjMxMDE1ODE=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/discussion","name":"discussion","color":"c5def5","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2018-07-31T22:11:17Z","updated_at":"2018-08-12T19:52:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi all,\r\n\r\nConsider the following (placed in `scripts/nmt/`):\r\n```python\r\nimport mxnet as mx\r\nimport loss\r\n\r\nif __name__ == '__main__':\r\n    BATCH_SIZE = 3\r\n    MAX_SEQ_LEN = 100\r\n    VOCAB_SIZE = 26\r\n    # Each time step has a uniform distribution over the vocabulary\r\n    uniform_tensor = 1./VOCAB_SIZE * mx.nd.ones(shape=(BATCH_SIZE, MAX_SEQ_LEN, VOCAB_SIZE))\r\n    # Sequences have different lengths\r\n    valid_lens = mx.nd.array((1, 10, 100))\r\n\r\n    loss = loss.SoftmaxCEMaskedLoss(sparse_label=False, from_logits=False)\r\n    ce_loss = loss(uniform_tensor, uniform_tensor, valid_lens)\r\n    print(ce_loss)\r\n```\r\nThis outputs:\r\n```\r\n[0.03258096 0.32580966 3.2580965 ]\r\n<NDArray 3 @cpu(0)>\r\n```\r\n**However, these should all be `3.2580965` (which is ln(26))**, as this is the (average) CE across all valid timesteps, per sequence.\r\n\r\nThe problem is [the averaging step of CE is not aware of `valid_length`](https://github.com/apache/incubator-mxnet/blob/master/python/mxnet/gluon/loss.py#L319), which leads to \"masking zeros\" being included in the mean. Compare with, e.g., [Sockeye's implementation](https://github.com/awslabs/sockeye/blob/master/sockeye/loss.py#L185).\r\n\r\nOne workaround is to increase sample weights so their weighted sum compensates for the zeros. For example, adding the following line before calling [super in SoftmaxCEMaskedLoss](https://github.com/dmlc/gluon-nlp/blob/master/scripts/nmt/loss.py#L56):\r\n```\r\nsample_weight = sample_weight * (sample_weight.shape[1] / F.reshape(valid_length, shape=(-1, 1, 1)))\r\n```\r\n\r\nI have not evaluated how this affects current MT models. This causes short sequences to have smaller per-token loss, weighting long sequences more.\r\n\r\nHowever, perhaps we should edit `SoftmaxCEMaskedLoss` for correctness (or someone point out if I'm mistaken); I can submit a PR. The CE loss of an sequence should not depend on the batch it is in.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/238/timeline","performed_via_github_app":null,"state_reason":null}