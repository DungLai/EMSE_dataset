{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/646","id":425465434,"node_id":"MDU6SXNzdWU0MjU0NjU0MzQ=","number":646,"title":"Transformer decoder shape mismatch","user":{"login":"lambdaofgod","id":3647577,"node_id":"MDQ6VXNlcjM2NDc1Nzc=","avatar_url":"https://avatars.githubusercontent.com/u/3647577?v=4","gravatar_id":"","url":"https://api.github.com/users/lambdaofgod","html_url":"https://github.com/lambdaofgod","followers_url":"https://api.github.com/users/lambdaofgod/followers","following_url":"https://api.github.com/users/lambdaofgod/following{/other_user}","gists_url":"https://api.github.com/users/lambdaofgod/gists{/gist_id}","starred_url":"https://api.github.com/users/lambdaofgod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lambdaofgod/subscriptions","organizations_url":"https://api.github.com/users/lambdaofgod/orgs","repos_url":"https://api.github.com/users/lambdaofgod/repos","events_url":"https://api.github.com/users/lambdaofgod/events{/privacy}","received_events_url":"https://api.github.com/users/lambdaofgod/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-03-26T14:50:43Z","updated_at":"2019-03-26T16:00:36Z","closed_at":"2019-03-26T16:00:36Z","author_association":"NONE","active_lock_reason":null,"body":"I tried to run Transformer model's decoder as in NMT example, but I get a mismatch\r\n\r\n\r\nI run either\r\n`translator.translate(example_input_batch, None)`\r\nor \r\n\r\n`translator.translate(example_input_batch, nd.full((example_output_batch.shape[0]), ctx=context, val=example_output_batch.shape[1]))`\r\n\r\nI get\r\n\r\n`MXNetError: Shape inconsistent, Provided = [n_hid,3200], inferred shape=(n_hid,n_hid)\r\n`\r\nWhere 3200 = sequence length * n_hid\r\n\r\n## Details:\r\n\r\nexample_input_batch is a tensor of shape batch_size=100, length=25\r\n\r\nI added print-debugs to your code to show intermediate tensor shapes, when I print before offending line (`samples, scores, sample_valid_length = self._sampler(inputs, decoder_states`) I get\r\n\r\n> inputs (100,)\r\n> decoder_states [(100, 25, 128), (100, 25)]\r\n> _ []\r\n> encoder_outputs (100, 25, 128)\r\n> src_valid_length (100,)\r\n> src_seq (100, 25)\r\n\r\n\r\n## Model definition\r\n```\r\n\r\nencoder, decoder = gluonnlp.model.transformer.get_transformer_encoder_decoder(\r\n    num_layers=2,\r\n    units=n_units,\r\n    hidden_size=hidden_size,\r\n    use_residual=False ,\r\n    max_src_length=seq_length,\r\n    max_tgt_length=seq_length,\r\n    num_heads=1\r\n)\r\n\r\n\r\nmodel = NMTModel(input_vocab, input_vocab, encoder, decoder, embed_size=n_units, share_embed=True)\r\nmodel.initialize(init.Xavier(), ctx=context)\r\n```\r\n\r\n## Full error log:\r\n\r\n```\r\n------------------------------------------------------------------\r\nMXNetError                       Traceback (most recent call last)\r\n<ipython-input-135-63776b590d33> in <module>()\r\n----> 1 translator.translate(example_input_batch, nd.full((example_output_batch.shape[0]), ctx=context, val=example_output_batch.shape[1]-1))\r\n\r\n<ipython-input-89-ebb059c7561b> in translate(self, src_seq, src_valid_length)\r\n     59 \r\n     60         print_debug_ndarrays()\r\n---> 61         samples, scores, sample_valid_length = self._sampler(inputs, decoder_states)\r\n     62         return samples, scores, sample_valid_length\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/sequence_sampler.py in __call__(self, inputs, states)\r\n    528         samples = step_input.reshape((batch_size, beam_size, 1))\r\n    529         for i in range(self._max_length):\r\n--> 530             log_probs, new_states = self._decoder(step_input, states)\r\n    531             vocab_size_nd = mx.nd.array([log_probs.shape[1]], ctx=ctx, dtype=np.int32)\r\n    532             batch_shift_nd = mx.nd.arange(0, batch_size * beam_size, beam_size, ctx=ctx,\r\n\r\n<ipython-input-89-ebb059c7561b> in _decode_logprob(self, step_input, states)\r\n     27 \r\n     28     def _decode_logprob(self, step_input, states):\r\n---> 29         out, states, _ = self._model.decode_step(step_input, states)\r\n     30         return mx.nd.log_softmax(out), states\r\n     31 \r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/translation.py in decode_step(self, step_input, states)\r\n    186         \"\"\"\r\n    187         step_output, states, step_additional_outputs =\\\r\n--> 188             self.decoder(self.tgt_embed(step_input), states)\r\n    189         step_output = self.tgt_proj(step_output)\r\n    190         return step_output, states, step_additional_outputs\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/transformer.py in __call__(self, step_input, states)\r\n    970             (batch_size, num_heads, length, mem_length)\r\n    971         \"\"\"\r\n--> 972         return super(TransformerDecoder, self).__call__(step_input, states)\r\n    973 \r\n    974     def forward(self, step_input, states, mask=None):  #pylint: disable=arguments-differ, missing-docstring\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/seq2seq_encoder_decoder.py in __call__(self, step_input, states)\r\n    216             Additional outputs of the step, e.g, the attention weights\r\n    217         \"\"\"\r\n--> 218         return super(Seq2SeqDecoder, self).__call__(step_input, states)\r\n    219 \r\n    220     def forward(self, step_input, states):  #pylint: disable=arguments-differ\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)\r\n    540             hook(self, args)\r\n    541 \r\n--> 542         out = self.forward(*args)\r\n    543 \r\n    544         for hook in self._forward_hooks.values():\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/transformer.py in forward(self, step_input, states, mask)\r\n   1011         # pylint: disable=too-many-function-args\r\n   1012         step_output, step_additional_outputs = \\\r\n-> 1013             super(TransformerDecoder, self).forward(scaled_step_input, states, mask)\r\n   1014         states = states[:-1]\r\n   1015         if has_mem_mask:\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in forward(self, x, *args)\r\n    917                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}\r\n    918 \r\n--> 919                 return self.hybrid_forward(ndarray, x, *args, **params)\r\n    920 \r\n    921         assert isinstance(x, Symbol), \\\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/transformer.py in hybrid_forward(self, F, step_input, states, mask, position_weight)\r\n   1062         attention_weights_l = []\r\n   1063         for cell in self.transformer_cells:\r\n-> 1064             outputs, attention_weights = cell(inputs, mem_value, mask, mem_mask)\r\n   1065             if self._output_attention:\r\n   1066                 attention_weights_l.append(attention_weights)\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)\r\n    540             hook(self, args)\r\n    541 \r\n--> 542         out = self.forward(*args)\r\n    543 \r\n    544         for hook in self._forward_hooks.values():\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in forward(self, x, *args)\r\n    917                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}\r\n    918 \r\n--> 919                 return self.hybrid_forward(ndarray, x, *args, **params)\r\n    920 \r\n    921         assert isinstance(x, Symbol), \\\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/gluonnlp/model/transformer.py in hybrid_forward(self, F, inputs, mem_value, mask, mem_mask)\r\n    769         outputs, attention_in_outputs =\\\r\n    770             self.attention_cell_in(inputs, inputs, inputs, mask)\r\n--> 771         outputs = self.proj_in(outputs)\r\n    772         outputs = self.dropout_layer(outputs)\r\n    773         if self._use_residual:\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in __call__(self, *args)\r\n    540             hook(self, args)\r\n    541 \r\n--> 542         out = self.forward(*args)\r\n    543 \r\n    544         for hook in self._forward_hooks.values():\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/block.py in forward(self, x, *args)\r\n    917                     params = {i: j.data(ctx) for i, j in self._reg_params.items()}\r\n    918 \r\n--> 919                 return self.hybrid_forward(ndarray, x, *args, **params)\r\n    920 \r\n    921         assert isinstance(x, Symbol), \\\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/nn/basic_layers.py in hybrid_forward(self, F, x, weight, bias)\r\n    219     def hybrid_forward(self, F, x, weight, bias=None):\r\n    220         act = F.FullyConnected(x, weight, bias, no_bias=bias is None, num_hidden=self._units,\r\n--> 221                                flatten=self._flatten, name='fwd')\r\n    222         if self.act is not None:\r\n    223             act = self.act(act)\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/ndarray/register.py in FullyConnected(data, weight, bias, num_hidden, no_bias, flatten, out, name, **kwargs)\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/_ctypes/ndarray.py in _imperative_invoke(handle, ndargs, keys, vals, out)\r\n     90         c_str_array(keys),\r\n     91         c_str_array([str(s) for s in vals]),\r\n---> 92         ctypes.byref(out_stypes)))\r\n     93 \r\n     94     if original_output is not None:\r\n\r\n~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/base.py in check_call(ret)\r\n    249     \"\"\"\r\n    250     if ret != 0:\r\n--> 251         raise MXNetError(py_str(_LIB.MXGetLastError()))\r\n    252 \r\n    253 \r\n\r\nMXNetError: Shape inconsistent, Provided = [128,3200], inferred shape=(128,128)\r\n```","closed_by":{"login":"lambdaofgod","id":3647577,"node_id":"MDQ6VXNlcjM2NDc1Nzc=","avatar_url":"https://avatars.githubusercontent.com/u/3647577?v=4","gravatar_id":"","url":"https://api.github.com/users/lambdaofgod","html_url":"https://github.com/lambdaofgod","followers_url":"https://api.github.com/users/lambdaofgod/followers","following_url":"https://api.github.com/users/lambdaofgod/following{/other_user}","gists_url":"https://api.github.com/users/lambdaofgod/gists{/gist_id}","starred_url":"https://api.github.com/users/lambdaofgod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lambdaofgod/subscriptions","organizations_url":"https://api.github.com/users/lambdaofgod/orgs","repos_url":"https://api.github.com/users/lambdaofgod/repos","events_url":"https://api.github.com/users/lambdaofgod/events{/privacy}","received_events_url":"https://api.github.com/users/lambdaofgod/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/646/timeline","performed_via_github_app":null,"state_reason":"completed"}