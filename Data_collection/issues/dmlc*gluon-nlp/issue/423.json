{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/423","id":383936794,"node_id":"MDU6SXNzdWUzODM5MzY3OTQ=","number":423,"title":"How to use use 2 embeddings with one acting as a backup?","user":{"login":"Ishitori","id":3286787,"node_id":"MDQ6VXNlcjMyODY3ODc=","avatar_url":"https://avatars.githubusercontent.com/u/3286787?v=4","gravatar_id":"","url":"https://api.github.com/users/Ishitori","html_url":"https://github.com/Ishitori","followers_url":"https://api.github.com/users/Ishitori/followers","following_url":"https://api.github.com/users/Ishitori/following{/other_user}","gists_url":"https://api.github.com/users/Ishitori/gists{/gist_id}","starred_url":"https://api.github.com/users/Ishitori/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ishitori/subscriptions","organizations_url":"https://api.github.com/users/Ishitori/orgs","repos_url":"https://api.github.com/users/Ishitori/repos","events_url":"https://api.github.com/users/Ishitori/events{/privacy}","received_events_url":"https://api.github.com/users/Ishitori/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2018-11-23T22:35:39Z","updated_at":"2018-12-08T15:52:30Z","closed_at":"2018-11-26T02:30:26Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I load 2 embeddings into my vocab: English and Chinese. I want to use English embedding when there is a value for a particular token, and if there is no value, I would like to use Embedding value from Chinese embedding. I thought that if I add English Embedding first and Chinese Embedding next, then I will have a desired behavior. \r\n\r\n```\r\nembedding_en = gluonnlp.embedding.create('fasttext', source='wiki.zh')\r\nembedding_zh = gluonnlp.embedding.create('fasttext', source='wiki.simple')\r\ncntr = data.count_tokens(['a', 'å“ˆ'])\r\nvocab = Vocab(cntr)\r\nvocab.set_embedding(embedding_en, embedding_zh)\r\n```\r\n\r\nBut actually I receive back a concatenation from both embeddings, e.g. if both embedding dimensions is 300 then I get 600 vector.  \r\n\r\nCan I avoid the concatenation and get back only the first hit? \r\n\r\nAt the moment, my workaround is to do embedding as a data preprocessing step, meaning I have to remove Embedding layer from the network, do token replacement not with indices, but with full-fledged vectors from embeddings using \"if\" and then pass vectors to the training.","closed_by":{"login":"leezu","id":946903,"node_id":"MDQ6VXNlcjk0NjkwMw==","avatar_url":"https://avatars.githubusercontent.com/u/946903?v=4","gravatar_id":"","url":"https://api.github.com/users/leezu","html_url":"https://github.com/leezu","followers_url":"https://api.github.com/users/leezu/followers","following_url":"https://api.github.com/users/leezu/following{/other_user}","gists_url":"https://api.github.com/users/leezu/gists{/gist_id}","starred_url":"https://api.github.com/users/leezu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leezu/subscriptions","organizations_url":"https://api.github.com/users/leezu/orgs","repos_url":"https://api.github.com/users/leezu/repos","events_url":"https://api.github.com/users/leezu/events{/privacy}","received_events_url":"https://api.github.com/users/leezu/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/423/timeline","performed_via_github_app":null,"state_reason":"completed"}