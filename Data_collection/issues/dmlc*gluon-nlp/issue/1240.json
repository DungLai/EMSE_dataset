{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1240","id":635978932,"node_id":"MDU6SXNzdWU2MzU5Nzg5MzI=","number":1240,"title":"[Numpy Refactor] Dataset Enhancement TODO","user":{"login":"sxjscience","id":5178350,"node_id":"MDQ6VXNlcjUxNzgzNTA=","avatar_url":"https://avatars.githubusercontent.com/u/5178350?v=4","gravatar_id":"","url":"https://api.github.com/users/sxjscience","html_url":"https://github.com/sxjscience","followers_url":"https://api.github.com/users/sxjscience/followers","following_url":"https://api.github.com/users/sxjscience/following{/other_user}","gists_url":"https://api.github.com/users/sxjscience/gists{/gist_id}","starred_url":"https://api.github.com/users/sxjscience/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sxjscience/subscriptions","organizations_url":"https://api.github.com/users/sxjscience/orgs","repos_url":"https://api.github.com/users/sxjscience/repos","events_url":"https://api.github.com/users/sxjscience/events{/privacy}","received_events_url":"https://api.github.com/users/sxjscience/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"},{"id":1689031381,"node_id":"MDU6TGFiZWwxNjg5MDMxMzgx","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/numpyrefactor","name":"numpyrefactor","color":"2f38ed","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-06-10T06:49:32Z","updated_at":"2020-06-10T07:35:48Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Now we have the PR about the new version of GluonNLP: https://github.com/dmlc/gluon-nlp/pull/1225, which refactors the major APIs and will rely on the DeepNumpy interface in MXNet.\r\n\r\nBasically, we refactored the way the user will download and prepare the common NLP datasets.  Previously, we will rely on python and create some `XXXDataset` object and access the data.\r\n\r\nNow, we have switched to the new `nlp_data` + `nlp_preprocess` CLI commands to help you download and prepare the dataset.\r\n\r\n```python\r\n# Prepare Squad\r\nnlp_data prepare_squad --version 2.0\r\n# Prepare WMT\r\nnlp_data prepare_wmt --dataset wmt2014 --lang-pair en-de --save-path wmt2014_en_de\r\n# Download Wikipedia\r\nnlp_data prepare_wikipedia --mode download --lang en --date latest -o ./\r\n```\r\n\r\nWe can enhance the dataset support by adding:\r\n\r\n- Pretrain Corpus\r\n   - [ ] CC-Net\r\n      - Code: https://github.com/facebookresearch/cc_net\r\n   - [ ] pushshift.io Reddit\r\n      - Paper: https://arxiv.org/pdf/2001.08435.pdf\r\n      - Dataset: https://files.pushshift.io/reddit/\r\n- Intent Classification and Slot Labeling\r\n   - [ ] ATIS\r\n   - [ ] SNIPS\r\n- Question Answering\r\n   - [ ] Natural Questions\r\n\r\nIn addition, we will consider to move part of the datasets to our internal S3, which will offer fast downloading speed (if license allows).\r\n- [ ] Mirror dataset in internal S3","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1240/timeline","performed_via_github_app":null,"state_reason":null}