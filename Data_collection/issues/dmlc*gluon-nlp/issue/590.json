{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/590","id":409196804,"node_id":"MDU6SXNzdWU0MDkxOTY4MDQ=","number":590,"title":"gluonnlp.data.batchify.Pad should add a parameter of maximum length","user":{"login":"fierceX","id":13912058,"node_id":"MDQ6VXNlcjEzOTEyMDU4","avatar_url":"https://avatars.githubusercontent.com/u/13912058?v=4","gravatar_id":"","url":"https://api.github.com/users/fierceX","html_url":"https://github.com/fierceX","followers_url":"https://api.github.com/users/fierceX/followers","following_url":"https://api.github.com/users/fierceX/following{/other_user}","gists_url":"https://api.github.com/users/fierceX/gists{/gist_id}","starred_url":"https://api.github.com/users/fierceX/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fierceX/subscriptions","organizations_url":"https://api.github.com/users/fierceX/orgs","repos_url":"https://api.github.com/users/fierceX/repos","events_url":"https://api.github.com/users/fierceX/events{/privacy}","received_events_url":"https://api.github.com/users/fierceX/received_events","type":"User","site_admin":false},"labels":[{"id":963101581,"node_id":"MDU6TGFiZWw5NjMxMDE1ODE=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/discussion","name":"discussion","color":"c5def5","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-02-12T09:22:09Z","updated_at":"2019-02-14T18:34:44Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"`gluonnlp.data.batchify.Pad` should be added with a parameter of maximum length.\r\nBecause variable-length input increases memory usage, it is more obvious to pre-train the bert model on the squad dataset.\r\n\r\nIn the https://github.com/dmlc/gluon-nlp/pull/493/commits/7219400545f15a7597d1bde98efd608ef96bcbd0 submission, I used `gluonnlp.data.batchify.Pad` to dynamically fill the length of the batch.\r\nHowever, it will cause a significant increase in the amount of memory used.\r\n\r\nThe specific performance is to use `python finetune_squad.py --optimizer adam --batch_size 12 --lr 3e-5 --epochs 2 --gpu` when running the script.\r\n\r\nPreviously occupied no more than 13G of GPU memory.\r\nHowever, after use, the 16G V100 will have insufficient memory.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/590/timeline","performed_via_github_app":null,"state_reason":null}