{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/986","id":512358600,"node_id":"MDU6SXNzdWU1MTIzNTg2MDA=","number":986,"title":"Consider streamlining BERT code?","user":{"login":"fierceX","id":13912058,"node_id":"MDQ6VXNlcjEzOTEyMDU4","avatar_url":"https://avatars.githubusercontent.com/u/13912058?v=4","gravatar_id":"","url":"https://api.github.com/users/fierceX","html_url":"https://github.com/fierceX","followers_url":"https://api.github.com/users/fierceX/followers","following_url":"https://api.github.com/users/fierceX/following{/other_user}","gists_url":"https://api.github.com/users/fierceX/gists{/gist_id}","starred_url":"https://api.github.com/users/fierceX/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fierceX/subscriptions","organizations_url":"https://api.github.com/users/fierceX/orgs","repos_url":"https://api.github.com/users/fierceX/repos","events_url":"https://api.github.com/users/fierceX/events{/privacy}","received_events_url":"https://api.github.com/users/fierceX/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2019-10-25T07:46:53Z","updated_at":"2019-10-25T23:17:33Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"## Description\r\nHave you considered simplifying the BERT model code? I think the BERT model code is too complicated now.\r\nNow the BERT main model code uses `attention_cell.py`, `seq2seq_encoder_decoder.py`, `transformer.py`, `bert.py`, a total of four files, which I think is unreasonable.\r\nFor example: `BERTEncoderCel`l inherits from `BaseTransformerEncoderCel`l, but only the default parameters are different.` BERTEncoder` inherits from `BaseTransformerEncoder`, but only the default parameters are different. `BERTLayerNorm` is inherited from `nn.LayerNorm`, but only the default parameters are different.\r\n`BERTPositionwiseFFN` inherits from `BasePositionwiseFFN`, but only the default parameters are different.\r\nAdded a `use_bert` parameter to the parameter, but only to use some default parameters, so why not set some default parameters directly. The bert-related code is placed in `bert.py`, and the base class is stored in the `transformer.py` file, but when use_bert=True, there is a need to import some subclass code with default parameters from `bert.py`. In addition:\r\nThe `BaseTransformerEncoder` base class also inherits from` Seq2SeqEncoder`, but `Seq2SeqEncoder` does not have additional code. The `_get_attention_cell` function I think should be in `utils.py` or `attention_cell.py` instead of `seq2seq_encoder_decoder.py`.\r\n@dmlc/gluon-nlp-committers \r\n\r\n## References\r\n- list reference and related literature\r\n- list known implementations\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/986/timeline","performed_via_github_app":null,"state_reason":null}