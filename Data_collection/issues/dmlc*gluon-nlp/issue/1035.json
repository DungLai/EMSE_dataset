{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1035","id":532528627,"node_id":"MDU6SXNzdWU1MzI1Mjg2Mjc=","number":1035,"title":"Optimize Inference Performance on CPU","user":{"login":"carter54","id":26741594,"node_id":"MDQ6VXNlcjI2NzQxNTk0","avatar_url":"https://avatars.githubusercontent.com/u/26741594?v=4","gravatar_id":"","url":"https://api.github.com/users/carter54","html_url":"https://github.com/carter54","followers_url":"https://api.github.com/users/carter54/followers","following_url":"https://api.github.com/users/carter54/following{/other_user}","gists_url":"https://api.github.com/users/carter54/gists{/gist_id}","starred_url":"https://api.github.com/users/carter54/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/carter54/subscriptions","organizations_url":"https://api.github.com/users/carter54/orgs","repos_url":"https://api.github.com/users/carter54/repos","events_url":"https://api.github.com/users/carter54/events{/privacy}","received_events_url":"https://api.github.com/users/carter54/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-12-04T08:37:02Z","updated_at":"2020-02-13T08:39:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Description\r\nthe news in https://github.com/dmlc/gluon-nlp/releases/tag/v0.8.1 shows BERT int8 quantization is presented in blog\r\nhttps://medium.com/apache-mxnet/optimization-for-bert-inference-performance-on-cpu-3bb2413d376c\r\nBut the blog only shows some results of BERT quantization test, \r\n>  The work on low precision deployment is still ongoing and involves un-released SW, the reproduction instructions will be available later.\r\n\r\n\r\nWhen will this work be released and can we apply this quantization method on GPT2? \r\n\r\nThanks a lot for the great work!\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1035/timeline","performed_via_github_app":null,"state_reason":null}