{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/970","id":505968389,"node_id":"MDU6SXNzdWU1MDU5NjgzODk=","number":970,"title":"Changes to GPT2BPETokenizer to allow loading custom vocab","user":{"login":"spandanagella","id":2839657,"node_id":"MDQ6VXNlcjI4Mzk2NTc=","avatar_url":"https://avatars.githubusercontent.com/u/2839657?v=4","gravatar_id":"","url":"https://api.github.com/users/spandanagella","html_url":"https://github.com/spandanagella","followers_url":"https://api.github.com/users/spandanagella/followers","following_url":"https://api.github.com/users/spandanagella/following{/other_user}","gists_url":"https://api.github.com/users/spandanagella/gists{/gist_id}","starred_url":"https://api.github.com/users/spandanagella/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spandanagella/subscriptions","organizations_url":"https://api.github.com/users/spandanagella/orgs","repos_url":"https://api.github.com/users/spandanagella/repos","events_url":"https://api.github.com/users/spandanagella/events{/privacy}","received_events_url":"https://api.github.com/users/spandanagella/received_events","type":"User","site_admin":false},"labels":[{"id":890393503,"node_id":"MDU6TGFiZWw4OTAzOTM1MDM=","url":"https://api.github.com/repos/dmlc/gluon-nlp/labels/enhancement","name":"enhancement","color":"135caf","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-10-11T17:21:14Z","updated_at":"2019-10-14T22:26:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Currently GPT2BPETokenizer do not allow us to load custom vocab and loads from a specific file location and the path cannot be overriden.\r\n\r\nCan you make changes to keep it consistent with BERTTokenizer so that we can load GPT2BPETokenizer with custom vocab and not always rely on downloading vocab from model zoo?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/970/timeline","performed_via_github_app":null,"state_reason":null}