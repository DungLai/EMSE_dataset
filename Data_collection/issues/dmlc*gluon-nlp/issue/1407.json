{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407","repository_url":"https://api.github.com/repos/dmlc/gluon-nlp","labels_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407/labels{/name}","comments_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407/comments","events_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407/events","html_url":"https://github.com/dmlc/gluon-nlp/issues/1407","id":730403955,"node_id":"MDU6SXNzdWU3MzA0MDM5NTU=","number":1407,"title":"[Proposal] Unified Interface/Implementation for Sparse Attention","user":{"login":"ZiyueHuang","id":24650346,"node_id":"MDQ6VXNlcjI0NjUwMzQ2","avatar_url":"https://avatars.githubusercontent.com/u/24650346?v=4","gravatar_id":"","url":"https://api.github.com/users/ZiyueHuang","html_url":"https://github.com/ZiyueHuang","followers_url":"https://api.github.com/users/ZiyueHuang/followers","following_url":"https://api.github.com/users/ZiyueHuang/following{/other_user}","gists_url":"https://api.github.com/users/ZiyueHuang/gists{/gist_id}","starred_url":"https://api.github.com/users/ZiyueHuang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ZiyueHuang/subscriptions","organizations_url":"https://api.github.com/users/ZiyueHuang/orgs","repos_url":"https://api.github.com/users/ZiyueHuang/repos","events_url":"https://api.github.com/users/ZiyueHuang/events{/privacy}","received_events_url":"https://api.github.com/users/ZiyueHuang/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2020-10-27T12:24:42Z","updated_at":"2020-10-28T10:57:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Currently several schemes of sparse attention (e.g. block-sparse, sliding window) relies on the handcrafted kernels, and it takes plenty of effort to implement new schemes (for research or other purpose). We may consider adopting a unified interface and implementation based on SpMM.\r\n\r\nWe can require an attention mask (defined by the user, and could be dynamically learned/generated in runtime) as the input. The attention mask is a CSR/COO matrix (or other) of shape `(n, n)`, where `n` denotes the sequence length, the `i`-th token should attend to the `j`-th token if `atten_mask[i, j] = 1`.\r\n\r\n- Given `query` and `key`, to compute `attention_score`, we just need compute a dot product (between `i`-th token and `j`-th token) for each non-zero coordinate in the attention mask. The output, `attention_score`, is also in CSR/COO format, same as `atten_mask`.\r\n- Given `attention_score`, to compute `attention_weight`, we need a softmax kernel over the sparse tensors, maybe we can reuse the kernel here https://github.com/google-research/sputnik/blob/master/sputnik/softmax/sparse_softmax.h. The output, `attention_weight`, is also in CSR/COO format, same as `attention_score`.\r\n- Given `attention_weight` and `value`, to compute `context`, we need a matrix multiplication kernel between CSR/COO tensor and the dense tensor, which may exist in CuSparse or we can reuse the kernels we develop in MXNet Sparse module years ago.\r\n\r\nThis paper could be useful https://arxiv.org/pdf/2006.10901.pdf, and the source code https://github.com/google-research/sputnik.\r\n\r\nThen, to provide sliding window attention, block-sparse attention or other schemes, we just need generate the corresponding attention mask, which should be quite easy.\r\n\r\ncc @szhengac @sxjscience ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/dmlc/gluon-nlp/issues/1407/timeline","performed_via_github_app":null,"state_reason":null}