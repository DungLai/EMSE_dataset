{"url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49","repository_url":"https://api.github.com/repos/GantMan/nsfw_model","labels_url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49/labels{/name}","comments_url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49/comments","events_url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49/events","html_url":"https://github.com/GantMan/nsfw_model/issues/49","id":572300502,"node_id":"MDU6SXNzdWU1NzIzMDA1MDI=","number":49,"title":"Better training on TF 2.1","user":{"login":"TechnikEmpire","id":11234763,"node_id":"MDQ6VXNlcjExMjM0NzYz","avatar_url":"https://avatars.githubusercontent.com/u/11234763?v=4","gravatar_id":"","url":"https://api.github.com/users/TechnikEmpire","html_url":"https://github.com/TechnikEmpire","followers_url":"https://api.github.com/users/TechnikEmpire/followers","following_url":"https://api.github.com/users/TechnikEmpire/following{/other_user}","gists_url":"https://api.github.com/users/TechnikEmpire/gists{/gist_id}","starred_url":"https://api.github.com/users/TechnikEmpire/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TechnikEmpire/subscriptions","organizations_url":"https://api.github.com/users/TechnikEmpire/orgs","repos_url":"https://api.github.com/users/TechnikEmpire/repos","events_url":"https://api.github.com/users/TechnikEmpire/events{/privacy}","received_events_url":"https://api.github.com/users/TechnikEmpire/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":19,"created_at":"2020-02-27T19:21:33Z","updated_at":"2020-03-03T17:24:23Z","closed_at":"2020-02-28T18:11:45Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Just FYI, if you preprocess images and shove them through the [`make_image_classifier.py`](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier) module of TF hub, you can train much faster and get to much higher accuracy, quicker.\r\n\r\nProcess is:\r\n - Merge all images into the five folders. Do not separate into train, test and val. The script will do this for you later.\r\n - Resize all images down to 224x before hand.\r\n - Set batch size to 1024-ish (depends on memory/video card memory)\r\n - Pass `--do_fine_tuning` to the script as well.\r\n - Before executing, edit the model compilation code in the TF-Hub module to separate the final softmax layer from the previous dense layer. The code (as it is in Google's repo) combines both with `layer.Dense(... activation:softmax...).` You need to delete the activation from that dense layer and append a separate softmax layer immediately after it as the final layer and explicitly set its data type to float32. The reason you're doing this is because you're going to also edit the script to configure TF to use mixed precision to drastically boost computation speed, so long as you have a fairly recent nvidia card. Follow [these instructions](https://www.tensorflow.org/guide/keras/mixed_precision). The config must be changed before initializing any layers.\r\n - Lastly, edit the `main()` definition of the script to call `set_memory_growth(YOUR_FIRST_GPU)` to true, as demonstrated [here](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth#for_example). CUDNN will die if you don't.\r\n\r\nIn order to do that last step, you'll need to ***not*** install TF hub from pip, but rather git clone tf hub's code. Once cloned, you'll need to modify the imports of the `make_image_classifier` script to pull the local files in the same dir as it, then execute those modified scripts.\r\n\r\nI'm only on epoch 3 of 10 (not 100 anymore) and I'm hitting 92.57% validation accuracy. I'm doing this on a pretty junky AMD machine that I popped an RTX 2060 into. Before doing the preprocessing, I was facing growing old waiting for this to get somewhere. I thought I'd share this because I've seen many other comments about people taking days to run training. It's only going to take ~2 hours to run through to epoch 10.\r\n\r\nFor greater clarity what's happening here is that you're not just transfer learning, you're telling TF to specialize the lower layers of the model to the domain, which increases accuracy by \"a few points\" [according to Google](https://www.tensorflow.org/tutorials/images/transfer_learning#continue_training_the_model).\r\n\r\nHope this helps someone.","closed_by":{"login":"TechnikEmpire","id":11234763,"node_id":"MDQ6VXNlcjExMjM0NzYz","avatar_url":"https://avatars.githubusercontent.com/u/11234763?v=4","gravatar_id":"","url":"https://api.github.com/users/TechnikEmpire","html_url":"https://github.com/TechnikEmpire","followers_url":"https://api.github.com/users/TechnikEmpire/followers","following_url":"https://api.github.com/users/TechnikEmpire/following{/other_user}","gists_url":"https://api.github.com/users/TechnikEmpire/gists{/gist_id}","starred_url":"https://api.github.com/users/TechnikEmpire/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TechnikEmpire/subscriptions","organizations_url":"https://api.github.com/users/TechnikEmpire/orgs","repos_url":"https://api.github.com/users/TechnikEmpire/repos","events_url":"https://api.github.com/users/TechnikEmpire/events{/privacy}","received_events_url":"https://api.github.com/users/TechnikEmpire/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/GantMan/nsfw_model/issues/49/timeline","performed_via_github_app":null,"state_reason":"completed"}