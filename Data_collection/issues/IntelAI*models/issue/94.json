{"url":"https://api.github.com/repos/IntelAI/models/issues/94","repository_url":"https://api.github.com/repos/IntelAI/models","labels_url":"https://api.github.com/repos/IntelAI/models/issues/94/labels{/name}","comments_url":"https://api.github.com/repos/IntelAI/models/issues/94/comments","events_url":"https://api.github.com/repos/IntelAI/models/issues/94/events","html_url":"https://github.com/IntelAI/models/issues/94","id":1072816239,"node_id":"I_kwDOCTQkLM4_8eBv","number":94,"title":"Error met when running BERT training on multiple nodes","user":{"login":"Eugene-Mark","id":51333308,"node_id":"MDQ6VXNlcjUxMzMzMzA4","avatar_url":"https://avatars.githubusercontent.com/u/51333308?v=4","gravatar_id":"","url":"https://api.github.com/users/Eugene-Mark","html_url":"https://github.com/Eugene-Mark","followers_url":"https://api.github.com/users/Eugene-Mark/followers","following_url":"https://api.github.com/users/Eugene-Mark/following{/other_user}","gists_url":"https://api.github.com/users/Eugene-Mark/gists{/gist_id}","starred_url":"https://api.github.com/users/Eugene-Mark/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Eugene-Mark/subscriptions","organizations_url":"https://api.github.com/users/Eugene-Mark/orgs","repos_url":"https://api.github.com/users/Eugene-Mark/repos","events_url":"https://api.github.com/users/Eugene-Mark/events{/privacy}","received_events_url":"https://api.github.com/users/Eugene-Mark/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-12-07T01:35:40Z","updated_at":"2021-12-07T01:35:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Issue:**\r\n/usr/bin/python3 common/tensorflow/run_tf_benchmark.py --framework=tensorflow --\r\nuse-case=language_modeling --model-name=bert_large --precision=fp32 --mode=train\r\ning --benchmark-dir=/dl/intel_train/models/benchmarks --intelai-models=/dl/intel\r\n_train/models/benchmarks/../models/language_modeling/tensorflow/bert_large --num\r\n-cores=-1 --batch-size=32 --socket-id=-1 --output-dir=/dl/intel_train/glue-outpu\r\nt --num-train-steps=1 --benchmark-only --num-intra-threads=10 --disable-tcmalloc\r\n=True --train-option=Classifier --init-checkpoint=/dl/intel_train/ckpt-bert-base\r\n/uncased_L-12_H-768_A-12/bert_model.ckpt --task-name=MRPC --vocab-file=/dl/intel\r\n_train/ckpt-bert-base/uncased_L-12_H-768_A-12/vocab.txt --config-file=/dl/intel_\r\ntrain/ckpt-bert-base/uncased_L-12_H-768_A-12/bert_config.json --do-train=true --\r\nnum-train-epochs=30 --learning-rate=2e-5 --max-seq-length=128 --do-eval=true --d\r\nata-dir=/dl/intel_train/glue/results4/MRPC --do-lower-case=True --experimental-g\r\nelu=False --optimized-softmax=True\r\n<2> is invalid\r\nlibnuma: Warning: node argument 2 is out of range\r\n\r\nusage: numactl [--all | -a] [--interleave= | -i <nodes>] [--preferred= | -p <nod\r\ne>]\r\n\r\n**Env:**\r\nOS: Fedora release 29 (Twenty Nine)   \r\nWhether container: bare metal  \r\nModel: bert base  \r\nReference guideline:    \r\nhttps://github.com/IntelAI/models/tree/master/benchmarks/language_modeling/tensorflow/bert_large/training/fp32  \r\nNumber_of_nodes: 2\r\nSocket_of_nodes: 2\r\n\r\n**The code where throws the issue:**\r\nStart.sh -> run_model()->eval ${CMD} 2>&1 | tee ${LOGFILE}\r\n\r\nNote, the ${CMD} after preprocessing is:\r\n_/usr/bin/python3 common/tensorflow/run_tf_benchmark.py --f\r\nramework=tensorflow --use-case=language_modeling --model-name=bert_large --preci\r\nsion=fp32 --mode=training --benchmark-dir=/dl/intel_train/models/benchmarks --in\r\ntelai-models=/dl/intel_train/models/benchmarks/../models/language_modeling/tenso\r\nrflow/bert_large --num-cores=-1 --batch-size=32 --socket-id=-1 --output-dir=/dl/\r\nintel_train/glue-output --num-train-steps=1   --benchmark-only    --disable-tcma\r\nlloc=True --train-option=Classifier --init-checkpoint=/dl/intel_train/ckpt-bert-\r\nbase/uncased_L-12_H-768_A-12/bert_model.ckpt --task-name=MRPC --vocab-file=/dl/i\r\nntel_train/ckpt-bert-base/uncased_L-12_H-768_A-12/vocab.txt --config-file=/dl/in\r\ntel_train/ckpt-bert-base/uncased_L-12_H-768_A-12/bert_config.json --do-train=tru\r\ne --num-train-epochs=30 --learning-rate=2e-5 --max-seq-length=128 --do-eval=true\r\n--data-dir=/dl/intel_train/glue/results4/MRPC --do-lower-case=True --experiment\r\nal-gelu=False --optimized-softmax=True_                                          \r\n\r\n**The step to launch the training:**\r\n•\tGit clone intel model on each node with same directory\r\n•\tPrepare glue data on each node with same directory\r\n•\tPrepare bert model on each node with same directory\r\n•\tMake sure for each node, distributed mode on two sockets is workable per the guide. The train result can be generated on each node.\r\n•\tTo support multiple instances, there is no guide for Bert so I’m referring the resnet related training https://github.com/IntelAI/models/blob/master/benchmarks/image_recognition/tensorflow/resnet50v1_5/training/fp32/Advanced.md\r\npython launch_benchmark.py \\\r\n  --verbose \\\r\n  --model-name resnet50v1_5 \\\r\n  --precision fp32 \\\r\n  --mode training \\\r\n  --framework tensorflow \\\r\n  --noinstall \\\r\n  --checkpoint ${OUTPUT_DIR} \\\r\n  --data-location ${DATASET_DIR} \\\r\n  --mpi_hostnames 'host1,host2' \\\r\n  --mpi_num_processes 4 2>&1\r\n•\tI just modify the fp32_classifier_training.sh directly in launching entry, something like below:\r\nsource \"${MODEL_DIR}/quickstart/common/utils.sh\"\r\n_command python3 ${MODEL_DIR}/benchmarks/launch_benchmark.py \\\r\n  --model-name=bert_large \\\r\n  --precision=fp32 \\\r\n  --mode=training \\\r\n  --framework=tensorflow \\\r\n  --batch-size=32 \\\r\n  ${mpi_num_proc_arg} \\\r\n  --mpi_hostnames='vsr140,vsr143' \\\r\n  --output-dir=$OUTPUT_DIR \\\r\n  $@ \\\r\n  -- train-option=Classifier \\\r\n  task-name=MRPC \\\r\n  do-train=true \\\r\n  do-eval=true \\\r\n  data-dir=$DATASET_DIR/MRPC \\\r\n  vocab-file=$CHECKPOINT_DIR/vocab.txt \\\r\n  config-file=$CHECKPOINT_DIR/bert_config.json \\\r\n  init-checkpoint=$CHECKPOINT_DIR/bert_model.ckpt \\\r\n  max-seq-length=128 \\\r\n  learning-rate=2e-5 \\\r\n  num-train-epochs=30 \\\r\n  optimized_softmax=True \\\r\n  experimental_gelu=False \\\r\n  do-lower-case=True \\\r\n\r\n**what we have tried**\r\nWe found some numactl related code under common/base_model_init.py so that we added some print out there, but found the error is threw before the numactl is invoked. \r\n\r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/IntelAI/models/issues/94/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/IntelAI/models/issues/94/timeline","performed_via_github_app":null,"state_reason":null}