[{"url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/comments/708519055","html_url":"https://github.com/ialhashim/DenseDepth/issues/152#issuecomment-708519055","issue_url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/152","id":708519055,"node_id":"MDEyOklzc3VlQ29tbWVudDcwODUxOTA1NQ==","user":{"login":"ialhashim","id":2434978,"node_id":"MDQ6VXNlcjI0MzQ5Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2434978?v=4","gravatar_id":"","url":"https://api.github.com/users/ialhashim","html_url":"https://github.com/ialhashim","followers_url":"https://api.github.com/users/ialhashim/followers","following_url":"https://api.github.com/users/ialhashim/following{/other_user}","gists_url":"https://api.github.com/users/ialhashim/gists{/gist_id}","starred_url":"https://api.github.com/users/ialhashim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ialhashim/subscriptions","organizations_url":"https://api.github.com/users/ialhashim/orgs","repos_url":"https://api.github.com/users/ialhashim/repos","events_url":"https://api.github.com/users/ialhashim/events{/privacy}","received_events_url":"https://api.github.com/users/ialhashim/received_events","type":"User","site_admin":false},"created_at":"2020-10-14T16:34:08Z","updated_at":"2020-10-14T16:34:08Z","author_association":"OWNER","body":"I tried looking up my old script for KITTI (using Keras). I relevant line of code is the resolution of my training\r\n\r\n```\r\nshape_rgb = (batch_size, 384, 1248, 3)\r\nshape_depth = (batch_size, 384 // 2, 1248 // 2, 1)\r\n```\r\n\r\nThat's one thing to consider. In any case, I hope this script helps.\r\n\r\n```\r\nimport os, time, argparse, pathlib, sys, os.path\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport keras as keras\r\nfrom keras import backend as K\r\nfrom keras import losses, applications\r\nfrom keras.models import Model, load_model\r\nfrom keras.layers import Input, InputLayer, Conv2D, Activation, LeakyReLU, Concatenate, Lambda\r\nfrom keras.applications.densenet import DenseNet121, DenseNet169, preprocess_input, decode_predictions\r\nfrom keras.utils import multi_gpu_model\r\nfrom keras.utils.vis_utils import plot_model\r\n\r\n# Custom layers\r\nfrom layers import BilinearUpSampling2D\r\n\r\nfrom keras_efficientnets import EfficientNetB5\r\n\r\n#========================================================================================\r\n# Hyperparameters\r\n#========================================================================================\r\nap = argparse.ArgumentParser()\r\nap.add_argument('--lr', type=float, default=0.0001, help='Learning rate (default 0.0001)')\r\nap.add_argument('--rgb', type=int, default=480, help='Input RGB size')\r\nap.add_argument('--depth', type=int, default=240, help='Input Depth size')\r\nap.add_argument('--bs', type=int, default=0, help='Batch size')\r\nap.add_argument('--epochs', type=int, default=50, help='Number of epochs')\r\nap.add_argument('--gpus', type=int, default=1, help='# of GPUs to use for training')\r\nap.add_argument('--gpuids', type=str, default='0')\r\nap.add_argument('--auglevel', type=int, default=0, help='Augmentation multiplier (default 0)')\r\nap.add_argument('--augment', dest='augment', action='store_true')\r\nap.add_argument('--usedepth', dest='usedepth', action='store_false')\r\nap.add_argument('--padam', dest='padam', action='store_true')\r\nap.add_argument('--halffeatures', dest='halffeatures', action='store_true')\r\nap.add_argument('--twohundred', dest='twohundred', action='store_true')\r\nap.add_argument('--theta', type=float, default=0.1, help='Loss parameter')\r\nap.add_argument('--debug', dest='debug', action='store_true')\r\nap.add_argument('--addnoise', dest='addnoise', action='store_true')\r\nap.add_argument('--erase', dest='erase', action='store_true')\r\nap.add_argument('--flip', dest='flip', action='store_true')\r\nap.add_argument('--noshuffle', dest='noshuffle', action='store_true')\r\nap.add_argument('--multirgb', dest='multirgb', action='store_true')\r\nap.add_argument('--multidepth', dest='multidepth', action='store_true')\r\nap.add_argument('--name', type=str, default='kitti_monodepth_efficent_net_b5')\r\nap.add_argument('--verbose', dest='verbose', action='store_true')\r\nap.add_argument('--from', type=str, default='', help='Start training from an existing model.')\r\nap.add_argument('--quick', dest='quick', action='store_true')\r\nap.add_argument('--leftonly', dest='leftonly', action='store_true')\r\n\r\nargs = vars(ap.parse_args())\r\n\r\nmynetname           = args['name']\r\nnumGPU              = args['gpus']\r\naugment_level       = args['auglevel']\r\nis_augment          = args['augment']\r\nis_invdepth         = args['usedepth']\r\nis_addnoise         = args['addnoise']\r\nis_erase            = args['erase']\r\nis_flip             = args['flip']\r\nis_padam            = args['padam']\r\nis_halffeatures     = args['halffeatures']\r\nis_twohundred       = args['twohundred']\r\nis_quick            = args['quick']\r\nis_leftonly         = args['leftonly']\r\nverbose             = args['verbose']\r\nresolution_RGB      = args['rgb']\r\nresolution_Depth    = args['depth']\r\nepochs              = args['epochs']\r\nbatch_size          = args['bs']\r\nlrate               = args['lr']\r\ntheta               = args['theta']\r\nexisting            = args['from']\r\n\r\n#========================================================================================\r\n# Depth values processing\r\n#========================================================================================\r\nminDepth = 1.0\r\nmaxDepth = 80.0\r\n\r\ndef DepthNorm(depth):\r\n    if not is_invdepth: return depth\r\n    return maxDepth / depth\r\n\r\nnormMaxDepth = maxDepth\r\nif is_invdepth: normMaxDepth = DepthNorm(minDepth)\r\n\r\nis_debug        = args['debug']\r\nisMultipleRGB   = args['multirgb']\r\nisMultipleDepth = args['multidepth']\r\n\r\nif is_invdepth: print('Using loss of 1.0 / depth.')\r\n\r\n#========================================================================================\r\n# Other\r\n#========================================================================================\r\nif numGPU == 1: \r\n    os.environ['CUDA_VISIBLE_DEVICES']=args['gpuids']\r\n    print('Will use GPU ' + args['gpuids'])\r\nelse:\r\n    print('Will use ' + str(numGPU) + ' GPUs.')\r\n\r\nif is_debug: print('Warning: debug dataset.')\r\n\r\nprint('Theta set to ', theta)\r\n\r\nif len(existing): \r\n    if os.path.isfile(existing):\r\n        print('Restarting training from:', existing)\r\n    else:\r\n        print('[!] Model file cannot be found:', existing)\r\n        exit(2)\r\n\r\n#========================================================================================\r\n# Data\r\n#========================================================================================\r\nwith open(__file__, 'r') as training_script:\r\n    training_script_content = training_script.read()\r\n\r\n# Input / output shapes\r\nshape_rgb = (batch_size, 384, 1248, 3)\r\nshape_depth = (batch_size, 384 // 2, 1248 // 2, 1)\r\n\r\n# Data root folder\r\ndata_root = 'F:/kitti' if (os.name == 'nt') else './kitti'\r\n\r\nif not os.path.exists(data_root):\r\n    data_root = '../kitti'\r\n\r\nfilled_depth_dir = 'kitti_filled_nyu' \r\ntraining_csv = 'kitti_train_left.csv'\r\n\r\nif is_quick:\r\n    filled_depth_dir = 'kitti_test_filled_nyu' \r\n    training_csv = 'kitti_test.csv'\r\n\r\n# Load training samples file and adjust paths\r\nimport csv\r\ncsv_reader = csv.DictReader(open(os.path.join(data_root, training_csv)), fieldnames=['id','rgb','depth'])\r\ntraining_samples = []\r\nfor row in csv_reader:\r\n    if is_leftonly and 'image_03' in row['rgb']:\r\n        continue    \r\n    for k,v in row.items(): \r\n        if 'id' not in k: row[k] = v.replace('F:/kitti', data_root)\r\n    training_samples.append(row)\r\n\r\n# Load validation samples\r\ntesting_samples = pathlib.Path(os.path.join(data_root, 'test_files_eigen.txt')).read_text().split('\\n')\r\nfor i,v in enumerate(testing_samples):\r\n    vsplit = v.split('/')\r\n    gt_path = os.path.join(data_root, 'kitti_eigen_test_gt/{}.npy'.format(i))\r\n    gt_interp_path = os.path.join(data_root, 'kitti_eigen_test_gt_interp/{}.npy'.format(i))\r\n    testing_samples[i] = {'id': i, 'rgb': os.path.join(data_root, v), 'gt': gt_path, 'gt_interp': gt_interp_path}\r\n\r\n# Data loaders:\r\nfrom keras.utils import Sequence\r\nfrom autoaugment import BasicPolicy\r\n\r\nfrom io import BytesIO\r\nfrom PIL import Image\r\nfrom skimage.transform import resize\r\nimport utils\r\n\r\nclass BasicAugmentRGBSequence(Sequence):\r\n    def __init__(self, dataset, batch_size):\r\n        self.dataset = dataset\r\n        self.policy = BasicPolicy( color_change_ratio=0.50, mirror_ratio=0.50, flip_ratio=0.0 if not is_flip else 0.2, \r\n                                    add_noise_peak=0 if not is_addnoise else 20, erase_ratio=-1.0 if not is_erase else 0.5)\r\n        self.batch_size = batch_size\r\n\r\n        from sklearn.utils import shuffle\r\n        self.dataset = shuffle(self.dataset, random_state=0)\r\n\r\n        self.N = len(self.dataset)\r\n\r\n    def __len__(self):\r\n        return int(np.ceil(self.N / float(self.batch_size)))\r\n\r\n    def __getitem__(self, idx, is_test=False):\r\n        batch_x, batch_y = np.zeros( shape_rgb ), np.zeros( shape_depth )\r\n        height, width = shape_rgb[1], shape_rgb[2]\r\n\r\n        # Augmentation of RGB images\r\n        for i in range(batch_x.shape[0]):\r\n            index = min((idx * self.batch_size) + i, self.N-1)\r\n\r\n            sample = self.dataset[index]\r\n\r\n            iid, rgb, depth = sample['id'], sample['rgb'], sample['depth']\r\n\r\n            #y_gt = np.clip(np.array(Image.open(depth)) / 256.0, 0.1, 80)\r\n            x = np.clip(np.array(Image.open(rgb)) / 255, 0, 1)\r\n            y_filled = np.clip(np.load(os.path.join(data_root, filled_depth_dir+'/'+iid.zfill(5)+'.npy')), 0.1, 80)\r\n            \r\n            #y_gt = DepthNorm(y_gt)\r\n\r\n            batch_x[i] = resize(x, (height, width), preserve_range=True, mode='reflect', anti_aliasing=True )\r\n            batch_y[i] = np.expand_dims(resize(y_filled, (height//2, width//2), preserve_range=True, mode='reflect', anti_aliasing=True ),-1)\r\n\r\n            if is_augment and not is_test:\r\n                x,y = batch_x[i], batch_y[i]\r\n\r\n                if is_erase:\r\n                    x,y = utils.cutout(x, y)\r\n                x,y = utils.shift_scale_rotate(x, y)\r\n\r\n                batch_x[i],batch_y[i] = x,np.clip(y, minDepth, maxDepth)\r\n\r\n            batch_y[i] = DepthNorm(batch_y[i])\r\n\r\n            batch_x[i], batch_y[i] = self.policy(batch_x[i], batch_y[i])\r\n\r\n            # DEBUG:\r\n            if is_debug: self.policy.debug_img(batch_x[i], np.clip(DepthNorm(batch_y[i])/maxDepth,0,1), idx, i)\r\n        #exit()\r\n\r\n        return batch_x, batch_y\r\n\r\nclass BasicRGBSequence(Sequence):\r\n    def __init__(self, dataset, batch_size):\r\n        self.dataset = dataset\r\n        self.batch_size = batch_size\r\n        self.N = len(self.dataset)\r\n\r\n    def __len__(self):\r\n        return int(np.ceil(self.N / float(self.batch_size)))\r\n\r\n    def __getitem__(self, idx):\r\n        batch_x, batch_y = np.zeros( shape_rgb ), np.zeros( shape_depth )        \r\n        height, width = shape_rgb[1], shape_rgb[2]\r\n\r\n        for i in range(self.batch_size):            \r\n            index = min((idx * self.batch_size) + i, self.N-1)\r\n\r\n            sample = self.dataset[index]\r\n\r\n            iid, rgb, gt, gt_interp = sample['id'], sample['rgb'], sample['gt'], sample['gt_interp']\r\n\r\n            x = np.clip(np.array(Image.open(rgb)) / 255, 0, 1)\r\n            y_gt = np.load(gt)\r\n\r\n            #y_gt = DepthNorm(y_gt)\r\n\r\n            batch_x[i] = resize(x, (height, width), preserve_range=True, mode='reflect', anti_aliasing=True )\r\n            batch_y[i] = np.expand_dims(resize(y_gt, (height//2, width//2), preserve_range=True, mode='reflect', anti_aliasing=False, order=0 ),-1)\r\n\r\n        return batch_x, batch_y\r\n\r\nif is_quick:\r\n    training_samples = training_samples[:10]\r\n    testing_samples = testing_samples[:10]\r\n\r\ntrain_generator = BasicAugmentRGBSequence(training_samples, batch_size=batch_size)\r\ntest_generator = BasicRGBSequence(testing_samples, batch_size=batch_size)\r\n\r\nif is_debug:\r\n    train_generator[i]\r\n    exit()\r\n\r\n#========================================================================================\r\n# Loss function\r\n#========================================================================================\r\ndef depth_loss_function(y_true, y_pred):\r\n    \r\n    # Weights\r\n    w1 = 1.0\r\n    w2 = 1.0\r\n    w3 = theta\r\n\r\n    # Special loss for testing\r\n    if K.learning_phase() is 0:\r\n        y_pred = DepthNorm(y_pred) # real depth values\r\n        return w3 * K.mean(K.abs(y_pred - y_true), axis=-1)\r\n\r\n    # Point-wise depth\r\n    l_depth = K.mean(K.abs(y_pred - y_true), axis=-1)\r\n\r\n    # Edges\r\n    dy_true, dx_true = tf.image.image_gradients(y_true)\r\n    dy_pred, dx_pred = tf.image.image_gradients(y_pred)\r\n    l_edges = K.mean(K.abs(dy_pred - dy_true) + K.abs(dx_pred - dx_true), axis=-1)\r\n\r\n    # Structural similarity (SSIM) index\r\n    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, normMaxDepth)) * 0.5, 0, 1)\r\n\r\n    return (w1 * l_ssim) + (w2 * K.mean(l_edges)) + (w3 * K.mean(l_depth))\r\n\r\n#========================================================================================\r\n# Preapare Outputs\r\n#========================================================================================\r\nrunID = str(int(time.time())) + '-n' + str(len(training_samples)) + '-r' + str(resolution_RGB)+'-d' + str(resolution_Depth) + \\\r\n    '-e' + str(epochs) + '-bs' + str(batch_size) + '-lr' + str(lrate) + '-thet' + str(theta) + '-' + mynetname\r\nif is_invdepth: runID = runID + '-invdepth'\r\nif is_debug: runID = runID + '-debug'\r\nif is_twohundred: runID = runID + '-201'\r\nif is_halffeatures: runID = runID + '-half'\r\nif is_padam: runID = runID + '-padam'\r\n\r\noutputPath = './Graphs/'\r\nrunPath = outputPath + runID\r\nprint('Output: ' + runPath)\r\npathlib.Path(runPath).mkdir(parents=True, exist_ok=True) \r\npathlib.Path(runPath+'/checkpoints').mkdir(parents=True, exist_ok=True) \r\npathlib.Path(runPath+'/samples').mkdir(parents=True, exist_ok=True) \r\n\r\n# Copy this script to output:\r\ntraining_script_content = '#' + str(sys.argv) + '\\n' + training_script_content\r\nwith open(runPath+'/'+__file__, 'w') as training_script: training_script.write(training_script_content)\r\n\r\nif len(existing) == 0:\r\n    #print('Loading base model (DenseNet)..')\r\n    # Encoder Layers\r\n    #if is_twohundred:\r\n    #    base_model = applications.DenseNet201(input_shape=(None, None, 3), include_top=False)\r\n    #else:\r\n    #    base_model = applications.DenseNet169(input_shape=(None, None, 3), include_top=False)\r\n\r\n    print('Loading base model (EfficentNet)..')\r\n\r\n    base_model = EfficientNetB5(input_shape=(384, 1248, 3), include_top=False, weights='imagenet')\r\n\r\n    print('Base model loaded.')\r\n\r\n    # Starting point for decoder\r\n    base_model_output_shape = base_model.layers[-1].output.shape\r\n\r\n    # Layer freezing?\r\n    for layer in base_model.layers: layer.trainable = True\r\n\r\n    # Starting number of decoder filters\r\n    if is_halffeatures:\r\n        decode_filters = int(int(base_model_output_shape[-1])/2)\r\n    else:\r\n        decode_filters = int(base_model_output_shape[-1])\r\n\r\n    # Define upsampling layer\r\n    def upproject(tensor, filters, name, concat_with):\r\n        up_i = BilinearUpSampling2D((2, 2), name=name+'_upsampling2d')(tensor)\r\n        up_i = Concatenate(name=name+'_concat')([up_i, base_model.get_layer(concat_with).output]) # Skip connection\r\n        up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convA')(up_i)\r\n        up_i = LeakyReLU(alpha=0.2)(up_i)\r\n        up_i = Conv2D(filters=filters, kernel_size=3, strides=1, padding='same', name=name+'_convB')(up_i)\r\n        up_i = LeakyReLU(alpha=0.2)(up_i)\r\n        return up_i\r\n\r\n    # Decoder Layers\r\n    decoder = Conv2D(filters=decode_filters, kernel_size=1, padding='same', input_shape=base_model_output_shape, name='conv2')(base_model.output)\r\n\r\n    decoder = upproject(decoder, int(decode_filters/2),  'up1', concat_with='depthwise_conv2d_15')\r\n    decoder = upproject(decoder, int(decode_filters/4),  'up2', concat_with='depthwise_conv2d_10')\r\n    decoder = upproject(decoder, int(decode_filters/8),  'up3', concat_with='depthwise_conv2d_5')\r\n    decoder = upproject(decoder, int(decode_filters/16), 'up4', concat_with='conv2d_1')\r\n\r\n    # Extract depths (final layer)\r\n    conv3 = Conv2D(filters=1, kernel_size=3, strides=1, padding='same', name='conv3')(decoder)\r\n\r\n    # Create the model\r\n    model = Model(inputs=base_model.input, outputs=conv3)\r\nelse:\r\n    # Load model from file\r\n    if not existing.endswith('.h5'):\r\n        sys.exit('Please provide a correct model file when using [existing] argument.')\r\n    custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': depth_loss_function}\r\n    model = load_model(existing, custom_objects=custom_objects)\r\n    print('\\nExisting model loaded.\\n')\r\n\r\nprint('Model created.')\r\n\r\n\r\n# Generate model plot\r\nplot_model(model, to_file=runPath+'/monodepth_model.svg', show_shapes=True, show_layer_names=True)\r\n\r\n# Save model summary to file\r\nfrom contextlib import redirect_stdout\r\nwith open(runPath+'/modelsummary.txt', 'w') as f:\r\n    with redirect_stdout(f):\r\n        model.summary()\r\nif verbose: model.summary()\r\n\r\n# Multi-gpu setup:\r\nbasemodel = model\r\nif numGPU > 1:\r\n    model = multi_gpu_model(model, gpus=numGPU)\r\n\r\n# Optimizer\r\nif is_padam:\r\n    from padam import Padam\r\n    optimizer = Padam(lr=lrate, partial=0.125, amsgrad=True)\r\nelse:\r\n    optimizer = keras.optimizers.Adam(lr=lrate, amsgrad=True)\r\n\r\n# Compile model\r\nprint('\\n\\n\\n', 'Compiling model..', runID, '\\n\\n\\tGPU ' \r\n        + (str(numGPU)+' gpus' if numGPU > 1 else args['gpuids'])\r\n        + '\\t\\tBatch size [ ' + str(batch_size) + ' ] '\r\n        + '\\t\\tRGB - Depth ' + str(resolution_RGB) + ' - ' + str(resolution_Depth) + ' \\n\\n')\r\nmodel.compile(loss=depth_loss_function, optimizer=optimizer)\r\n\r\n#========================================================================================\r\n# Training\r\n#========================================================================================\r\n\r\n## Callbacks:\r\ncallbacks = []\r\n\r\n# Callback: Learning Rate Scheduler\r\ncallbacks.append(keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=0.00009, min_delta=1e-2)) # reduce learning rate when stuck\r\n\r\n# Callback: show intermediate results and save current model\r\nimport utils\r\nfrom PIL import Image\r\ndef MyCheckPointCallback(epoch):\r\n\r\n    # Error metrics on KITTI's Eigen test set    \r\n    #if epoch % 3 == 0:\r\n    if True:\r\n        p = pathlib.Path(os.path.join(data_root, 'test_files_eigen.txt'))\r\n        files = p.read_text().split('\\n')\r\n        for i,v in enumerate(files): files[i] = os.path.join(p.parent, v)\r\n        our_results = np.zeros((len(files), 192, 624))\r\n\r\n        # Predict depth function with flip augmentation\r\n        def predictDepth(img_filename):\r\n            height, width = shape_rgb[1], shape_rgb[2]\r\n            im = resize(np.array(Image.open(img_filename)) / 255, (height, width), preserve_range=True, mode='reflect', anti_aliasing=True )\r\n            im_f = np.fliplr(im)\r\n            prediction = utils.predict(model, im, numChannels=1).reshape(shape_depth[1], shape_depth[2], 3)\r\n            prediction_f = utils.predict(model, im_f, numChannels=1).reshape(shape_depth[1], shape_depth[2], 3)        \r\n            return (0.5 * prediction) + (0.5 * np.fliplr(prediction_f))\r\n\r\n        # Compute depth for test dataset\r\n        print('Computing error meterics for test datset..')\r\n        count = 1\r\n        for i,v in enumerate(files):\r\n            our_results[i] = predictDepth(v)[:,:,0]\r\n            if count % 10 == 0: print('.', end='')\r\n            if count % 100 == 0: print('#', end='')\r\n            count = count + 1\r\n        import evaluate_kitti\r\n        print('Done depth.')\r\n        \r\n        # Evaluate metrics\r\n        evaluate_kitti.evaluate(pred_data=our_results, kitti_dir=data_root)\r\n        print('Done metrics.')\r\n\r\n    # Save intermediate model\r\n    #if epoch % 5 == 0 and not is_debug:\r\n    if True:\r\n        model_filename = runPath+'/model_epoch_'+str(epoch)+'.h5'\r\n        basemodel.save(model_filename)\r\n        print('Saved model to file:\\t', model_filename)\r\n\r\n    # Sampling\r\n    if epoch % 3 == 0 if not is_debug else epoch % 3 == 0:\r\n        # Sample validation set\r\n        for i in range(0,len(test_generator),int(len(test_generator)/5)):\r\n            img_x,img_y = test_generator[i]\r\n            img_x,img_y = img_x[0,:,:,:], img_y[0,:,:,:]\r\n\r\n            gt_rgb, gt_depth = img_x, img_y\r\n            prediction =  utils.predict(model, img_x, numChannels=1).reshape(shape_depth[1],shape_depth[2],3)\r\n\r\n            if is_invdepth:\r\n                #gt_depth = DepthNorm(gt_depth)\r\n                prediction = DepthNorm(prediction)   \r\n\r\n            prediction = np.clip(prediction, minDepth, maxDepth)/maxDepth*255\r\n            utils.save_png(np.clip(prediction,0,255), runPath+'/samples/i' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.OURS.SceneDepth.png')\r\n            \r\n            if epoch == 0:\r\n                def quick_fill(gt_depth):\r\n                    gt_depth = gt_depth[:,:,0]\r\n                    from scipy import interpolate\r\n                    valid_mask = ~(gt_depth==0)\r\n                    coords = np.array(np.nonzero(valid_mask)).T\r\n                    values = gt_depth[valid_mask]\r\n                    it = interpolate.LinearNDInterpolator(coords, values, fill_value=0)\r\n                    return it(list(np.ndindex(gt_depth.shape))).reshape(gt_depth.shape)\r\n                    \r\n                utils.save_png(gt_rgb*255, runPath+'/samples/i' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.GT.FinalImage.png')\r\n                gt_depth = np.clip(quick_fill(gt_depth).reshape(gt_depth.shape), minDepth, maxDepth)/maxDepth*255            \r\n                utils.save_png(np.clip(gt_depth,0,255), runPath+'/samples/i' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.GT.SceneDepth.png')\r\n\r\n        # Sample training set\r\n        gt_count = 4\r\n        gt_range = gt_count * 30\r\n        #for i in range(0, gt_range, int(gt_range / gt_count)):\r\n        for i in range(0, gt_range):\r\n            img_x,img_y = train_generator.__getitem__(i, True)\r\n            img_x,img_y = img_x[0,:,:,:], img_y[0,:,:,:]\r\n\r\n            gt_rgb, gt_depth = img_x, img_y\r\n            prediction =  utils.predict(model, img_x, numChannels=1).reshape(shape_depth[1],shape_depth[2],3)\r\n\r\n            if is_invdepth:\r\n                gt_depth = DepthNorm(gt_depth)\r\n                prediction = DepthNorm(prediction)  \r\n\r\n            prediction = np.clip(prediction, minDepth, maxDepth)/maxDepth*255\r\n            utils.save_png(np.clip(prediction,0,255), runPath+'/samples/gt' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.OURS.SceneDepth.png')\r\n            \r\n            if epoch == 0:\r\n                utils.save_png(gt_rgb*255, runPath+'/samples/gt' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.GT.FinalImage.png')\r\n                gt_depth = np.clip(gt_depth, minDepth, maxDepth)/maxDepth*255\r\n                utils.save_png(np.clip(gt_depth,0,255), runPath+'/samples/gt' + f'{i:04}' +'.e'+ f'{epoch:05}' + '.GT.SceneDepth.png')\r\n\r\ncallbacks.append( keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: MyCheckPointCallback(epoch)) )\r\n\r\n# Callback: Tensorboard\r\nclass LRTensorBoard(keras.callbacks.TensorBoard):\r\n    def __init__(self, log_dir):\r\n        super().__init__(log_dir=log_dir)\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        logs.update({'zlr': K.eval(self.model.optimizer.lr)})\r\n        super().on_epoch_end(epoch, logs)\r\ncallbacks.append( LRTensorBoard(log_dir=runPath) )\r\n\r\nmodel.fit_generator(train_generator, callbacks=callbacks, validation_data=test_generator, epochs=epochs, shuffle=True)\r\n\r\n## Save trained model:\r\nbasemodel.save(runPath+'/model.h5')\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/comments/708519055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"ialhashim","id":2434978,"node_id":"MDQ6VXNlcjI0MzQ5Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2434978?v=4","gravatar_id":"","url":"https://api.github.com/users/ialhashim","html_url":"https://github.com/ialhashim","followers_url":"https://api.github.com/users/ialhashim/followers","following_url":"https://api.github.com/users/ialhashim/following{/other_user}","gists_url":"https://api.github.com/users/ialhashim/gists{/gist_id}","starred_url":"https://api.github.com/users/ialhashim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ialhashim/subscriptions","organizations_url":"https://api.github.com/users/ialhashim/orgs","repos_url":"https://api.github.com/users/ialhashim/repos","events_url":"https://api.github.com/users/ialhashim/events{/privacy}","received_events_url":"https://api.github.com/users/ialhashim/received_events","type":"User","site_admin":false}},{"id":3877505975,"node_id":"MDExOkNsb3NlZEV2ZW50Mzg3NzUwNTk3NQ==","url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/events/3877505975","actor":{"login":"ialhashim","id":2434978,"node_id":"MDQ6VXNlcjI0MzQ5Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2434978?v=4","gravatar_id":"","url":"https://api.github.com/users/ialhashim","html_url":"https://github.com/ialhashim","followers_url":"https://api.github.com/users/ialhashim/followers","following_url":"https://api.github.com/users/ialhashim/following{/other_user}","gists_url":"https://api.github.com/users/ialhashim/gists{/gist_id}","starred_url":"https://api.github.com/users/ialhashim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ialhashim/subscriptions","organizations_url":"https://api.github.com/users/ialhashim/orgs","repos_url":"https://api.github.com/users/ialhashim/repos","events_url":"https://api.github.com/users/ialhashim/events{/privacy}","received_events_url":"https://api.github.com/users/ialhashim/received_events","type":"User","site_admin":false},"event":"closed","commit_id":null,"commit_url":null,"created_at":"2020-10-14T16:34:08Z","state_reason":null,"performed_via_github_app":null},{"url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/comments/708520772","html_url":"https://github.com/ialhashim/DenseDepth/issues/152#issuecomment-708520772","issue_url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/152","id":708520772,"node_id":"MDEyOklzc3VlQ29tbWVudDcwODUyMDc3Mg==","user":{"login":"arijitgupta42","id":42406413,"node_id":"MDQ6VXNlcjQyNDA2NDEz","avatar_url":"https://avatars.githubusercontent.com/u/42406413?v=4","gravatar_id":"","url":"https://api.github.com/users/arijitgupta42","html_url":"https://github.com/arijitgupta42","followers_url":"https://api.github.com/users/arijitgupta42/followers","following_url":"https://api.github.com/users/arijitgupta42/following{/other_user}","gists_url":"https://api.github.com/users/arijitgupta42/gists{/gist_id}","starred_url":"https://api.github.com/users/arijitgupta42/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arijitgupta42/subscriptions","organizations_url":"https://api.github.com/users/arijitgupta42/orgs","repos_url":"https://api.github.com/users/arijitgupta42/repos","events_url":"https://api.github.com/users/arijitgupta42/events{/privacy}","received_events_url":"https://api.github.com/users/arijitgupta42/received_events","type":"User","site_admin":false},"created_at":"2020-10-14T16:37:15Z","updated_at":"2020-10-16T18:45:36Z","author_association":"CONTRIBUTOR","body":"Thanks a lot for the script and for your time. I'll see if I can make it work using this.\r\n\r\nEDIT: The dimensions weren't the issue as I had accounted for that in the model as well. I managed to get the training loss and validation loss much lower by simply replacing the minDepth and maxDepth values by 1 and 80 in the train.py, loss.py, data.py and utils.py. ","reactions":{"url":"https://api.github.com/repos/ialhashim/DenseDepth/issues/comments/708520772/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"arijitgupta42","id":42406413,"node_id":"MDQ6VXNlcjQyNDA2NDEz","avatar_url":"https://avatars.githubusercontent.com/u/42406413?v=4","gravatar_id":"","url":"https://api.github.com/users/arijitgupta42","html_url":"https://github.com/arijitgupta42","followers_url":"https://api.github.com/users/arijitgupta42/followers","following_url":"https://api.github.com/users/arijitgupta42/following{/other_user}","gists_url":"https://api.github.com/users/arijitgupta42/gists{/gist_id}","starred_url":"https://api.github.com/users/arijitgupta42/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arijitgupta42/subscriptions","organizations_url":"https://api.github.com/users/arijitgupta42/orgs","repos_url":"https://api.github.com/users/arijitgupta42/repos","events_url":"https://api.github.com/users/arijitgupta42/events{/privacy}","received_events_url":"https://api.github.com/users/arijitgupta42/received_events","type":"User","site_admin":false}}]