{"url":"https://api.github.com/repos/allenai/scibert/issues/29","repository_url":"https://api.github.com/repos/allenai/scibert","labels_url":"https://api.github.com/repos/allenai/scibert/issues/29/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scibert/issues/29/comments","events_url":"https://api.github.com/repos/allenai/scibert/issues/29/events","html_url":"https://github.com/allenai/scibert/issues/29","id":428889239,"node_id":"MDU6SXNzdWU0Mjg4ODkyMzk=","number":29,"title":"Question Over Punctuation Charts in Vocab Creation","user":{"login":"antonyscerri","id":881190,"node_id":"MDQ6VXNlcjg4MTE5MA==","avatar_url":"https://avatars.githubusercontent.com/u/881190?v=4","gravatar_id":"","url":"https://api.github.com/users/antonyscerri","html_url":"https://github.com/antonyscerri","followers_url":"https://api.github.com/users/antonyscerri/followers","following_url":"https://api.github.com/users/antonyscerri/following{/other_user}","gists_url":"https://api.github.com/users/antonyscerri/gists{/gist_id}","starred_url":"https://api.github.com/users/antonyscerri/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/antonyscerri/subscriptions","organizations_url":"https://api.github.com/users/antonyscerri/orgs","repos_url":"https://api.github.com/users/antonyscerri/repos","events_url":"https://api.github.com/users/antonyscerri/events{/privacy}","received_events_url":"https://api.github.com/users/antonyscerri/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-04-03T17:32:52Z","updated_at":"2022-11-17T01:10:29Z","closed_at":"2019-06-29T06:29:00Z","author_association":"NONE","active_lock_reason":null,"body":"Hi\r\n\r\nInteresting to see you look at creating your own vocab. It appears with BERT they used a special variant and code hasn't been made available nor exact details of what was run. In your cheatsheet i've found reference to your use of Google's SentencePiece through the python wrapper by the looks of thing. I wondered if you had any more specific details on preparation and post processing as the output by default won't include custom tokens nor does it use ## etc. Pretty good idea how you have likely done most of it but would be nice to know for sure. Also in the cheatsheet the command used set the vocab to 31K not 30K and the length of your vocab files differs from those of BERT too.\r\n\r\nCould you also comment on the significance of the \"[unused15]\" kind of entries as the base BERT vocab has 994 of them where as you only have 100. I havent found any details on what these are used for and what created them.\r\n\r\nMore significantly (maybe) i was curious as to whether you had looked at any preprocessing (additional tokenisation) before running SentencePiece. The reason being that the BERT tokeniser does whitespace and punctuation splitting before word piece tokenisation over the resulting tokens. It looks as though this could part of what WordPiece does as well, based on the occurrence of lots of single character entries (with and without ## prefixed) and the lack of entries with punctuation characters in them along with letters or digits. For example you have entries like \"(1997),\", where as the BERT one doesnt have anything like this (with the exception of symbol characters not classes as punctuation). One issue with these entries as they stand is that when you apply the BERT tokeniser as part of task training you are never going to use this entry, so even though you have a ~30K vocab size a portion of it will not be used and therefor the neural model is going to have unused capacity. There is also the potential to change the number of possible occurrence of UNK tokens resulting from the tokenisation steps. \r\n\r\nFollowing on from that there is a question as to whether this has possible impacted (negatively or positively) the results you have seen as a possible side effect of this difference. So any more information on what may also have been tried in this area would be of interest to hear.\r\n\r\nThanks\r\n\r\nTony","closed_by":{"login":"kyleclo","id":13603748,"node_id":"MDQ6VXNlcjEzNjAzNzQ4","avatar_url":"https://avatars.githubusercontent.com/u/13603748?v=4","gravatar_id":"","url":"https://api.github.com/users/kyleclo","html_url":"https://github.com/kyleclo","followers_url":"https://api.github.com/users/kyleclo/followers","following_url":"https://api.github.com/users/kyleclo/following{/other_user}","gists_url":"https://api.github.com/users/kyleclo/gists{/gist_id}","starred_url":"https://api.github.com/users/kyleclo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kyleclo/subscriptions","organizations_url":"https://api.github.com/users/kyleclo/orgs","repos_url":"https://api.github.com/users/kyleclo/repos","events_url":"https://api.github.com/users/kyleclo/events{/privacy}","received_events_url":"https://api.github.com/users/kyleclo/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/allenai/scibert/issues/29/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scibert/issues/29/timeline","performed_via_github_app":null,"state_reason":"completed"}