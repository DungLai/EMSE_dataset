{"url":"https://api.github.com/repos/allenai/scibert/issues/68","repository_url":"https://api.github.com/repos/allenai/scibert","labels_url":"https://api.github.com/repos/allenai/scibert/issues/68/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scibert/issues/68/comments","events_url":"https://api.github.com/repos/allenai/scibert/issues/68/events","html_url":"https://github.com/allenai/scibert/issues/68","id":531843245,"node_id":"MDU6SXNzdWU1MzE4NDMyNDU=","number":68,"title":"Context embedding shows anomaly, independent of sentence and token","user":{"login":"RommeZetaAlpha","id":58464344,"node_id":"MDQ6VXNlcjU4NDY0MzQ0","avatar_url":"https://avatars.githubusercontent.com/u/58464344?v=4","gravatar_id":"","url":"https://api.github.com/users/RommeZetaAlpha","html_url":"https://github.com/RommeZetaAlpha","followers_url":"https://api.github.com/users/RommeZetaAlpha/followers","following_url":"https://api.github.com/users/RommeZetaAlpha/following{/other_user}","gists_url":"https://api.github.com/users/RommeZetaAlpha/gists{/gist_id}","starred_url":"https://api.github.com/users/RommeZetaAlpha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RommeZetaAlpha/subscriptions","organizations_url":"https://api.github.com/users/RommeZetaAlpha/orgs","repos_url":"https://api.github.com/users/RommeZetaAlpha/repos","events_url":"https://api.github.com/users/RommeZetaAlpha/events{/privacy}","received_events_url":"https://api.github.com/users/RommeZetaAlpha/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2019-12-03T09:27:35Z","updated_at":"2019-12-16T17:35:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Problem\r\nWhile doing some analysis on the pre-trained SciBert transformer networks, we found that there is an anomaly in the context embedding on index 422. Doing some more tests we found that this anomaly was there, independent of the context or the specific token and position. We think that the distance metrics in the contextualized embedding-space, such as cosine similarity, are heavily dominated by this exploding component. \r\n\r\n### scibert-scivocab-uncased\r\n\r\nTo see the actual contextual representation of words, we infered a sentence using the pretrained model. We performed this using both a TF and PyTorch version of the model to see if the results were similar.  Below we can see a crude representation of the embedding space (word + positional embedding) simply as a linear plot of the 768 components of the embedding for the word `linear` in the context of a regular sentence. The values seem nicely distributed around 0 and peaking just a few times just above 2 or -2, which is just as someone would expect the embedding space to look like.\r\n\r\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/YXFnDxX/embedding1.png\" alt=\"embedding1\" border=\"0\"></a>\r\n\r\nHowever, if we look at the contextualized embedding, after the last layer of the SciBert model, we find the embedding has turned into this representation below. The representation \"explodes\" at a particular index (422) very consistently across tokens and sentences.\r\n\r\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/jk9L1T4/embedding2.png\" alt=\"embedding2\" border=\"0\"></a>\r\n\r\nthe token is `linear` is used in the context of the sentence` How are linear regression and gradient descent related, is gradient descent a type of linear regression, and is it similar to ordinary least squares (OLS) and generalized least squares (GLS)?`\r\n\r\n### scibert-scivocab-cased\r\n\r\nWe now performed the same analysis using the cased version of the model, which gives the following results. Here we have the token: `difference`  for the sentence:\r\n`Computer Vision: What is the difference between HOG and SIFT feature descriptor?` The exploding index in this embedding is 421, meaning that the anomaly occurs at a different position.\r\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/dgPYG6N/Captura-de-Pantalla-2019-12-03-a-les-10-13-19.png\" alt=\"Captura-de-Pantalla-2019-12-03-a-les-10-13-19\" border=\"0\"></a>\r\n\r\nTo compared with the uncased model we did the same for the token `linear` for the sentence ` How are linear regression and gradient descent related, is gradient descent a type of linear regression, and is it similar to ordinary least squares (OLS) and generalized least squares (GLS)?` Here we see that the anomaly again is consistent, this also holds for other examples.\r\n\r\n<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/jLJMsVM/Captura-de-Pantalla-2019-12-03-a-les-10-16-39.png\" alt=\"Captura-de-Pantalla-2019-12-03-a-les-10-16-39\" border=\"0\"></a> \r\n\r\nFrom this we see that a similar problem occurs both in the cased and the uncased model, yet the issue is found on a different location from which we figure that something odd is happening in the training process. Again, we have tested this is a behavior in many other tokens and sentences and it's consistent across all circumstances.\r\n\r\nCould you explain if this behavior is expected by you and why? And otherwise could you explain what causes this behavior and how it can potentially be overcome?\r\n\r\nThanks in advance!d","closed_by":null,"reactions":{"url":"https://api.github.com/repos/allenai/scibert/issues/68/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scibert/issues/68/timeline","performed_via_github_app":null,"state_reason":null}