{"url":"https://api.github.com/repos/allenai/scibert/issues/100","repository_url":"https://api.github.com/repos/allenai/scibert","labels_url":"https://api.github.com/repos/allenai/scibert/issues/100/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scibert/issues/100/comments","events_url":"https://api.github.com/repos/allenai/scibert/issues/100/events","html_url":"https://github.com/allenai/scibert/issues/100","id":671515586,"node_id":"MDU6SXNzdWU2NzE1MTU1ODY=","number":100,"title":"Better performance than the reported.","user":{"login":"shizhediao","id":18120087,"node_id":"MDQ6VXNlcjE4MTIwMDg3","avatar_url":"https://avatars.githubusercontent.com/u/18120087?v=4","gravatar_id":"","url":"https://api.github.com/users/shizhediao","html_url":"https://github.com/shizhediao","followers_url":"https://api.github.com/users/shizhediao/followers","following_url":"https://api.github.com/users/shizhediao/following{/other_user}","gists_url":"https://api.github.com/users/shizhediao/gists{/gist_id}","starred_url":"https://api.github.com/users/shizhediao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shizhediao/subscriptions","organizations_url":"https://api.github.com/users/shizhediao/orgs","repos_url":"https://api.github.com/users/shizhediao/repos","events_url":"https://api.github.com/users/shizhediao/events{/privacy}","received_events_url":"https://api.github.com/users/shizhediao/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-08-02T05:02:51Z","updated_at":"2020-08-04T08:14:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\nI am reproducing the fine-tune results following your instruction.\r\nI work on your default code and my setting is as below. I work on ebmnlp and pico task without fine-tuning\r\n> `DATASET='ebmnlp', \r\n    TASK='pico',\r\nwith_finetuning='' #'_finetune'  # or '' for not fine tuning,\r\ndataset_size=38124`,\r\n\r\nHowever, I found my results are much better than the reported number in your paper. In scibert paper, the micro F1 is 68.30 (Frozen) and 72.28 (fine-tune), but in my experiment, the F1 is 75.48 (frozen).\r\n\r\n>   \"test_accuracy\": 0.8668599033816425,                                                                                                                                           \r\n  \"test_accuracy3\": 0.9632463768115942,                                                                                                                                          \r\n  \"test_F1_O\": 0.9153885841369629,                                                                                                                                               \r\n  \"test_F1_I-OUT\": 0.673623263835907,                                                                                                                                            \r\n  \"test_F1_I-PAR\": 0.7656552195549011,                                                                                                                                           \r\n  \"test_F1_I-INT\": 0.6646913886070251,                                                                                                                                           \r\n  \"test_avg_f1\": 0.754839614033699,                                                                                                                                              \r\n  \"test_loss\": 4.438140077646389  \r\n\r\nI am really confused. \r\nThanks","closed_by":null,"reactions":{"url":"https://api.github.com/repos/allenai/scibert/issues/100/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scibert/issues/100/timeline","performed_via_github_app":null,"state_reason":null}