{"url":"https://api.github.com/repos/allenai/scibert/issues/115","repository_url":"https://api.github.com/repos/allenai/scibert","labels_url":"https://api.github.com/repos/allenai/scibert/issues/115/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scibert/issues/115/comments","events_url":"https://api.github.com/repos/allenai/scibert/issues/115/events","html_url":"https://github.com/allenai/scibert/issues/115","id":876405771,"node_id":"MDU6SXNzdWU4NzY0MDU3NzE=","number":115,"title":"Is it expected the same sentence gives different features?","user":{"login":"towr","id":4736480,"node_id":"MDQ6VXNlcjQ3MzY0ODA=","avatar_url":"https://avatars.githubusercontent.com/u/4736480?v=4","gravatar_id":"","url":"https://api.github.com/users/towr","html_url":"https://github.com/towr","followers_url":"https://api.github.com/users/towr/followers","following_url":"https://api.github.com/users/towr/following{/other_user}","gists_url":"https://api.github.com/users/towr/gists{/gist_id}","starred_url":"https://api.github.com/users/towr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/towr/subscriptions","organizations_url":"https://api.github.com/users/towr/orgs","repos_url":"https://api.github.com/users/towr/repos","events_url":"https://api.github.com/users/towr/events{/privacy}","received_events_url":"https://api.github.com/users/towr/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-05-05T12:38:05Z","updated_at":"2021-05-06T08:08:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm a bit puzzled by something I encountered trying to encode sentences as embeddings. When I ran the sentences through the model one at a time, I got slightly different results from when I ran batches of sentences. \r\n\r\nI've reduced an example down to:\r\n```\r\nfrom transformers import pipeline\r\nimport numpy as np\r\n\r\np = pipeline('feature-extraction', model='allenai/scibert_scivocab_uncased')\r\ns = 'the scurvy dog walked home alone'.split()\r\n\r\nfor l in range(1,len(s)+1):\r\n    txt = ' '.join(s[:l])\r\n    \r\n    res1 = p(txt)\r\n    res2 = p(txt)\r\n    res1_2 = p([txt, txt])\r\n    print(l, txt, len(res1[0]))\r\n    print(all( np.allclose(i, j) for i, j in zip(res1[0], res2[0])),\r\n          all( np.allclose(i, j) for i, j in zip(res2[0], res1_2[0])),\r\n          all( np.allclose(i, j) for i, j in zip(res1_2[0], res1_2[1])))\r\n```\r\nThe output I get is:\r\n```\r\n1 the 3\r\nTrue False True\r\n2 the scurvy 6\r\nTrue True True\r\n3 the scurvy dog 7\r\nTrue False False\r\n4 the scurvy dog walked 9\r\nTrue False True\r\n5 the scurvy dog walked home 10\r\nTrue True False\r\n6 the scurvy dog walked home alone 11\r\nTrue True True\r\n```\r\n\r\nSo running a single sentence through the model seems to give the same output each time, but if I run a batch with the same sentence twice, it's sometimes different (between the two outputs, and compared to the single-sentence case)\r\n\r\nIs this expected/explainable?\r\n\r\nFurther context, I'm running it on CPU (laptop), python 3.8.9, freshly installed venv.\r\nThe difference is usually just in a few indices of the embeddings, and can be up to 1e-3. The difference is negligible when comparing the embeddings with cosine distance. But I'd like to understand where it comes from before dismissing it.\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/allenai/scibert/issues/115/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scibert/issues/115/timeline","performed_via_github_app":null,"state_reason":null}