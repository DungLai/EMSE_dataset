{"url":"https://api.github.com/repos/allenai/scibert/issues/107","repository_url":"https://api.github.com/repos/allenai/scibert","labels_url":"https://api.github.com/repos/allenai/scibert/issues/107/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scibert/issues/107/comments","events_url":"https://api.github.com/repos/allenai/scibert/issues/107/events","html_url":"https://github.com/allenai/scibert/issues/107","id":727993068,"node_id":"MDU6SXNzdWU3Mjc5OTMwNjg=","number":107,"title":"Relation Extraction in PyTorch","user":{"login":"PetterBerntsson","id":22333371,"node_id":"MDQ6VXNlcjIyMzMzMzcx","avatar_url":"https://avatars.githubusercontent.com/u/22333371?v=4","gravatar_id":"","url":"https://api.github.com/users/PetterBerntsson","html_url":"https://github.com/PetterBerntsson","followers_url":"https://api.github.com/users/PetterBerntsson/followers","following_url":"https://api.github.com/users/PetterBerntsson/following{/other_user}","gists_url":"https://api.github.com/users/PetterBerntsson/gists{/gist_id}","starred_url":"https://api.github.com/users/PetterBerntsson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/PetterBerntsson/subscriptions","organizations_url":"https://api.github.com/users/PetterBerntsson/orgs","repos_url":"https://api.github.com/users/PetterBerntsson/repos","events_url":"https://api.github.com/users/PetterBerntsson/events{/privacy}","received_events_url":"https://api.github.com/users/PetterBerntsson/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-10-23T07:43:50Z","updated_at":"2020-10-23T07:43:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\nWe have our model fine-tuned over the chemprot dataset you've provided, and have now downloaded the model to run predictions locally. However, loading and running the model seems to present several issues:\r\n\r\n\r\nFirst of all, there are 13 classes the model can predict. But running it over the whole dataset it seems the model can't reproduce the metrics from training (even when running over the training set). The model seems not to be able to distinguish the classes. For example, the most predicted index for each class is as follows:\r\n\r\n- ACTIVATOR : 12\r\n- AGONIST-ACTIVATOR : 8\r\n- AGONIST-INHIBITOR : 8\r\n- AGONIST : 8\r\n- DOWNREGULATOR : 8\r\n- ANTAGONIST : 0\r\n- DOWNREGULATOR : 8\r\n- INDIRECT-DOWNREGULATOR : 8\r\n- INDIRECT-UPREGULATOR : 8\r\n- INHIBITOR : 8\r\n- PRODUCT-OF : 9\r\n- SUBSTRATE : 8\r\n- SUBSTRATE_PRODUCT-OF : 9\r\n- UPREGULATOR : 12\r\n\r\n_(That's another question, where could we find which index represents which text-label?)_\r\n\r\nIt seems also when loading that we need to rename all the keys, and we haven't found a way to load the model simply (as you see from this hacky solution). The fine-tuned model was the AllenNLP scivocab-uncased, would AllenNLP be a dependency when running our models in PyTorch?\r\n\r\nIf you have suggestions, or find a problem with our code, it would be very helpful if you could point it out. We have so far assumed that the code works, as it is runnable and produces consistent results and that fine-tuning changes these results. The metrics however are not as good as during the fine-tuning using your repository, but no amount of fine-tuning seems to solve the issue.\r\n\r\n```python\r\ntokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\r\nmodel = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\r\ncheckpoint = torch.load(\"./scibert_model/model2/pytorch_model.bin\", map_location=device)\r\n\r\nfor key in checkpoint:\r\n    name = str(key).replace(\"text_field_embedder.token_embedder_bert.bert_model.\", \"\")\r\n    renamed_dict[name] = checkpoint.get(key)\r\n\r\ncfw = renamed_dict.pop(\"classifier_feedforward.weight\")\r\ncfb = renamed_dict.pop(\"classifier_feedforward.bias\")\r\n\r\nout_layer = torch.nn.Linear(768, 13)\r\nwith torch.no_grad():\r\n    out_layer.weight.copy_(cfw)\r\n    out_layer.bias.copy_(cfb)\r\n\r\nmodel.load_state_dict(renamed_dict)\r\nmodel.to(device)\r\nmodel.eval()\r\n\r\nwith torch.no_grad():\r\n    \r\ntext = \"Beta-1,4-galactosyltransferase I (beta4Gal-T1) normally transfers Gal from UDP-Gal to GlcNAc in the presence of Mn(2+) ion (Gal-T activity) and also transfers Glc from << UDP-Glc >> to GlcNAc ([[ Glc-T ]] activity), albeit at only 0.3% efficiency.\"\r\n\r\n    encoded = tokenizer.encode_plus(\r\n        text,\r\n        max_length=128,\r\n        add_special_tokens=True,\r\n        return_token_type_ids=False,\r\n        pad_to_max_length=True,\r\n        return_attention_mask=True,\r\n        return_tensors='pt'\r\n    )\r\n\r\n   input_ids = encoded['input_ids'].to(device)\r\n    attention_mask = encoded['attention_mask'].to(device)\r\n    out = model(input_ids=input_ids, attention_mask=attention_mask)\r\n    preds = torch.softmax(out_layer(out[1]), dim=1)[0]\r\n    print(torch.argmax(preds))\r\n```\r\n\r\n\r\nThank you in advance","closed_by":null,"reactions":{"url":"https://api.github.com/repos/allenai/scibert/issues/107/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scibert/issues/107/timeline","performed_via_github_app":null,"state_reason":null}