{"url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318","repository_url":"https://api.github.com/repos/neo-ai/neo-ai-dlr","labels_url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318/labels{/name}","comments_url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318/comments","events_url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318/events","html_url":"https://github.com/neo-ai/neo-ai-dlr/issues/318","id":791817021,"node_id":"MDU6SXNzdWU3OTE4MTcwMjE=","number":318,"title":"Failure in running TensorRT while running inference on different models ","user":{"login":"nwils","id":65023541,"node_id":"MDQ6VXNlcjY1MDIzNTQx","avatar_url":"https://avatars.githubusercontent.com/u/65023541?v=4","gravatar_id":"","url":"https://api.github.com/users/nwils","html_url":"https://github.com/nwils","followers_url":"https://api.github.com/users/nwils/followers","following_url":"https://api.github.com/users/nwils/following{/other_user}","gists_url":"https://api.github.com/users/nwils/gists{/gist_id}","starred_url":"https://api.github.com/users/nwils/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nwils/subscriptions","organizations_url":"https://api.github.com/users/nwils/orgs","repos_url":"https://api.github.com/users/nwils/repos","events_url":"https://api.github.com/users/nwils/events{/privacy}","received_events_url":"https://api.github.com/users/nwils/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-01-22T08:46:54Z","updated_at":"2021-02-08T08:56:26Z","closed_at":"2021-02-08T08:56:26Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nI am trying to run two different SageMaker Neo compiled models on Jetson Nano. \r\nDLR version - 1.3\r\nJetpack -4.3\r\n\r\nThe models were compiled for ARM64 platform with the following parameters. \r\nCompilation configuration - \r\nArch - ARM64\r\nOS = LINUX\r\nAccelerator = Nvidia\r\nCompiler options = {\"cuda-ver\": \"10.0\", \"trt-ver\": \"6.0.1\", \"gpu-code\": \"sm_53\"}\r\n\r\nThe libdlr.so module in the model artifact was used to run inference. Loading a model followed by the other causes error as follows.\r\n\r\n`dlr.dlr_model.DLRError: TVMError: \r\nAn internal invariant was violated during the execution of TVM.\r\nPlease read TVM's error reporting guidelines.\r\nMore details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.\r\n  Check failed: context->execute(batch_size_, bindings.data()) == false: Running TensorRT failed.\r\nStack trace:\r\n  File \"/home/nvidia/neo-ai-dlr/3rdparty/tvm/src/runtime/contrib/tensorrt/tensorrt_runtime.cc\", line 154\r\n  [bt] (0) /app/model_cache/libdlr.so(+0x201134) [0x7f3342f134]\r\n  [bt] (1) /app/model_cache/libdlr.so(tvm::runtime::contrib::TensorRTRuntime::Run()+0xb40) [0x7f33440558]\r\n  [bt] (2) /app/model_cache/libdlr.so(std::_Function_handler<void (tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*), tvm::runtime::json::JSONRuntimeBase::GetFunction(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#3}>::_M_invoke(std::_Any_data const&, tvm::runtime::TVMArgs&&, tvm::runtime::TVMRetValue*&&)+0xdc) [0x7f33436afc]\r\n  [bt] (3) /app/model_cache/libdlr.so(+0x27d7b4) [0x7f334ab7b4]\r\n  [bt] (4) /app/model_cache/libdlr.so(+0x27d83c) [0x7f334ab83c]\r\n  [bt] (5) /app/model_cache/libdlr.so(dlr::TVMModel::Run()+0xc4) [0x7f333208b4]\r\n  [bt] (6) /app/model_cache/libdlr.so(RunDLRModel+0x1c) [0x7f332d712c]\r\n[File : run.py, Function : main, Line : 161] ERROR: failed to load model\r\nTraceback (most recent call last):\r\n  File \"/app/neo/neo_model.py\", line 133, in load_model\r\n    self._model.run(tmp_input_data)\r\n  File \"/usr/local/lib/python3.8/site-packages/dlr/api.py\", line 135, in run\r\n    raise ex\r\n  File \"/usr/local/lib/python3.8/site-packages/dlr/api.py\", line 132, in run\r\n    return self._impl.run(input_values)\r\n  File \"/usr/local/lib/python3.8/site-packages/dlr/dlr_model.py\", line 470, in run\r\n    self._run()\r\n  File \"/usr/local/lib/python3.8/site-packages/dlr/dlr_model.py\", line 351, in _run\r\n    self._check_call(self._lib.RunDLRModel(byref(self.handle)))\r\n  File \"/usr/local/lib/python3.8/site-packages/dlr/dlr_model.py\", line 185, in _check_call\r\n    raise DLRError(self._lib.DLRGetLastError().decode('ascii'))`\r\n\r\nThe tensorrt cache is same for both models. It has been noted that a new tensorrt plan is not created for the next model.\r\nIs it possible to generate different tensorrt plans for different models? Or is there any other solution for this?\r\nCan you please help us to solve this problem?\r\n\r\nThanks in advance.","closed_by":{"login":"nwils","id":65023541,"node_id":"MDQ6VXNlcjY1MDIzNTQx","avatar_url":"https://avatars.githubusercontent.com/u/65023541?v=4","gravatar_id":"","url":"https://api.github.com/users/nwils","html_url":"https://github.com/nwils","followers_url":"https://api.github.com/users/nwils/followers","following_url":"https://api.github.com/users/nwils/following{/other_user}","gists_url":"https://api.github.com/users/nwils/gists{/gist_id}","starred_url":"https://api.github.com/users/nwils/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nwils/subscriptions","organizations_url":"https://api.github.com/users/nwils/orgs","repos_url":"https://api.github.com/users/nwils/repos","events_url":"https://api.github.com/users/nwils/events{/privacy}","received_events_url":"https://api.github.com/users/nwils/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/neo-ai/neo-ai-dlr/issues/318/timeline","performed_via_github_app":null,"state_reason":"completed"}