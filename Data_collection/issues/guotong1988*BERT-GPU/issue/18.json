{"url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18","repository_url":"https://api.github.com/repos/guotong1988/BERT-GPU","labels_url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18/labels{/name}","comments_url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18/comments","events_url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18/events","html_url":"https://github.com/guotong1988/BERT-GPU/issues/18","id":494382075,"node_id":"MDU6SXNzdWU0OTQzODIwNzU=","number":18,"title":"Output model files compatible with Official Bert's pre-trained models?","user":{"login":"1e0ng","id":1264873,"node_id":"MDQ6VXNlcjEyNjQ4NzM=","avatar_url":"https://avatars.githubusercontent.com/u/1264873?v=4","gravatar_id":"","url":"https://api.github.com/users/1e0ng","html_url":"https://github.com/1e0ng","followers_url":"https://api.github.com/users/1e0ng/followers","following_url":"https://api.github.com/users/1e0ng/following{/other_user}","gists_url":"https://api.github.com/users/1e0ng/gists{/gist_id}","starred_url":"https://api.github.com/users/1e0ng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/1e0ng/subscriptions","organizations_url":"https://api.github.com/users/1e0ng/orgs","repos_url":"https://api.github.com/users/1e0ng/repos","events_url":"https://api.github.com/users/1e0ng/events{/privacy}","received_events_url":"https://api.github.com/users/1e0ng/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2019-09-17T03:35:00Z","updated_at":"2019-09-19T06:44:50Z","closed_at":"2019-09-19T06:44:50Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, I tried to pre-train a Bert model with this project. I find the output of the model is not compatible with the official Bert's pre-trained model. Is it easy to make it compatible?\r\n\r\nFor example, I can use `pytorch_transformers` to read the official Bert's pre-trained models, but when I do this same for the model trained by this project, I get some errors about some shape sizes are not the same.\r\n\r\n```\r\nRuntimeError: Error(s) in loading state_dict for BertForMultiLabelSequenceClassification:\r\n\tsize mismatch for bert.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n\tsize mismatch for bert.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n\tsize mismatch for bert.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n\tsize mismatch for bert.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n\tsize mismatch for bert.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n\tsize mismatch for bert.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([256, 128]).\r\n\tsize mismatch for bert.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([256, 128]) from checkpoint, the shape in current model is torch.Size([128, 256]).\r\n```","closed_by":{"login":"guotong1988","id":4702353,"node_id":"MDQ6VXNlcjQ3MDIzNTM=","avatar_url":"https://avatars.githubusercontent.com/u/4702353?v=4","gravatar_id":"","url":"https://api.github.com/users/guotong1988","html_url":"https://github.com/guotong1988","followers_url":"https://api.github.com/users/guotong1988/followers","following_url":"https://api.github.com/users/guotong1988/following{/other_user}","gists_url":"https://api.github.com/users/guotong1988/gists{/gist_id}","starred_url":"https://api.github.com/users/guotong1988/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/guotong1988/subscriptions","organizations_url":"https://api.github.com/users/guotong1988/orgs","repos_url":"https://api.github.com/users/guotong1988/repos","events_url":"https://api.github.com/users/guotong1988/events{/privacy}","received_events_url":"https://api.github.com/users/guotong1988/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/guotong1988/BERT-GPU/issues/18/timeline","performed_via_github_app":null,"state_reason":"completed"}