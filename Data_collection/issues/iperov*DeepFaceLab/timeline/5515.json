[{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/comments/1122200802","html_url":"https://github.com/iperov/DeepFaceLab/issues/5515#issuecomment-1122200802","issue_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5515","id":1122200802,"node_id":"IC_kwDOCBuapc5C42zi","user":{"login":"Twenkid","id":23367640,"node_id":"MDQ6VXNlcjIzMzY3NjQw","avatar_url":"https://avatars.githubusercontent.com/u/23367640?v=4","gravatar_id":"","url":"https://api.github.com/users/Twenkid","html_url":"https://github.com/Twenkid","followers_url":"https://api.github.com/users/Twenkid/followers","following_url":"https://api.github.com/users/Twenkid/following{/other_user}","gists_url":"https://api.github.com/users/Twenkid/gists{/gist_id}","starred_url":"https://api.github.com/users/Twenkid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Twenkid/subscriptions","organizations_url":"https://api.github.com/users/Twenkid/orgs","repos_url":"https://api.github.com/users/Twenkid/repos","events_url":"https://api.github.com/users/Twenkid/events{/privacy}","received_events_url":"https://api.github.com/users/Twenkid/received_events","type":"User","site_admin":false},"created_at":"2022-05-10T10:18:49Z","updated_at":"2022-05-10T11:11:15Z","author_association":"NONE","body":"I found a solution: I just manually set the paprameters of the device and skipped the check... :D\r\n\r\n```python\r\nC:\\DFL\\DeepFaceLab_NVIDIA_up_to_RTX2080Ti\\_internal\\DeepFaceLab\\core\\leras\\device.py\r\n...\r\n\r\nskip_physical_devices = True\r\n...\r\n@staticmethod\r\n    def _get_tf_devices_proc(q : multiprocessing.Queue):\r\n     #do not call: device_lib.list_local_devices()\r\n     devices = []\r\n     physical_devices_f = {}\r\n     ...\r\n       if not skip_physical_devices:\r\n          print(f\"list_local_devices()={device_lib.list_local_devices()}\")\r\n        \r\n        max_memory = 1556925644 # 1.45 GB\r\n        physical_devices_f = {}\r\n        physical_devices_f[0] = ('GPU', '750 Ti', 1556925644)\r\n        print(physical_devices_f)\r\n        q.put(physical_devices_f)\r\n        time.sleep(0.1)\r\n        if not skip_physical_devices: \r\n              physical_devices = device_lib.list_local_devices()\r\n               physical_devices_f = {}\r\n               ...\r\n\r\n```\r\n\r\nThere was another apparent issue, initially it run through the model initialization, but then returned OOM where it wasn't supposed to do given a very small model,  HWMonitor stayed at GPU memory 97% and I was unable to release it, I think it is that TF clearing-the-memory issue: https://github.com/tensorflow/tensorflow/issues/36465?msclkid=0b0cb26dd03c11ec9d212ba18e7d751d\r\n\r\nThe solution with numba didn't solve it.\r\n```\r\nfrom numba import cuda\r\ncuda.select_device(0)\r\ncuda.close()\r\n```\r\n\r\nHowever when running the DirectX12 DFL with that frozen 97% indication, it successfully run a big model which needs fills the memory, so possibly the indication of HWMonitor was not correct.\r\n\r\nIn my settings, the speed up of CUDA vs DX12 seemed about 33%. For:\r\nRes: 96; AE-E-D-M: 128-64-64-16, saved model size: 467 MB.\r\n\r\nThe model fits in GPU and saturates the GPU load.  (```[y] Place models and optimizer on GPU ( y/n ?:help ) : y  [y] Use AdaBelief optimizer? ( y/n ?:help ) : ``` Res: 96; AE-E-D-M: 128-64-64-16, saved model size: 467 MB, \r\n\r\nIndeed, that 1.45 GB limit out of 2 GB was another issue and I know it's for any GPU, a friend with 1070 Ti reports 6.63 GB out of 8 GB - Windows not letting a program to use all, or at least more, of the memory.\r\n\r\nOne solution I tried was to make/to \"trick\" Windows to use the integrated GPU for the GUI, by waking it up or restarting with one monitor connected to the integrated GPU output. \r\n\r\nThen the dedicated GPU's memory usage stays at 0% if not used, however after it is fully loaded with a big model it just tops at a lower %, instead of close to 100%.\r\n\r\nI will try to set that allocation size higher and see will it fit a bigger model or crash, but a bit later when I am comfortable with that possibility.\r\n","reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/comments/1122200802/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"Twenkid","id":23367640,"node_id":"MDQ6VXNlcjIzMzY3NjQw","avatar_url":"https://avatars.githubusercontent.com/u/23367640?v=4","gravatar_id":"","url":"https://api.github.com/users/Twenkid","html_url":"https://github.com/Twenkid","followers_url":"https://api.github.com/users/Twenkid/followers","following_url":"https://api.github.com/users/Twenkid/following{/other_user}","gists_url":"https://api.github.com/users/Twenkid/gists{/gist_id}","starred_url":"https://api.github.com/users/Twenkid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Twenkid/subscriptions","organizations_url":"https://api.github.com/users/Twenkid/orgs","repos_url":"https://api.github.com/users/Twenkid/repos","events_url":"https://api.github.com/users/Twenkid/events{/privacy}","received_events_url":"https://api.github.com/users/Twenkid/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/comments/1122423287","html_url":"https://github.com/iperov/DeepFaceLab/issues/5515#issuecomment-1122423287","issue_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5515","id":1122423287,"node_id":"IC_kwDOCBuapc5C5tH3","user":{"login":"Twenkid","id":23367640,"node_id":"MDQ6VXNlcjIzMzY3NjQw","avatar_url":"https://avatars.githubusercontent.com/u/23367640?v=4","gravatar_id":"","url":"https://api.github.com/users/Twenkid","html_url":"https://github.com/Twenkid","followers_url":"https://api.github.com/users/Twenkid/followers","following_url":"https://api.github.com/users/Twenkid/following{/other_user}","gists_url":"https://api.github.com/users/Twenkid/gists{/gist_id}","starred_url":"https://api.github.com/users/Twenkid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Twenkid/subscriptions","organizations_url":"https://api.github.com/users/Twenkid/orgs","repos_url":"https://api.github.com/users/Twenkid/repos","events_url":"https://api.github.com/users/Twenkid/events{/privacy}","received_events_url":"https://api.github.com/users/Twenkid/received_events","type":"User","site_admin":false},"created_at":"2022-05-10T13:48:52Z","updated_at":"2022-05-10T13:48:52Z","author_association":"NONE","body":"Setting the CUDA memory higher/up to almost the maximum (1.98 GB) didn't crash the system. It returns just OOM errors eventually. However I didn't manage to fit a bigger batch size, that amount maybe is just for \"info\" purposes and Windows reserves whatever it wants anyway.\r\n\r\nInitially there was unknown memory issue with running big models, one which I run with a batch size 6 on DirectX12 managed to fit only a batch=4 in CUDA. I assumed either CUDA was taking more memory than the DX version or it wasn't cleared properly. That was resolved after performing the procedure which I mentioned in the previous message:\r\n\r\n1) PC-->Sleep\r\n2) Connect the monitor to the integrated GPU output\r\n3) Resume \r\n4) GPU memory usage = 0%\r\n\r\nThen the batch size 6 was encompassed, but unfortunately the GPU couldn't fit 7, even with 1.98 GB set manually, while batch 6 fits within the  default returned size: 1.45 GB. So it seems that the returned number is only  a \"suggestion\" and it couldn't be overriden so simply.\r\n","reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/comments/1122423287/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"Twenkid","id":23367640,"node_id":"MDQ6VXNlcjIzMzY3NjQw","avatar_url":"https://avatars.githubusercontent.com/u/23367640?v=4","gravatar_id":"","url":"https://api.github.com/users/Twenkid","html_url":"https://github.com/Twenkid","followers_url":"https://api.github.com/users/Twenkid/followers","following_url":"https://api.github.com/users/Twenkid/following{/other_user}","gists_url":"https://api.github.com/users/Twenkid/gists{/gist_id}","starred_url":"https://api.github.com/users/Twenkid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Twenkid/subscriptions","organizations_url":"https://api.github.com/users/Twenkid/orgs","repos_url":"https://api.github.com/users/Twenkid/repos","events_url":"https://api.github.com/users/Twenkid/events{/privacy}","received_events_url":"https://api.github.com/users/Twenkid/received_events","type":"User","site_admin":false}}]