{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/5239","id":783037402,"node_id":"MDU6SXNzdWU3ODMwMzc0MDI=","number":5239,"title":"Model collapse when training with GAN + Adabelief (no Learning Rate Dropout) and Gradient Clipping","user":{"login":"ThomasBardem","id":57501789,"node_id":"MDQ6VXNlcjU3NTAxNzg5","avatar_url":"https://avatars.githubusercontent.com/u/57501789?v=4","gravatar_id":"","url":"https://api.github.com/users/ThomasBardem","html_url":"https://github.com/ThomasBardem","followers_url":"https://api.github.com/users/ThomasBardem/followers","following_url":"https://api.github.com/users/ThomasBardem/following{/other_user}","gists_url":"https://api.github.com/users/ThomasBardem/gists{/gist_id}","starred_url":"https://api.github.com/users/ThomasBardem/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ThomasBardem/subscriptions","organizations_url":"https://api.github.com/users/ThomasBardem/orgs","repos_url":"https://api.github.com/users/ThomasBardem/repos","events_url":"https://api.github.com/users/ThomasBardem/events{/privacy}","received_events_url":"https://api.github.com/users/ThomasBardem/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-01-11T03:26:59Z","updated_at":"2021-01-11T16:30:59Z","closed_at":"2021-01-11T15:52:02Z","author_association":"NONE","active_lock_reason":null,"body":"Iperov reply: \"Offer a fix. Or dont use gan\" Clearly you're the developer and users are testers, we find bugs and report them so you can fix it. You can keep acting like a child or investigate the issue, if you think gradient clipping works fine with adabelief and LRD doesn't cause any issues too test it and prove it works.\r\n\r\nUsing the latest release, training a pretrained DF-UD 320 model with dims 320/72/72/16 on bunch of random faces before actual training.\r\n\r\nModel has AB enabled from the start (both pretraining and training done with it enabled). I've ran during the training set most options, starting with Random Warp, then no RW, Uniform Yaw, bit of true face, mouth and eye priority until I've enabled learning rate dropout for a while and then disabled it and enabled GAN at 0.1 power with default patch size and dims.\r\n\r\nModel trained fine for some time until it collapsed, I've decided to enabled gradient clipping despite reading up about some potential issues with it running on models using adabelief optimizer but still it also collapsed after some time producing results like this: https://imgur.com/a/fHDQc4v\r\n\r\nWhen gradient clipping was disabled the collapsed face previews were completely black with DST faces looking fine, with gradient clipping enabled I'm getting these black/blue swirls and DST faces are all distorted and just look bad.\r\n\r\nIn both cases loss values go up to 4.100 like values. This happened also when the GAN values for patch size and dims were changed (lower than default).\r\n\r\nRest of the setup: updated Windows 10, AVX supported CPU, 1080Ti, Win 10 fix applied (GPU scheduling one)\r\n\r\nThis is really frustrating as I don't see any obvious reasons for why this model would collapse 3 times already, old GAN never caused any such issues and I was also able to turn on and off gradient clipping with no issues too, now with AB turning gradient clipping causes issues and training with GAN collapses model after just 1-3 hours of training no matter if gradient clipping is enabled or not.\r\n\r\nOther thing I've noticed too is that models sometimes seem to train much faster, before AB models would always run at the same speed and use up same amount of VRAM, now with AB enabled models VRAM requirements for the same model can differ and sometimes the model may use 1-1,5GB more or less, which also affects training speed which when working correctly is under 800ms and when it's consuming more for some reason speed also goes down with iterations reaching 3000ms with usual 16th iteration raises going as high as 8000ms. I always make sure the available VRAM is the same before running the model and doesn't fully load the cards VRAM (usage never goes above 10,6GB on 11GB card and minimum usage when just the display is being driven by the GPU ranges between 0.4-0.8GB before launching the training.","closed_by":{"login":"iperov","id":8076202,"node_id":"MDQ6VXNlcjgwNzYyMDI=","avatar_url":"https://avatars.githubusercontent.com/u/8076202?v=4","gravatar_id":"","url":"https://api.github.com/users/iperov","html_url":"https://github.com/iperov","followers_url":"https://api.github.com/users/iperov/followers","following_url":"https://api.github.com/users/iperov/following{/other_user}","gists_url":"https://api.github.com/users/iperov/gists{/gist_id}","starred_url":"https://api.github.com/users/iperov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iperov/subscriptions","organizations_url":"https://api.github.com/users/iperov/orgs","repos_url":"https://api.github.com/users/iperov/repos","events_url":"https://api.github.com/users/iperov/events{/privacy}","received_events_url":"https://api.github.com/users/iperov/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":1,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5239/timeline","performed_via_github_app":null,"state_reason":"completed"}