{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/518","id":538039034,"node_id":"MDU6SXNzdWU1MzgwMzkwMzQ=","number":518,"title":"Training isn't working, but it used to work. Why?","user":{"login":"Jamie1605","id":58906078,"node_id":"MDQ6VXNlcjU4OTA2MDc4","avatar_url":"https://avatars.githubusercontent.com/u/58906078?v=4","gravatar_id":"","url":"https://api.github.com/users/Jamie1605","html_url":"https://github.com/Jamie1605","followers_url":"https://api.github.com/users/Jamie1605/followers","following_url":"https://api.github.com/users/Jamie1605/following{/other_user}","gists_url":"https://api.github.com/users/Jamie1605/gists{/gist_id}","starred_url":"https://api.github.com/users/Jamie1605/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Jamie1605/subscriptions","organizations_url":"https://api.github.com/users/Jamie1605/orgs","repos_url":"https://api.github.com/users/Jamie1605/repos","events_url":"https://api.github.com/users/Jamie1605/events{/privacy}","received_events_url":"https://api.github.com/users/Jamie1605/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-12-15T11:40:18Z","updated_at":"2020-03-28T05:41:59Z","closed_at":"2020-03-28T05:41:59Z","author_association":"NONE","active_lock_reason":null,"body":"I used to use DeepFaceLab and it worked perfectly fine. Yesterday I wanted to try it again, I installed DeepFaceLab, Cuda and CuDNN. But now when I try to train (I tried every way, H64, Sae, ... etc.), theres this Error:\r\n\r\nRunning trainer.\r\n\r\nLoading model...\r\n\r\nModel first run. Enter model options as default for each run.\r\nEnable autobackup? (y/n ?:help skip:n) : y\r\nWrite preview history? (y/n ?:help skip:n) :\r\nn\r\nTarget iteration (skip:unlimited/default) :\r\n0\r\nBatch_size (?:help skip:0) : 1\r\nFeed faces to network sorted by yaw? (y/n ?:help skip:n) :\r\nn\r\nFlip faces randomly? (y/n ?:help skip:y) :\r\ny\r\nSrc face scale modifier % ( -30...30, ?:help skip:0) :\r\n0\r\nUse lightweight autoencoder? (y/n, ?:help skip:n) :\r\nn\r\nUse pixel loss? (y/n, ?:help skip: n/default ) :\r\nn\r\nUsing TensorFlow backend.\r\nLoading: 100%|####################################################################| 5409/5409 [00:12<00:00, 425.83it/s]\r\nLoading: 100%|######################################################################| 356/356 [00:00<00:00, 439.23it/s]\r\n============= Model Summary =============\r\n==                                     ==\r\n==        Model name: H64              ==\r\n==                                     ==\r\n== Current iteration: 0                ==\r\n==                                     ==\r\n==----------- Model Options -----------==\r\n==                                     ==\r\n==        autobackup: True             ==\r\n==       sort_by_yaw: False            ==\r\n==       random_flip: True             ==\r\n==        lighter_ae: False            ==\r\n==        pixel_loss: False            ==\r\n==        batch_size: 1                ==\r\n==                                     ==\r\n==------------ Running On -------------==\r\n==                                     ==\r\n==      Device index: 0                ==\r\n==              Name: GeForce GTX 1050 ==\r\n==              VRAM: 2.00GB           ==\r\n==                                     ==\r\n=========================================\r\n/!\\\r\n/!\\ WARNING:\r\n/!\\ You are using a GPU with 2GB or less VRAM. This may significantly reduce the quality of your result!\r\n/!\\ If training does not start, close all programs and try again.\r\n/!\\ Also you can disable Windows Aero Desktop to increase available VRAM.\r\n/!\\\r\nStarting. Press \"Enter\" to stop training and save model.\r\n2019-12-15 12:24:41.106601: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 237.97M (249527808 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:41.112333: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 214.17M (224575232 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:41.118872: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 192.75M (202117888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:41.164972: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:41.169588: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:51.176914: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:51.194472: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:51.210262: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:24:51.218788: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:01.225303: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:01.238103: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:01.252586: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:01.264183: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:11.271293: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:26.178159: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:26.194196: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:26.202083: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:36.209152: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:36.222070: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:36.228803: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2019-12-15 12:25:36.239554: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 64.49M (67621632 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\nError: OOM when allocating tensor with shape[3,3,512,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node training/Adam/Variable_14/Assign (defined at C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)  = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/Variable_14, training/Adam/zeros_14)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'training/Adam/Variable_14/Assign', defined at:\r\n  File \"threading.py\", line 884, in _bootstrap\r\n  File \"threading.py\", line 916, in _bootstrap_inner\r\n  File \"threading.py\", line 864, in run\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 109, in trainerThread\r\n    iter, iter_time = model.train_one_iter()\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 525, in train_one_iter\r\n    losses = self.onTrainOneIter(sample, self.generator_list)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\Model_H64\\Model.py\", line 89, in onTrainOneIter\r\n    total, loss_src_bgr, loss_src_mask, loss_dst_bgr, loss_dst_mask = self.ae.train_on_batch( [warped_src, target_src_full_mask, warped_dst, target_dst_full_mask], [target_src, target_src_full_mask, target_dst, target_dst_full_mask] )\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in train_on_batch\r\n    self._make_train_function()\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 509, in _make_train_function\r\n    loss=self.total_loss)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\nnlib\\nnlib.py\", line 867, in get_updates\r\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\nnlib\\nnlib.py\", line 867, in <listcomp>\r\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 704, in zeros\r\n    return variable(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 402, in variable\r\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\r\n    expected_shape=expected_shape, import_scope=import_scope)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\r\n    constraint=constraint)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1481, in _init_from_args\r\n    validate_shape=validate_shape).op\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 221, in assign\r\n    validate_shape=validate_shape)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 61, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,512,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node training/Adam/Variable_14/Assign (defined at C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)  = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/Variable_14, training/Adam/zeros_14)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[3,3,512,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[{{node training/Adam/Variable_14/Assign}} = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/Variable_14, training/Adam/zeros_14)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 109, in trainerThread\r\n    iter, iter_time = model.train_one_iter()\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 525, in train_one_iter\r\n    losses = self.onTrainOneIter(sample, self.generator_list)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\Model_H64\\Model.py\", line 89, in onTrainOneIter\r\n    total, loss_src_bgr, loss_src_mask, loss_dst_bgr, loss_dst_mask = self.ae.train_on_batch( [warped_src, target_src_full_mask, warped_dst, target_dst_full_mask], [target_src, target_src_full_mask, target_dst, target_dst_full_mask] )\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1217, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2697, in __call__\r\n    if hasattr(get_session(), '_make_callable_from_options'):\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 206, in get_session\r\n    session.run(tf.variables_initializer(uninitialized_vars))\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[3,3,512,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node training/Adam/Variable_14/Assign (defined at C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)  = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/Variable_14, training/Adam/zeros_14)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'training/Adam/Variable_14/Assign', defined at:\r\n  File \"threading.py\", line 884, in _bootstrap\r\n  File \"threading.py\", line 916, in _bootstrap_inner\r\n  File \"threading.py\", line 864, in run\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 109, in trainerThread\r\n    iter, iter_time = model.train_one_iter()\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 525, in train_one_iter\r\n    losses = self.onTrainOneIter(sample, self.generator_list)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\models\\Model_H64\\Model.py\", line 89, in onTrainOneIter\r\n    total, loss_src_bgr, loss_src_mask, loss_dst_bgr, loss_dst_mask = self.ae.train_on_batch( [warped_src, target_src_full_mask, warped_dst, target_dst_full_mask], [target_src, target_src_full_mask, target_dst, target_dst_full_mask] )\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in train_on_batch\r\n    self._make_train_function()\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\engine\\training.py\", line 509, in _make_train_function\r\n    loss=self.total_loss)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\nnlib\\nnlib.py\", line 867, in get_updates\r\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\DeepFaceLab\\nnlib\\nnlib.py\", line 867, in <listcomp>\r\n    ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 704, in zeros\r\n    return variable(v, dtype=dtype, name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 402, in variable\r\n    v = tf.Variable(value, dtype=tf.as_dtype(dtype), name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\r\n    return cls._variable_v1_call(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\r\n    aggregation=aggregation)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\r\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\r\n    expected_shape=expected_shape, import_scope=import_scope)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\r\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\r\n    constraint=constraint)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1481, in _init_from_args\r\n    validate_shape=validate_shape).op\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 221, in assign\r\n    validate_shape=validate_shape)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 61, in assign\r\n    use_locking=use_locking, name=name)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3,3,512,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n         [[node training/Adam/Variable_14/Assign (defined at C:\\Users\\jamie\\Desktop\\DeepFaceLab_CUDA_9.2_SSE\\_internal\\python-3.6.8\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:402)  = Assign[T=DT_FLOAT, _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/Variable_14, training/Adam/zeros_14)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n What could I have done wrong? It can't be the VRAM because, it used to work, with the same PC, as I said.\r\n\r\n- Windows 10\r\n- I'm using the prebuilt version.","closed_by":{"login":"iperov","id":8076202,"node_id":"MDQ6VXNlcjgwNzYyMDI=","avatar_url":"https://avatars.githubusercontent.com/u/8076202?v=4","gravatar_id":"","url":"https://api.github.com/users/iperov","html_url":"https://github.com/iperov","followers_url":"https://api.github.com/users/iperov/followers","following_url":"https://api.github.com/users/iperov/following{/other_user}","gists_url":"https://api.github.com/users/iperov/gists{/gist_id}","starred_url":"https://api.github.com/users/iperov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iperov/subscriptions","organizations_url":"https://api.github.com/users/iperov/orgs","repos_url":"https://api.github.com/users/iperov/repos","events_url":"https://api.github.com/users/iperov/events{/privacy}","received_events_url":"https://api.github.com/users/iperov/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/518/timeline","performed_via_github_app":null,"state_reason":"completed"}