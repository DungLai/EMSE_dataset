{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/851","id":673646381,"node_id":"MDU6SXNzdWU2NzM2NDYzODE=","number":851,"title":"LR-Dropout always causes OOM error when training with multi-GPU setup.","user":{"login":"GravitationalLensing","id":69250530,"node_id":"MDQ6VXNlcjY5MjUwNTMw","avatar_url":"https://avatars.githubusercontent.com/u/69250530?v=4","gravatar_id":"","url":"https://api.github.com/users/GravitationalLensing","html_url":"https://github.com/GravitationalLensing","followers_url":"https://api.github.com/users/GravitationalLensing/followers","following_url":"https://api.github.com/users/GravitationalLensing/following{/other_user}","gists_url":"https://api.github.com/users/GravitationalLensing/gists{/gist_id}","starred_url":"https://api.github.com/users/GravitationalLensing/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GravitationalLensing/subscriptions","organizations_url":"https://api.github.com/users/GravitationalLensing/orgs","repos_url":"https://api.github.com/users/GravitationalLensing/repos","events_url":"https://api.github.com/users/GravitationalLensing/events{/privacy}","received_events_url":"https://api.github.com/users/GravitationalLensing/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-08-05T15:48:29Z","updated_at":"2020-08-05T15:48:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"## Expected behaviour:\r\nAfter training a DF-UD model on dual 1070s at batch size 22 for some 300k+ iterations, the time came to enable learning rate dropout. Per the description for the function I did expect an increase in memory usage and was prepared to drop the batch size somewhat as needed.\r\n\r\n## Actual behaviour:\r\nDespite Dropping the batch size low as 4 (from 22) the OOM errors persisted and I was not able to train with LR-Dropout enabled on both GPUs. On a whim I tried training at BS11 on a single GPU and oddly enough, it worked without any issue.\r\n\r\nThis behaviour is consistent regardless of whether LR-Dropout is set to CPU or GPU.\r\n\r\n## Steps to reproduce:\r\n(Tested on latest Windows release as of 2020/08/05)\r\n- Train a model as normal to the point it is ready for LR-Dropout\r\n- Enable LR-Dropout and continue training the existing model with both GPUs.\r\n- Observe as it crashes after a few iterations with a output along the lines of:\r\n\r\n`2020-08-05 16:00:40.623215: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623215: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623215: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623247: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623251: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623221: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623251: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623253: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623216: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623246: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623252: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623253: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623220: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623217: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623216: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623221: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623261: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623247: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623260: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623221: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623272: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623272: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n2020-08-05 16:00:40.623277: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.623282: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:740] failed to record completion event; therefore, failed to create inter-stream dependency\r\n2020-08-05 16:00:40.627669: E tensorflow/stream_executor/stream.cc:328] Error recording event in stream: error recording CUDA event on stream 0000015D187467D0: CUDA_ERROR_OUT_OF_MEMORY: out of memory; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.\r\n2020-08-05 16:00:40.634641: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1\r\n2020-08-05 16:00:40.635982: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_OUT_OF_MEMORY: out of memory`\r\n\r\n- Repeat with single GPU and observe as the model will without issue train with LR-Dropout despite having a higher batchsize than the size that yielded a OOM error when running with dual GPUs.\r\n\r\n## System Specifications:\r\nOS: Windows 10 Pro (v2004)\r\nCPU: R9 3900x\r\nGPU(s): 2 x Nvidia GTX 1070\r\nRAM: 32GB DDR4\r\n\r\n## TLDR:\r\nOOM error when training with LR-Dropout at batch size low as 4 on dual 1070s, no OOM error training with it enabled at BS11 on a single 1070.\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/851/timeline","performed_via_github_app":null,"state_reason":null}