{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/847","id":672047653,"node_id":"MDU6SXNzdWU2NzIwNDc2NTM=","number":847,"title":"Inconsistent iterations times and issues with RW override in new version.","user":{"login":"ThomasBardem","id":57501789,"node_id":"MDQ6VXNlcjU3NTAxNzg5","avatar_url":"https://avatars.githubusercontent.com/u/57501789?v=4","gravatar_id":"","url":"https://api.github.com/users/ThomasBardem","html_url":"https://github.com/ThomasBardem","followers_url":"https://api.github.com/users/ThomasBardem/followers","following_url":"https://api.github.com/users/ThomasBardem/following{/other_user}","gists_url":"https://api.github.com/users/ThomasBardem/gists{/gist_id}","starred_url":"https://api.github.com/users/ThomasBardem/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ThomasBardem/subscriptions","organizations_url":"https://api.github.com/users/ThomasBardem/orgs","repos_url":"https://api.github.com/users/ThomasBardem/repos","events_url":"https://api.github.com/users/ThomasBardem/events{/privacy}","received_events_url":"https://api.github.com/users/ThomasBardem/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2020-08-03T12:39:58Z","updated_at":"2020-08-03T20:15:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Expected behavior:\r\n\r\nCorrect me if I'm wrong but the recent updates should be overriding random warp setting state to disabled (n) as long as pretrain is enabled (y).\r\n\r\nActual behavior:\r\n\r\nDespite this override update it seems like the behavior is not how one would expect, either explanation or bug fix is required, whenever random warp is set to disabled (n) overall VRAM usage is at about 9,3GB (batch 14, 224 DF-UD 288/80/80/16, optimizer set to GPU, LR disabled, GTX 1080Ti 11GB, Windows 10, 2004 update, display running of the same GPU) whereas when random warp is enabled (y) VRAM usage is close to 10,6GB.\r\n\r\nI also tested this very same model on older version of DFL before the RW override during pretrain (before July 26), I've tested it being enabled/disabled couple times just to be sure and sure enough, old version same VRAM usage, new one with RW enabled, 10,6 even saw 10,7GB VRAM usage after a minute of training.\r\n\r\nAlso overall iterations since few updated ago have the issue where every nth one will be higher, usually twice but not always, for me it was usually all consistent -/+ 50-100ms, after some update it changed and then every 16th iteration was about twice of the previous 15 (600ms for 15 iterration, 1200ms at 16 and then back to 600ms) and now it seems to have gotten even worse, CPU usage is pinned at 100% (before it was lower) and while at the beginning it still behaves the same (16th iter twice of the rest) after few minutes it gets worse and then either every 3rd or 4th iteration is like 1000ms, 1500ms, even 2200 or up to 3000ms. It's worse with new update and RW enabled, bit better with it disabled or enabled/disabled on older version (probably due to that lower vram usage). I'm thinking it might be the issue with my CPU being tad weak since it's pinned at 95-100% all the time, but this wasn't the case before, is the -U or -D (or -UD) heavier on CPU? Or some other thing made it heavier on CPU overall?\r\n\r\nShouldn't then this override update keep RW disabled all the time when pretrain is enabled? Why higher VRAM usage? What's with the iteration times being all over the place, this is not an issue just for me, there have been many people who reported the same issue over past month.\r\n\r\nUPDATE: It seems like I was able to resolve this issue, not sure why but after I've downloaded the newest 08.02 release and ran it once it fixed it in the version I had and now VRAM usage after RW override update is the same no matter the RW setting (about 9,3-9,4 GB). I will now leave it to run for few minutes to see if maybe the iteration time issue was fixed or at least improved.\r\n\r\nSteps to reproduce:\r\n\r\nDownload newest version of DFL (08.02) and run a model with or without random warp enabled, observe VRAM usage and iteration times.\r\n\r\nOther issues/questions:\r\n\r\nRecent updates also changed behavior of random warp, changed value in warp.py, introduced more changes in the code as well as forced it to be disable during pretraining without even a simple explanation as to why, this raises couple questions about how models should be now correctly trained:\r\n\r\n1. Does this mean pretrain is not required at all or just for pretraining?\r\n2. How does this change affect lower resolution models that still use base DF and LIAE as well as -U variants?\r\n3. Do models still correctly generalize with RW being disabled during pretraining?\r\n4. Is changing batch size still allowed during pretraining/training or should it remain now at mostly the same setting throughout the training?\r\n5. How should LR be used now that RW is disabled, previous updated mentioned workflow like this: RW:Y LR:N -> RW:Y LR:Y -> RW:N LR:???? (enable before GAN). Should LR be used at all during pretraining?\r\n\r\nOther relevant information:\r\n\r\nGTX 1080Ti, 16GB of RAM, 4 core, 8 thread i7 4,1GHZ when load on all 8 threads\r\nWindows 10 64bit with 2004 update, display running of the GPU\r\nDF-UD 224 Full Face model, dims: 288/80/80/16, Pretrain mode enabled","closed_by":null,"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/847/timeline","performed_via_github_app":null,"state_reason":null}