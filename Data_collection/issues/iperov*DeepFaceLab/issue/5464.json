{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/5464","id":1108018936,"node_id":"I_kwDOCBuapc5CCwb4","number":5464,"title":"Feature Request","user":{"login":"ddade","id":1152172,"node_id":"MDQ6VXNlcjExNTIxNzI=","avatar_url":"https://avatars.githubusercontent.com/u/1152172?v=4","gravatar_id":"","url":"https://api.github.com/users/ddade","html_url":"https://github.com/ddade","followers_url":"https://api.github.com/users/ddade/followers","following_url":"https://api.github.com/users/ddade/following{/other_user}","gists_url":"https://api.github.com/users/ddade/gists{/gist_id}","starred_url":"https://api.github.com/users/ddade/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ddade/subscriptions","organizations_url":"https://api.github.com/users/ddade/orgs","repos_url":"https://api.github.com/users/ddade/repos","events_url":"https://api.github.com/users/ddade/events{/privacy}","received_events_url":"https://api.github.com/users/ddade/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2022-01-19T12:05:04Z","updated_at":"2022-01-23T22:32:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Apologies if this is not the proper mechanism.\r\n\r\nI'm an amateur filmmaker, and I would like to experiment with replacing a live actor's face with a 3D model from software like Unreal Metahuman, or Daz 3D.\r\n\r\nSince I can render photorealistic images, theoretically, I should be able to create very effective source training sets because I could match the destination lighting, facial expressions, and orientation of the head very closely.\r\n\r\nFor this reason, during the face extraction phase of the destination images, is it possible for the software to output a JSON file describing its best guess at the orientation of the head as a quaternion, Euler angles, or anything really, for each image? With that information, I could easily create an animation to provide the perfect source training data.\r\n\r\nIs this possible? I'd like to discuss with a developer about adding this feature. I've got budget for it.\r\n\r\nAside from protecting production against actors who don't show up on shoot day, or later abandon their commitments as soon as they get a better gig, this would also solve the problem of retargeting facial expressions to the 3D model, a technical problem many would prefer to forget about.\r\n\r\nIs there someone here with whom I could further discuss?\r\n\r\nThanks!\r\n\r\nThanks! ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5464/timeline","performed_via_github_app":null,"state_reason":null}