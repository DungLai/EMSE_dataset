{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/586","id":555914593,"node_id":"MDU6SXNzdWU1NTU5MTQ1OTM=","number":586,"title":"Multi-GPU Issue (Titan RTX x2)","user":{"login":"The-Digital-Void","id":57318447,"node_id":"MDQ6VXNlcjU3MzE4NDQ3","avatar_url":"https://avatars.githubusercontent.com/u/57318447?v=4","gravatar_id":"","url":"https://api.github.com/users/The-Digital-Void","html_url":"https://github.com/The-Digital-Void","followers_url":"https://api.github.com/users/The-Digital-Void/followers","following_url":"https://api.github.com/users/The-Digital-Void/following{/other_user}","gists_url":"https://api.github.com/users/The-Digital-Void/gists{/gist_id}","starred_url":"https://api.github.com/users/The-Digital-Void/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/The-Digital-Void/subscriptions","organizations_url":"https://api.github.com/users/The-Digital-Void/orgs","repos_url":"https://api.github.com/users/The-Digital-Void/repos","events_url":"https://api.github.com/users/The-Digital-Void/events{/privacy}","received_events_url":"https://api.github.com/users/The-Digital-Void/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2020-01-28T00:27:21Z","updated_at":"2020-11-24T00:14:11Z","closed_at":"2020-01-30T00:40:11Z","author_association":"NONE","active_lock_reason":null,"body":"My training runs perfectly fine if I use either index 0 or 1. However, when I try to use both I get the following error.\r\n\r\n==------------- Running On -------------==\r\n==                                      ==\r\n==          Device index: 0             ==\r\n==                  Name: TITAN RTX     ==\r\n==                  VRAM: 24.00GB       ==\r\n==          Device index: 1             ==\r\n==                  Name: TITAN RTX     ==\r\n==                  VRAM: 24.00GB       ==\r\n==                                      ==\r\n==========================================\r\nStarting. Target iteration: 100000. Press \"Enter\" to stop training and save model.\r\n2020-01-27 19:09:33.301000: E tensorflow/stream_executor/cuda/cuda_driver.cc:806] failed to allocate 8.00G (8589934592 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\r\n\r\n**(The above line repeats about 40 times, removed for readability)**\r\n\r\nError: OOM when allocating tensor with shape[2,276480,384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[node concat_49 (defined at C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py:58) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'concat_49', defined at:\r\n  File \"threading.py\", line 884, in _bootstrap\r\n  File \"threading.py\", line 916, in _bootstrap_inner\r\n  File \"threading.py\", line 864, in run\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 57, in trainerThread\r\n    debug=debug,\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 173, in __init__\r\n    self.on_initialize()\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\Model_SAEHD\\Model.py\", line 563, in on_initialize\r\n    src_dst_loss_gv = nn.tf_average_gv_list (gpu_src_dst_loss_gvs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py\", line 58, in tf_average_gv_list\r\n    result[i] = ( tf.reduce_mean( tf.concat (gs, 0), 0 ), v )\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1256, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1149, in concat_v2\r\n    \"ConcatV2\", values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2,276480,384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[node concat_49 (defined at C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py:58) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1334, in _do_call\r\n    return fn(*args)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1319, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1407, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2,276480,384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[{{node concat_49}}]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 119, in trainerThread\r\n    iter, iter_time = model.train_one_iter()\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 411, in train_one_iter\r\n    losses = self.onTrainOneIter()\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\Model_SAEHD\\Model.py\", line 704, in onTrainOneIter\r\n    src_loss, dst_loss = self.src_dst_train (warped_src, target_src, target_srcm, warped_dst, target_dst, target_dstm)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\Model_SAEHD\\Model.py\", line 580, in src_dst_train\r\n    self.target_dstm:target_dstm,\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[2,276480,384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[node concat_49 (defined at C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py:58) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'concat_49', defined at:\r\n  File \"threading.py\", line 884, in _bootstrap\r\n  File \"threading.py\", line 916, in _bootstrap_inner\r\n  File \"threading.py\", line 864, in run\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\mainscripts\\Trainer.py\", line 57, in trainerThread\r\n    debug=debug,\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\ModelBase.py\", line 173, in __init__\r\n    self.on_initialize()\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\models\\Model_SAEHD\\Model.py\", line 563, in on_initialize\r\n    src_dst_loss_gv = nn.tf_average_gv_list (gpu_src_dst_loss_gvs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py\", line 58, in tf_average_gv_list\r\n    result[i] = ( tf.reduce_mean( tf.concat (gs, 0), 0 ), v )\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\", line 180, in wrapper\r\n    return target(*args, **kwargs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1256, in concat\r\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 1149, in concat_v2\r\n    \"ConcatV2\", values=values, axis=axis, name=name)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3300, in create_op\r\n    op_def=op_def)\r\n  File \"C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\python-3.6.8\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1801, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[2,276480,384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cuda_host_bfc\r\n         [[node concat_49 (defined at C:\\Users\\Dantevus\\Desktop\\DeepFaceLab_NVIDIA\\_internal\\DeepFaceLab\\core\\leras\\tensor_ops.py:58) ]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDone.\r\nPress any key to continue . . .\r\n","closed_by":{"login":"The-Digital-Void","id":57318447,"node_id":"MDQ6VXNlcjU3MzE4NDQ3","avatar_url":"https://avatars.githubusercontent.com/u/57318447?v=4","gravatar_id":"","url":"https://api.github.com/users/The-Digital-Void","html_url":"https://github.com/The-Digital-Void","followers_url":"https://api.github.com/users/The-Digital-Void/followers","following_url":"https://api.github.com/users/The-Digital-Void/following{/other_user}","gists_url":"https://api.github.com/users/The-Digital-Void/gists{/gist_id}","starred_url":"https://api.github.com/users/The-Digital-Void/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/The-Digital-Void/subscriptions","organizations_url":"https://api.github.com/users/The-Digital-Void/orgs","repos_url":"https://api.github.com/users/The-Digital-Void/repos","events_url":"https://api.github.com/users/The-Digital-Void/events{/privacy}","received_events_url":"https://api.github.com/users/The-Digital-Void/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/586/timeline","performed_via_github_app":null,"state_reason":"completed"}