{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604","repository_url":"https://api.github.com/repos/iperov/DeepFaceLab","labels_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604/labels{/name}","comments_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604/comments","events_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604/events","html_url":"https://github.com/iperov/DeepFaceLab/issues/5604","id":1504618314,"node_id":"I_kwDOCBuapc5ZrqdK","number":5604,"title":"Why there isn't an option for BatchNormalization?","user":{"login":"Jerry-Master","id":34888496,"node_id":"MDQ6VXNlcjM0ODg4NDk2","avatar_url":"https://avatars.githubusercontent.com/u/34888496?v=4","gravatar_id":"","url":"https://api.github.com/users/Jerry-Master","html_url":"https://github.com/Jerry-Master","followers_url":"https://api.github.com/users/Jerry-Master/followers","following_url":"https://api.github.com/users/Jerry-Master/following{/other_user}","gists_url":"https://api.github.com/users/Jerry-Master/gists{/gist_id}","starred_url":"https://api.github.com/users/Jerry-Master/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Jerry-Master/subscriptions","organizations_url":"https://api.github.com/users/Jerry-Master/orgs","repos_url":"https://api.github.com/users/Jerry-Master/repos","events_url":"https://api.github.com/users/Jerry-Master/events{/privacy}","received_events_url":"https://api.github.com/users/Jerry-Master/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-12-20T14:06:35Z","updated_at":"2022-12-20T14:06:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have seen that you have included a BatchNorm2D layer in core.leras but haven't used it in any model. Why is so? Batch normalization can help reducing the number of iterations needed for convergence. Also, your current implementation is wrong, you don't update the mean and the variance. As the official tensorflow implementation states, you must do:\r\n\r\n- `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`\r\n- `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`\r\n\r\nWhich you are not doing. ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/iperov/DeepFaceLab/issues/5604/timeline","performed_via_github_app":null,"state_reason":null}