{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234","repository_url":"https://api.github.com/repos/openvinotoolkit/model_server","labels_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234/labels{/name}","comments_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234/comments","events_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234/events","html_url":"https://github.com/openvinotoolkit/model_server/issues/234","id":616798781,"node_id":"MDU6SXNzdWU2MTY3OTg3ODE=","number":234,"title":"Using the correct configurations to get the best performance","user":{"login":"Steedance","id":45638338,"node_id":"MDQ6VXNlcjQ1NjM4MzM4","avatar_url":"https://avatars.githubusercontent.com/u/45638338?v=4","gravatar_id":"","url":"https://api.github.com/users/Steedance","html_url":"https://github.com/Steedance","followers_url":"https://api.github.com/users/Steedance/followers","following_url":"https://api.github.com/users/Steedance/following{/other_user}","gists_url":"https://api.github.com/users/Steedance/gists{/gist_id}","starred_url":"https://api.github.com/users/Steedance/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Steedance/subscriptions","organizations_url":"https://api.github.com/users/Steedance/orgs","repos_url":"https://api.github.com/users/Steedance/repos","events_url":"https://api.github.com/users/Steedance/events{/privacy}","received_events_url":"https://api.github.com/users/Steedance/received_events","type":"User","site_admin":false},"labels":[{"id":1070321717,"node_id":"MDU6TGFiZWwxMDcwMzIxNzE3","url":"https://api.github.com/repos/openvinotoolkit/model_server/labels/help%20wanted","name":"help wanted","color":"008672","default":true,"description":"Extra attention is needed"}],"state":"closed","locked":false,"assignee":{"login":"dtrawins","id":20400806,"node_id":"MDQ6VXNlcjIwNDAwODA2","avatar_url":"https://avatars.githubusercontent.com/u/20400806?v=4","gravatar_id":"","url":"https://api.github.com/users/dtrawins","html_url":"https://github.com/dtrawins","followers_url":"https://api.github.com/users/dtrawins/followers","following_url":"https://api.github.com/users/dtrawins/following{/other_user}","gists_url":"https://api.github.com/users/dtrawins/gists{/gist_id}","starred_url":"https://api.github.com/users/dtrawins/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dtrawins/subscriptions","organizations_url":"https://api.github.com/users/dtrawins/orgs","repos_url":"https://api.github.com/users/dtrawins/repos","events_url":"https://api.github.com/users/dtrawins/events{/privacy}","received_events_url":"https://api.github.com/users/dtrawins/received_events","type":"User","site_admin":false},"assignees":[{"login":"dtrawins","id":20400806,"node_id":"MDQ6VXNlcjIwNDAwODA2","avatar_url":"https://avatars.githubusercontent.com/u/20400806?v=4","gravatar_id":"","url":"https://api.github.com/users/dtrawins","html_url":"https://github.com/dtrawins","followers_url":"https://api.github.com/users/dtrawins/followers","following_url":"https://api.github.com/users/dtrawins/following{/other_user}","gists_url":"https://api.github.com/users/dtrawins/gists{/gist_id}","starred_url":"https://api.github.com/users/dtrawins/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dtrawins/subscriptions","organizations_url":"https://api.github.com/users/dtrawins/orgs","repos_url":"https://api.github.com/users/dtrawins/repos","events_url":"https://api.github.com/users/dtrawins/events{/privacy}","received_events_url":"https://api.github.com/users/dtrawins/received_events","type":"User","site_admin":false},{"login":"mzegla","id":41325006,"node_id":"MDQ6VXNlcjQxMzI1MDA2","avatar_url":"https://avatars.githubusercontent.com/u/41325006?v=4","gravatar_id":"","url":"https://api.github.com/users/mzegla","html_url":"https://github.com/mzegla","followers_url":"https://api.github.com/users/mzegla/followers","following_url":"https://api.github.com/users/mzegla/following{/other_user}","gists_url":"https://api.github.com/users/mzegla/gists{/gist_id}","starred_url":"https://api.github.com/users/mzegla/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzegla/subscriptions","organizations_url":"https://api.github.com/users/mzegla/orgs","repos_url":"https://api.github.com/users/mzegla/repos","events_url":"https://api.github.com/users/mzegla/events{/privacy}","received_events_url":"https://api.github.com/users/mzegla/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2020-05-12T16:37:01Z","updated_at":"2020-06-12T10:45:55Z","closed_at":"2020-06-12T10:45:55Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nI'm using openvino model server to run inference on multiple models. I've read the documentation but I'm not completely sure how I should set up the config.json. The target hardware is an Intel Xeon Silver 4216 (16 cores, 32 threads).\r\n\r\nBelow is what I have been using.\r\n\r\n```\r\n{\r\n   \"model_config_list\":[\r\n      {\r\n         \"config\":{\r\n            \"name\":\"face-detection-retail-0004\",\r\n            \"base_path\":\"/opt/ml/face-detection-retail-0004\",\r\n            \"shape\": \"auto\",\r\n            \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n      },\r\n      {\r\n         \"config\":{\r\n            \"name\":\"age-gender-recognition-retail-0013\",\r\n            \"base_path\":\"/opt/ml/age-gender-recognition-retail-0013\",\r\n            \"batch_size\": \"auto\",\r\n            \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n      },\r\n      {\r\n         \"config\":{\r\n            \"name\":\"emotions-recognition-retail-0003\",\r\n            \"base_path\":\"/opt/ml/emotions-recognition-retail-0003\",\r\n            \"batch_size\": \"auto\",\r\n            \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n      },\r\n      {\r\n         \"config\":{\r\n            \"name\":\"head-pose-estimation-adas-0001\",\r\n            \"base_path\":\"/opt/ml/head-pose-estimation-adas-0001\",\r\n            \"batch_size\": \"auto\",\r\n            \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n      },\r\n     {\r\n        \"config\":{\r\n           \"name\":\"person-detection-retail-0013\",\r\n           \"base_path\":\"/opt/ml/person-detection-retail-0013\",\r\n           \"batch_size\": \"auto\",\r\n           \"shape\": \"auto\",\r\n           \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n     },\r\n     {\r\n        \"config\":{\r\n           \"name\":\"person-reidentification-retail-0079\",\r\n           \"base_path\":\"/opt/ml/person-reidentification-retail-0079\",\r\n           \"batch_size\": \"auto\",\r\n           \"nireq\": 8\r\n         },\r\n         \"plugin_config\": {\"CPU_THROUGHPUT_STREAMS\": 8, \"CPU_THREADS_NUM\": 32}\r\n     }\r\n   ]\r\n}\r\n```\r\n\r\nI'm using:\r\n \"CPU_THROUGHPUT_STREAMS\": 8 because this is what the benchmark app determined was the optimal setup.\r\n\"CPU_THREADS_NUM\": 32 because the hardware has 32 threads.\r\n\"nireq\": 8 because that would be the maximum requests we would send per model.\r\n\r\n\r\nI have a few questions regarding the config.json file:\r\n-  Is there anything that looks incorrect?\r\n-  Should the throughput streams and threads be set per model and should the values all be the same?\r\n-  Should I include the grpc_workers property in each model's config or in the docker run command?\r\n-  If grpc_workers should be in the config.json, is setting it to the same value as nireq fine or would I get better performance with a higher grpc_worker value?\r\n\r\n\r\nI also have another question regarding sending batches to OVMS (not sure if I should make another issue for this). I have noticed that the fps is lower when sending larger batches. For example, I made a container with just 1 model loaded onto it. I then sent 500 images into it (asynchronously) in batches of 1, 4, 10 and 50. Using batches of 4 processed the images the fastest.\r\nIt is my understanding that using higher batches should produce a higher throughput,  is this not the case when processing asynchronously?\r\n\r\nAny help would be be appreciated.\r\n","closed_by":{"login":"Steedance","id":45638338,"node_id":"MDQ6VXNlcjQ1NjM4MzM4","avatar_url":"https://avatars.githubusercontent.com/u/45638338?v=4","gravatar_id":"","url":"https://api.github.com/users/Steedance","html_url":"https://github.com/Steedance","followers_url":"https://api.github.com/users/Steedance/followers","following_url":"https://api.github.com/users/Steedance/following{/other_user}","gists_url":"https://api.github.com/users/Steedance/gists{/gist_id}","starred_url":"https://api.github.com/users/Steedance/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Steedance/subscriptions","organizations_url":"https://api.github.com/users/Steedance/orgs","repos_url":"https://api.github.com/users/Steedance/repos","events_url":"https://api.github.com/users/Steedance/events{/privacy}","received_events_url":"https://api.github.com/users/Steedance/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/234/timeline","performed_via_github_app":null,"state_reason":"completed"}