{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207","repository_url":"https://api.github.com/repos/openvinotoolkit/model_server","labels_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207/labels{/name}","comments_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207/comments","events_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207/events","html_url":"https://github.com/openvinotoolkit/model_server/issues/1207","id":1175000049,"node_id":"I_kwDOCPfP385GCRPx","number":1207,"title":"how to do batch predictions in openvino model server?","user":{"login":"yuguanxiang","id":41687188,"node_id":"MDQ6VXNlcjQxNjg3MTg4","avatar_url":"https://avatars.githubusercontent.com/u/41687188?v=4","gravatar_id":"","url":"https://api.github.com/users/yuguanxiang","html_url":"https://github.com/yuguanxiang","followers_url":"https://api.github.com/users/yuguanxiang/followers","following_url":"https://api.github.com/users/yuguanxiang/following{/other_user}","gists_url":"https://api.github.com/users/yuguanxiang/gists{/gist_id}","starred_url":"https://api.github.com/users/yuguanxiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yuguanxiang/subscriptions","organizations_url":"https://api.github.com/users/yuguanxiang/orgs","repos_url":"https://api.github.com/users/yuguanxiang/repos","events_url":"https://api.github.com/users/yuguanxiang/events{/privacy}","received_events_url":"https://api.github.com/users/yuguanxiang/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2022-03-21T07:47:27Z","updated_at":"2022-03-29T09:05:52Z","closed_at":"2022-03-29T09:05:52Z","author_association":"NONE","active_lock_reason":null,"body":"how to do batch predirections in openvino server?\r\n\r\ni’ve a DeepFM model in video recommendation system, i trained the model in tensorflow, and provide online serving by tensorflow serving currently.\r\nin tensorflow serving, i use http rest api to do batch prediction(batch size maybe any number in [100,1000])\r\n//in tensorflow serving, i use curl to do http request to tensorflow serving (here for demostration, i send batch(size=2) request)\r\n _curl -X POST http://172.24.72.2:9601/v1/models/ctrdeepfm/versions/9037:predict –d ‘\r\n {\r\n \t“instances”: [\r\n           { “feat_ids”:[7,58,....], //1*301 vector\r\n            “feat_vals”:[1.0,1.0,...], //1*301 vector\r\n           },\r\n           { “feat_ids”:[7,58,....], //1*301 vector\r\n            “feat_vals”:[1.0,1.0,...], //1*301 vector\r\n           }\r\n     ]\r\n }\r\n ‘_\r\n\r\n //output, get 2 scores\r\n_{\r\n    \"predictions\": [0.0528144817, 0.0528254332\r\n    ]\r\n}_\r\n\r\n\r\nhere, i tried use openvino (openvino_2021.4.752) & model server , expecting to accelerate prediction , reduce predict latency,\r\n\r\n1. run model optimizer and convert the model to  get IR\r\n_python mo.py --log_level=DEBUG --input \"feat_ids,feat_vals\" --input_shape \"[1,301],[1,301]\" --saved_model_dir /tmp/1647427562/ --output_dir /tmp/1647427562out_\r\n[root@Router_A model_optimizer]# ls /tmp/1647427562out\r\nsaved_model.bin  saved_model.mapping  saved_model.xml_\r\n\r\n\r\n2.Then, i start openvino model server\r\n_docker pull openvino/model_server\r\ndocker run -d -u $(id -u):$(id -g) -v $(pwd)/model_1647427562out:/models/deepfm -p 9000:9000 openvino/model_server:latest \\\r\n        --shape auto\\\r\n        --model_path /models/deepfm --model_name deepfm --rest_port 9000 --plugin_config '{\"CPU_THROUGHPUT_STREAMS\": \"1\"}'_\r\n\r\n3. i send one prediction(batchsize=1) in one requet to model server, i get correct response\r\n_curl -X POST http://10.2.177.40:9000/v1/models/deepfm/versions/1:predict -d '\r\n{\r\n\"instances\":[\r\n{\"feat_ids\":[7,58,251,....],\r\n\"feat_vals\":[1.0,1.0,1.0,...]\r\n}\r\n]\r\n}'_ \r\n\r\n//output\r\n_{\r\n    \"predictions\": [\r\n        {\r\n            \"Reshape_14\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\r\n            \"CTR_PDT\": 0.0529434271,\r\n            \"Multi-Head-Attention-Layer/Sum_1\": -0.0317770354\r\n        }\r\n    ]\r\n}_\r\n\r\n4. i send two predictions(batchsize=2) in one request to model server\r\n_curl -X POST http://10.2.177.40:9000/v1/models/deepfm/versions/1:predict -d '\r\n{\r\n\"instances\":[\r\n{\"feat_ids\":[7,58,251,....],   //prediction 1\r\n\"feat_vals\":[1.0,1.0,1.0,...]\r\n},\r\n{\"feat_ids\":[7,58,251,....],  //prediction 2\r\n\"feat_vals\":[1.0,1.0,1.0,...]\r\n}\r\n]\r\n}'_ \r\n\r\ni got error message\r\n_{\"error\": \"Model could not be reshaped with requested shape\"}_\r\n\r\n5. i also tried to  change \"shape auto\" to \"batchsize error\", i get this error message \r\n_{\"error\": \"Error while loading a network\"}_\r\n\r\nand also this logs\r\n_[modelinstance.cpp:514] exception occurred while loading network: Check 'PartialShape::broadcast_merge_into( pshape, node->get_input_partial_shape(i), autob)' failed at core/src/op/util/elementwise_args.cpp:37:\r\nWhile validating node 'v1::Multiply Multi-Head-Attention-Layer/multi_head_attention/scale_dot_product_attention/truediv_copy (Multi-Head-Attention-Layer/multi_head_attention/concat_1[0]:f32{8,20,24}, Multi-Head-Attention-Layer/multi_head_attention/scale_dot_product_attention/truediv_copy/const3249965853[0]:f32{4,1,1}) -> (f32{4,20,24})' with friendly_name 'Multi-Head-Attention-Layer/multi_head_attention/scale_dot_product_attention/truediv_copy':\r\nArgument shapes are inconsistent._\r\n\r\nso, my question is: how to do batch predictions for any batchsize? which are correct option for mo.py and model server?\r\nthanks !\r\n","closed_by":{"login":"yuguanxiang","id":41687188,"node_id":"MDQ6VXNlcjQxNjg3MTg4","avatar_url":"https://avatars.githubusercontent.com/u/41687188?v=4","gravatar_id":"","url":"https://api.github.com/users/yuguanxiang","html_url":"https://github.com/yuguanxiang","followers_url":"https://api.github.com/users/yuguanxiang/followers","following_url":"https://api.github.com/users/yuguanxiang/following{/other_user}","gists_url":"https://api.github.com/users/yuguanxiang/gists{/gist_id}","starred_url":"https://api.github.com/users/yuguanxiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yuguanxiang/subscriptions","organizations_url":"https://api.github.com/users/yuguanxiang/orgs","repos_url":"https://api.github.com/users/yuguanxiang/repos","events_url":"https://api.github.com/users/yuguanxiang/events{/privacy}","received_events_url":"https://api.github.com/users/yuguanxiang/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/1207/timeline","performed_via_github_app":null,"state_reason":"completed"}