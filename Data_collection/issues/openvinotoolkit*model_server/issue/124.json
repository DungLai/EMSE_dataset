{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124","repository_url":"https://api.github.com/repos/openvinotoolkit/model_server","labels_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124/labels{/name}","comments_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124/comments","events_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124/events","html_url":"https://github.com/openvinotoolkit/model_server/issues/124","id":548270496,"node_id":"MDU6SXNzdWU1NDgyNzA0OTY=","number":124,"title":"Reproduce OpenVINO efficiency","user":{"login":"dkurt","id":25801568,"node_id":"MDQ6VXNlcjI1ODAxNTY4","avatar_url":"https://avatars.githubusercontent.com/u/25801568?v=4","gravatar_id":"","url":"https://api.github.com/users/dkurt","html_url":"https://github.com/dkurt","followers_url":"https://api.github.com/users/dkurt/followers","following_url":"https://api.github.com/users/dkurt/following{/other_user}","gists_url":"https://api.github.com/users/dkurt/gists{/gist_id}","starred_url":"https://api.github.com/users/dkurt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dkurt/subscriptions","organizations_url":"https://api.github.com/users/dkurt/orgs","repos_url":"https://api.github.com/users/dkurt/repos","events_url":"https://api.github.com/users/dkurt/events{/privacy}","received_events_url":"https://api.github.com/users/dkurt/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2020-01-10T20:16:54Z","updated_at":"2020-02-10T14:09:19Z","closed_at":"2020-02-10T14:09:19Z","author_association":"NONE","active_lock_reason":null,"body":"Hi!\r\n\r\nCannot reproduce the performance of plain OpenVINO with Model Server. Benchmarking tool gives more than 12FPS however the best efficiency from OMS is only 8.4 FPS. Can you guide me if I'm using OMS in a wrong way? Shall I try to run multiple instances of docker / clients to achieve performance close to target?\r\n\r\nbenchmark_app.py:\r\n```\r\n$ python3 /opt/intel/openvino/deployment_tools/tools/benchmark_tool/benchmark_app.py -m graph.xml -l /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so\r\n\r\n[Step 1/11] Parsing and validating input arguments\r\n[ WARNING ]  -nstreams default value is determined automatically for a device. Although the automatic selection usually provides a reasonable performance, but it still may be non-optimal for some cases, for more information look at README.\r\n[Step 2/11] Loading Inference Engine\r\n[ INFO ] CPU extensions is loaded /opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so\r\n[ INFO ] InferenceEngine:\r\n         API version............. 2.1.custom_releases/2019/R3_cb6cad9663aea3d282e0e8b3e0bf359df665d5d0\r\n[ INFO ] Device info\r\n         CPU\r\n         MKLDNNPlugin............ version 2.1\r\n         Build................... 30677\r\n\r\n[Step 3/11] Reading the Intermediate Representation network\r\n[Step 4/11] Resizing network to match image sizes and given batch\r\n[ INFO ] Network batch size: 1, precision: MIXED\r\n[Step 5/11] Configuring input of the model\r\n[Step 6/11] Setting device configuration\r\n[Step 7/11] Loading the model to the device\r\n[Step 8/11] Setting optimal runtime parameters\r\n[Step 9/11] Creating infer requests and filling input blobs with images\r\n[ INFO ] Network input 'input_ids' precision FP32, dimensions (NC): 1 128\r\n[ INFO ] Network input 'input_type_ids' precision FP32, dimensions (NC): 1 128\r\n[ INFO ] Network input 'input_mask' precision FP32, dimensions (NC): 1 128\r\n[ WARNING ] No input files were given: all inputs will be filled with random values!\r\n[ INFO ] Infer Request 0 filling\r\n[ INFO ] Fill input 'input_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_type_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_mask' with random values (some binary data is expected)\r\n[ INFO ] Infer Request 1 filling\r\n[ INFO ] Fill input 'input_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_type_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_mask' with random values (some binary data is expected)\r\n[ INFO ] Infer Request 2 filling\r\n[ INFO ] Fill input 'input_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_type_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_mask' with random values (some binary data is expected)\r\n[ INFO ] Infer Request 3 filling\r\n[ INFO ] Fill input 'input_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_type_ids' with random values (some binary data is expected)\r\n[ INFO ] Fill input 'input_mask' with random values (some binary data is expected)\r\n[Step 10/11] Measuring performance (Start inference asyncronously, 4 inference requests using 4 streams for CPU, limits: 60000 ms duration)\r\n[Step 11/11] Dumping statistics report\r\nCount:      744 iterations\r\nDuration:   60606.23 ms\r\nLatency:    322.86 ms\r\nThroughput: 12.28 FPS\r\n```\r\n\r\nClient script (tried to use 16 async requests):\r\n```python\r\nimport grpc\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport time\r\nimport argparse\r\n\r\nfrom tensorflow_serving.apis import predict_pb2\r\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\r\n\r\nchannel = grpc.insecure_channel('127.0.0.1:9001')\r\n\r\nstub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\r\nrequest = predict_pb2.PredictRequest()\r\n\r\nrequest.model_spec.name = 'bert'\r\n\r\ninput_ids = np.random.randint(0, 255, (1, 128))\r\ninput_mask = np.random.randint(0, 255, (1, 128))\r\ninput_type_ids = np.random.randint(0, 255, (1, 128))\r\n\r\nrequest.inputs['input_ids'].CopyFrom(tf.make_tensor_proto(input_ids))\r\nrequest.inputs['input_mask'].CopyFrom(tf.make_tensor_proto(input_mask))\r\nrequest.inputs['input_type_ids'].CopyFrom(tf.make_tensor_proto(input_type_ids))\r\n\r\nnireq = 16\r\n\r\nfutures = [stub.Predict.future(request, 5.0) for i in range(nireq)]\r\nfor future in futures:\r\n    future.result()\r\n\r\n\r\nstart = time.time()\r\nn = 64\r\nfor i in range(n // nireq):\r\n    futures = [stub.Predict.future(request, 5.0) for i in range(nireq)]\r\n    for future in futures:\r\n        future.result()\r\n\r\nprint((time.time() - start) / n)\r\n\r\ntime.sleep(5)\r\n```\r\n\r\n\r\n1. CPU_THROUGHPUT_NUMA and CPU_THREADS_NUM\r\n\r\n```\r\ndocker run --rm -d  -v /path/to/models/:/opt/ml:ro -p 9001:9001 -p 8001:8001 ie-serving-py:latest /ie-serving-py/start_server.sh ie_serving model --model_path /opt/ml/bert --model_name bert --port 9001 --rest_port 8001 --nireq 4 --grpc_workers 8 --plugin_config \"{\\\"CPU_THROUGHPUT_STREAMS\\\": \\\"CPU_THROUGHPUT_NUMA\\\",\\\"CPU_THREADS_NUM\\\": \\\"4\\\"}\"\r\n```\r\n\r\n7.35 FPS\r\n\r\n2. CPU_THROUGHPUT_AUTO\r\n\r\n```\r\ndocker run --rm -d  -v /path/to/models/:/opt/ml:ro -p 9001:9001 -p 8001:8001 ie-serving-py:latest /ie-serving-py/start_server.sh ie_serving model --model_path /opt/ml/bert --model_name bert --port 9001 --rest_port 8001 --nireq 4 --grpc_workers 8 --plugin_config \"{\\\"CPU_THROUGHPUT_STREAMS\\\": \\\"CPU_THROUGHPUT_AUTO\\\"}\"\r\n```\r\n\r\n  8.33 FPS\r\n\r\n3. CPU_THROUGHPUT_AUTO and `nireq = 8` in the Python script\r\n\r\n  7.63 FPS\r\n\r\n4. CPU_THROUGHPUT_AUTO and `nireq = 4` in the Python script\r\n\r\n  6.53 FPS\r\n\r\nModel: BERT base uncased, [graph.bin](https://www.dropbox.com/s/benurld444gv70a/graph.bin?dl=0), [graph.xml](https://www.dropbox.com/s/2g6yk9m0ln7ierx/graph.xml?dl=0)\r\n\r\n\r\nThanks in advance!","closed_by":{"login":"dkurt","id":25801568,"node_id":"MDQ6VXNlcjI1ODAxNTY4","avatar_url":"https://avatars.githubusercontent.com/u/25801568?v=4","gravatar_id":"","url":"https://api.github.com/users/dkurt","html_url":"https://github.com/dkurt","followers_url":"https://api.github.com/users/dkurt/followers","following_url":"https://api.github.com/users/dkurt/following{/other_user}","gists_url":"https://api.github.com/users/dkurt/gists{/gist_id}","starred_url":"https://api.github.com/users/dkurt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dkurt/subscriptions","organizations_url":"https://api.github.com/users/dkurt/orgs","repos_url":"https://api.github.com/users/dkurt/repos","events_url":"https://api.github.com/users/dkurt/events{/privacy}","received_events_url":"https://api.github.com/users/dkurt/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/openvinotoolkit/model_server/issues/124/timeline","performed_via_github_app":null,"state_reason":"completed"}