[{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/comments/414953690","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/7#issuecomment-414953690","issue_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/7","id":414953690,"node_id":"MDEyOklzc3VlQ29tbWVudDQxNDk1MzY5MA==","user":{"login":"XiaoLiuAI","id":1553482,"node_id":"MDQ6VXNlcjE1NTM0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/1553482?v=4","gravatar_id":"","url":"https://api.github.com/users/XiaoLiuAI","html_url":"https://github.com/XiaoLiuAI","followers_url":"https://api.github.com/users/XiaoLiuAI/followers","following_url":"https://api.github.com/users/XiaoLiuAI/following{/other_user}","gists_url":"https://api.github.com/users/XiaoLiuAI/gists{/gist_id}","starred_url":"https://api.github.com/users/XiaoLiuAI/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/XiaoLiuAI/subscriptions","organizations_url":"https://api.github.com/users/XiaoLiuAI/orgs","repos_url":"https://api.github.com/users/XiaoLiuAI/repos","events_url":"https://api.github.com/users/XiaoLiuAI/events{/privacy}","received_events_url":"https://api.github.com/users/XiaoLiuAI/received_events","type":"User","site_admin":false},"created_at":"2018-08-22T08:25:53Z","updated_at":"2018-08-22T09:03:50Z","author_association":"NONE","body":"My current implementation is \r\n```\r\nclass TransformerEncoderCrf(Wrapper):\r\n    def __init__(self, config):\r\n        self.len_limit = config.len_limit\r\n        self.load_model(config)\r\n\r\n    def get_pos_seq(self, x):\r\n        mask = K.cast(K.not_equal(x, 0), 'int32')\r\n        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\r\n        return pos * mask # TODO add length limit\r\n\r\n    @staticmethod\r\n    def get_loss(args):\r\n        y_pred, y_true = args\r\n        y_true = K.cast(y_true, 'int32')\r\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true[:,1:], logits=y_pred[:,:-1])\r\n        mask = K.cast(tf.not_equal(y_true, 0), 'float32')\r\n        loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1) # batch_size * 1\r\n        loss = K.mean(loss) \r\n        return loss\r\n\r\n    def load_model(self, config):\r\n        nvocab = config.nvocab\r\n        len_limit = config.len_limit\r\n        d_embed = config.d_embed\r\n        share_word_emb = config.share_word_emb\r\n        n_head = config.n_head\r\n        d_k = config.d_k\r\n        d_v = config.d_v\r\n        d_inner_hid = config.d_inner_hid\r\n        n_layers = config.n_layers\r\n        dropout = config.dropout\r\n\r\n        if config.pos_trainable:\r\n            pos_emb = Embedding(len_limit, d_embed, trainable=True)\r\n        else:\r\n            pos_emb = Embedding(len_limit, d_embed, trainable=False, weights=[GetPosEncodingMatrix(len_limit, d_embed)])\r\n\r\n        word_emb = Embedding(nvocab, d_embed)\r\n\r\n        self.encoder = FullSeqEncoder(d_embed, d_inner_hid, n_head, d_k, d_v, n_layers, dropout,\r\n                                     word_emb=word_emb, pos_emb=pos_emb)\r\n\r\n        self.tok_input = Input(shape=(None,), dtype='int32')\r\n        self.tok_output = Input(shape=(None,), dtype='int32')\r\n\r\n        self.position_input = Lambda(self.get_pos_seq)(self.tok_input)\r\n\r\n        enc_output = self.encoder(self.tok_input, self.position_input)\r\n        self.encoder_model = Model(inputs=self.tok_input, outputs=enc_output)  # for possible pre-training\r\n\r\n        lm_output = TimeDistributed(TiedEmbeddingsTransposed(tied_to=word_emb))(enc_output)\r\n        lm_loss = Lambda(self.get_loss)([lm_output, self.tok_input])\r\n\r\n        if config.use_crf:\r\n            fully_connected_layer = TimeDistributed(Dense(config.num_fully_connect, activation='tanh'))\r\n            crf_layer = CRF(config.ntags, sparse_target=False)\r\n            ner_output = crf_layer(fully_connected_layer(self.encoder_model))\r\n\r\n            self.loss = [crf_layer.loss_function, lm_loss]\r\n            self.metrics = crf_layer.accuracy\r\n\r\n            self.model = Model(inputs=self.tok_input, outputs=ner_output)\r\n        else:\r\n            # TO BE DONE\r\n            output_layer = TimeDistributed(Dense(config.ntags, activation='softmax'))\r\n            self.model = Model(inputs=self.tok_input, outputs=output_layer)\r\n            self.loss = 'categorical_crossentropy'\r\n            self.metrics = 'accuracy'\r\n\r\n        print(self.model.summary())\r\n\r\n    def compile(self, *args, **kwargs):\r\n        # TO BE DONE\r\n        if 'metrics' in kwargs:\r\n            kwargs['metrics'].append(self.metrics)\r\n        else:\r\n            kwargs['metrics'] = [self.metrics]\r\n        self.model.compile(*args, loss=self.loss, **kwargs)\r\n```","reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/comments/414953690/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"XiaoLiuAI","id":1553482,"node_id":"MDQ6VXNlcjE1NTM0ODI=","avatar_url":"https://avatars.githubusercontent.com/u/1553482?v=4","gravatar_id":"","url":"https://api.github.com/users/XiaoLiuAI","html_url":"https://github.com/XiaoLiuAI","followers_url":"https://api.github.com/users/XiaoLiuAI/followers","following_url":"https://api.github.com/users/XiaoLiuAI/following{/other_user}","gists_url":"https://api.github.com/users/XiaoLiuAI/gists{/gist_id}","starred_url":"https://api.github.com/users/XiaoLiuAI/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/XiaoLiuAI/subscriptions","organizations_url":"https://api.github.com/users/XiaoLiuAI/orgs","repos_url":"https://api.github.com/users/XiaoLiuAI/repos","events_url":"https://api.github.com/users/XiaoLiuAI/events{/privacy}","received_events_url":"https://api.github.com/users/XiaoLiuAI/received_events","type":"User","site_admin":false}}]