{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/13","id":387922750,"node_id":"MDU6SXNzdWUzODc5MjI3NTA=","number":13,"title":"'nan' loss function when using layer normalization","user":{"login":"McKracken","id":7049505,"node_id":"MDQ6VXNlcjcwNDk1MDU=","avatar_url":"https://avatars.githubusercontent.com/u/7049505?v=4","gravatar_id":"","url":"https://api.github.com/users/McKracken","html_url":"https://github.com/McKracken","followers_url":"https://api.github.com/users/McKracken/followers","following_url":"https://api.github.com/users/McKracken/following{/other_user}","gists_url":"https://api.github.com/users/McKracken/gists{/gist_id}","starred_url":"https://api.github.com/users/McKracken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/McKracken/subscriptions","organizations_url":"https://api.github.com/users/McKracken/orgs","repos_url":"https://api.github.com/users/McKracken/repos","events_url":"https://api.github.com/users/McKracken/events{/privacy}","received_events_url":"https://api.github.com/users/McKracken/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2018-12-05T20:02:55Z","updated_at":"2019-07-13T01:17:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, \r\n\r\nI was using only the LayerNormalization from your code in mine. I didn't change anything from the code, apart from overriding the `compute_mask` function, as my input is an Embedding with `mask_zero=True`\r\n\r\nCode\r\n\r\n```\r\nclass LayerNormalization(Layer):\r\n\r\n    def __init__(self, eps=1e-6, **kwargs):\r\n        self.eps = eps\r\n        super(LayerNormalization, self).__init__(**kwargs)\r\n\r\n    def build(self, input_shape):\r\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\r\n                                     initializer=Ones(), trainable=True)\r\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\r\n                                    initializer=Zeros(), trainable=True)\r\n        super(LayerNormalization, self).build(input_shape)\r\n\r\n    def call(self, x):\r\n        mean = K.mean(x, axis=-1, keepdims=True)\r\n        std = K.std(x, axis=-1, keepdims=True)\r\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\r\n\r\n    def compute_output_shape(self, input_shape):\r\n        return input_shape\r\n\r\n    def compute_mask(self, inputs, input_mask=None):\r\n        return input_mask\r\n```\r\n\r\n\r\nbut strangely I get all `nan` for all the measurements I do while training and tuning (loss function and others). I tried using other implementations of the LayerNormalization layer (e.g. [https://github.com/CyberZHG/keras-layer-normalization](https://github.com/CyberZHG/keras-layer-normalization)), and everything works without problem. I was wondering whether you have any clue about that. ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/13/timeline","performed_via_github_app":null,"state_reason":null}