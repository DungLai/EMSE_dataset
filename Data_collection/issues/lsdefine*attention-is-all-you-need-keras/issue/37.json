{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/37","id":616301616,"node_id":"MDU6SXNzdWU2MTYzMDE2MTY=","number":37,"title":"after embedding layer","user":{"login":"lidongxing","id":16410042,"node_id":"MDQ6VXNlcjE2NDEwMDQy","avatar_url":"https://avatars.githubusercontent.com/u/16410042?v=4","gravatar_id":"","url":"https://api.github.com/users/lidongxing","html_url":"https://github.com/lidongxing","followers_url":"https://api.github.com/users/lidongxing/followers","following_url":"https://api.github.com/users/lidongxing/following{/other_user}","gists_url":"https://api.github.com/users/lidongxing/gists{/gist_id}","starred_url":"https://api.github.com/users/lidongxing/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lidongxing/subscriptions","organizations_url":"https://api.github.com/users/lidongxing/orgs","repos_url":"https://api.github.com/users/lidongxing/repos","events_url":"https://api.github.com/users/lidongxing/events{/privacy}","received_events_url":"https://api.github.com/users/lidongxing/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-05-12T02:01:34Z","updated_at":"2020-05-19T07:40:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\nafter embedding layer, the new token embedding includes learned token embedding and static postional embedding. Of course, the <pad> have the postional embedding value. So, before the embedding are entered into the encoder or decoder, if the embedding sequences are needed to multiply padding mask to delete the influence of <pad> embeddings?\r\n\r\nTks,\r\nLook forward to your reply.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/37/timeline","performed_via_github_app":null,"state_reason":null}