{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/36","id":579694662,"node_id":"MDU6SXNzdWU1Nzk2OTQ2NjI=","number":36,"title":"reshape may not match","user":{"login":"pengxingang","id":44219076,"node_id":"MDQ6VXNlcjQ0MjE5MDc2","avatar_url":"https://avatars.githubusercontent.com/u/44219076?v=4","gravatar_id":"","url":"https://api.github.com/users/pengxingang","html_url":"https://github.com/pengxingang","followers_url":"https://api.github.com/users/pengxingang/followers","following_url":"https://api.github.com/users/pengxingang/following{/other_user}","gists_url":"https://api.github.com/users/pengxingang/gists{/gist_id}","starred_url":"https://api.github.com/users/pengxingang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pengxingang/subscriptions","organizations_url":"https://api.github.com/users/pengxingang/orgs","repos_url":"https://api.github.com/users/pengxingang/repos","events_url":"https://api.github.com/users/pengxingang/events{/privacy}","received_events_url":"https://api.github.com/users/pengxingang/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-03-12T05:02:41Z","updated_at":"2020-11-12T02:08:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, thanks a lot for your code. It seems that I find a bug.\r\n\r\nIn the `MultiHeadAttention` layer, the `reshape1` function\r\n```python\r\nx = tf.reshape(x, [s[0], s[1], n_head, s[2]//n_head])\r\nx = tf.transpose(x, [2, 0, 1, 3]) \r\nx = tf.reshape(x, [-1, s[1], s[2]//n_head])\r\n```\r\nThe transpose puts the head axis before the batch axis. After reshaping, the first axis should be like this (suppose N samples and only 2 heads):\r\n```\r\nsample_0_head_0\r\nsample_1_head_0\r\nsample_2_head_0\r\n...\r\nsample_N-1_head_0\r\nsample_0_head_1\r\nsample_1_head_1\r\nsample_2_head_1\r\n...\r\nsample_N-1_head_1\r\n```\r\n\r\nBut the repeats of `mask`:\r\n```python\r\nmask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\r\n```\r\nwill return `mask` like this:\r\n```\r\nmask_0,\r\nmask_0,\r\nmask_1,\r\nmask_1,\r\n...\r\nmask_N,\r\nmask_N,\r\n```\r\n(find the useage of repeat_elements [here](https://www.tensorflow.org/api_docs/python/tf/keras/backend/repeat_elements))\r\n\r\nHowever, actually we want `mask` to be like this:\r\n```\r\nmask_0,\r\nmask_1,\r\n...\r\nmask_N-1,\r\nmask_0,\r\nmask_1,\r\n...\r\nmask_N-1\r\n```\r\nSo I think the reshape function `reshape1` should change `x = tf.transpose(x, [2, 0, 1, 3]) ` into `x = tf.transpose(x, [0, 2, 1, 3])`. And so does the `reshape2`.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/36/timeline","performed_via_github_app":null,"state_reason":null}