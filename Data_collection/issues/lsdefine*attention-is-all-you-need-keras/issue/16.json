{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/16","id":399170740,"node_id":"MDU6SXNzdWUzOTkxNzA3NDA=","number":16,"title":"Issue with attention mask","user":{"login":"LorrinWWW","id":20911161,"node_id":"MDQ6VXNlcjIwOTExMTYx","avatar_url":"https://avatars.githubusercontent.com/u/20911161?v=4","gravatar_id":"","url":"https://api.github.com/users/LorrinWWW","html_url":"https://github.com/LorrinWWW","followers_url":"https://api.github.com/users/LorrinWWW/followers","following_url":"https://api.github.com/users/LorrinWWW/following{/other_user}","gists_url":"https://api.github.com/users/LorrinWWW/gists{/gist_id}","starred_url":"https://api.github.com/users/LorrinWWW/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/LorrinWWW/subscriptions","organizations_url":"https://api.github.com/users/LorrinWWW/orgs","repos_url":"https://api.github.com/users/LorrinWWW/repos","events_url":"https://api.github.com/users/LorrinWWW/events{/privacy}","received_events_url":"https://api.github.com/users/LorrinWWW/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-01-15T03:13:53Z","updated_at":"2019-01-15T03:34:48Z","closed_at":"2019-01-15T03:34:48Z","author_association":"NONE","active_lock_reason":null,"body":"Hello, I check the source code and found the implementation of mask is define as following:\r\n```python\r\nclass ScaledDotProductAttention():\r\n\tdef __init__(self, d_model, attn_dropout=0.1):\r\n\t\tself.temper = np.sqrt(d_model)\r\n\t\tself.dropout = Dropout(attn_dropout)\r\n\tdef __call__(self, q, k, v, mask):\r\n\t\tattn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])/self.temper)([q, k])\r\n\t\tif mask is not None:\r\n\t\t\tmmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\r\n\t\t\tattn = Add()([attn, mmask])\r\n\t\tattn = Activation('softmax')(attn)\r\n\t\tattn = self.dropout(attn)\r\n\t\toutput = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\r\n\t\treturn output, attn\r\n```\r\nas far as i am concerned, the \"Add()([attn, mmask])\" operation will broadcast \"mmask\" to the shape of \"attn\", which will mask some rows of \"attn\". But this may cause the following softmax operation to be meaningless as the softmax layer takes effect on each row. To be clearer,\r\n\r\n```python\r\n'''\r\n## we neglect the batch dimension\r\nattn = [\r\n\ta_11, a_12, a_13\r\n\ta_21, a_22, a_23\r\n\ta_31, a_32, a_33\r\n](q=3, k=3)\r\n\r\nmmask = [\r\n\t0.0,\r\n\t0.0,\r\n\t-inf, \r\n](q=3)\r\n\r\n## after broadcasting:\r\nattn += mmask \r\n== [\r\n\ta_11, a_12, a_13\r\n\ta_21, a_22, a_23\r\n\t-inf, -inf, -inf\r\n](q=3, k=3)\r\n\r\nattn = softmax(attn)\r\n== [\r\n\tsoftmax(a_11, a_12, a_13)\r\n\tsoftmax(a_21, a_22, a_23)\r\n\t1/3, 1/3, 1/3  <-------- is not what we want\r\n](q=3, k=3)\r\n'''\r\n```\r\n\r\nAm I missing something? or should the mask operation take effect after softmax layer?","closed_by":{"login":"LorrinWWW","id":20911161,"node_id":"MDQ6VXNlcjIwOTExMTYx","avatar_url":"https://avatars.githubusercontent.com/u/20911161?v=4","gravatar_id":"","url":"https://api.github.com/users/LorrinWWW","html_url":"https://github.com/LorrinWWW","followers_url":"https://api.github.com/users/LorrinWWW/followers","following_url":"https://api.github.com/users/LorrinWWW/following{/other_user}","gists_url":"https://api.github.com/users/LorrinWWW/gists{/gist_id}","starred_url":"https://api.github.com/users/LorrinWWW/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/LorrinWWW/subscriptions","organizations_url":"https://api.github.com/users/LorrinWWW/orgs","repos_url":"https://api.github.com/users/LorrinWWW/repos","events_url":"https://api.github.com/users/LorrinWWW/events{/privacy}","received_events_url":"https://api.github.com/users/LorrinWWW/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/16/timeline","performed_via_github_app":null,"state_reason":"completed"}