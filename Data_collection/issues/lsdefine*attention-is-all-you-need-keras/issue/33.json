{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/33","id":570782366,"node_id":"MDU6SXNzdWU1NzA3ODIzNjY=","number":33,"title":"layer norm end of the encoder?","user":{"login":"salihgunduz","id":38071762,"node_id":"MDQ6VXNlcjM4MDcxNzYy","avatar_url":"https://avatars.githubusercontent.com/u/38071762?v=4","gravatar_id":"","url":"https://api.github.com/users/salihgunduz","html_url":"https://github.com/salihgunduz","followers_url":"https://api.github.com/users/salihgunduz/followers","following_url":"https://api.github.com/users/salihgunduz/following{/other_user}","gists_url":"https://api.github.com/users/salihgunduz/gists{/gist_id}","starred_url":"https://api.github.com/users/salihgunduz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/salihgunduz/subscriptions","organizations_url":"https://api.github.com/users/salihgunduz/orgs","repos_url":"https://api.github.com/users/salihgunduz/repos","events_url":"https://api.github.com/users/salihgunduz/events{/privacy}","received_events_url":"https://api.github.com/users/salihgunduz/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-02-25T19:29:28Z","updated_at":"2020-02-25T19:29:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"should a layer norm be at the end of encoder layer like below? if I search orginal paper there is norm layer after pos-ffn.\r\nclass EncoderLayer():\r\n\tdef __init__(self, d_model, d_inner_hid, n_head, dropout=0.1):\r\n\t\tself.self_att_layer = MultiHeadAttention(n_head, d_model, dropout=dropout)\r\n\t\tself.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\r\n\t\tself.norm_layer = LayerNormalization()\r\n\tdef __call__(self, enc_input, mask=None):\r\n\t\toutput, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\r\n\t\toutput1 = self.norm_layer(Add()([enc_input, output]))\r\n\t\toutput = self.pos_ffn_layer(output1)\r\n                **_output = self.norm_layer(Add()([output1 , output]))_**\r\n\t\treturn output, slf_attn\r\n\r\n![encoder](https://user-images.githubusercontent.com/38071762/75279984-a85a2f80-581d-11ea-877b-9111dc617faa.png)\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/33/timeline","performed_via_github_app":null,"state_reason":null}