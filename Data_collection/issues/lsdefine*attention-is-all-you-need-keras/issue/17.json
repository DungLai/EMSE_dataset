{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/17","id":401081199,"node_id":"MDU6SXNzdWU0MDEwODExOTk=","number":17,"title":"Skip-connection in Transformer","user":{"login":"hoangcuong2011","id":8759715,"node_id":"MDQ6VXNlcjg3NTk3MTU=","avatar_url":"https://avatars.githubusercontent.com/u/8759715?v=4","gravatar_id":"","url":"https://api.github.com/users/hoangcuong2011","html_url":"https://github.com/hoangcuong2011","followers_url":"https://api.github.com/users/hoangcuong2011/followers","following_url":"https://api.github.com/users/hoangcuong2011/following{/other_user}","gists_url":"https://api.github.com/users/hoangcuong2011/gists{/gist_id}","starred_url":"https://api.github.com/users/hoangcuong2011/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hoangcuong2011/subscriptions","organizations_url":"https://api.github.com/users/hoangcuong2011/orgs","repos_url":"https://api.github.com/users/hoangcuong2011/repos","events_url":"https://api.github.com/users/hoangcuong2011/events{/privacy}","received_events_url":"https://api.github.com/users/hoangcuong2011/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-01-20T09:53:58Z","updated_at":"2019-07-13T01:12:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\n\r\nThanks for a great project, which helps me build model on top of that.\r\n\r\nI was wondering one thing: it seems like you do not implement skip connection (residual network) in Transformer?\r\n\r\nIs it because you implemented it and you didn't observe improvement?\r\n\r\nOr is it just because you didn't implement it?\r\n\r\nI asked because when I use more layers, I got worser performance actually. I am not sure whether it is what it is (i.e. having more layers does not help), or it is because I don't have skip connections, which usually helps build a deeper model.\r\n\r\nBest,\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/17/timeline","performed_via_github_app":null,"state_reason":null}