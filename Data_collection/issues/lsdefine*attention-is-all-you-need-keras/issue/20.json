{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20","repository_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras","labels_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20/labels{/name}","comments_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20/comments","events_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20/events","html_url":"https://github.com/lsdefine/attention-is-all-you-need-keras/issues/20","id":408089810,"node_id":"MDU6SXNzdWU0MDgwODk4MTA=","number":20,"title":"K.mean() in computing loss doesn't make any sense.","user":{"login":"mayurnewase","id":12967587,"node_id":"MDQ6VXNlcjEyOTY3NTg3","avatar_url":"https://avatars.githubusercontent.com/u/12967587?v=4","gravatar_id":"","url":"https://api.github.com/users/mayurnewase","html_url":"https://github.com/mayurnewase","followers_url":"https://api.github.com/users/mayurnewase/followers","following_url":"https://api.github.com/users/mayurnewase/following{/other_user}","gists_url":"https://api.github.com/users/mayurnewase/gists{/gist_id}","starred_url":"https://api.github.com/users/mayurnewase/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mayurnewase/subscriptions","organizations_url":"https://api.github.com/users/mayurnewase/orgs","repos_url":"https://api.github.com/users/mayurnewase/repos","events_url":"https://api.github.com/users/mayurnewase/events{/privacy}","received_events_url":"https://api.github.com/users/mayurnewase/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-02-08T10:26:01Z","updated_at":"2019-02-08T10:26:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In\r\n ```\r\ndef get_loss(args):\r\n            y_pred, y_true = args\r\n            y_true = tf.cast(y_true, 'int32')\r\n            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\r\n            mask = tf.cast(tf.not_equal(y_true, 0), 'float32')\r\n            loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1)\r\n            loss = K.mean(loss)\r\n            return loss\r\n\r\n```\r\n```loss = tf.reduce_sum(loss * mask, -1) / tf.reduce_sum(mask, -1)```\r\nproduce single element, it's mean doesn't make difference.\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/lsdefine/attention-is-all-you-need-keras/issues/20/timeline","performed_via_github_app":null,"state_reason":null}