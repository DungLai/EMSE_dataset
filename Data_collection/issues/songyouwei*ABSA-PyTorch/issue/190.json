{"url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190","repository_url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch","labels_url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190/labels{/name}","comments_url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190/comments","events_url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190/events","html_url":"https://github.com/songyouwei/ABSA-PyTorch/issues/190","id":925394691,"node_id":"MDU6SXNzdWU5MjUzOTQ2OTE=","number":190,"title":"Why we build tokenizer using both train and test set? ","user":{"login":"minhdang241","id":38881541,"node_id":"MDQ6VXNlcjM4ODgxNTQx","avatar_url":"https://avatars.githubusercontent.com/u/38881541?v=4","gravatar_id":"","url":"https://api.github.com/users/minhdang241","html_url":"https://github.com/minhdang241","followers_url":"https://api.github.com/users/minhdang241/followers","following_url":"https://api.github.com/users/minhdang241/following{/other_user}","gists_url":"https://api.github.com/users/minhdang241/gists{/gist_id}","starred_url":"https://api.github.com/users/minhdang241/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/minhdang241/subscriptions","organizations_url":"https://api.github.com/users/minhdang241/orgs","repos_url":"https://api.github.com/users/minhdang241/repos","events_url":"https://api.github.com/users/minhdang241/events{/privacy}","received_events_url":"https://api.github.com/users/minhdang241/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-06-19T13:29:20Z","updated_at":"2021-06-20T02:44:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I don't think we should build the tokenizer with both train and test set for non-BERT model situations. The chances are there will be words in the production environment which are not available in our vocabulary. If we use the test set to build the vocabulary, the performance will be bias. \r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/songyouwei/ABSA-PyTorch/issues/190/timeline","performed_via_github_app":null,"state_reason":null}