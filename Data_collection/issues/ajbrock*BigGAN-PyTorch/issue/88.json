{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88","repository_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch","labels_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88/labels{/name}","comments_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88/comments","events_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88/events","html_url":"https://github.com/ajbrock/BigGAN-PyTorch/issues/88","id":902772907,"node_id":"MDU6SXNzdWU5MDI3NzI5MDc=","number":88,"title":"Correct attention layer implementation","user":{"login":"valillon","id":2164412,"node_id":"MDQ6VXNlcjIxNjQ0MTI=","avatar_url":"https://avatars.githubusercontent.com/u/2164412?v=4","gravatar_id":"","url":"https://api.github.com/users/valillon","html_url":"https://github.com/valillon","followers_url":"https://api.github.com/users/valillon/followers","following_url":"https://api.github.com/users/valillon/following{/other_user}","gists_url":"https://api.github.com/users/valillon/gists{/gist_id}","starred_url":"https://api.github.com/users/valillon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/valillon/subscriptions","organizations_url":"https://api.github.com/users/valillon/orgs","repos_url":"https://api.github.com/users/valillon/repos","events_url":"https://api.github.com/users/valillon/events{/privacy}","received_events_url":"https://api.github.com/users/valillon/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-05-26T18:19:02Z","updated_at":"2021-05-26T21:24:39Z","closed_at":"2021-05-26T18:59:07Z","author_association":"NONE","active_lock_reason":null,"body":"There is an interesting [comment](https://github.com/ajbrock/BigGAN-PyTorch/blob/98459431a5d618d644d54cd1e9fceb1e5045648d/layers.py#L142) regarding the original attention layer proposed in SAGAN. Actually, the published method and the actual implementation significantly differ as issued in the [official repo](https://github.com/brain-research/self-attention-gan/issues/30) and another well-known [pytorch implementation](https://github.com/heykeetae/Self-Attention-GAN/issues/54). Therein, I recently proposed an implementation which strictly follows the original paper description, at least to my understanding of it.\r\n\r\nSo, with not other explanation, which is then the correct method, the one published or the implemented one?\r\n","closed_by":{"login":"ajbrock","id":7751273,"node_id":"MDQ6VXNlcjc3NTEyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/7751273?v=4","gravatar_id":"","url":"https://api.github.com/users/ajbrock","html_url":"https://github.com/ajbrock","followers_url":"https://api.github.com/users/ajbrock/followers","following_url":"https://api.github.com/users/ajbrock/following{/other_user}","gists_url":"https://api.github.com/users/ajbrock/gists{/gist_id}","starred_url":"https://api.github.com/users/ajbrock/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajbrock/subscriptions","organizations_url":"https://api.github.com/users/ajbrock/orgs","repos_url":"https://api.github.com/users/ajbrock/repos","events_url":"https://api.github.com/users/ajbrock/events{/privacy}","received_events_url":"https://api.github.com/users/ajbrock/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/88/timeline","performed_via_github_app":null,"state_reason":"completed"}