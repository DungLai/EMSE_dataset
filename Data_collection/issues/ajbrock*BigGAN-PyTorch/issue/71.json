{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71","repository_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch","labels_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71/labels{/name}","comments_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71/comments","events_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71/events","html_url":"https://github.com/ajbrock/BigGAN-PyTorch/issues/71","id":704525752,"node_id":"MDU6SXNzdWU3MDQ1MjU3NTI=","number":71,"title":"ValueError: __len__() should return >= 0","user":{"login":"JanineCHEN","id":37527041,"node_id":"MDQ6VXNlcjM3NTI3MDQx","avatar_url":"https://avatars.githubusercontent.com/u/37527041?v=4","gravatar_id":"","url":"https://api.github.com/users/JanineCHEN","html_url":"https://github.com/JanineCHEN","followers_url":"https://api.github.com/users/JanineCHEN/followers","following_url":"https://api.github.com/users/JanineCHEN/following{/other_user}","gists_url":"https://api.github.com/users/JanineCHEN/gists{/gist_id}","starred_url":"https://api.github.com/users/JanineCHEN/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JanineCHEN/subscriptions","organizations_url":"https://api.github.com/users/JanineCHEN/orgs","repos_url":"https://api.github.com/users/JanineCHEN/repos","events_url":"https://api.github.com/users/JanineCHEN/events{/privacy}","received_events_url":"https://api.github.com/users/JanineCHEN/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-09-18T17:01:25Z","updated_at":"2021-12-19T05:53:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Great thanks for the open-sourced models. I have encountered the following issue when trying to resume the training process. I trained the model using my own dataset, the previous training and checkpoints saving were successful without any prompted error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 227, in <module>\r\n    main()\r\n  File \"train.py\", line 224, in main\r\n    run(config)\r\n  File \"train.py\", line 171, in run\r\n    for i, (x, y) in enumerate(pbar):\r\n  File \"/home/projects/11002043/BIGGAN_archdaily_outdoor_128_bs110x237/utils.py\", line 834, in progress\r\n    total = total or len(items)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 316, in __len__\r\n    return len(self._index_sampler)\r\n  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/sampler.py\", line 212, in __len__\r\n    return (len(self.sampler) + self.batch_size - 1) // self.batch_size\r\nValueError: __len__() should return >= 0\r\n```\r\nThe configurations are as following:\r\n```\r\n$ sh scripts/launch_BigGAN_bs110x237.sh\r\n{'dataset': 'I128_hdf5', 'augment': False, 'num_workers': 4, 'pin_memory': True, 'shuffle': True, 'load_in_mem': True, 'use_multiepoch_sampler': True, 'model': 'BigGAN', 'G_param': 'SN', 'D_param': 'SN', 'G_ch': 32, 'D_ch': 32, 'G_depth': 1, 'D_depth': 1, 'D_wide': True, 'G_shared': True, 'shared_dim': 128, 'dim_z': 120, 'z_var': 1.0, 'hier': True, 'cross_replica': False, 'mybn': False, 'G_nl': 'inplace_relu', 'D_nl': 'inplace_relu', 'G_attn': '32', 'D_attn': '32', 'norm_style': 'bn', 'seed': 0, 'G_init': 'ortho', 'D_init': 'ortho', 'skip_init': False, 'G_lr': 0.0001, 'D_lr': 0.0004, 'G_B1': 0.0, 'D_B1': 0.0, 'G_B2': 0.999, 'D_B2': 0.999, 'batch_size': 110, 'G_batch_size': 0, 'num_G_accumulations': 237, 'num_D_steps': 1, 'num_D_accumulations': 237, 'split_D': False, 'num_epochs': 100, 'parallel': True, 'G_fp16': False, 'D_fp16': False, 'D_mixed_precision': False, 'G_mixed_precision': False, 'accumulate_stats': False, 'num_standing_accumulations': 16, 'G_eval_mode': True, 'save_every': 100, 'num_save_copies': 2, 'num_best_copies': 5, 'which_best': 'FID', 'no_fid': False, 'test_every': 100, 'num_inception_images': 50000, 'hashname': False, 'base_root': '', 'data_root': 'data', 'weights_root': 'weights', 'logs_root': 'logs', 'samples_root': 'samples', 'pbar': 'mine', 'name_suffix': '', 'experiment_name': 'BigGAN_I128_hdf5_seed0_Gch32_Dch32_bs110_nDa237_nGa237_Glr1.0e-04_Dlr4.0e-04_Gnlinplace_relu_Dnlinplace_relu_Ginitortho_Dinitortho_Gattn32_Dattn32_Gshared_hier_ema', 'config_from_name': False, 'ema': True, 'ema_decay': 0.9999, 'use_ema': True, 'ema_start': 300, 'adam_eps': 1e-06, 'BN_eps': 1e-05, 'SN_eps': 1e-06, 'num_G_SVs': 1, 'num_D_SVs': 1, 'num_G_SV_itrs': 1, 'num_D_SV_itrs': 1, 'G_ortho': 0.0, 'D_ortho': 0.0, 'toggle_grads': True, 'which_train_fn': 'GAN', 'load_weights': '', 'resume': True, 'logstyle': '%3.3e', 'log_G_spectra': False, 'log_D_spectra': False, 'sv_log_interval': 10}\r\nSkipping initialization for training resumption...\r\nExperiment name is BigGAN_I128_hdf5_seed0_Gch32_Dch32_bs110_nDa237_nGa237_Glr1.0e-04_Dlr4.0e-04_Gnlinplace_relu_Dnlinplace_relu_Ginitortho_Dinitortho_Gattn32_Dattn32_Gshared_hier_ema\r\nAdding attention layer in G at resolution 32\r\nAdding attention layer in D at resolution 32\r\nPreparing EMA for G with decay of 0.9999\r\nAdding attention layer in G at resolution 32\r\nInitializing EMA parameters to be source parameters...\r\nGenerator(\r\n  (activation): ReLU(inplace=True)\r\n  (shared): Embedding(160, 128)\r\n  (linear): SNLinear(in_features=20, out_features=8192, bias=True)\r\n  (blocks): ModuleList(\r\n    (0): ModuleList(\r\n      (0): GBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\r\n        (bn1): ccbn(\r\n          out: 512, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=512, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=512, bias=False)\r\n        )\r\n        (bn2): ccbn(\r\n          out: 512, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=512, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=512, bias=False)\r\n        )\r\n      )\r\n    )\r\n    (1): ModuleList(\r\n      (0): GBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\r\n        (bn1): ccbn(\r\n          out: 512, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=512, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=512, bias=False)\r\n        )\r\n        (bn2): ccbn(\r\n          out: 256, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=256, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=256, bias=False)\r\n        )\r\n      )\r\n    )\r\n    (2): ModuleList(\r\n      (0): GBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\r\n        (bn1): ccbn(\r\n          out: 256, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=256, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=256, bias=False)\r\n        )\r\n        (bn2): ccbn(\r\n          out: 128, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=128, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=128, bias=False)\r\n        )\r\n      )\r\n      (1): Attention(\r\n        (theta): SNConv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (phi): SNConv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (g): SNConv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (o): SNConv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      )\r\n    )\r\n    (3): ModuleList(\r\n      (0): GBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\r\n        (bn1): ccbn(\r\n          out: 128, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=128, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=128, bias=False)\r\n        )\r\n        (bn2): ccbn(\r\n          out: 64, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=64, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=64, bias=False)\r\n        )\r\n      )\r\n    )\r\n    (4): ModuleList(\r\n      (0): GBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\r\n        (bn1): ccbn(\r\n          out: 64, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=64, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=64, bias=False)\r\n        )\r\n        (bn2): ccbn(\r\n          out: 32, in: 148, cross_replica=False\r\n          (gain): SNLinear(in_features=148, out_features=32, bias=False)\r\n          (bias): SNLinear(in_features=148, out_features=32, bias=False)\r\n        )\r\n      )\r\n    )\r\n  )\r\n  (output_layer): Sequential(\r\n    (0): bn()\r\n    (1): ReLU(inplace=True)\r\n    (2): SNConv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n  )\r\n)\r\nDiscriminator(\r\n  (activation): ReLU(inplace=True)\r\n  (blocks): ModuleList(\r\n    (0): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n        (conv1): SNConv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\r\n      )\r\n    )\r\n    (1): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n        (conv1): SNConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\r\n      )\r\n      (1): Attention(\r\n        (theta): SNConv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (phi): SNConv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (g): SNConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n        (o): SNConv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\r\n      )\r\n    )\r\n    (2): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n        (conv1): SNConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\r\n      )\r\n    )\r\n    (3): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n        (conv1): SNConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\r\n      )\r\n    )\r\n    (4): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (downsample): AvgPool2d(kernel_size=2, stride=2, padding=0)\r\n        (conv1): SNConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv_sc): SNConv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\r\n      )\r\n    )\r\n    (5): ModuleList(\r\n      (0): DBlock(\r\n        (activation): ReLU(inplace=True)\r\n        (conv1): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n        (conv2): SNConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\r\n      )\r\n    )\r\n  )\r\n  (linear): SNLinear(in_features=512, out_features=1, bias=True)\r\n  (embed): SNEmbedding(160, 512)\r\n)\r\nNumber of params in G: 8451140 D: 9694562\r\nLoading weights...\r\nLoading weights from weights/BigGAN_I128_hdf5_seed0_Gch32_Dch32_bs110_nDa237_nGa237_Glr1.0e-04_Dlr4.0e-04_Gnlinplace_relu_Dnlinplace_relu_Ginitortho_Dinitortho_Gattn32_Dattn32_Gshared_hier_ema...\r\nInception Metrics will be saved to logs/BigGAN_I128_hdf5_seed0_Gch32_Dch32_bs110_nDa237_nGa237_Glr1.0e-04_Dlr4.0e-04_Gnlinplace_relu_Dnlinplace_relu_Ginitortho_Dinitortho_Gattn32_Dattn32_Gshared_hier_ema_log.jsonl\r\nTraining Metrics will be saved to logs/BigGAN_I128_hdf5_seed0_Gch32_Dch32_bs110_nDa237_nGa237_Glr1.0e-04_Dlr4.0e-04_Gnlinplace_relu_Dnlinplace_relu_Ginitortho_Dinitortho_Gattn32_Dattn32_Gshared_hier_ema\r\nUsing dataset root location data/ILSVRC128.hdf5\r\nLoading data/ILSVRC128.hdf5 into memory...\r\nUsing multiepoch sampler from start_itr 200...\r\nParallelizing Inception module...\r\nBeginning training at epoch 1...\r\n```\r\nAny idea why this error takes place? Any help will be highly appreciated!\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/71/timeline","performed_via_github_app":null,"state_reason":null}