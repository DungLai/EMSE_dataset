{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17","repository_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch","labels_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17/labels{/name}","comments_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17/comments","events_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17/events","html_url":"https://github.com/ajbrock/BigGAN-PyTorch/issues/17","id":430243439,"node_id":"MDU6SXNzdWU0MzAyNDM0Mzk=","number":17,"title":"Any difference between Attention and SAGAN?","user":{"login":"jayzhan211","id":15869383,"node_id":"MDQ6VXNlcjE1ODY5Mzgz","avatar_url":"https://avatars.githubusercontent.com/u/15869383?v=4","gravatar_id":"","url":"https://api.github.com/users/jayzhan211","html_url":"https://github.com/jayzhan211","followers_url":"https://api.github.com/users/jayzhan211/followers","following_url":"https://api.github.com/users/jayzhan211/following{/other_user}","gists_url":"https://api.github.com/users/jayzhan211/gists{/gist_id}","starred_url":"https://api.github.com/users/jayzhan211/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jayzhan211/subscriptions","organizations_url":"https://api.github.com/users/jayzhan211/orgs","repos_url":"https://api.github.com/users/jayzhan211/repos","events_url":"https://api.github.com/users/jayzhan211/events{/privacy}","received_events_url":"https://api.github.com/users/jayzhan211/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-04-08T03:51:48Z","updated_at":"2019-04-08T08:32:09Z","closed_at":"2019-04-08T08:32:09Z","author_association":"NONE","active_lock_reason":null,"body":"[https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py](https://github.com/heykeetae/Self-Attention-GAN/blob/master/sagan_models.py)\r\nIn this code\r\n```py\r\nclass Self_Attn(nn.Module):\r\n    \"\"\" Self attention Layer\"\"\"\r\n    def __init__(self,in_dim,activation):\r\n        super(Self_Attn,self).__init__()\r\n        self.chanel_in = in_dim\r\n        self.activation = activation\r\n        \r\n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\r\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\r\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\r\n        self.gamma = nn.Parameter(torch.zeros(1))\r\n\r\n        self.softmax  = nn.Softmax(dim=-1) #\r\n    def forward(self,x):\r\n        \"\"\"\r\n            inputs :\r\n                x : input feature maps( B X C X W X H)\r\n            returns :\r\n                out : self attention value + input feature \r\n                attention: B X N X N (N is Width*Height)\r\n        \"\"\"\r\n        m_batchsize,C,width ,height = x.size()\r\n        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X CX(N)\r\n        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\r\n        energy =  torch.bmm(proj_query,proj_key) # transpose check\r\n        attention = self.softmax(energy) # BX (N) X (N) \r\n        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\r\n\r\n        out = torch.bmm(proj_value,attention.permute(0,2,1) )\r\n        out = out.view(m_batchsize,C,width,height)\r\n        \r\n        out = self.gamma*out + x\r\n        return out,attention\r\n```\r\n```\r\n# A non-local block as used in SA-GAN\r\n# Note that the implementation as described in the paper is largely incorrect;\r\n# refer to the released code for the actual implementation.\r\n```\r\nwhich one do you mean is largely incorrect and actual implementation? Thanks","closed_by":{"login":"ajbrock","id":7751273,"node_id":"MDQ6VXNlcjc3NTEyNzM=","avatar_url":"https://avatars.githubusercontent.com/u/7751273?v=4","gravatar_id":"","url":"https://api.github.com/users/ajbrock","html_url":"https://github.com/ajbrock","followers_url":"https://api.github.com/users/ajbrock/followers","following_url":"https://api.github.com/users/ajbrock/following{/other_user}","gists_url":"https://api.github.com/users/ajbrock/gists{/gist_id}","starred_url":"https://api.github.com/users/ajbrock/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajbrock/subscriptions","organizations_url":"https://api.github.com/users/ajbrock/orgs","repos_url":"https://api.github.com/users/ajbrock/repos","events_url":"https://api.github.com/users/ajbrock/events{/privacy}","received_events_url":"https://api.github.com/users/ajbrock/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ajbrock/BigGAN-PyTorch/issues/17/timeline","performed_via_github_app":null,"state_reason":"completed"}