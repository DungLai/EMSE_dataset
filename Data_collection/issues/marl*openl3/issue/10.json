{"url":"https://api.github.com/repos/marl/openl3/issues/10","repository_url":"https://api.github.com/repos/marl/openl3","labels_url":"https://api.github.com/repos/marl/openl3/issues/10/labels{/name}","comments_url":"https://api.github.com/repos/marl/openl3/issues/10/comments","events_url":"https://api.github.com/repos/marl/openl3/issues/10/events","html_url":"https://github.com/marl/openl3/issues/10","id":376549173,"node_id":"MDU6SXNzdWUzNzY1NDkxNzM=","number":10,"title":"Supporting multiple GPU models","user":{"login":"auroracramer","id":1388668,"node_id":"MDQ6VXNlcjEzODg2Njg=","avatar_url":"https://avatars.githubusercontent.com/u/1388668?v=4","gravatar_id":"","url":"https://api.github.com/users/auroracramer","html_url":"https://github.com/auroracramer","followers_url":"https://api.github.com/users/auroracramer/followers","following_url":"https://api.github.com/users/auroracramer/following{/other_user}","gists_url":"https://api.github.com/users/auroracramer/gists{/gist_id}","starred_url":"https://api.github.com/users/auroracramer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/auroracramer/subscriptions","organizations_url":"https://api.github.com/users/auroracramer/orgs","repos_url":"https://api.github.com/users/auroracramer/repos","events_url":"https://api.github.com/users/auroracramer/events{/privacy}","received_events_url":"https://api.github.com/users/auroracramer/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2018-11-01T20:04:50Z","updated_at":"2018-11-01T22:34:09Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Should supporting running the embedding models on multiple-GPUs be prioritized? Here are the pros/cons as I see it (not necessarily equally weighted in terms of importance):\r\n\r\n## Pros\r\n- Allows users to take advantage of multiple GPUs for faster running time\r\n\r\n## Cons\r\n- Adds an extra parameter to most API calls, though this can be optional\r\n- Adds meat to the codebase (though we already have it)\r\n- Can we test this on Travis?\r\n\r\nAll in all, I think that if we believe that using multiple GPUs will be a common use case, then we should include it. But if it's something that will be rarely used, if at all, we shouldn't prioritize it (at least for an MVP).\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/marl/openl3/issues/10/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/marl/openl3/issues/10/timeline","performed_via_github_app":null,"state_reason":null}