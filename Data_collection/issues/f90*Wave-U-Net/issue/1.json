{"url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1","repository_url":"https://api.github.com/repos/f90/Wave-U-Net","labels_url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1/labels{/name}","comments_url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1/comments","events_url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1/events","html_url":"https://github.com/f90/Wave-U-Net/issues/1","id":341731205,"node_id":"MDU6SXNzdWUzNDE3MzEyMDU=","number":1,"title":"Training halts after the first epoch","user":{"login":"leoybkim","id":21056900,"node_id":"MDQ6VXNlcjIxMDU2OTAw","avatar_url":"https://avatars.githubusercontent.com/u/21056900?v=4","gravatar_id":"","url":"https://api.github.com/users/leoybkim","html_url":"https://github.com/leoybkim","followers_url":"https://api.github.com/users/leoybkim/followers","following_url":"https://api.github.com/users/leoybkim/following{/other_user}","gists_url":"https://api.github.com/users/leoybkim/gists{/gist_id}","starred_url":"https://api.github.com/users/leoybkim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leoybkim/subscriptions","organizations_url":"https://api.github.com/users/leoybkim/orgs","repos_url":"https://api.github.com/users/leoybkim/repos","events_url":"https://api.github.com/users/leoybkim/events{/privacy}","received_events_url":"https://api.github.com/users/leoybkim/received_events","type":"User","site_admin":false},"labels":[{"id":912642574,"node_id":"MDU6TGFiZWw5MTI2NDI1NzQ=","url":"https://api.github.com/repos/f90/Wave-U-Net/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2018-07-17T01:17:36Z","updated_at":"2018-11-16T14:21:24Z","closed_at":"2018-08-27T13:36:33Z","author_association":"NONE","active_lock_reason":null,"body":"Hello,\r\n\r\nI tried to train your model with `full_multi_instrument` mode using 4 GPUs (NVIDIA Tesla P100) and with the same datasets (musdb). It took 2 hours to finish the first epoch followed by a very long hanging with no progress. \r\n\r\nHere is the stack trace after stopping the script manually\r\n```\r\n\r\n\r\n2018-07-16 16:21:31.186201: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_25/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186236: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_25/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186247: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_25/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186262: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_25/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186272: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_26/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186285: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_26/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186297: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_26/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186309: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_26/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186322: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_27/kernel/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186334: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_27/kernel/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186347: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_27/bias/Adam/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186359: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/separator/conv1d_27/bias/Adam_1/Initializer/zeros: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186384: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/Adam/beta1: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186406: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/Adam/beta2: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186427: I tensorflow/core/common_runtime/placer.cc:886] separator_solver/Adam/epsilon: (Const)/job:localhost/replica:0/task:0/device:GPU:0\r\n2018-07-16 16:21:31.186440: I tensorflow/core/common_runtime/placer.cc:886] sep_loss/tags: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186451: I tensorflow/core/common_runtime/placer.cc:886] save/Const: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186464: I tensorflow/core/common_runtime/placer.cc:886] save/SaveV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186477: I tensorflow/core/common_runtime/placer.cc:886] save/SaveV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186490: I tensorflow/core/common_runtime/placer.cc:886] save/RestoreV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186503: I tensorflow/core/common_runtime/placer.cc:886] save/RestoreV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186516: I tensorflow/core/common_runtime/placer.cc:886] save_1/Const: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186528: I tensorflow/core/common_runtime/placer.cc:886] save_1/SaveV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186542: I tensorflow/core/common_runtime/placer.cc:886] save_1/SaveV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186554: I tensorflow/core/common_runtime/placer.cc:886] save_1/RestoreV2/tensor_names: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n2018-07-16 16:21:31.186567: I tensorflow/core/common_runtime/placer.cc:886] save_1/RestoreV2/shape_and_slices: (Const)/job:localhost/replica:0/task:0/device:CPU:0\r\n\r\n\r\n^CWARNING - Waveunet - Aborted after 6:05:45!\r\nTraceback (most recent call last):\r\n  File \"Training.py\", line 326, in <module>\r\n    @ex.automain\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/experiment.py\", line 137, in automain\r\n    self.run_commandline()\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/experiment.py\", line 260, in run_commandline\r\n    return self.run(cmd_name, config_updates, named_configs, {}, args)\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/experiment.py\", line 209, in run\r\n    run()\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/run.py\", line 221, in __call__\r\n    self.result = self.main_function(*args)\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/config/captured_function.py\", line 46, in captured_function\r\n    result = wrapped(*args, **kwargs)\r\n  File \"Training.py\", line 373, in dsd_100_experiment\r\n    sup_model_path, sup_loss = optimise(dataset=dataset)\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/config/captured_function.py\", line 46, in captured_function\r\n    result = wrapped(*args, **kwargs)\r\n  File \"Training.py\", line 311, in optimise\r\n    model_path = train(sup_dataset=dataset[\"train_sup\"], load_model=model_path)\r\n  File \"/home/leo/.local/lib/python2.7/site-packages/sacred/config/captured_function.py\", line 46, in captured_function\r\n    result = wrapped(*args, **kwargs)\r\n  File \"Training.py\", line 269, in train\r\n    sup_batch = sup_batch_gen.get_batch()\r\n  File \"/home/leo/Wave-U-Net/Input/batchgenerators.py\", line 123, in get_batch\r\n    self.cache.update_cache_from_queue()\r\n  File \"/home/leo/Wave-U-Net/Input/multistreamcache.py\", line 84, in update_cache_from_queue\r\n    self.update_next_cache_item(self.communication_queue.get())\r\n  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 117, in get\r\n    res = self._recv()\r\nKeyboardInterrupt\r\n^C\r\n\r\n```\r\n\r\nThe full stack trace can be found here [stack trace gist] (https://gist.github.com/leoybkim/789d367a0ee2c63db8a513613270b017)\r\n\r\nIt seems like something failed while fetching the next batch from queue. I also found your TODO comment on `update_cache_from_queue()` func about empty queues in \r\n[multistreamcache.py](https://github.com/f90/Wave-U-Net/blob/c3b9492267d2da3e16cbc3e45b5ceed034487415/Input/multistreamcache.py#L64-L88). I wonder if it has any relation to this. ","closed_by":{"login":"f90","id":23065103,"node_id":"MDQ6VXNlcjIzMDY1MTAz","avatar_url":"https://avatars.githubusercontent.com/u/23065103?v=4","gravatar_id":"","url":"https://api.github.com/users/f90","html_url":"https://github.com/f90","followers_url":"https://api.github.com/users/f90/followers","following_url":"https://api.github.com/users/f90/following{/other_user}","gists_url":"https://api.github.com/users/f90/gists{/gist_id}","starred_url":"https://api.github.com/users/f90/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/f90/subscriptions","organizations_url":"https://api.github.com/users/f90/orgs","repos_url":"https://api.github.com/users/f90/repos","events_url":"https://api.github.com/users/f90/events{/privacy}","received_events_url":"https://api.github.com/users/f90/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/f90/Wave-U-Net/issues/1/timeline","performed_via_github_app":null,"state_reason":"completed"}