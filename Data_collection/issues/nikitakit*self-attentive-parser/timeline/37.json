[{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/518928052","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/37#issuecomment-518928052","issue_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/37","id":518928052,"node_id":"MDEyOklzc3VlQ29tbWVudDUxODkyODA1Mg==","user":{"login":"nikitakit","id":252225,"node_id":"MDQ6VXNlcjI1MjIyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/252225?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitakit","html_url":"https://github.com/nikitakit","followers_url":"https://api.github.com/users/nikitakit/followers","following_url":"https://api.github.com/users/nikitakit/following{/other_user}","gists_url":"https://api.github.com/users/nikitakit/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitakit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitakit/subscriptions","organizations_url":"https://api.github.com/users/nikitakit/orgs","repos_url":"https://api.github.com/users/nikitakit/repos","events_url":"https://api.github.com/users/nikitakit/events{/privacy}","received_events_url":"https://api.github.com/users/nikitakit/received_events","type":"User","site_admin":false},"created_at":"2019-08-07T03:47:18Z","updated_at":"2019-08-07T03:47:18Z","author_association":"OWNER","body":"The 300-word limit is inherent to the pre-trained model; it can't be changed without modifying the model. There are multiple places in the model architecture that place a limit on the maximum sentence length, each for a different reason. Overall I'd say that removing the length limit is not straightforward and would require re-thinking several aspects of the overall parser architecture.","reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/518928052/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"nikitakit","id":252225,"node_id":"MDQ6VXNlcjI1MjIyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/252225?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitakit","html_url":"https://github.com/nikitakit","followers_url":"https://api.github.com/users/nikitakit/followers","following_url":"https://api.github.com/users/nikitakit/following{/other_user}","gists_url":"https://api.github.com/users/nikitakit/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitakit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitakit/subscriptions","organizations_url":"https://api.github.com/users/nikitakit/orgs","repos_url":"https://api.github.com/users/nikitakit/repos","events_url":"https://api.github.com/users/nikitakit/events{/privacy}","received_events_url":"https://api.github.com/users/nikitakit/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/518930118","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/37#issuecomment-518930118","issue_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/37","id":518930118,"node_id":"MDEyOklzc3VlQ29tbWVudDUxODkzMDExOA==","user":{"login":"FengXuas","id":33900935,"node_id":"MDQ6VXNlcjMzOTAwOTM1","avatar_url":"https://avatars.githubusercontent.com/u/33900935?v=4","gravatar_id":"","url":"https://api.github.com/users/FengXuas","html_url":"https://github.com/FengXuas","followers_url":"https://api.github.com/users/FengXuas/followers","following_url":"https://api.github.com/users/FengXuas/following{/other_user}","gists_url":"https://api.github.com/users/FengXuas/gists{/gist_id}","starred_url":"https://api.github.com/users/FengXuas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/FengXuas/subscriptions","organizations_url":"https://api.github.com/users/FengXuas/orgs","repos_url":"https://api.github.com/users/FengXuas/repos","events_url":"https://api.github.com/users/FengXuas/events{/privacy}","received_events_url":"https://api.github.com/users/FengXuas/received_events","type":"User","site_admin":false},"created_at":"2019-08-07T04:00:12Z","updated_at":"2019-08-07T04:00:12Z","author_association":"NONE","body":"I am using the benepar_en2 model. When the sentence length limit is removed during the training model phase, is it possible to generate a model with the same effect as benepar_en2. The method of training the model follows the methods provided in your github","reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/518930118/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"FengXuas","id":33900935,"node_id":"MDQ6VXNlcjMzOTAwOTM1","avatar_url":"https://avatars.githubusercontent.com/u/33900935?v=4","gravatar_id":"","url":"https://api.github.com/users/FengXuas","html_url":"https://github.com/FengXuas","followers_url":"https://api.github.com/users/FengXuas/followers","following_url":"https://api.github.com/users/FengXuas/following{/other_user}","gists_url":"https://api.github.com/users/FengXuas/gists{/gist_id}","starred_url":"https://api.github.com/users/FengXuas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/FengXuas/subscriptions","organizations_url":"https://api.github.com/users/FengXuas/orgs","repos_url":"https://api.github.com/users/FengXuas/repos","events_url":"https://api.github.com/users/FengXuas/events{/privacy}","received_events_url":"https://api.github.com/users/FengXuas/received_events","type":"User","site_admin":false}},{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/520074142","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/37#issuecomment-520074142","issue_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/37","id":520074142,"node_id":"MDEyOklzc3VlQ29tbWVudDUyMDA3NDE0Mg==","user":{"login":"nikitakit","id":252225,"node_id":"MDQ6VXNlcjI1MjIyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/252225?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitakit","html_url":"https://github.com/nikitakit","followers_url":"https://api.github.com/users/nikitakit/followers","following_url":"https://api.github.com/users/nikitakit/following{/other_user}","gists_url":"https://api.github.com/users/nikitakit/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitakit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitakit/subscriptions","organizations_url":"https://api.github.com/users/nikitakit/orgs","repos_url":"https://api.github.com/users/nikitakit/repos","events_url":"https://api.github.com/users/nikitakit/events{/privacy}","received_events_url":"https://api.github.com/users/nikitakit/received_events","type":"User","site_admin":false},"created_at":"2019-08-09T21:45:43Z","updated_at":"2019-08-09T21:45:43Z","author_association":"OWNER","body":"BERT has an inherent length limit of 512 sub-word tokens, so you can only raise the limit from 300 words to 512 sub-words before you hit a limit that requires re-thinking the overall architecture.\r\n\r\nIf the 512 sub-word limit is fine, you can load the tensorflow graph for `benepar_en2`, find the `300xN` embedding matrix for positions, and add another 212 entries to it that are populated with random values. This would be far easier than re-training the model, and have the same effect. There are no sentences longer than 300 words long in the training data, so those position embeddings wouldn't be trained anyway even if you re-ran the training code from scratch. This relates to another limitation of the current modeling approach: when parsing extra-long sentences, you will be using randomly-initialized parameters that have never been touched during training (and hoping that the parser works anyway).","reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/comments/520074142/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"nikitakit","id":252225,"node_id":"MDQ6VXNlcjI1MjIyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/252225?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitakit","html_url":"https://github.com/nikitakit","followers_url":"https://api.github.com/users/nikitakit/followers","following_url":"https://api.github.com/users/nikitakit/following{/other_user}","gists_url":"https://api.github.com/users/nikitakit/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitakit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitakit/subscriptions","organizations_url":"https://api.github.com/users/nikitakit/orgs","repos_url":"https://api.github.com/users/nikitakit/repos","events_url":"https://api.github.com/users/nikitakit/events{/privacy}","received_events_url":"https://api.github.com/users/nikitakit/received_events","type":"User","site_admin":false}}]