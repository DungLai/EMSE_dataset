{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76","repository_url":"https://api.github.com/repos/nikitakit/self-attentive-parser","labels_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76/labels{/name}","comments_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76/comments","events_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76/events","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/76","id":836968159,"node_id":"MDU6SXNzdWU4MzY5NjgxNTk=","number":76,"title":"Fails with CUBLAS_STATUS_INTERNAL_ERROR on linux, benepar 0.2.0, spacy 3.0","user":{"login":"yanvirin","id":7925833,"node_id":"MDQ6VXNlcjc5MjU4MzM=","avatar_url":"https://avatars.githubusercontent.com/u/7925833?v=4","gravatar_id":"","url":"https://api.github.com/users/yanvirin","html_url":"https://github.com/yanvirin","followers_url":"https://api.github.com/users/yanvirin/followers","following_url":"https://api.github.com/users/yanvirin/following{/other_user}","gists_url":"https://api.github.com/users/yanvirin/gists{/gist_id}","starred_url":"https://api.github.com/users/yanvirin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanvirin/subscriptions","organizations_url":"https://api.github.com/users/yanvirin/orgs","repos_url":"https://api.github.com/users/yanvirin/repos","events_url":"https://api.github.com/users/yanvirin/events{/privacy}","received_events_url":"https://api.github.com/users/yanvirin/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-21T02:41:26Z","updated_at":"2021-03-29T19:47:31Z","closed_at":"2021-03-29T19:47:31Z","author_association":"NONE","active_lock_reason":null,"body":"I am trying to run the code from the README which creates a simple doc instance with nlp object from spacy and I get the following error:\r\nRuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`\r\n\r\n- Interestingly enough, the older version benepar 0.1.3, spacy 2.3 works fine with the GPU (with tensorflow).\r\n- Other Torch based models run fine on my computer.\r\n- Running with CUDA_VISIBLE_DEVICES=\"\" makes it work, with no exception, so the problem is related to the GPU.\r\n- I am wondering if this can be something like this: https://discuss.pytorch.org/t/cuda-error-cublas-status-internal-error-probably-related-to-memory/96167/2 (where only a portion of the model is committed to cuda?)\r\n\r\nAny idea why this might be happening with this library in particular?\r\n\r\n---------------------------------------------------------------------------------------------------------------------------\r\noutput of nvidia-smi while the model is loaded:\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 460.56       Driver Version: 460.56       CUDA Version: 11.2     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce RTX 207...  Off  | 00000000:01:00.0 Off |                  N/A |\r\n| N/A   34C    P5     9W /  N/A |   1309MiB /  7982MiB |      0%      Default |\r\n|                               |                      |                  N/A |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                                  |\r\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n|        ID   ID                                                   Usage      |\r\n|=============================================================================|\r\n|    0   N/A  N/A      2786      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A     11405      C   python                           1301MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\nFull stacktrace:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/spacy/language.py\", line 995, in __call__\r\n    error_handler(name, proc, [doc], e)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/spacy/util.py\", line 1498, in raise_error\r\n    raise e\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/spacy/language.py\", line 990, in __call__\r\n    doc = proc(doc, **component_cfg.get(name, {}))\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/benepar/integrations/spacy_plugin.py\", line 151, in __call__\r\n    self._parser.parse(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/benepar/parse_chart.py\", line 416, in parse\r\n    res = subbatching.map(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/benepar/subbatching.py\", line 60, in map\r\n    for item_id, item_out in zip(item_ids, subbatch_out):\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/benepar/parse_chart.py\", line 366, in _parse_encoded\r\n    span_scores, tag_scores = self.forward(batch)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/benepar/parse_chart.py\", line 284, in forward\r\n    pretrained_out = self.pretrained_model(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 1304, in forward\r\n    encoder_outputs = self.encoder(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 951, in forward\r\n    layer_outputs = layer_module(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 633, in forward\r\n    self_attention_outputs = self.layer[0](\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 540, in forward\r\n    attention_output = self.SelfAttention(\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 468, in forward\r\n    query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/modules/linear.py\", line 94, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File \"/home/yanvirin/code/benepar/env/lib/python3.8/site-packages/torch/nn/functional.py\", line 1753, in linear\r\n    return torch._C._nn.linear(input, weight, bias)\r\nRuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`\r\n","closed_by":{"login":"yanvirin","id":7925833,"node_id":"MDQ6VXNlcjc5MjU4MzM=","avatar_url":"https://avatars.githubusercontent.com/u/7925833?v=4","gravatar_id":"","url":"https://api.github.com/users/yanvirin","html_url":"https://github.com/yanvirin","followers_url":"https://api.github.com/users/yanvirin/followers","following_url":"https://api.github.com/users/yanvirin/following{/other_user}","gists_url":"https://api.github.com/users/yanvirin/gists{/gist_id}","starred_url":"https://api.github.com/users/yanvirin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanvirin/subscriptions","organizations_url":"https://api.github.com/users/yanvirin/orgs","repos_url":"https://api.github.com/users/yanvirin/repos","events_url":"https://api.github.com/users/yanvirin/events{/privacy}","received_events_url":"https://api.github.com/users/yanvirin/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/76/timeline","performed_via_github_app":null,"state_reason":"completed"}