{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71","repository_url":"https://api.github.com/repos/nikitakit/self-attentive-parser","labels_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71/labels{/name}","comments_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71/comments","events_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71/events","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/71","id":781978328,"node_id":"MDU6SXNzdWU3ODE5NzgzMjg=","number":71,"title":"Using huggingface/transformers as BERT pre-trained language model ?","user":{"login":"HiiamCong","id":29277146,"node_id":"MDQ6VXNlcjI5Mjc3MTQ2","avatar_url":"https://avatars.githubusercontent.com/u/29277146?v=4","gravatar_id":"","url":"https://api.github.com/users/HiiamCong","html_url":"https://github.com/HiiamCong","followers_url":"https://api.github.com/users/HiiamCong/followers","following_url":"https://api.github.com/users/HiiamCong/following{/other_user}","gists_url":"https://api.github.com/users/HiiamCong/gists{/gist_id}","starred_url":"https://api.github.com/users/HiiamCong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HiiamCong/subscriptions","organizations_url":"https://api.github.com/users/HiiamCong/orgs","repos_url":"https://api.github.com/users/HiiamCong/repos","events_url":"https://api.github.com/users/HiiamCong/events{/privacy}","received_events_url":"https://api.github.com/users/HiiamCong/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-01-08T09:28:12Z","updated_at":"2021-01-08T09:29:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Dear authors,\r\n\r\nI currently want to use [(PhoBERT: Pre-trained language models for Vietnamese)](https://github.com/VinAIResearch/PhoBERT) as a BERT pre-trained language model for this repo. PhoBERT was built and deploy in huggingface/transformers library [(huggingface/transformers)](https://github.com/huggingface/transformers).\r\nAs I know, self-attentive-parser is using `pytorch_pretrained_bert` for getting BERT Model. I have tried to change the code of function `get_bert` in `parse_nk.py` to use PhoBERT:\r\n```python\r\ndef get_bert(bert_model, bert_do_lower_case):\r\n    from transformers import AutoModel, AutoTokenizer\r\n    phobert = AutoModel.from_pretrained(bert_model)\r\n    tokenizer = AutoTokenizer.from_pretrained(bert_model, do_lower_case=bert_do_lower_case)\r\n    return tokenizer, phobert\r\n```\r\nBut get this error:\r\n```sh\r\nTraceback (most recent call last):\r\n  File \"src/main.py\", line 612, in <module>\r\n    main()\r\n  File \"src/main.py\", line 608, in main\r\n    args.callback(args)\r\n  File \"src/main.py\", line 564, in <lambda>\r\n    subparser.set_defaults(callback=lambda args: run_train(args, hparams))\r\n  File \"src/main.py\", line 312, in run_train\r\n    _, loss = parser.parse_batch(subbatch_sentences, subbatch_trees)\r\n  File \"/home/kynh/codes/self-attentive-parser/src/parse_nk.py\", line 1026, in parse_batch\r\n    features_packed = features.masked_select(all_word_end_mask.to(torch.bool).unsqueeze(-1)).reshape(-1, features.shape[-1])\r\nAttributeError: 'str' object has no attribute 'masked_select'\r\n```\r\nI read a [paper](https://arxiv.org/pdf/2010.09623.pdf) using PhoBert for training in this repo so I am pretty sure this can be done, but do not know how to do it.\r\nAny solution? thanks!","closed_by":null,"reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/71/timeline","performed_via_github_app":null,"state_reason":null}