{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25","repository_url":"https://api.github.com/repos/nikitakit/self-attentive-parser","labels_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25/labels{/name}","comments_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25/comments","events_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25/events","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/25","id":446670219,"node_id":"MDU6SXNzdWU0NDY2NzAyMTk=","number":25,"title":"RuntimeError when trying to train a new model","user":{"login":"nitedl","id":50877445,"node_id":"MDQ6VXNlcjUwODc3NDQ1","avatar_url":"https://avatars.githubusercontent.com/u/50877445?v=4","gravatar_id":"","url":"https://api.github.com/users/nitedl","html_url":"https://github.com/nitedl","followers_url":"https://api.github.com/users/nitedl/followers","following_url":"https://api.github.com/users/nitedl/following{/other_user}","gists_url":"https://api.github.com/users/nitedl/gists{/gist_id}","starred_url":"https://api.github.com/users/nitedl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nitedl/subscriptions","organizations_url":"https://api.github.com/users/nitedl/orgs","repos_url":"https://api.github.com/users/nitedl/repos","events_url":"https://api.github.com/users/nitedl/events{/privacy}","received_events_url":"https://api.github.com/users/nitedl/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-05-21T15:00:15Z","updated_at":"2019-06-12T09:21:43Z","closed_at":"2019-06-12T09:21:43Z","author_association":"NONE","active_lock_reason":null,"body":"First of all, thank you for sharing the code for this great work.\r\n\r\nI'm trying to train a model in the most simple setup using the following command-line arguments:\r\n(python self-attentive-parser-master/src/main.py) train --model-path-base . --train-path self-attentive-parser-master\\data\\02-21.10way.clean --use-words\r\n\r\nusing python 3.6 on windows 10 with the latest pytorch, cython etc. I get the following error after \"Training...\":\r\n\r\nTraceback (most recent call last):\r\n  File \"self-attentive-parser-master/src/main.py\", line 612, in <module>\r\n    main()\r\n  File \"self-attentive-parser-master/src/main.py\", line 608, in main\r\n    args.callback(args)\r\n  File \"self-attentive-parser-master/src/main.py\", line 564, in <lambda>\r\n    subparser.set_defaults(callback=lambda args: run_train(args, hparams))\r\n  File \"self-attentive-parser-master/src/main.py\", line 312, in run_train\r\n    _, loss = parser.parse_batch(subbatch_sentences, subbatch_trees)\r\n  File \"self-attentive-parser-master\\src\\parse_nk.py\", line 1010, in parse_batch\r\n    annotations, _ = self.encoder(emb_idxs, batch_idxs, extra_content_annotations=extra_content_annotations)\r\n  File \"venv_parsing_36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"self-attentive-parser-master\\src\\parse_nk.py\", line 607, in forward\r\n    res, timing_signal, batch_idxs = emb(xs, batch_idxs, extra_content_annotations=extra_content_annotations)\r\n  File \"venv_parsing_36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"self-attentive-parser-master\\src\\parse_nk.py\", line 486, in forward\r\n    for x, emb, emb_dropout in zip(xs, self.embs, self.emb_dropouts)\r\n  File \"self-attentive-parser-master\\src\\parse_nk.py\", line 486, in <listcomp>\r\n    for x, emb, emb_dropout in zip(xs, self.embs, self.emb_dropouts)\r\n  File \"venv_parsing_36\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n    result = self.forward(*input, **kwargs)\r\n  File \"venv_parsing_36\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 118, in forward\r\n    self.norm_type, self.scale_grad_by_freq, self.sparse)\r\n  File \"venv_parsing_36\\lib\\site-packages\\torch\\nn\\functional.py\", line 1454, in embedding\r\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\r\nRuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.IntTensor instead (while checking arguments for embedding)\r\n\r\nProcess finished with exit code 1\r\n\r\nTrying to run with 'use_cuda = False' in parse_nk.py I get the same error (with 'torch.IntTensor' instead of 'torch.cuda.IntTensor'), so it doesn't seem to be cuda-related.\r\n\r\nTo make sure this is not a compatibility issue, I tried running in another virtual environment with python 3.6, cython 0.25.2 and pytorch 0.4.1 (with which the code was originally tested, according to the documentation), and I get the same error, with 'torch.cpu.IntTensor' replaced by 'CUDAIntTensor'.\r\n\r\nI found some references for this error on the web but nothing helpful. Have you encountered this error? Any idea what's causing it?\r\n\r\nThanks","closed_by":{"login":"nitedl","id":50877445,"node_id":"MDQ6VXNlcjUwODc3NDQ1","avatar_url":"https://avatars.githubusercontent.com/u/50877445?v=4","gravatar_id":"","url":"https://api.github.com/users/nitedl","html_url":"https://github.com/nitedl","followers_url":"https://api.github.com/users/nitedl/followers","following_url":"https://api.github.com/users/nitedl/following{/other_user}","gists_url":"https://api.github.com/users/nitedl/gists{/gist_id}","starred_url":"https://api.github.com/users/nitedl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nitedl/subscriptions","organizations_url":"https://api.github.com/users/nitedl/orgs","repos_url":"https://api.github.com/users/nitedl/repos","events_url":"https://api.github.com/users/nitedl/events{/privacy}","received_events_url":"https://api.github.com/users/nitedl/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/25/timeline","performed_via_github_app":null,"state_reason":"completed"}