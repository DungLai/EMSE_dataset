{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42","repository_url":"https://api.github.com/repos/nikitakit/self-attentive-parser","labels_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42/labels{/name}","comments_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42/comments","events_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42/events","html_url":"https://github.com/nikitakit/self-attentive-parser/issues/42","id":495161735,"node_id":"MDU6SXNzdWU0OTUxNjE3MzU=","number":42,"title":"Training script crashes on pytorch 1.2","user":{"login":"Genius1237","id":15867363,"node_id":"MDQ6VXNlcjE1ODY3MzYz","avatar_url":"https://avatars.githubusercontent.com/u/15867363?v=4","gravatar_id":"","url":"https://api.github.com/users/Genius1237","html_url":"https://github.com/Genius1237","followers_url":"https://api.github.com/users/Genius1237/followers","following_url":"https://api.github.com/users/Genius1237/following{/other_user}","gists_url":"https://api.github.com/users/Genius1237/gists{/gist_id}","starred_url":"https://api.github.com/users/Genius1237/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Genius1237/subscriptions","organizations_url":"https://api.github.com/users/Genius1237/orgs","repos_url":"https://api.github.com/users/Genius1237/repos","events_url":"https://api.github.com/users/Genius1237/events{/privacy}","received_events_url":"https://api.github.com/users/Genius1237/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-09-18T11:18:15Z","updated_at":"2021-02-06T04:50:36Z","closed_at":"2021-02-06T04:50:36Z","author_association":"NONE","active_lock_reason":null,"body":"The current version of the code does not run with pytorch 1.2, which is the current latest version. I am running the training script on the ptb data with `--use-words` as the only flag.\r\n\r\nThe error is in the call of `FeatureDropoutFunction.apply()`, in the line https://github.com/nikitakit/self-attentive-parser/blob/1ee43a8f93d6f3259c09ea1ff57cf5124ec32efc/src/parse_nk.py#L107\r\n.\r\noutput is of shape ([2016, 1024]) and ctx.noise is of shape ([1379, 1024]), due to which the mul operation fails.\r\n\r\nNote that this does not happen in every call of `FeatureDropoutFunction.apply()`. While stepping through, this exception is seen in the second call only. In the first time it's called, both the dimensions match and there is no exception thrown.\r\n\r\nWith Pytorch 1.1, these errors do not seem to appear. In a trial run, output and ctx.noise are of shape (1413, 1024) and there is no problem.\r\n\r\nI can provide further stack traces if needed.\r\n\r\n","closed_by":{"login":"nikitakit","id":252225,"node_id":"MDQ6VXNlcjI1MjIyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/252225?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitakit","html_url":"https://github.com/nikitakit","followers_url":"https://api.github.com/users/nikitakit/followers","following_url":"https://api.github.com/users/nikitakit/following{/other_user}","gists_url":"https://api.github.com/users/nikitakit/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitakit/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitakit/subscriptions","organizations_url":"https://api.github.com/users/nikitakit/orgs","repos_url":"https://api.github.com/users/nikitakit/repos","events_url":"https://api.github.com/users/nikitakit/events{/privacy}","received_events_url":"https://api.github.com/users/nikitakit/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nikitakit/self-attentive-parser/issues/42/timeline","performed_via_github_app":null,"state_reason":"completed"}