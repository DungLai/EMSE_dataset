{"url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181","repository_url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter","labels_url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181/labels{/name}","comments_url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181/comments","events_url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181/events","html_url":"https://github.com/HunterMcGushion/hyperparameter_hunter/issues/181","id":479007910,"node_id":"MDU6SXNzdWU0NzkwMDc5MTA=","number":181,"title":"Support for nested parameters/parameterizing objects that can't be called by name.","user":{"login":"ben-arnao","id":8053809,"node_id":"MDQ6VXNlcjgwNTM4MDk=","avatar_url":"https://avatars.githubusercontent.com/u/8053809?v=4","gravatar_id":"","url":"https://api.github.com/users/ben-arnao","html_url":"https://github.com/ben-arnao","followers_url":"https://api.github.com/users/ben-arnao/followers","following_url":"https://api.github.com/users/ben-arnao/following{/other_user}","gists_url":"https://api.github.com/users/ben-arnao/gists{/gist_id}","starred_url":"https://api.github.com/users/ben-arnao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ben-arnao/subscriptions","organizations_url":"https://api.github.com/users/ben-arnao/orgs","repos_url":"https://api.github.com/users/ben-arnao/repos","events_url":"https://api.github.com/users/ben-arnao/events{/privacy}","received_events_url":"https://api.github.com/users/ben-arnao/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-08-09T14:11:21Z","updated_at":"2019-08-20T14:04:41Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In a few different scenarios you'd want to have nested parameters, but sometimes a parameter isn't just a numerical/string value but instead a layer or object. An obvious example would be an optimizer with a learn rate. You need to call the learn rate inside of the optimizer initialization function.\r\n\r\nSo while you can have an optimizer as a hyper parameter like in the default example...\r\n\r\n```\r\ndef build_fn(input_shape):\r\n    model = Sequential([\r\n        Dense(Integer(50, 150), input_shape=input_shape, activation='relu'),\r\n        Dropout(Real(0.2, 0.7)),\r\n        Dense(1, activation=Categorical(['sigmoid', 'softmax']))\r\n    ])\r\n    model.compile(\r\n        optimizer=Categorical(['adam', 'rmsprop', 'sgd', 'adadelta']),\r\n        loss='binary_crossentropy', metrics=['accuracy']\r\n    )\r\n    return model\r\n```\r\n\r\nAnd you can have learn rate as a param for a **static** optimizer\r\n\r\n```\r\ndef build_fn(input_shape):\r\n    model = Sequential([\r\n        Dense(Integer(50, 150), input_shape=input_shape, activation='relu'),\r\n        Dropout(Real(0.2, 0.7)),\r\n        Dense(1, activation=Categorical(['sigmoid', 'softmax']))\r\n    ])\r\n    model.compile(\r\n        optimizer=Adam(lr=Real(0.001, 0.1)),\r\n        loss='binary_crossentropy', metrics=['accuracy']\r\n    )\r\n    return model\r\n```\r\n\r\nI'm not sure how you can have both as parameters simultaneously.\r\n\r\nWhat i have done in my own custom setup to get around a similar issue is to have a wrapper function that returns a optimizer. This way i can return optimizers that can't be called by name (Ie. Adamax or Nadam). With this same approach i think i could also have learn rate in the wrapper function as well. So i could do something like to get the functionality i want.\r\n\r\nget_custom_optimizer(Categorical(['adam', 'nadam')], Real(0.001, 0.01))\r\n\r\nWhich should just return an Adam or Nadam optimizer, with a random learn rate.\r\n\r\nThe problem is i don't think you can call any non-native function inside your build_fn...\r\n\r\nFor example:\r\n\r\n```\r\ndef get_opt():\r\n    return Adamax()\r\n\r\ndef build_fn(input_shape):\r\n    model = Sequential([\r\n        Dense(Integer(50, 150), input_shape=input_shape, activation='relu'),\r\n        Dropout(Real(0.2, 0.7)),\r\n        Dense(1, activation=Categorical(['sigmoid', 'softmax']))\r\n    ])\r\n    model.compile(\r\n        optimizer=get_opt(),\r\n        loss='binary_crossentropy', metrics=['accuracy']\r\n    )\r\n    return model\r\n```\r\n\r\nResults in an error\r\n`NameError: name 'get_opt' is not defined`\r\n\r\n\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/HunterMcGushion/hyperparameter_hunter/issues/181/timeline","performed_via_github_app":null,"state_reason":null}