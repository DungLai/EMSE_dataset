{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120","repository_url":"https://api.github.com/repos/sksq96/pytorch-summary","labels_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120/labels{/name}","comments_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120/comments","events_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120/events","html_url":"https://github.com/sksq96/pytorch-summary/issues/120","id":581828380,"node_id":"MDU6SXNzdWU1ODE4MjgzODA=","number":120,"title":"Functional Layers won't get summarised","user":{"login":"franz101","id":18228395,"node_id":"MDQ6VXNlcjE4MjI4Mzk1","avatar_url":"https://avatars.githubusercontent.com/u/18228395?v=4","gravatar_id":"","url":"https://api.github.com/users/franz101","html_url":"https://github.com/franz101","followers_url":"https://api.github.com/users/franz101/followers","following_url":"https://api.github.com/users/franz101/following{/other_user}","gists_url":"https://api.github.com/users/franz101/gists{/gist_id}","starred_url":"https://api.github.com/users/franz101/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/franz101/subscriptions","organizations_url":"https://api.github.com/users/franz101/orgs","repos_url":"https://api.github.com/users/franz101/repos","events_url":"https://api.github.com/users/franz101/events{/privacy}","received_events_url":"https://api.github.com/users/franz101/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-03-15T21:13:34Z","updated_at":"2020-04-17T17:51:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n`import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nfrom torchvision import models\r\nfrom torchsummary import summary\r\n\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 32, 5, 1)\r\n        self.conv2 = nn.Conv2d(32, 64, 5, 1)\r\n        self.dropout1 = nn.Dropout2d(0.4)\r\n        self.dropout2 = nn.Dropout2d(0.5)\r\n        self.fc1 = nn.Linear(2048, 1024)\r\n        self.fc2 = nn.Linear(1024, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        print(x.size())\r\n        x = F.relu(x)\r\n        print(x.size())\r\n        x = F.max_pool2d(x, 2,2)\r\n        print(x.size())\r\n        x = self.conv2(x)\r\n        print(x.size())\r\n        x = F.relu(x)\r\n        print(x.size())\r\n        x = F.max_pool2d(x, 2,2)\r\n        print(x.size())\r\n        x = x.view(-1,2048)\r\n        x = self.fc1(x)\r\n        x = F.relu(x)\r\n        x = self.dropout1(x)\r\n        x = self.fc2(x)\r\n        output = F.log_softmax(x, dim=1)\r\n        return output\r\n\r\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\nmodel = Net().to(device)\r\nsummary(model, (1,28, 28))`\r\n\r\nOUTPUT:\r\n`torch.Size([2, 32, 24, 24])\r\ntorch.Size([2, 32, 24, 24])\r\ntorch.Size([2, 32, 12, 12])\r\ntorch.Size([2, 64, 8, 8])\r\ntorch.Size([2, 64, 8, 8])\r\ntorch.Size([2, 64, 4, 4])\r\n----------------------------------------------------------------\r\n        Layer (type)               Output Shape         Param #\r\n================================================================\r\n            Conv2d-1           [-1, 32, 24, 24]             832\r\n            Conv2d-2             [-1, 64, 8, 8]          51,264\r\n            Linear-3                 [-1, 1024]       2,098,176\r\n         Dropout2d-4                 [-1, 1024]               0\r\n            Linear-5                   [-1, 10]          10,250\r\n================================================================\r\nTotal params: 2,160,522\r\nTrainable params: 2,160,522\r\nNon-trainable params: 0\r\n----------------------------------------------------------------\r\nInput size (MB): 0.00\r\nForward/backward pass size (MB): 0.19\r\nParams size (MB): 8.24\r\nEstimated Total Size (MB): 8.43\r\n----------------------------------------------------------------`\r\n\r\n\r\nEXPECTED OUTPUT:\r\nThe Max Pooling Layers should be mentioned...\r\n\r\nWhat am I missing here?","closed_by":null,"reactions":{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/120/timeline","performed_via_github_app":null,"state_reason":null}