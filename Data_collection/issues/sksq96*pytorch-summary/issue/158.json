{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158","repository_url":"https://api.github.com/repos/sksq96/pytorch-summary","labels_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158/labels{/name}","comments_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158/comments","events_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158/events","html_url":"https://github.com/sksq96/pytorch-summary/issues/158","id":752089449,"node_id":"MDU6SXNzdWU3NTIwODk0NDk=","number":158,"title":"RuntimeWarning: overflow encountered in long_scalars","user":{"login":"HamaguchiKazuki","id":37105228,"node_id":"MDQ6VXNlcjM3MTA1MjI4","avatar_url":"https://avatars.githubusercontent.com/u/37105228?v=4","gravatar_id":"","url":"https://api.github.com/users/HamaguchiKazuki","html_url":"https://github.com/HamaguchiKazuki","followers_url":"https://api.github.com/users/HamaguchiKazuki/followers","following_url":"https://api.github.com/users/HamaguchiKazuki/following{/other_user}","gists_url":"https://api.github.com/users/HamaguchiKazuki/gists{/gist_id}","starred_url":"https://api.github.com/users/HamaguchiKazuki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HamaguchiKazuki/subscriptions","organizations_url":"https://api.github.com/users/HamaguchiKazuki/orgs","repos_url":"https://api.github.com/users/HamaguchiKazuki/repos","events_url":"https://api.github.com/users/HamaguchiKazuki/events{/privacy}","received_events_url":"https://api.github.com/users/HamaguchiKazuki/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-11-27T08:58:00Z","updated_at":"2021-02-28T05:25:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"An overflow occurred when I ran the following code.\r\nThis is why the model estimation, including batch size, is not successful.\r\n\r\n``` python\r\n import torch\r\n from torchvision import models\r\n from torchsummary import summary\r\n\r\n device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n vgg = models.vgg16().to(device)\r\n\r\n summary(vgg, (3, 600, 600), 20)\r\n```\r\nThe output result is this.\r\n\r\n```bash\r\n----------------------------------------------------------------\r\n        Layer (type)               Output Shape         Param #\r\n================================================================\r\n            Conv2d-1         [20, 64, 600, 600]           1,792\r\n              ReLU-2         [20, 64, 600, 600]               0\r\n            Conv2d-3         [20, 64, 600, 600]          36,928\r\n              ReLU-4         [20, 64, 600, 600]               0\r\n         MaxPool2d-5         [20, 64, 300, 300]               0\r\ntorchsummary.py:93: RuntimeWarning: overflow encountered in long_scalars\r\n  total_output += np.prod(summary[layer][\"output_shape\"])\r\n            Conv2d-6        [20, 128, 300, 300]          73,856\r\n              ReLU-7        [20, 128, 300, 300]               0\r\n            Conv2d-8        [20, 128, 300, 300]         147,584\r\n              ReLU-9        [20, 128, 300, 300]               0\r\n        MaxPool2d-10        [20, 128, 150, 150]               0\r\n           Conv2d-11        [20, 256, 150, 150]         295,168\r\n             ReLU-12        [20, 256, 150, 150]               0\r\n           Conv2d-13        [20, 256, 150, 150]         590,080\r\n             ReLU-14        [20, 256, 150, 150]               0\r\n           Conv2d-15        [20, 256, 150, 150]         590,080\r\n             ReLU-16        [20, 256, 150, 150]               0\r\n        MaxPool2d-17          [20, 256, 75, 75]               0\r\n           Conv2d-18          [20, 512, 75, 75]       1,180,160\r\n             ReLU-19          [20, 512, 75, 75]               0\r\n           Conv2d-20          [20, 512, 75, 75]       2,359,808\r\n             ReLU-21          [20, 512, 75, 75]               0\r\n           Conv2d-22          [20, 512, 75, 75]       2,359,808\r\n             ReLU-23          [20, 512, 75, 75]               0\r\n        MaxPool2d-24          [20, 512, 37, 37]               0\r\n           Conv2d-25          [20, 512, 37, 37]       2,359,808\r\n             ReLU-26          [20, 512, 37, 37]               0\r\n           Conv2d-27          [20, 512, 37, 37]       2,359,808\r\n             ReLU-28          [20, 512, 37, 37]               0\r\n           Conv2d-29          [20, 512, 37, 37]       2,359,808\r\n             ReLU-30          [20, 512, 37, 37]               0\r\n        MaxPool2d-31          [20, 512, 18, 18]               0\r\nAdaptiveAvgPool2d-32            [20, 512, 7, 7]               0\r\n           Linear-33                 [20, 4096]     102,764,544\r\n             ReLU-34                 [20, 4096]               0\r\n          Dropout-35                 [20, 4096]               0\r\n           Linear-36                 [20, 4096]      16,781,312\r\n             ReLU-37                 [20, 4096]               0\r\n          Dropout-38                 [20, 4096]               0\r\n           Linear-39                 [20, 1000]       4,097,000\r\n================================================================\r\nTotal params: 138,357,544\r\nTrainable params: 138,357,544\r\nNon-trainable params: 0\r\n----------------------------------------------------------------\r\nInput size (MB): 82.40\r\nForward/backward pass size (MB): 1444.29\r\nParams size (MB): 527.79\r\nEstimated Total Size (MB): 2054.48\r\n----------------------------------------------------------------\r\n```\r\n\r\nDevelopment Environment\r\n- Windows 10\r\n- CUDA 10.2\r\n- Python 3.8.6\r\n- PyTorch 1.6.0+cu10.2\r\n- torchsummary 1.5.1","closed_by":null,"reactions":{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/158/timeline","performed_via_github_app":null,"state_reason":null}