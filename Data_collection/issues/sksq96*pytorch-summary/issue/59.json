{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59","repository_url":"https://api.github.com/repos/sksq96/pytorch-summary","labels_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59/labels{/name}","comments_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59/comments","events_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59/events","html_url":"https://github.com/sksq96/pytorch-summary/issues/59","id":426267187,"node_id":"MDU6SXNzdWU0MjYyNjcxODc=","number":59,"title":"summary() is work for the correct network.","user":{"login":"zking-a","id":30771326,"node_id":"MDQ6VXNlcjMwNzcxMzI2","avatar_url":"https://avatars.githubusercontent.com/u/30771326?v=4","gravatar_id":"","url":"https://api.github.com/users/zking-a","html_url":"https://github.com/zking-a","followers_url":"https://api.github.com/users/zking-a/followers","following_url":"https://api.github.com/users/zking-a/following{/other_user}","gists_url":"https://api.github.com/users/zking-a/gists{/gist_id}","starred_url":"https://api.github.com/users/zking-a/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zking-a/subscriptions","organizations_url":"https://api.github.com/users/zking-a/orgs","repos_url":"https://api.github.com/users/zking-a/repos","events_url":"https://api.github.com/users/zking-a/events{/privacy}","received_events_url":"https://api.github.com/users/zking-a/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-03-28T02:34:16Z","updated_at":"2019-04-23T08:38:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\nHere is my network.\r\n\r\n\timport torch\r\n\timport torch.nn as nn\r\n\timport torch.nn.functional as F\r\n\tfrom torch.nn import init\r\n\tfrom torch.nn import modules\r\n\ttorch.backends.cudnn.version()\r\n\r\n\tdef init_weights(net, init_type='normal', gain=0.02):\r\n\t    def init_func(m):\r\n\t        classname = m.__class__.__name__\r\n\t        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\r\n\t            if init_type == 'normal':\r\n\t                init.normal_(m.weight.data, 0.0, gain)\r\n\t            elif init_type == 'xavier':\r\n\t                init.xavier_normal_(m.weight.data, gain=gain)\r\n\t            elif init_type == 'kaiming':\r\n\t                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\r\n\t            elif init_type == 'orthogonal':\r\n\t                init.orthogonal_(m.weight.data, gain=gain)\r\n\t            else:\r\n\t                raise NotImplementedError(\r\n\t                    'initialization method [%s] is not implemented' % init_type)\r\n\t            if hasattr(m, 'bias') and m.bias is not None:\r\n\t                init.constant_(m.bias.data, 0.0)\r\n\t        elif classname.find('BatchNorm2d') != -1:\r\n\t            init.normal_(m.weight.data, 1.0, gain)\r\n\t            init.constant_(m.bias.data, 0.0)\r\n\t\r\n\t    print('initialize network with %s' % init_type)\r\n\t    net.apply(init_func)\r\n\t\r\n\t\r\n\tclass conv_block(nn.Module):\r\n\t    def __init__(self, ch_in, ch_out):\r\n\t        super(conv_block, self).__init__()\r\n\t        self.conv = nn.Sequential(\r\n\t            nn.Conv2d(ch_in, ch_out, kernel_size=3,\r\n\t                      stride=1, padding=1, bias=True),\r\n\t            nn.BatchNorm2d(ch_out),\r\n\t            nn.ReLU(inplace=True),\r\n\t            nn.Conv2d(ch_out, ch_out, kernel_size=3,\r\n\t                      stride=1, padding=1, bias=True),\r\n\t            nn.BatchNorm2d(ch_out),\r\n\t            nn.ReLU(inplace=True)\r\n\t        )\r\n\t\r\n\t    def forward(self, x):\r\n\t        x = self.conv(x)\r\n\t        return x\r\n\t\r\n\t\r\n\tclass up_conv(nn.Module):\r\n\t    def __init__(self, ch_in, ch_out):\r\n\t        super(up_conv, self).__init__()\r\n\t        self.up = nn.Sequential(\r\n\t            nn.Upsample(scale_factor=2),\r\n\t            nn.Conv2d(ch_in, ch_out, kernel_size=3,\r\n\t                      stride=1, padding=1, bias=True),\r\n\t            nn.BatchNorm2d(ch_out),\r\n\t            nn.ReLU(inplace=True)\r\n\t        )\r\n\t\r\n\t    def forward(self, x):\r\n\t        x = self.up(x)\r\n\t        return x\r\n\t\r\n\t\r\n\tclass Attention_block(nn.Module):\r\n\t    def __init__(self, F_g, F_l, F_int):\r\n\t        super(Attention_block, self).__init__()\r\n\t        self.W_g = nn.Sequential(\r\n\t            nn.Conv2d(F_g, F_int, kernel_size=1,\r\n\t                      stride=1, padding=0, bias=True),\r\n\t            nn.BatchNorm2d(F_int)\r\n\t        )\r\n\t\r\n\t        self.W_x = nn.Sequential(\r\n\t            nn.Conv2d(F_l, F_int, kernel_size=1,\r\n\t                      stride=1, padding=0, bias=True),\r\n\t            nn.BatchNorm2d(F_int)\r\n\t        )\r\n\t\r\n\t        self.psi = nn.Sequential(\r\n\t            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\r\n\t            nn.BatchNorm2d(1),\r\n\t            nn.Sigmoid()\r\n\t        )\r\n\t\r\n\t        self.relu = nn.ReLU(inplace=True)\r\n\t\r\n\t    def forward(self, g, x):\r\n\t        g1 = self.W_g(g)\r\n\t        x1 = self.W_x(x)\r\n\t        psi = self.relu(g1+x1)\r\n\t        psi = self.psi(psi)\r\n\t\r\n\t        return x*psi\r\n\t\r\n\t\r\n\tclass AttU_Net(nn.Module):\r\n\t    def __init__(self, img_ch=3, output_ch=3):\r\n\t        super(AttU_Net, self).__init__()\r\n\t\r\n\t        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\r\n\t\r\n\t        self.Conv1 = conv_block(ch_in=img_ch, ch_out=64)\r\n\t        self.Conv2 = conv_block(ch_in=64, ch_out=128)\r\n\t        self.Conv3 = conv_block(ch_in=128, ch_out=256)\r\n\t        self.Conv4 = conv_block(ch_in=256, ch_out=512)\r\n\t        self.Conv5 = conv_block(ch_in=512, ch_out=1024)\r\n\t\r\n\t        self.Up5 = up_conv(ch_in=1024, ch_out=512)\r\n\t        self.Att5 = Attention_block(F_g=512, F_l=512, F_int=256)\r\n\t        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\r\n\t\r\n\t        self.Up4 = up_conv(ch_in=512, ch_out=256)\r\n\t        self.Att4 = Attention_block(F_g=256, F_l=256, F_int=128)\r\n\t        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\r\n\t\r\n\t        self.Up3 = up_conv(ch_in=256, ch_out=128)\r\n\t        self.Att3 = Attention_block(F_g=128, F_l=128, F_int=64)\r\n\t        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\r\n\t\r\n\t        self.Up2 = up_conv(ch_in=128, ch_out=64)\r\n\t        self.Att2 = Attention_block(F_g=64, F_l=64, F_int=32)\r\n\t        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\r\n\t\r\n\t        self.Conv_1x1 = nn.Conv2d(\r\n\t            64, output_ch, kernel_size=1, stride=1, padding=0)\r\n\t\r\n\t    def forward(self, x):\r\n\t        # encoding path\r\n\t        x1 = self.Conv1(x)\r\n\t\r\n\t        x2 = self.Maxpool(x1)\r\n\t        x2 = self.Conv2(x2)\r\n\t\r\n\t        x3 = self.Maxpool(x2)\r\n\t        x3 = self.Conv3(x3)\r\n\t\r\n\t        x4 = self.Maxpool(x3)\r\n\t        x4 = self.Conv4(x4)\r\n\t\r\n\t        x5 = self.Maxpool(x4)\r\n\t        x5 = self.Conv5(x5)\r\n\t\r\n\t        # decoding + concat path\r\n\t        d5 = self.Up5(x5)\r\n\t        x4 = self.Att5(g=d5, x=x4)\r\n\t        d5 = torch.cat((x4, d5), dim=1)\r\n\t        d5 = self.Up_conv5(d5)\r\n\t\r\n\t        d4 = self.Up4(d5)\r\n\t        x3 = self.Att4(g=d4, x=x3)\r\n\t        d4 = torch.cat((x3, d4), dim=1)\r\n\t        d4 = self.Up_conv4(d4)\r\n\t\r\n\t        d3 = self.Up3(d4)\r\n\t        x2 = self.Att3(g=d3, x=x2)\r\n\t        d3 = torch.cat((x2, d3), dim=1)\r\n\t        d3 = self.Up_conv3(d3)\r\n\t\r\n\t        d2 = self.Up2(d3)\r\n\t        x1 = self.Att2(g=d2, x=x1)\r\n\t        d2 = torch.cat((x1, d2), dim=1)\r\n\t        d2 = self.Up_conv2(d2)\r\n\t\r\n\t        d1 = self.Conv_1x1(d2)\r\n\t\r\n\t        return d1\r\n\t\r\n\tif __name__ == '__main__':\r\n\t    from torchsummary import summary\r\n\t    g = R2AttU_Net()\r\n\t    t = torch.ones((3, 160, 160))\r\n\t    print(summary(g, tuple([3, 160, 160])))\r\n\r\nAnd when i run the code to check the model shape with summary,the bug is show ,but code is can train.how to solve it, would you help me?\r\n\r\n\tTraceback (most recent call last):\r\n\t  File \"E:\\work\\project\\Model\\torch_unet3.py\", line 446, in <module>\r\n\t    print(summary(g, tuple([3, 160, 160])))\r\n\t  File \"D:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchsummary\\torchsummary.py\", line 72, in summary\r\n\t    model(*x)\r\n\t  File \"D:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 489, in __call__\r\n\t    result = self.forward(*input, **kwargs)\r\n\t  File \"E:\\work\\project\\Model\\torch_unet3.py\", line 418, in forward\r\n\t    x4 = self.Att5(g=d5, x=x4)\r\n\t  File \"D:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 491, in __call__\r\n\t    hook_result = hook(self, input, result)\r\n\t  File \"D:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchsummary\\torchsummary.py\", line 19, in hook\r\n\t    summary[m_key][\"input_shape\"] = list(input[0].size())\r\n\tIndexError: tuple index out of range\r\n\t[Finished in 3.3s with exit code 1]","closed_by":null,"reactions":{"url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/sksq96/pytorch-summary/issues/59/timeline","performed_via_github_app":null,"state_reason":null}