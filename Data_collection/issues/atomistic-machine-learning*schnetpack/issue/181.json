{"url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181","repository_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack","labels_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181/labels{/name}","comments_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181/comments","events_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181/events","html_url":"https://github.com/atomistic-machine-learning/schnetpack/issues/181","id":518162232,"node_id":"MDU6SXNzdWU1MTgxNjIyMzI=","number":181,"title":"change inputs and embedding layer","user":{"login":"junwoony","id":34072912,"node_id":"MDQ6VXNlcjM0MDcyOTEy","avatar_url":"https://avatars.githubusercontent.com/u/34072912?v=4","gravatar_id":"","url":"https://api.github.com/users/junwoony","html_url":"https://github.com/junwoony","followers_url":"https://api.github.com/users/junwoony/followers","following_url":"https://api.github.com/users/junwoony/following{/other_user}","gists_url":"https://api.github.com/users/junwoony/gists{/gist_id}","starred_url":"https://api.github.com/users/junwoony/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/junwoony/subscriptions","organizations_url":"https://api.github.com/users/junwoony/orgs","repos_url":"https://api.github.com/users/junwoony/repos","events_url":"https://api.github.com/users/junwoony/events{/privacy}","received_events_url":"https://api.github.com/users/junwoony/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-11-06T00:42:35Z","updated_at":"2019-11-06T17:07:27Z","closed_at":"2019-11-06T17:07:27Z","author_association":"NONE","active_lock_reason":null,"body":"Dear Schnetpack people,\r\n\r\nI'm trying to replace the embedding layer by a Linear layer so that my new input can pass it. \r\nI tried to remove the embedding layer and add a new `nn.Linear` layer in the` __init__ `and `foward` in `SchNet` class, but it says \r\n\r\n```\r\n~/Desktop/SchNet/schnetpack/src/schnetpack/train/trainer.py` in __init__(self, model_path, model, loss_fn, optimizer, train_loader, validation_loader, keep_n_checkpoints, checkpoint_interval, validation_interval, hooks, loss_is_normalized)\r\n\r\n     60 #         self.scheduler = scheduler ###\r\n     61         if os.path.exists(self.checkpoint_path):\r\n---> 62             self.restore_checkpoint()\r\n     63         else:\r\n     64             os.makedirs(self.checkpoint_path)\r\n\r\n~/Desktop/SchNet/schnetpack/src/schnetpack/train/trainer.py in restore_checkpoint(self, epoch)\r\n    143             self.checkpoint_path, \"checkpoint-\" + str(epoch) + \".pth.tar\"\r\n    144         )\r\n--> 145         self.state_dict = torch.load(chkpt)\r\n    146 \r\n    147     def train(self, device, n_epochs=sys.maxsize):\r\n\r\n~/Desktop/SchNet/schnetpack/src/schnetpack/train/trainer.py in state_dict(self, state_dict)\r\n    111         self.step = state_dict[\"step\"]\r\n    112         self.best_loss = state_dict[\"best_loss\"]\r\n--> 113         self.optimizer.load_state_dict(state_dict[\"optimizer\"])\r\n    114         self._load_model_state_dict(state_dict[\"model\"])\r\n    115 \r\n\r\n~/miniconda3/envs/schnet2/lib/python3.7/site-packages/torch/optim/optimizer.py in load_state_dict(self, state_dict)\r\n    114         saved_lens = (len(g['params']) for g in saved_groups)\r\n    115         if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\r\n--> 116             raise ValueError(\"loaded state dict contains a parameter group \"\r\n    117                              \"that doesn't match the size of optimizer's group\")\r\n    118 \r\n\r\nValueError: loaded state dict contains a parameter group that doesn't match the size of optimizer's group\r\n```\r\n\r\nDo you have any idea how to add/remove layers from the model?\r\nIs there anything I need to update when I add/remove layers?","closed_by":{"login":"junwoony","id":34072912,"node_id":"MDQ6VXNlcjM0MDcyOTEy","avatar_url":"https://avatars.githubusercontent.com/u/34072912?v=4","gravatar_id":"","url":"https://api.github.com/users/junwoony","html_url":"https://github.com/junwoony","followers_url":"https://api.github.com/users/junwoony/followers","following_url":"https://api.github.com/users/junwoony/following{/other_user}","gists_url":"https://api.github.com/users/junwoony/gists{/gist_id}","starred_url":"https://api.github.com/users/junwoony/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/junwoony/subscriptions","organizations_url":"https://api.github.com/users/junwoony/orgs","repos_url":"https://api.github.com/users/junwoony/repos","events_url":"https://api.github.com/users/junwoony/events{/privacy}","received_events_url":"https://api.github.com/users/junwoony/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/181/timeline","performed_via_github_app":null,"state_reason":"completed"}