{"url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288","repository_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack","labels_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288/labels{/name}","comments_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288/comments","events_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288/events","html_url":"https://github.com/atomistic-machine-learning/schnetpack/issues/288","id":893450415,"node_id":"MDU6SXNzdWU4OTM0NTA0MTU=","number":288,"title":"High Training Loss on very Small DBs with PBCs","user":{"login":"JordD04","id":1126249,"node_id":"MDQ6VXNlcjExMjYyNDk=","avatar_url":"https://avatars.githubusercontent.com/u/1126249?v=4","gravatar_id":"","url":"https://api.github.com/users/JordD04","html_url":"https://github.com/JordD04","followers_url":"https://api.github.com/users/JordD04/followers","following_url":"https://api.github.com/users/JordD04/following{/other_user}","gists_url":"https://api.github.com/users/JordD04/gists{/gist_id}","starred_url":"https://api.github.com/users/JordD04/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JordD04/subscriptions","organizations_url":"https://api.github.com/users/JordD04/orgs","repos_url":"https://api.github.com/users/JordD04/repos","events_url":"https://api.github.com/users/JordD04/events{/privacy}","received_events_url":"https://api.github.com/users/JordD04/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2021-05-17T15:27:09Z","updated_at":"2021-11-14T03:35:05Z","closed_at":"2021-05-21T13:57:37Z","author_association":"NONE","active_lock_reason":null,"body":"I've been having trouble getting SchNetPack to perform as well as I would expect on my custom database (MAE Energy 1.5 eV / atom and MAE Force 0.5 eV / Ã…) so I decided to do some testing using databases that only have 10 structures (I just pick 10 structures at random from a full database and save them into a new database file). My understanding is that when training a NN on a very small dataset, we should expect terrible validation loss but a near-zero training loss.\r\n\r\nI've been using spk_run.py to minimise user-error on my part and my args.json looks like this:\r\n```\r\n{\r\n    \"aggregation_mode\": \"sum\",\r\n    \"batch_size\": 100,\r\n    \"checkpoint_interval\": 1,\r\n    \"contributions\": null,\r\n    \"cuda\": false,\r\n    \"cutoff\": 10.0,\r\n    \"cutoff_function\": \"cosine\",\r\n    \"datapath\": \"model/QM9_10_set2.db\",\r\n    \"dataset\": \"custom\",\r\n    \"derivative\": null,\r\n    \"environment_provider\": \"simple\",\r\n    \"features\": 128,\r\n    \"force\": null,\r\n    \"interactions\": 6,\r\n    \"keep_n_checkpoints\": 3,\r\n    \"log_every_n_epochs\": 1,\r\n    \"logger\": \"csv\",\r\n    \"lr\": 0.0001,\r\n    \"lr_decay\": 0.8,\r\n    \"lr_min\": 1e-06,\r\n    \"lr_patience\": 25,\r\n    \"max_epochs\": 5000,\r\n    \"max_steps\": null,\r\n    \"mode\": \"train\",\r\n    \"model\": \"schnet\",\r\n    \"modelpath\": \"model/\",\r\n    \"n_epochs\": 1000,\r\n    \"negative_dr\": false,\r\n    \"num_gaussians\": 50,\r\n    \"output_module\": \"atomwise\",\r\n    \"overwrite\": false,\r\n    \"parallel\": false,\r\n    \"property\": \"energy_U0\",\r\n    \"rho\": {},\r\n    \"seed\": null,\r\n    \"split\": [\r\n        7,\r\n        3\r\n    ],\r\n    \"split_path\": null,\r\n    \"stress\": null\r\n}\r\n```\r\nFor QM9, I had trouble getting a decent train loss with the default parameters from the benchmark (you'll notice I've specified a custom dataset in the args.json but I think I'm using the parameters that come with the QM9 benchmark) but I was able to consistently achieve a train loss of 0 when I increased the lr from 1e-4 to 1e-3.\r\nThe figure shows the train loss converging to zero.\r\n![QM9-best](https://user-images.githubusercontent.com/1126249/118511209-9669f280-b729-11eb-8bac-1187bc714295.png)\r\n\r\nI've tried training on energy_per_atom and total_energy (I defined this later property myself) and I got a training loss range of 0 - 0.404. I've tested with both lr=1e-4 to Lr=1e-3 but it didn't change very much.\r\nThe figure shows the train loss converging to zero.\r\n![MatProj-best](https://user-images.githubusercontent.com/1126249/118511228-9bc73d00-b729-11eb-8117-a553a8039129.png)\r\n\r\nI experimented with a few parameters for my custom dataset. The parameters that gave me the best results are as follows: lr = 1e-3, lr_decay = 0.8, lr_min = 1e-6, lr_patience= 50 and converged to a train loss over the range of 0.143 - 4.778 (units are in eV). Simply changing my lr from 1e-4 to 1e-3 gave worse results. \r\n\r\nI've also tried training on Nongnuch Artrith's TiO2 benchmark (xsf files converted to a SchNetPack database) but got even worse results. Here, an lr=1e-3 converged to train loss of a range between 0.114 and 6895. \r\nThe figure shows the train loss in the worst case scenario.\r\n![TiO2-worst](https://user-images.githubusercontent.com/1126249/118512067-64a55b80-b72a-11eb-8f3d-950fed025551.png)\r\nTraining on the full TiO2 database gives similarly terrible results (MAE energy = 35 eV / atom).\r\n\r\nI've experimented with some of the other non-lr-related arguments and I'm not sure they're the problem. I don't think i'm being conservative with the architecture parameters or cfconv parameters. I don't think I even have enough RAM for the current parameters when scaling up to a full dataset.\r\n\r\nI've seen another issue reported on [github](https://github.com/atomistic-machine-learning/schnetpack/issues/226) concerning training on the MatProj benchmark. Does SchNetPack not work with PBCs right now or am I making a mistake somewhere?","closed_by":{"login":"ktschuett","id":6585114,"node_id":"MDQ6VXNlcjY1ODUxMTQ=","avatar_url":"https://avatars.githubusercontent.com/u/6585114?v=4","gravatar_id":"","url":"https://api.github.com/users/ktschuett","html_url":"https://github.com/ktschuett","followers_url":"https://api.github.com/users/ktschuett/followers","following_url":"https://api.github.com/users/ktschuett/following{/other_user}","gists_url":"https://api.github.com/users/ktschuett/gists{/gist_id}","starred_url":"https://api.github.com/users/ktschuett/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ktschuett/subscriptions","organizations_url":"https://api.github.com/users/ktschuett/orgs","repos_url":"https://api.github.com/users/ktschuett/repos","events_url":"https://api.github.com/users/ktschuett/events{/privacy}","received_events_url":"https://api.github.com/users/ktschuett/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/atomistic-machine-learning/schnetpack/issues/288/timeline","performed_via_github_app":null,"state_reason":"completed"}