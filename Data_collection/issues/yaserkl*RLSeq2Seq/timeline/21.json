[{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/comments/434192390","html_url":"https://github.com/yaserkl/RLSeq2Seq/issues/21#issuecomment-434192390","issue_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/21","id":434192390,"node_id":"MDEyOklzc3VlQ29tbWVudDQzNDE5MjM5MA==","user":{"login":"crystina-z","id":31640436,"node_id":"MDQ6VXNlcjMxNjQwNDM2","avatar_url":"https://avatars.githubusercontent.com/u/31640436?v=4","gravatar_id":"","url":"https://api.github.com/users/crystina-z","html_url":"https://github.com/crystina-z","followers_url":"https://api.github.com/users/crystina-z/followers","following_url":"https://api.github.com/users/crystina-z/following{/other_user}","gists_url":"https://api.github.com/users/crystina-z/gists{/gist_id}","starred_url":"https://api.github.com/users/crystina-z/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/crystina-z/subscriptions","organizations_url":"https://api.github.com/users/crystina-z/orgs","repos_url":"https://api.github.com/users/crystina-z/repos","events_url":"https://api.github.com/users/crystina-z/events{/privacy}","received_events_url":"https://api.github.com/users/crystina-z/received_events","type":"User","site_admin":false},"created_at":"2018-10-30T07:00:16Z","updated_at":"2018-10-30T07:03:06Z","author_association":"NONE","body":"A follow-up question, again I apologize for my possible misunderstanding. \r\nSo far as I see, \r\n1. **the input for the Loss_ml and Loss_rl seems to be different in the original paper?** \r\n   Since the loss_ml part is basically the same with the traditional teacher forcing way, it uses ground truth as input for next timestep, which is also how the current implementation does. \r\n   However, in the Reinforcement Learning part, considering the paper is trying to address the _exposure bias_ which 'come from the fact that the network **has knowledge of the ground truth sequence up to the next token**' (at the bottom of page 4), and **the baseline y^ is obtained essentially by greedy search**(at the top of page 5), i feel in the RL part, ground truth should not be given in the training mode, i.e. the decode input should come from last prediction other than the _batch_. \r\n\r\n2. Based on the above (input of RL come from the last timestep), i'm thinking maybe **the procedure of generating y^ and ys should also be separated** (now they are both depend on the ground truth input): \r\n  since at timestep t, y^(t) is obtained by maximizing p( y^(t)| y^(t-1), ...y^(1) ) , while ys(t) is _sampled_ from p(ys(t)| ys(t-1), ...ys(1)). As u see, these 2 distributions are different, so i'm thinking maybe we are supposed to have 2 generative procedures here: one always takes y^(t-1) as next timestep input and generates y^(t), another always takes ys(t-1) as input and generates ys(t). in this way, we ultimately receive 2 sequences y^ and ys and their corresponding ROUGE value. \r\n\r\nPlease correct me if you think the other way. I'm still on my way about understanding RL in Summarization. Thanks!!","reactions":{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/comments/434192390/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null,"event":"commented","actor":{"login":"crystina-z","id":31640436,"node_id":"MDQ6VXNlcjMxNjQwNDM2","avatar_url":"https://avatars.githubusercontent.com/u/31640436?v=4","gravatar_id":"","url":"https://api.github.com/users/crystina-z","html_url":"https://github.com/crystina-z","followers_url":"https://api.github.com/users/crystina-z/followers","following_url":"https://api.github.com/users/crystina-z/following{/other_user}","gists_url":"https://api.github.com/users/crystina-z/gists{/gist_id}","starred_url":"https://api.github.com/users/crystina-z/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/crystina-z/subscriptions","organizations_url":"https://api.github.com/users/crystina-z/orgs","repos_url":"https://api.github.com/users/crystina-z/repos","events_url":"https://api.github.com/users/crystina-z/events{/privacy}","received_events_url":"https://api.github.com/users/crystina-z/received_events","type":"User","site_admin":false}}]