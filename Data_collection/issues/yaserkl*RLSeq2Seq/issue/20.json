{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20","repository_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq","labels_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20/labels{/name}","comments_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20/comments","events_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20/events","html_url":"https://github.com/yaserkl/RLSeq2Seq/issues/20","id":374410719,"node_id":"MDU6SXNzdWUzNzQ0MTA3MTk=","number":20,"title":"A problem about Q updates","user":{"login":"painterner","id":21156030,"node_id":"MDQ6VXNlcjIxMTU2MDMw","avatar_url":"https://avatars.githubusercontent.com/u/21156030?v=4","gravatar_id":"","url":"https://api.github.com/users/painterner","html_url":"https://github.com/painterner","followers_url":"https://api.github.com/users/painterner/followers","following_url":"https://api.github.com/users/painterner/following{/other_user}","gists_url":"https://api.github.com/users/painterner/gists{/gist_id}","starred_url":"https://api.github.com/users/painterner/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/painterner/subscriptions","organizations_url":"https://api.github.com/users/painterner/orgs","repos_url":"https://api.github.com/users/painterner/repos","events_url":"https://api.github.com/users/painterner/events{/privacy}","received_events_url":"https://api.github.com/users/painterner/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2018-10-26T14:31:49Z","updated_at":"2018-10-26T14:31:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello! I can't understand this (389 - 407 line in run_summarization.py), why the \"dqn_best_action\" use \r\nstate other than state_prime ? I think  dist_q_val = -tf.log(dist) * q_value (model.py) which means we should let dist and q_value be close each other , right ?  Shouldn't we use ||Q-q||^2 [(https://arxiv.org/pdf/1805.09461.pdf](url)   Eq. 29)\r\n\r\n         # 389 line\r\n         q_estimates = dqn_results['estimates'] # shape (len(transitions), vocab_size)\r\n          dqn_best_action = dqn_results['best_action']\r\n          #dqn_q_estimate_loss = dqn_results['loss']\r\n\r\n          # use target DQN to estimate values for the next decoder state\r\n          dqn_target_results = self.dqn_target.run_test_steps(self.dqn_sess, x= b_prime._x)\r\n          q_vals_new_t = dqn_target_results['estimates'] # shape (len(transitions), vocab_size)\r\n          \r\n          # 407 line\r\n          q_estimates[i][tr.action] = tr.reward + FLAGS.gamma * q_vals_new_t[i][dqn_best_action[i]]\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/20/timeline","performed_via_github_app":null,"state_reason":null}