{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28","repository_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq","labels_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28/labels{/name}","comments_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28/comments","events_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28/events","html_url":"https://github.com/yaserkl/RLSeq2Seq/issues/28","id":412888784,"node_id":"MDU6SXNzdWU0MTI4ODg3ODQ=","number":28,"title":"Maybe interested in these papers?","user":{"login":"khanhptnk","id":1854828,"node_id":"MDQ6VXNlcjE4NTQ4Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/1854828?v=4","gravatar_id":"","url":"https://api.github.com/users/khanhptnk","html_url":"https://github.com/khanhptnk","followers_url":"https://api.github.com/users/khanhptnk/followers","following_url":"https://api.github.com/users/khanhptnk/following{/other_user}","gists_url":"https://api.github.com/users/khanhptnk/gists{/gist_id}","starred_url":"https://api.github.com/users/khanhptnk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/khanhptnk/subscriptions","organizations_url":"https://api.github.com/users/khanhptnk/orgs","repos_url":"https://api.github.com/users/khanhptnk/repos","events_url":"https://api.github.com/users/khanhptnk/events{/privacy}","received_events_url":"https://api.github.com/users/khanhptnk/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2019-02-21T11:57:16Z","updated_at":"2019-02-21T11:57:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi Authors, \r\n\r\nThanks for the excellent work! For this research direction, I think you maybe interested in work on using RL in practical applications where user ratings or preferences are used to improve models. New challenges arise when we deploy RL methods to these real-world scenarios (e.g., noisy reward, sample efficiency). Addressing these challenges is an important research direction. \r\n\r\nReliability and learnability of human bandit feedback for sequence-to-sequence reinforcement learning (https://arxiv.org/abs/1707.07402)\r\nReinforcement learning for bandit neural machine translation with simulated human feedback (https://arxiv.org/abs/1805.10627)\r\nLearning from human feedback: https://deepmind.com/blog/learning-through-human-feedback/\r\n\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/yaserkl/RLSeq2Seq/issues/28/timeline","performed_via_github_app":null,"state_reason":null}