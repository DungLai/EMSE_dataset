{"url":"https://api.github.com/repos/allenai/scispacy/issues/440","repository_url":"https://api.github.com/repos/allenai/scispacy","labels_url":"https://api.github.com/repos/allenai/scispacy/issues/440/labels{/name}","comments_url":"https://api.github.com/repos/allenai/scispacy/issues/440/comments","events_url":"https://api.github.com/repos/allenai/scispacy/issues/440/events","html_url":"https://github.com/allenai/scispacy/issues/440","id":1305354697,"node_id":"I_kwDOCPNzOc5NziHJ","number":440,"title":"Training scispacy pipelines require recreating the vocab file","user":{"login":"Hammad-NobleAI","id":108357284,"node_id":"U_kgDOBnVmpA","avatar_url":"https://avatars.githubusercontent.com/u/108357284?v=4","gravatar_id":"","url":"https://api.github.com/users/Hammad-NobleAI","html_url":"https://github.com/Hammad-NobleAI","followers_url":"https://api.github.com/users/Hammad-NobleAI/followers","following_url":"https://api.github.com/users/Hammad-NobleAI/following{/other_user}","gists_url":"https://api.github.com/users/Hammad-NobleAI/gists{/gist_id}","starred_url":"https://api.github.com/users/Hammad-NobleAI/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Hammad-NobleAI/subscriptions","organizations_url":"https://api.github.com/users/Hammad-NobleAI/orgs","repos_url":"https://api.github.com/users/Hammad-NobleAI/repos","events_url":"https://api.github.com/users/Hammad-NobleAI/events{/privacy}","received_events_url":"https://api.github.com/users/Hammad-NobleAI/received_events","type":"User","site_admin":false},"labels":[{"id":1068015159,"node_id":"MDU6TGFiZWwxMDY4MDE1MTU5","url":"https://api.github.com/repos/allenai/scispacy/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2022-07-14T22:36:42Z","updated_at":"2022-09-16T05:49:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm attempting to use your \"en_core_sci_lg\" pipeline to extract chemical entities from documents, and then using those entities as a basis to train Spacy's Entity Linker (as shown in this [document](https://github.com/explosion/projects/blob/master/nel-emerson/scripts/notebook_video.ipynb)). Here are the relevant portions of my code:\r\n\r\n```\r\nimport spacy\r\nimport scispacy\r\nnlp = spacy.load(\"en_core_sci_lg\")\r\n\r\n... prepare training documentation as Spacy specified in the form [tuples of form (text, {\"links\": (span.start, span.end), {qID: probability})]...\r\n\r\nentity_linker = nlp.create_pipe(\"entity_linker\", config={\"incl_prior\": False})\r\n\r\ndef create_kb(vocab):\r\n    kb = KnowledgeBase(vocab=nlp.vocab, entity_vector_length=200)\r\n\r\n    for qid, desc in desc_dict.items():\r\n        desc_doc = nlp(desc)\r\n        desc_enc = desc_doc.vector\r\n        kb.add_entity(entity=qid, entity_vector=desc_enc, freq=342)\r\n    return kb\r\n\r\nentity_linker.set_kb(create_kb)\r\nnlp.add_pipe(\"entity_linker\", last=True)\r\n\r\nfrom random import random\r\nfrom spacy.util import minibatch, compounding\r\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"entity_linker\"]\r\nwith nlp.disable_pipes(*other_pipes):   # train only the entity_linker\r\n        optimizer = nlp.begin_training() ## ERROR HERE\r\n        for itn in range(500):   # 500 iterations takes about a minute to train on this small dataset\r\n            random.shuffle(TRAIN_DOCS)\r\n            batches = minibatch(TRAIN_DOCS, size=compounding(4.0, 32.0, 1.001))   # increasing batch size\r\n            losses = {}\r\n            for batch in batches:\r\n                texts, annotations = zip(*batch)\r\n                nlp.update(\r\n                    texts,\r\n                    annotations,\r\n                    drop=0.2,   # prevent overfitting\r\n                    losses=losses,\r\n                    sgd=optimizer,\r\n                )\r\n            if itn % 50 == 0:\r\n                print(itn, \"Losses\", losses)   # print the training loss\r\nprint(itn, \"Losses\", losses)\r\n```\r\nWhen I get to the error line (commented towards the end of the code block), I get the following error:\r\n```\r\nRegistryError: [E893] Could not find function 'replace_tokenizer' in function registry 'callbacks'. If you're using a custom function, make sure the code is available. If the function is provided by a third-party package, e.g. spacy-transformers, make sure the package is installed in your environment.\r\n\r\nAvailable names: spacy.copy_from_base_model.v1, spacy.models_and_pipes_with_nvtx_range.v1, spacy.models_with_nvtx_range.v1\r\n```\r\n\r\nI'm running on Mac OS 12.4, M1 Pro, 16 GB unified memory. Scispacy==0.5.0, spacy==3.2.4. Are Scispacy models compatible with this workflow, or is that something that hasn't/won't be implemented? Thanks in advance!","closed_by":{"login":"dakinggg","id":43149077,"node_id":"MDQ6VXNlcjQzMTQ5MDc3","avatar_url":"https://avatars.githubusercontent.com/u/43149077?v=4","gravatar_id":"","url":"https://api.github.com/users/dakinggg","html_url":"https://github.com/dakinggg","followers_url":"https://api.github.com/users/dakinggg/followers","following_url":"https://api.github.com/users/dakinggg/following{/other_user}","gists_url":"https://api.github.com/users/dakinggg/gists{/gist_id}","starred_url":"https://api.github.com/users/dakinggg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dakinggg/subscriptions","organizations_url":"https://api.github.com/users/dakinggg/orgs","repos_url":"https://api.github.com/users/dakinggg/repos","events_url":"https://api.github.com/users/dakinggg/events{/privacy}","received_events_url":"https://api.github.com/users/dakinggg/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/allenai/scispacy/issues/440/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/allenai/scispacy/issues/440/timeline","performed_via_github_app":null,"state_reason":"reopened"}