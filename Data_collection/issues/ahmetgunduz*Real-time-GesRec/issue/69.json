{"url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69","repository_url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec","labels_url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69/labels{/name}","comments_url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69/comments","events_url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69/events","html_url":"https://github.com/ahmetgunduz/Real-time-GesRec/issues/69","id":627000362,"node_id":"MDU6SXNzdWU2MjcwMDAzNjI=","number":69,"title":"error in testing with video","user":{"login":"niuwenju","id":37408618,"node_id":"MDQ6VXNlcjM3NDA4NjE4","avatar_url":"https://avatars.githubusercontent.com/u/37408618?v=4","gravatar_id":"","url":"https://api.github.com/users/niuwenju","html_url":"https://github.com/niuwenju","followers_url":"https://api.github.com/users/niuwenju/followers","following_url":"https://api.github.com/users/niuwenju/following{/other_user}","gists_url":"https://api.github.com/users/niuwenju/gists{/gist_id}","starred_url":"https://api.github.com/users/niuwenju/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/niuwenju/subscriptions","organizations_url":"https://api.github.com/users/niuwenju/orgs","repos_url":"https://api.github.com/users/niuwenju/repos","events_url":"https://api.github.com/users/niuwenju/events{/privacy}","received_events_url":"https://api.github.com/users/niuwenju/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2020-05-29T05:03:02Z","updated_at":"2020-09-18T20:00:45Z","closed_at":"2020-06-02T11:15:52Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,i have retrain the model into RGB,and have successfully tested with images(with test dataset),but when i test with video,prediction_det is alway 0.there is my script for video\r\n\r\n```\r\nimport os\r\nimport glob\r\nimport json\r\nimport pandas as pd\r\nimport numpy as np\r\nimport csv\r\nimport torch\r\nimport time\r\nfrom torch.autograd import Variable\r\nfrom PIL import Image\r\nimport cv2\r\nfrom torch.nn import functional as F\r\n\r\nfrom opts import parse_opts_online\r\nfrom model import generate_model\r\nfrom mean import get_mean, get_std\r\nfrom spatial_transforms import *\r\nfrom temporal_transforms import *\r\nfrom target_transforms import ClassLabel\r\nfrom dataset import get_online_data\r\nfrom utils import  AverageMeter, LevenshteinDistance, Queue\r\n\r\nimport pdb\r\nimport numpy as np\r\nimport datetime\r\n\r\n\r\ndef weighting_func(x):\r\n    return (1 / (1 + np.exp(-0.2 * (x - 9))))\r\n\r\n\r\nopt = parse_opts_online()\r\n\r\n\r\ndef load_models(opt):\r\n    opt.resume_path = opt.resume_path_det\r\n    opt.pretrain_path = opt.pretrain_path_det\r\n    opt.sample_duration = opt.sample_duration_det\r\n    opt.model = opt.model_det\r\n    opt.model_depth = opt.model_depth_det\r\n    opt.width_mult = opt.width_mult_det\r\n    opt.modality = opt.modality_det\r\n    opt.resnet_shortcut = opt.resnet_shortcut_det\r\n    opt.n_classes = opt.n_classes_det\r\n    opt.n_finetune_classes = opt.n_finetune_classes_det\r\n\r\n    if opt.root_path != '':\r\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\r\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\r\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\r\n        if opt.resume_path:\r\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        if opt.pretrain_path:\r\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\r\n\r\n    opt.scales = [opt.initial_scale]\r\n    for i in range(1, opt.n_scales):\r\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\r\n    opt.arch = '{}'.format(opt.model)\r\n    opt.mean = get_mean(opt.norm_value)\r\n    opt.std = get_std(opt.norm_value)\r\n\r\n    print(opt)\r\n    with open(os.path.join(opt.result_path, 'opts_det.json'), 'w') as opt_file:\r\n        json.dump(vars(opt), opt_file)\r\n\r\n    torch.manual_seed(opt.manual_seed)\r\n\r\n    detector, parameters = generate_model(opt)\r\n    detector = detector.cuda()\r\n    if opt.resume_path:\r\n        opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        print('loading checkpoint {}'.format(opt.resume_path))\r\n        checkpoint = torch.load(opt.resume_path)\r\n\r\n        detector.load_state_dict(checkpoint['state_dict'])\r\n\r\n    print('Model 1 \\n', detector)\r\n    pytorch_total_params = sum(p.numel() for p in detector.parameters() if\r\n                               p.requires_grad)\r\n    print(\"Total number of trainable parameters: \", pytorch_total_params)\r\n\r\n    opt.resume_path = opt.resume_path_clf\r\n    opt.pretrain_path = opt.pretrain_path_clf\r\n    opt.sample_duration = opt.sample_duration_clf\r\n    opt.model = opt.model_clf\r\n    opt.model_depth = opt.model_depth_clf\r\n    opt.width_mult = opt.width_mult_clf\r\n    opt.modality = opt.modality_clf\r\n    opt.resnet_shortcut = opt.resnet_shortcut_clf\r\n    opt.n_classes = opt.n_classes_clf\r\n    opt.n_finetune_classes = opt.n_finetune_classes_clf\r\n    if opt.root_path != '':\r\n        opt.video_path = os.path.join(opt.root_path, opt.video_path)\r\n        opt.annotation_path = os.path.join(opt.root_path, opt.annotation_path)\r\n        opt.result_path = os.path.join(opt.root_path, opt.result_path)\r\n        if opt.resume_path:\r\n            opt.resume_path = os.path.join(opt.root_path, opt.resume_path)\r\n        if opt.pretrain_path:\r\n            opt.pretrain_path = os.path.join(opt.root_path, opt.pretrain_path)\r\n\r\n    opt.scales = [opt.initial_scale]\r\n    for i in range(1, opt.n_scales):\r\n        opt.scales.append(opt.scales[-1] * opt.scale_step)\r\n    opt.arch = '{}'.format(opt.model)\r\n    opt.mean = get_mean(opt.norm_value)\r\n    opt.std = get_std(opt.norm_value)\r\n\r\n    print(opt)\r\n    with open(os.path.join(opt.result_path, 'opts_clf.json'), 'w') as opt_file:\r\n        json.dump(vars(opt), opt_file)\r\n\r\n    torch.manual_seed(opt.manual_seed)\r\n    classifier, parameters = generate_model(opt)\r\n    classifier = classifier.cuda()\r\n    if opt.resume_path:\r\n        print('loading checkpoint {}'.format(opt.resume_path))\r\n        checkpoint = torch.load(opt.resume_path)\r\n\r\n        classifier.load_state_dict(checkpoint['state_dict'])\r\n\r\n    print('Model 2 \\n', classifier)\r\n    pytorch_total_params = sum(p.numel() for p in classifier.parameters() if\r\n                               p.requires_grad)\r\n    print(\"Total number of trainable parameters: \", pytorch_total_params)\r\n\r\n    return detector, classifier\r\n\r\n\r\ndetector, classifier = load_models(opt)\r\n\r\nif opt.no_mean_norm and not opt.std_norm:\r\n    norm_method = Normalize([0, 0, 0], [1, 1, 1])\r\nelif not opt.std_norm:\r\n    norm_method = Normalize(opt.mean, [1, 1, 1])\r\nelse:\r\n    norm_method = Normalize(opt.mean, opt.std)\r\n\r\nspatial_transform = Compose([\r\n    Scale(112),\r\n    CenterCrop(112),\r\n    ToTensor(opt.norm_value), norm_method\r\n])\r\n\r\nopt.sample_duration = max(opt.sample_duration_clf, opt.sample_duration_det)\r\nfps = \"\"\r\ncap = cv2.VideoCapture('rgb7.mp4')\r\nnum_frame = 0\r\nclip = []\r\nwhile cap.isOpened():\r\n    \r\n    ret, frame = cap.read()\r\n    print(\"num_frame\",num_frame)\r\n    if num_frame == 0:\r\n        cur_frame = Image.fromarray(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\r\n        cur_frame = cur_frame.convert('RGB')\r\n        for i in range(opt.sample_duration):\r\n            clip.append(cur_frame)\r\n        clip = [spatial_transform(img) for img in clip]\r\n    num_frame += 1\r\n    active_index = 0\r\n    passive_count = 0\r\n    active = False\r\n    prev_active = False\r\n    finished_prediction = None\r\n    pre_predict = False\r\n\r\n    cum_sum = np.zeros(opt.n_classes_clf, )\r\n    clf_selected_queue = np.zeros(opt.n_classes_clf, )\r\n    det_selected_queue = np.zeros(opt.n_classes_det, )\r\n    myqueue_det = Queue(opt.det_queue_size, n_classes=opt.n_classes_det)\r\n    myqueue_clf = Queue(opt.clf_queue_size, n_classes=opt.n_classes_clf)\r\n    clip.pop(0)\r\n    _frame = Image.fromarray(cv2.cvtColor(frame,cv2.COLOR_BGR2RGB))\r\n    _frame = cur_frame.convert('RGB')\r\n    _frame = spatial_transform(_frame)\r\n    clip.append(_frame)\r\n    im_dim = clip[0].size()[-2:]\r\n    try:\r\n        test_data = torch.cat(clip, 0).view((opt.sample_duration, -1) + im_dim).permute(1, 0, 2, 3)\r\n    except Exception as e:\r\n        pdb.set_trace()\r\n        raise e\r\n    test_data = (test_data,0)\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_data,\r\n        batch_size=opt.batch_size,\r\n        shuffle=False,\r\n        num_workers=opt.n_threads,\r\n        pin_memory=True)\r\n\r\n    results = []\r\n    prev_best1 = opt.n_classes_clf\r\n    dataset_len = len(test_loader.dataset)\r\n    num_i = 0\r\n    for i, (inputs) in enumerate(test_loader):\r\n        if num_i == 0:\r\n            ground_truth_array = np.zeros(opt.n_classes_clf + 1, )\r\n            t2 = time.time()\r\n            with torch.no_grad():\r\n                inputs = Variable(inputs)\r\n                print(\"*****************\",inputs.size())\r\n                if opt.modality_det == 'RGB':\r\n                    inputs_det = inputs[:, :, -opt.sample_duration_det:, :, :]\r\n                elif opt.modality_det == 'Depth':\r\n                    inputs_det = inputs[:, -1, -opt.sample_duration_det:, :, :].unsqueeze(1)\r\n                elif opt.modality_det == 'RGB-D':\r\n                    inputs_det = inputs[:, :, -opt.sample_duration_det:, :, :]\r\n\r\n                outputs_det = detector(inputs_det)\r\n                outputs_det = F.softmax(outputs_det, dim=1)\r\n                outputs_det = outputs_det.cpu().numpy()[0].reshape(-1, )\r\n\r\n                myqueue_det.enqueue(outputs_det.tolist())\r\n\r\n                if opt.det_strategy == 'raw':\r\n                    det_selected_queue = outputs_det\r\n                elif opt.det_strategy == 'median':\r\n                    det_selected_queue = myqueue_det.median\r\n                elif opt.det_strategy == 'ma':\r\n                    det_selected_queue = myqueue_det.ma\r\n                elif opt.det_strategy == 'ewma':\r\n                    det_selected_queue = myqueue_det.ewma\r\n\r\n                prediction_det = np.argmax(det_selected_queue)\r\n                prob_det = det_selected_queue[prediction_det]\r\n                print(\"prediction_det:\",prediction_det)\r\n                \r\n                if prediction_det == 1:\r\n                    if opt.modality_clf == 'RGB':\r\n                        inputs_clf = inputs[:, :, :, :, :]\r\n                    elif opt.modality_clf == 'Depth':\r\n                        inputs_clf = inputs[:, -1, :, :, :].unsqueeze(1)\r\n                    elif opt.modality_clf == 'RGB-D':\r\n                        inputs_clf = inputs[:, :, :, :, :]\r\n                    inputs_clf = torch.Tensor(inputs_clf.numpy()[:,:,::2,:,:])\r\n                    outputs_clf = classifier(inputs_clf)\r\n                    outputs_clf = F.softmax(outputs_clf, dim=1)\r\n                    outputs_clf = outputs_clf.cpu().numpy()[0].reshape(-1, )\r\n\r\n                    myqueue_clf.enqueue(outputs_clf.tolist())\r\n                    passive_count = 0\r\n\r\n                    if opt.clf_strategy == 'raw':\r\n                        clf_selected_queue = outputs_clf\r\n                    elif opt.clf_strategy == 'median':\r\n                        clf_selected_queue = myqueue_clf.median\r\n                    elif opt.clf_strategy == 'ma':\r\n                        clf_selected_queue = myqueue_clf.ma\r\n                    elif opt.clf_strategy == 'ewma':\r\n                        clf_selected_queue = myqueue_clf.ewma\r\n\r\n                else:\r\n                    outputs_clf = np.zeros(opt.n_classes_clf, )\r\n                    # Push the probabilities to queue\r\n                    myqueue_clf.enqueue(outputs_clf.tolist())\r\n                    passive_count += 1\r\n            \r\n            if passive_count >= opt.det_counter or i == (dataset_len -2):\r\n                active = False\r\n            else:\r\n                active = True\r\n\r\n            if active:\r\n                active_index += 1\r\n                cum_sum = ((cum_sum * (active_index - 1)) + (\r\n                            weighting_func(active_index) * clf_selected_queue)) / active_index  # Weighted Aproach\r\n\r\n                best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\r\n                if float(cum_sum[best1] - cum_sum[best2]) > opt.clf_threshold_pre:\r\n                    finished_prediction = True\r\n                    pre_predict = True\r\n\r\n            else:\r\n                active_index = 0\r\n\r\n            if active == False and prev_active == True:\r\n                finished_prediction = True\r\n            elif active == True and prev_active == False:\r\n                finished_prediction = False\r\n\r\n\r\n            if finished_prediction == True:\r\n                best2, best1 = tuple(cum_sum.argsort()[-2:][::1])\r\n                if cum_sum[best1] > opt.clf_threshold_final:\r\n                    if pre_predict == True:\r\n                        if best1 != prev_best1:\r\n                            if cum_sum[best1] > opt.clf_threshold_final:\r\n                                results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n                                print('Early Detected - class : {} with prob : {} at frame {}'.format(best1, cum_sum[best1],\r\n                                                                                                      (\r\n                                                                                                                  i * opt.stride_len) + opt.sample_duration_clf))\r\n                    else:\r\n                        if cum_sum[best1] > opt.clf_threshold_final:\r\n                            if best1 == prev_best1:\r\n                                if cum_sum[best1] > 5:\r\n                                    results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n                                    print('Late Detected - class : {} with prob : {} at frame {}'.format(best1,\r\n                                                                                                         cum_sum[best1], (\r\n                                                                                                                     i * opt.stride_len) + opt.sample_duration_clf))\r\n                            else:\r\n                                results.append(((i * opt.stride_len) + opt.sample_duration_clf, best1))\r\n\r\n                                print('Late Detected - class : {} with prob : {} at frame {}'.format(best1, cum_sum[best1],\r\n                                                                                                     (\r\n                                                                                                                 i * opt.stride_len) + opt.sample_duration_clf))\r\n\r\n                    finished_prediction = False\r\n                    prev_best1 = best1\r\n\r\n                cum_sum = np.zeros(opt.n_classes_clf, )\r\n            \r\n            if active == False and prev_active == True:\r\n                pre_predict = False\r\n\r\n            prev_active = active\r\n            num_i += 1\r\n            elapsedTime = time.time() - t2\r\n            fps2 = \"(Playback) {:.1f} FPS\".format(1/elapsedTime)\r\n            t1 = time.time()\r\n            break\r\n        else:\r\n            break\r\n    elapsedTime = time.time() - t1\r\n    fps1 = \"(Playback) {:.1f} FPS\".format(1/elapsedTime)\r\n    if len(results) != 0:\r\n        predicted = np.array(results)[:, 1]\r\n    else:\r\n        predicted = []\r\n    print('predicted classes: \\t', predicted)\r\n    cv2.putText(frame, fps, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (38, 0, 255), 1, cv2.LINE_AA)\r\n    cv2.imshow(\"Result\", frame)\r\n    if cv2.waitKey(1)&0xFF == ord('q'):\r\n        break\r\n    #elapsedTime = time.time() - t1\r\n    #fps = \"(Playback) {:.1f} FPS\".format(1/elapsedTime)\r\ncv2.destroyAllWindows()\r\n```\r\n\r\n\r\n\r\nand the result is:\r\n ***************** torch.Size([1, 3, 32, 112, 112])\r\n**prediction_det: 0\r\npredicted classes: \t []**\r\nnum_frame 437\r\n","closed_by":{"login":"ahmetgunduz","id":16335853,"node_id":"MDQ6VXNlcjE2MzM1ODUz","avatar_url":"https://avatars.githubusercontent.com/u/16335853?v=4","gravatar_id":"","url":"https://api.github.com/users/ahmetgunduz","html_url":"https://github.com/ahmetgunduz","followers_url":"https://api.github.com/users/ahmetgunduz/followers","following_url":"https://api.github.com/users/ahmetgunduz/following{/other_user}","gists_url":"https://api.github.com/users/ahmetgunduz/gists{/gist_id}","starred_url":"https://api.github.com/users/ahmetgunduz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ahmetgunduz/subscriptions","organizations_url":"https://api.github.com/users/ahmetgunduz/orgs","repos_url":"https://api.github.com/users/ahmetgunduz/repos","events_url":"https://api.github.com/users/ahmetgunduz/events{/privacy}","received_events_url":"https://api.github.com/users/ahmetgunduz/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ahmetgunduz/Real-time-GesRec/issues/69/timeline","performed_via_github_app":null,"state_reason":"completed"}