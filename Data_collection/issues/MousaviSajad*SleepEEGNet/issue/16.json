{"url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16","repository_url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet","labels_url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16/labels{/name}","comments_url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16/comments","events_url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16/events","html_url":"https://github.com/MousaviSajad/SleepEEGNet/issues/16","id":832580448,"node_id":"MDU6SXNzdWU4MzI1ODA0NDg=","number":16,"title":"the loss function is inconsistent with that in the paper","user":{"login":"wannibar","id":25928537,"node_id":"MDQ6VXNlcjI1OTI4NTM3","avatar_url":"https://avatars.githubusercontent.com/u/25928537?v=4","gravatar_id":"","url":"https://api.github.com/users/wannibar","html_url":"https://github.com/wannibar","followers_url":"https://api.github.com/users/wannibar/followers","following_url":"https://api.github.com/users/wannibar/following{/other_user}","gists_url":"https://api.github.com/users/wannibar/gists{/gist_id}","starred_url":"https://api.github.com/users/wannibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wannibar/subscriptions","organizations_url":"https://api.github.com/users/wannibar/orgs","repos_url":"https://api.github.com/users/wannibar/repos","events_url":"https://api.github.com/users/wannibar/events{/privacy}","received_events_url":"https://api.github.com/users/wannibar/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-16T09:13:56Z","updated_at":"2021-03-16T09:13:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"hi,  I love what you do and open source work.\r\nand I have a question about the loss function.\r\nI read your paper, the paper said loss function is MFE and MSFE.\r\nbut I found the code is:\r\n\r\n       for i in range(logits.get_shape().as_list()[-1]): # [128, None, 7]\r\n            class_fill_targets = tf.fill(tf.shape(targets), i) #[?,?]\r\n            weights_i = tf.cast(tf.equal(targets, class_fill_targets), \"float\") #[?,?] \r\n            loss_is.append(tf.contrib.seq2seq.sequence_loss(logits, targets, weights_i, average_across_batch=False))\r\n\r\nI googled this function. It compute  cross-entropy loss. also weights_i parameter is my another question.\r\nfor example, when i=7 the weight is 7.\r\nIf I have something wrong, can you help me clarify it. thanks a lot! ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/MousaviSajad/SleepEEGNet/issues/16/timeline","performed_via_github_app":null,"state_reason":null}