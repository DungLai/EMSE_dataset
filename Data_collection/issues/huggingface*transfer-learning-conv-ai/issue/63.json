{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/63","id":577500628,"node_id":"MDU6SXNzdWU1Nzc1MDA2Mjg=","number":63,"title":"Questions about ppl when using gpt2","user":{"login":"ssxy00","id":29683793,"node_id":"MDQ6VXNlcjI5NjgzNzkz","avatar_url":"https://avatars.githubusercontent.com/u/29683793?v=4","gravatar_id":"","url":"https://api.github.com/users/ssxy00","html_url":"https://github.com/ssxy00","followers_url":"https://api.github.com/users/ssxy00/followers","following_url":"https://api.github.com/users/ssxy00/following{/other_user}","gists_url":"https://api.github.com/users/ssxy00/gists{/gist_id}","starred_url":"https://api.github.com/users/ssxy00/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssxy00/subscriptions","organizations_url":"https://api.github.com/users/ssxy00/orgs","repos_url":"https://api.github.com/users/ssxy00/repos","events_url":"https://api.github.com/users/ssxy00/events{/privacy}","received_events_url":"https://api.github.com/users/ssxy00/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2020-03-08T13:25:56Z","updated_at":"2022-03-18T00:32:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi! I meet some problems when running ConvAI2 evaluation scripts:\r\n\r\nI first trained a model from OpenAI GPT. I increased the number of cumulative gradients because I only have one card.\r\n```\r\npython train.py --model_checkpoint /path/to/pretrained/gpt \\\r\n--gradient_accumulation_steps=32 --lm_coef=2.0 --max_history=2 \\\r\n--n_epochs=1 --num_candidates=4 --personality_permutations=2 \\\r\n--train_batch_size=2 --valid_batch_size=2\r\n```\r\nthis gives ConvAI2 evalution results:\r\n```\r\nFinal Hits@1: 0.761\r\nFINAL F1: 0.1659\r\nFINAL PPL: 20.7\r\n```\r\nThen I tried to train from GPT2-small with the same config:\r\n```\r\npython train.py --model_checkpoint /path/to/pretrained/gpt2 \\\r\n--gradient_accumulation_steps=32 --lm_coef=2.0 --max_history=2 \\\r\n--n_epochs=1 --num_candidates=4 --personality_permutations=2 \\\r\n--train_batch_size=2 --valid_batch_size=2\r\n```\r\nand the evaluation results are:\r\n```\r\nFinal Hits@1: 0.737\r\nFINAL F1: 0.1643\r\nFINAL PPL: 178.9\r\n```\r\nThe command I used to run convai_evalution.py is:\r\n```\r\npython convai_evaluation.py --eval_type ppl --model_checkpoint /path/to/finetuned/model\r\n```\r\n\r\nThe ppl of GPT2 is strangely high. \r\n\r\nIs there anything that needs to be modified when testing finetuned-gpt2 with convai_evalution.py?\r\n\r\nI'm also curious about the best test results and hyperparameters when you finetuned from GPT2. Thank you!","closed_by":null,"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/63/timeline","performed_via_github_app":null,"state_reason":null}