{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/92","id":738497331,"node_id":"MDU6SXNzdWU3Mzg0OTczMzE=","number":92,"title":"I can't replicate the training","user":{"login":"albusdemens","id":276459,"node_id":"MDQ6VXNlcjI3NjQ1OQ==","avatar_url":"https://avatars.githubusercontent.com/u/276459?v=4","gravatar_id":"","url":"https://api.github.com/users/albusdemens","html_url":"https://github.com/albusdemens","followers_url":"https://api.github.com/users/albusdemens/followers","following_url":"https://api.github.com/users/albusdemens/following{/other_user}","gists_url":"https://api.github.com/users/albusdemens/gists{/gist_id}","starred_url":"https://api.github.com/users/albusdemens/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/albusdemens/subscriptions","organizations_url":"https://api.github.com/users/albusdemens/orgs","repos_url":"https://api.github.com/users/albusdemens/repos","events_url":"https://api.github.com/users/albusdemens/events{/privacy}","received_events_url":"https://api.github.com/users/albusdemens/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2020-11-08T16:04:03Z","updated_at":"2020-11-08T17:07:42Z","closed_at":"2020-11-08T17:07:42Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, first of all hats off for your work and for the nice blog post! I have an Ubuntu VM with 4 V100s on AWS; if I try to replicate your training (command: `python train.py`) I get the error below. Do you have suggestions on how to fix this?\r\n\r\nHere is the error:\r\n\r\n```\r\nINFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin from cache at /home/ubuntu/.cache/torch/transformers/e45ee1afb14c5d77c946e66cb0fa70073a77882097a1a2cefd51fd24b172355e.e7ee3fcd07c695a4c9f31ca735502c090230d988de03202f7af9ebe1c3a4054c\r\nINFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\r\nINFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\r\nINFO:train.py:Prepare datasets\r\nINFO:/home/ubuntu/transfer-learning-conv-ai/utils.py:Load tokenized dataset from cache at ./dataset_cache_OpenAIGPTTokenizer\r\nINFO:train.py:Build inputs and labels\r\nINFO:train.py:Pad inputs and convert to Tensor\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 267, in <module>\r\n    train()\r\n  File \"train.py\", line 171, in train\r\n    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\r\n  File \"train.py\", line 98, in get_data_loaders\r\n    dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\r\n  File \"train.py\", line 43, in pad_dataset\r\n    max_l = max(len(x) for x in dataset[\"input_ids\"])\r\nValueError: max() arg is an empty sequence\r\n```","closed_by":{"login":"albusdemens","id":276459,"node_id":"MDQ6VXNlcjI3NjQ1OQ==","avatar_url":"https://avatars.githubusercontent.com/u/276459?v=4","gravatar_id":"","url":"https://api.github.com/users/albusdemens","html_url":"https://github.com/albusdemens","followers_url":"https://api.github.com/users/albusdemens/followers","following_url":"https://api.github.com/users/albusdemens/following{/other_user}","gists_url":"https://api.github.com/users/albusdemens/gists{/gist_id}","starred_url":"https://api.github.com/users/albusdemens/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/albusdemens/subscriptions","organizations_url":"https://api.github.com/users/albusdemens/orgs","repos_url":"https://api.github.com/users/albusdemens/repos","events_url":"https://api.github.com/users/albusdemens/events{/privacy}","received_events_url":"https://api.github.com/users/albusdemens/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/92/timeline","performed_via_github_app":null,"state_reason":"completed"}