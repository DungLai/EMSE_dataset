{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/106","id":865941117,"node_id":"MDU6SXNzdWU4NjU5NDExMTc=","number":106,"title":"Question on Next Utterance Classification task","user":{"login":"wsp317","id":30275007,"node_id":"MDQ6VXNlcjMwMjc1MDA3","avatar_url":"https://avatars.githubusercontent.com/u/30275007?v=4","gravatar_id":"","url":"https://api.github.com/users/wsp317","html_url":"https://github.com/wsp317","followers_url":"https://api.github.com/users/wsp317/followers","following_url":"https://api.github.com/users/wsp317/following{/other_user}","gists_url":"https://api.github.com/users/wsp317/gists{/gist_id}","starred_url":"https://api.github.com/users/wsp317/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wsp317/subscriptions","organizations_url":"https://api.github.com/users/wsp317/orgs","repos_url":"https://api.github.com/users/wsp317/repos","events_url":"https://api.github.com/users/wsp317/events{/privacy}","received_events_url":"https://api.github.com/users/wsp317/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-04-23T09:41:38Z","updated_at":"2021-04-23T09:41:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n    In your paper, you said the \"next-utterance classification\" task and \"language modeling\" task were trained in a multi-task learning setting, and also in train.py, there is a function load the persona dataset as described. However, there is a logical gap that I can not fully understand that is you also trained the wrong candidates (i.e. replies) for language modeling task. Can anyone explain this to me? \r\n \r\n```\r\ndef get_data_loaders(args, tokenizer):\r\n    \"\"\" Prepare the dataset for training and evaluation \"\"\"\r\n    personachat = get_dataset(tokenizer, args.dataset_path, args.dataset_cache)\r\n\r\n    logger.info(\"Build inputs and labels\")\r\n    datasets = {\"train\": defaultdict(list), \"valid\": defaultdict(list)}\r\n    for dataset_name, dataset in personachat.items():\r\n        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\r\n        if args.num_candidates > 0 and dataset_name == 'train':\r\n            num_candidates = min(args.num_candidates, num_candidates)\r\n        for dialog in dataset:\r\n            persona = dialog[\"personality\"].copy()\r\n            for _ in range(args.personality_permutations):\r\n                for utterance in dialog[\"utterances\"]:\r\n                    history = utterance[\"history\"][-(2*args.max_history+1):]\r\n                    for j, candidate in enumerate(utterance[\"candidates\"][-num_candidates:]):\r\n                        lm_labels = bool(j == num_candidates-1)\r\n                        instance = build_input_from_segments(persona, history, candidate, tokenizer, lm_labels)\r\n                        for input_name, input_array in instance.items():\r\n                            datasets[dataset_name][input_name].append(input_array)\r\n                    datasets[dataset_name][\"mc_labels\"].append(num_candidates - 1)\r\n                    datasets[dataset_name][\"n_candidates\"] = num_candidates\r\n                persona = [persona[-1]] + persona[:-1]  # permuted personalities\r\n\r\n    logger.info(\"Pad inputs and convert to Tensor\")\r\n    tensor_datasets = {\"train\": [], \"valid\": []}\r\n    for dataset_name, dataset in datasets.items():\r\n        dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\r\n        for input_name in MODEL_INPUTS:\r\n            tensor = torch.tensor(dataset[input_name])\r\n            if input_name != \"mc_labels\":\r\n                tensor = tensor.view((-1, datasets[dataset_name][\"n_candidates\"]) + tensor.shape[1:])\r\n            tensor_datasets[dataset_name].append(tensor)\r\n\r\n    logger.info(\"Build train and validation dataloaders\")\r\n    train_dataset, valid_dataset = TensorDataset(*tensor_datasets[\"train\"]), TensorDataset(*tensor_datasets[\"valid\"])\r\n    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if args.distributed else None\r\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset) if args.distributed else None\r\n    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, shuffle=(not args.distributed))\r\n    valid_loader = DataLoader(valid_dataset, sampler=valid_sampler, batch_size=args.valid_batch_size, shuffle=False)\r\n\r\n    logger.info(\"Train dataset (Batch, Candidates, Seq length): {}\".format(train_dataset.tensors[0].shape))\r\n    logger.info(\"Valid dataset (Batch, Candidates, Seq length): {}\".format(valid_dataset.tensors[0].shape))\r\n    return train_loader, valid_loader, train_sampler, valid_sampler\r\n\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/106/timeline","performed_via_github_app":null,"state_reason":null}