{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/100","id":788387381,"node_id":"MDU6SXNzdWU3ODgzODczODE=","number":100,"title":"Error on GPT2-model: \"generating special token with probability 1\"","user":{"login":"dnns92","id":39763670,"node_id":"MDQ6VXNlcjM5NzYzNjcw","avatar_url":"https://avatars.githubusercontent.com/u/39763670?v=4","gravatar_id":"","url":"https://api.github.com/users/dnns92","html_url":"https://github.com/dnns92","followers_url":"https://api.github.com/users/dnns92/followers","following_url":"https://api.github.com/users/dnns92/following{/other_user}","gists_url":"https://api.github.com/users/dnns92/gists{/gist_id}","starred_url":"https://api.github.com/users/dnns92/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnns92/subscriptions","organizations_url":"https://api.github.com/users/dnns92/orgs","repos_url":"https://api.github.com/users/dnns92/repos","events_url":"https://api.github.com/users/dnns92/events{/privacy}","received_events_url":"https://api.github.com/users/dnns92/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-01-18T16:06:52Z","updated_at":"2021-01-18T16:06:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Using a blank install, I can't get the GPT2 model to work nicely. Maybe someone had the same problems. Seeing that the online bot also does not work may indicate that something is wrong with the pretrained model\r\n\r\nMy Setup:\r\n- Windows 10\r\n- fresh conda env using python=3.6*\r\n- Also tried creating a interpreter in wsl1, which lead to the same error\r\n\r\nBug description:\r\n- When interacting with the Bot, I get this warning, that apparently there is a special token generated with probabilty 1. After that, no interaction gets done. I checked the history list and you can see my word embeddings in the history (its not empty)\r\n \r\n See:\r\n\r\n```\r\n>>> How are you?\r\n\r\nC:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:81: UserWarning: Warning: model generating special token with probability 1.\r\n  warnings.warn(\"Warning: model generating special token with probability 1.\")\r\n>>> How are you now?\r\n\r\n\r\n```\r\n\r\nFull Scrollback:\r\n\r\n```\r\n\r\nC:\\Users\\nano\\.conda\\envs\\convAI\\python.exe -- C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py --model gpt2 --model_checkpoint gpt2\r\n2021-01-18 16:57:56.320089: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-18 16:57:56.320235: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Namespace(dataset_cache='./dataset_cache', dataset_path='', device='cpu', max_history=2, max_length=20, min_length=1, model='gpt2', model_checkpoint='gpt2', no_sample=False, seed=0, temperature=0.7, top_k=0, top_p=0.9)\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Get pretrained model and tokenizer\r\nINFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\r\nINFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\r\nINFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\r\nINFO:transformers.configuration_utils:Model config GPT2Config {\r\n  \"activation_function\": \"gelu_new\",\r\n  \"architectures\": [\r\n    \"GPT2LMHeadModel\"\r\n  ],\r\n  \"attn_pdrop\": 0.1,\r\n  \"bos_token_id\": 50256,\r\n  \"do_sample\": false,\r\n  \"embd_pdrop\": 0.1,\r\n  \"eos_token_id\": 50256,\r\n  \"eos_token_ids\": null,\r\n  \"finetuning_task\": null,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\"\r\n  },\r\n  \"initializer_range\": 0.02,\r\n  \"is_decoder\": false,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1\r\n  },\r\n  \"layer_norm_epsilon\": 1e-05,\r\n  \"length_penalty\": 1.0,\r\n  \"max_length\": 20,\r\n  \"model_type\": \"gpt2\",\r\n  \"n_ctx\": 1024,\r\n  \"n_embd\": 768,\r\n  \"n_head\": 12,\r\n  \"n_layer\": 12,\r\n  \"n_positions\": 1024,\r\n  \"num_beams\": 1,\r\n  \"num_labels\": 2,\r\n  \"num_return_sequences\": 1,\r\n  \"output_attentions\": false,\r\n  \"output_hidden_states\": false,\r\n  \"output_past\": true,\r\n  \"pad_token_id\": null,\r\n  \"pruned_heads\": {},\r\n  \"repetition_penalty\": 1.0,\r\n  \"resid_pdrop\": 0.1,\r\n  \"summary_activation\": null,\r\n  \"summary_first_dropout\": 0.1,\r\n  \"summary_proj_to_labels\": true,\r\n  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n  \"task_specific_params\": {\r\n    \"text-generation\": {\r\n      \"do_sample\": true,\r\n      \"max_length\": 50\r\n    }\r\n  },\r\n  \"temperature\": 1.0,\r\n  \"top_k\": 50,\r\n  \"top_p\": 1.0,\r\n  \"torchscript\": false,\r\n  \"use_bfloat16\": false,\r\n  \"vocab_size\": 50257\r\n}\r\n\r\nINFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\r\nINFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\r\nINFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Sample a personality\r\nINFO:C:\\Users\\nano\\Documents\\repos\\transfer-learning-conv-ai\\utils.py:Load tokenized dataset from cache at ./dataset_cache_GPT2Tokenizer\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Selected personality: i love to drink wine and dance in the moonlight.i remember when nobody had a television.i am very strong for my age.i feel like i might live forever.i am 100 years old.\r\n>>> How are you?\r\n\r\nC:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:81: UserWarning: Warning: model generating special token with probability 1.\r\n  warnings.warn(\"Warning: model generating special token with probability 1.\")\r\n>>> How are you now?\r\n\r\n>>> \r\n\r\n\r\n\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/100/timeline","performed_via_github_app":null,"state_reason":null}