{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/37","id":503138394,"node_id":"MDU6SXNzdWU1MDMxMzgzOTQ=","number":37,"title":"Training with gpt2-large and got ValueError: max() arg is an empty sequence","user":{"login":"GraphGrailAi","id":4690353,"node_id":"MDQ6VXNlcjQ2OTAzNTM=","avatar_url":"https://avatars.githubusercontent.com/u/4690353?v=4","gravatar_id":"","url":"https://api.github.com/users/GraphGrailAi","html_url":"https://github.com/GraphGrailAi","followers_url":"https://api.github.com/users/GraphGrailAi/followers","following_url":"https://api.github.com/users/GraphGrailAi/following{/other_user}","gists_url":"https://api.github.com/users/GraphGrailAi/gists{/gist_id}","starred_url":"https://api.github.com/users/GraphGrailAi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GraphGrailAi/subscriptions","organizations_url":"https://api.github.com/users/GraphGrailAi/orgs","repos_url":"https://api.github.com/users/GraphGrailAi/repos","events_url":"https://api.github.com/users/GraphGrailAi/events{/privacy}","received_events_url":"https://api.github.com/users/GraphGrailAi/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-10-06T18:13:13Z","updated_at":"2020-06-23T08:28:29Z","closed_at":"2019-10-21T18:20:14Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, i have prepared my dataset with 2 personalities my.json (the same with the original 200mb dataset) and tried to start training with parameter `--model=\"gpt2-large\"`, here is output:\r\n\r\n```\r\njoo@joo-tf:~/Документы/LocalRepository/transfer-learning-conv-ai$ python  ./train.py --gradient_accumulation_steps=4 --lm_coef=2.0 --max_history=2 --n_epochs=1 --num_candidates=4 --personality_permutations=2 --train_batch_size=1 --valid_batch_size=1 --dataset_path=\"my.json\" --model=\"gpt2-large\"\r\nWARNING:./train.py:Running process -1\r\nINFO:./train.py:Arguments: Namespace(dataset_cache='./dataset_cache', dataset_path='ze_personality_dataset_forgpt2.json', device='cuda', eval_before_start=False, fp16='', gradient_accumulation_steps=4, lm_coef=2.0, local_rank=-1, lr=6.25e-05, max_history=2, max_norm=1.0, mc_coef=1.0, model_checkpoint='gpt2-large', n_epochs=1, num_candidates=4, personality_permutations=2, train_batch_size=1, valid_batch_size=1)\r\nINFO:./train.py:Prepare tokenizer, pretrained model and optimizer.\r\nINFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json from cache at /home/joo/.cache/torch/pytorch_transformers/69f8d734111f39eaa51a85907bfdc81a7ef42242d638ffab6f77df305402b2b2.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\r\nINFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt from cache at /home/joo/.cache/torch/pytorch_transformers/38d28acc17953e356348dca948e152c653c0ccf5058a552eea30168e27f02046.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\r\nINFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json from cache at /home/joo/.cache/torch/pytorch_transformers/c8f887cdfff4327916f4b7ed06a379c0add42bd9c66e1fe3b4a5a8525a4b2678.bc44facd742477605da5434f20a32607ead98e78fff95c5ca9523e47b453e1ad\r\nINFO:pytorch_transformers.modeling_utils:Model config {\r\n  \"attn_pdrop\": 0.1,\r\n  \"embd_pdrop\": 0.1,\r\n  \"finetuning_task\": null,\r\n  \"initializer_range\": 0.02,\r\n  \"layer_norm_epsilon\": 1e-05,\r\n  \"n_ctx\": 1024,\r\n  \"n_embd\": 1280,\r\n  \"n_head\": 20,\r\n  \"n_layer\": 36,\r\n  \"n_positions\": 1024,\r\n  \"num_labels\": 1,\r\n  \"output_attentions\": false,\r\n  \"output_hidden_states\": false,\r\n  \"pruned_heads\": {},\r\n  \"resid_pdrop\": 0.1,\r\n  \"summary_activation\": null,\r\n  \"summary_first_dropout\": 0.1,\r\n  \"summary_proj_to_labels\": true,\r\n  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n  \"torchscript\": false,\r\n  \"vocab_size\": 50257\r\n}\r\n\r\nINFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin from cache at /home/joo/.cache/torch/pytorch_transformers/bcc61dff8b1b03d0fd33a1eb1dc4db00875cae33296848155c6882d4bab03db4.999a50942f8e31ea6fa89ec2580cb38fa40e3db5aa46102d0406bcfa77d9142d\r\nINFO:pytorch_transformers.tokenization_utils:Adding <bos> to the vocabulary\r\nINFO:pytorch_transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\r\nINFO:pytorch_transformers.tokenization_utils:Adding <eos> to the vocabulary\r\nINFO:pytorch_transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\r\nINFO:pytorch_transformers.tokenization_utils:Adding <pad> to the vocabulary\r\nINFO:pytorch_transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\r\nINFO:pytorch_transformers.tokenization_utils:Adding <speaker1> to the vocabulary\r\nINFO:pytorch_transformers.tokenization_utils:Adding <speaker2> to the vocabulary\r\nINFO:pytorch_transformers.tokenization_utils:Assigning ('<speaker1>', '<speaker2>') to the additional_special_tokens key of the tokenizer\r\nINFO:./train.py:Prepare datasets\r\nINFO:/home/joo/Документы/LocalRepository/transfer-learning-conv-ai/utils.py:Load tokenized dataset from cache at ./dataset_cache_GPT2Tokenizer\r\nINFO:./train.py:Build inputs and labels\r\nINFO:./train.py:Pad inputs and convert to Tensor\r\nTraceback (most recent call last):\r\n  File \"./train.py\", line 271, in <module>\r\n    train()\r\n  File \"./train.py\", line 175, in train\r\n    train_loader, val_loader, train_sampler, valid_sampler = get_data_loaders(args, tokenizer)\r\n  File \"./train.py\", line 102, in get_data_loaders\r\n    dataset = pad_dataset(dataset, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1]))\r\n  File \"./train.py\", line 43, in pad_dataset\r\n    max_l = max(len(x) for x in dataset[\"input_ids\"])\r\nValueError: max() arg is an empty sequence\r\n```\r\n\r\nI checked other issues, but no similar","closed_by":{"login":"sshleifer","id":6045025,"node_id":"MDQ6VXNlcjYwNDUwMjU=","avatar_url":"https://avatars.githubusercontent.com/u/6045025?v=4","gravatar_id":"","url":"https://api.github.com/users/sshleifer","html_url":"https://github.com/sshleifer","followers_url":"https://api.github.com/users/sshleifer/followers","following_url":"https://api.github.com/users/sshleifer/following{/other_user}","gists_url":"https://api.github.com/users/sshleifer/gists{/gist_id}","starred_url":"https://api.github.com/users/sshleifer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sshleifer/subscriptions","organizations_url":"https://api.github.com/users/sshleifer/orgs","repos_url":"https://api.github.com/users/sshleifer/repos","events_url":"https://api.github.com/users/sshleifer/events{/privacy}","received_events_url":"https://api.github.com/users/sshleifer/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/37/timeline","performed_via_github_app":null,"state_reason":"completed"}