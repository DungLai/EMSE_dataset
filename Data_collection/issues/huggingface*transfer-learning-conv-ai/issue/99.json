{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99","repository_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai","labels_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99/labels{/name}","comments_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99/comments","events_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99/events","html_url":"https://github.com/huggingface/transfer-learning-conv-ai/issues/99","id":787766570,"node_id":"MDU6SXNzdWU3ODc3NjY1NzA=","number":99,"title":"dataset tokenization returns None values","user":{"login":"dnns92","id":39763670,"node_id":"MDQ6VXNlcjM5NzYzNjcw","avatar_url":"https://avatars.githubusercontent.com/u/39763670?v=4","gravatar_id":"","url":"https://api.github.com/users/dnns92","html_url":"https://github.com/dnns92","followers_url":"https://api.github.com/users/dnns92/followers","following_url":"https://api.github.com/users/dnns92/following{/other_user}","gists_url":"https://api.github.com/users/dnns92/gists{/gist_id}","starred_url":"https://api.github.com/users/dnns92/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnns92/subscriptions","organizations_url":"https://api.github.com/users/dnns92/orgs","repos_url":"https://api.github.com/users/dnns92/repos","events_url":"https://api.github.com/users/dnns92/events{/privacy}","received_events_url":"https://api.github.com/users/dnns92/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-01-17T18:33:58Z","updated_at":"2021-01-18T15:30:48Z","closed_at":"2021-01-18T15:30:48Z","author_association":"NONE","active_lock_reason":null,"body":"My System:\r\n- win10\r\n- rtx 2060\r\n\r\nWhat happened?\r\n- Using GPT works fine. However using gpt2 results in weird returns of the get_dataset-function. dataset gets set as:\r\n\r\n`dataset[\"train\"][0] = {'personality': [[11, None, 14594, None, 571, None, 30678, None, 5279, 14, None, 1], [11, None, 14594, None, 571, None, 581, None, 1108, 4407, None, 1], [11, None, 14594, None, 571, None, 3895, 3 ..`\r\n\r\nwhich is not supposed to happen I guess. Someone else having the same errors?\r\n\r\nWhat I tried so far:\r\n\r\nI tried to play around with the arguments a bit, having no luck so far. \r\n\r\n\r\nFull Error:\r\n```\r\n  File \"C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py\", line 157, in <module>\r\n    run()\r\n  File \"C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py\", line 139, in run\r\n    logger.info(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\r\n  File \"C:\\Users\\nano\\.conda\\envs\\convAI\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 1528, in decode\r\n    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\r\n  File \"C:\\Users\\nano\\.conda\\envs\\convAI\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 1498, in convert_ids_to_tokens\r\n    index = int(index)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n```\r\n\r\nFull Scrollback:\r\n\r\n```\r\nC:\\Users\\nano\\.conda\\envs\\convAI\\python.exe -- C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py --model gpt2 --model_checkpoint gpt2\r\n2021-01-17 19:24:14.047383: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\r\n2021-01-17 19:24:14.047530: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Namespace(dataset_cache='./dataset_cache', dataset_path='', device='cpu', max_history=2, max_length=20, min_length=1, model='gpt2', model_checkpoint='gpt2', no_sample=False, seed=0, temperature=0.7, top_k=0, top_p=0.9)\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Get pretrained model and tokenizer\r\nINFO:filelock:Lock 2441686875720 acquired on C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\r\nINFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to C:\\Users\\nano\\.cache\\torch\\transformers\\tmp07zony6g\r\nDownloading: 100%|██████████| 1.04M/1.04M [00:01<00:00, 975kB/s]\r\nINFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\r\nINFO:transformers.file_utils:creating metadata file for C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\r\nINFO:filelock:Lock 2441686875720 released on C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\r\nINFO:filelock:Lock 2441686874320 acquired on C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\r\nINFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to C:\\Users\\nano\\.cache\\torch\\transformers\\tmp_mcocmdo\r\nDownloading: 100%|██████████| 456k/456k [00:00<00:00, 653kB/s]\r\nINFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\r\nINFO:transformers.file_utils:creating metadata file for C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\r\nINFO:filelock:Lock 2441686874320 released on C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\r\nINFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\r\nINFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\r\nINFO:filelock:Lock 2441686874992 acquired on C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\r\nINFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to C:\\Users\\nano\\.cache\\torch\\transformers\\tmp0c9u5l10\r\nDownloading: 100%|██████████| 665/665 [00:00<00:00, 626kB/s]\r\nINFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\r\nINFO:transformers.file_utils:creating metadata file for C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\r\nINFO:filelock:Lock 2441686874992 released on C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e.lock\r\nINFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\r\nINFO:transformers.configuration_utils:Model config GPT2Config {\r\n  \"activation_function\": \"gelu_new\",\r\n  \"architectures\": [\r\n    \"GPT2LMHeadModel\"\r\n  ],\r\n  \"attn_pdrop\": 0.1,\r\n  \"bos_token_id\": 50256,\r\n  \"do_sample\": false,\r\n  \"embd_pdrop\": 0.1,\r\n  \"eos_token_id\": 50256,\r\n  \"eos_token_ids\": null,\r\n  \"finetuning_task\": null,\r\n  \"id2label\": {\r\n    \"0\": \"LABEL_0\",\r\n    \"1\": \"LABEL_1\"\r\n  },\r\n  \"initializer_range\": 0.02,\r\n  \"is_decoder\": false,\r\n  \"label2id\": {\r\n    \"LABEL_0\": 0,\r\n    \"LABEL_1\": 1\r\n  },\r\n  \"layer_norm_epsilon\": 1e-05,\r\n  \"length_penalty\": 1.0,\r\n  \"max_length\": 20,\r\n  \"model_type\": \"gpt2\",\r\n  \"n_ctx\": 1024,\r\n  \"n_embd\": 768,\r\n  \"n_head\": 12,\r\n  \"n_layer\": 12,\r\n  \"n_positions\": 1024,\r\n  \"num_beams\": 1,\r\n  \"num_labels\": 2,\r\n  \"num_return_sequences\": 1,\r\n  \"output_attentions\": false,\r\n  \"output_hidden_states\": false,\r\n  \"output_past\": true,\r\n  \"pad_token_id\": null,\r\n  \"pruned_heads\": {},\r\n  \"repetition_penalty\": 1.0,\r\n  \"resid_pdrop\": 0.1,\r\n  \"summary_activation\": null,\r\n  \"summary_first_dropout\": 0.1,\r\n  \"summary_proj_to_labels\": true,\r\n  \"summary_type\": \"cls_index\",\r\n  \"summary_use_proj\": true,\r\n  \"task_specific_params\": {\r\n    \"text-generation\": {\r\n      \"do_sample\": true,\r\n      \"max_length\": 50\r\n    }\r\n  },\r\n  \"temperature\": 1.0,\r\n  \"top_k\": 50,\r\n  \"top_p\": 1.0,\r\n  \"torchscript\": false,\r\n  \"use_bfloat16\": false,\r\n  \"vocab_size\": 50257\r\n}\r\n\r\nINFO:filelock:Lock 2440366262592 acquired on C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\r\nINFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\nano\\.cache\\torch\\transformers\\tmpf1zbtwlp\r\nDownloading: 100%|██████████| 548M/548M [00:30<00:00, 17.8MB/s]\r\nINFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\r\nINFO:transformers.file_utils:creating metadata file for C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\r\nINFO:filelock:Lock 2440366262592 released on C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\r\nINFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at C:\\Users\\nano\\.cache\\torch\\transformers\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\r\nINFO:transformers.tokenization_utils:Adding <bos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <bos> to the bos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <eos> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <eos> to the eos_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <pad> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning <pad> to the pad_token key of the tokenizer\r\nINFO:transformers.tokenization_utils:Adding <speaker1> to the vocabulary\r\nINFO:transformers.tokenization_utils:Adding <speaker2> to the vocabulary\r\nINFO:transformers.tokenization_utils:Assigning ['<speaker1>', '<speaker2>'] to the additional_special_tokens key of the tokenizer\r\nINFO:C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py:Sample a personality\r\nINFO:C:\\Users\\nano\\Documents\\repos\\transfer-learning-conv-ai\\utils.py:Load tokenized dataset from cache at ./dataset_cache_GPT2Tokenizer\r\nTraceback (most recent call last):\r\n  File \"C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py\", line 157, in <module>\r\n    run()\r\n  File \"C:/Users/nano/Documents/repos/transfer-learning-conv-ai/interact.py\", line 139, in run\r\n    logger.info(\"Selected personality: %s\", tokenizer.decode(chain(*personality)))\r\n  File \"C:\\Users\\nano\\.conda\\envs\\convAI\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 1528, in decode\r\n    filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\r\n  File \"C:\\Users\\nano\\.conda\\envs\\convAI\\lib\\site-packages\\transformers\\tokenization_utils.py\", line 1498, in convert_ids_to_tokens\r\n    index = int(index)\r\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'\r\n\r\n```","closed_by":{"login":"dnns92","id":39763670,"node_id":"MDQ6VXNlcjM5NzYzNjcw","avatar_url":"https://avatars.githubusercontent.com/u/39763670?v=4","gravatar_id":"","url":"https://api.github.com/users/dnns92","html_url":"https://github.com/dnns92","followers_url":"https://api.github.com/users/dnns92/followers","following_url":"https://api.github.com/users/dnns92/following{/other_user}","gists_url":"https://api.github.com/users/dnns92/gists{/gist_id}","starred_url":"https://api.github.com/users/dnns92/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dnns92/subscriptions","organizations_url":"https://api.github.com/users/dnns92/orgs","repos_url":"https://api.github.com/users/dnns92/repos","events_url":"https://api.github.com/users/dnns92/events{/privacy}","received_events_url":"https://api.github.com/users/dnns92/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/huggingface/transfer-learning-conv-ai/issues/99/timeline","performed_via_github_app":null,"state_reason":"completed"}