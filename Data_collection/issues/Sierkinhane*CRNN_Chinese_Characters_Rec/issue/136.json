{"url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136","repository_url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec","labels_url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136/labels{/name}","comments_url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136/comments","events_url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136/events","html_url":"https://github.com/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136","id":483173924,"node_id":"MDU6SXNzdWU0ODMxNzM5MjQ=","number":136,"title":"用作者的模型在360万数据集的测试集的36万多张图片上测试，val accuracy只有78.4%","user":{"login":"Cocoalate","id":48081275,"node_id":"MDQ6VXNlcjQ4MDgxMjc1","avatar_url":"https://avatars.githubusercontent.com/u/48081275?v=4","gravatar_id":"","url":"https://api.github.com/users/Cocoalate","html_url":"https://github.com/Cocoalate","followers_url":"https://api.github.com/users/Cocoalate/followers","following_url":"https://api.github.com/users/Cocoalate/following{/other_user}","gists_url":"https://api.github.com/users/Cocoalate/gists{/gist_id}","starred_url":"https://api.github.com/users/Cocoalate/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Cocoalate/subscriptions","organizations_url":"https://api.github.com/users/Cocoalate/orgs","repos_url":"https://api.github.com/users/Cocoalate/repos","events_url":"https://api.github.com/users/Cocoalate/events{/privacy}","received_events_url":"https://api.github.com/users/Cocoalate/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-08-21T03:00:53Z","updated_at":"2020-10-26T01:50:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"由于作者的代码train和validation是在一起的，我把validation的部分提出来单独用作者训练好的模型对360万数据集的36万多张图片做validation，但是效果并不好,val accuracy只有78.4%\r\n![image](https://user-images.githubusercontent.com/48081275/63398740-1520e700-c401-11e9-84ae-61528f906cb2.png)\r\n作者说的验证准确率可以finetune到97.7%是指我需要在mixed_second_finetune_acc97p7.pth模型的基础行再进行finetune才能val accuracy才能从78.4%变成97.7%么？\r\n或者是我validation出了问题？\r\n```\r\nfrom __future__ import print_function\r\nfrom torch.utils.data import DataLoader\r\nimport argparse\r\nimport random\r\nimport torch\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.optim as optim\r\nimport torch.utils.data\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\n# from warpctc_pytorch import CTCLoss\r\nimport os\r\nimport utils\r\n# import dataset\r\nimport models.crnn as crnn\r\nimport re\r\nimport params\r\nfrom dataset_v2 import baiduDataset\r\n\r\n# def init_args():\r\n#     args = argparse.ArgumentParser()\r\n#     args.add_argument('--trainroot', help='path to dataset', default='./to_lmdb/train')\r\n#     args.add_argument('--valroot', help='path to dataset', default='./to_lmdb/train')\r\n#     args.add_argument('--cuda', action='store_true', help='enables cuda', default=False)\r\n\r\n#     return args.parse_args()\r\n\r\n# custom weights initialization called on crnn\r\n\r\ndef weights_init(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find('Conv') != -1:\r\n        m.weight.data.normal_(0.0, 0.02)\r\n    elif classname.find('BatchNorm') != -1:\r\n        m.weight.data.normal_(1.0, 0.02)\r\n        m.bias.data.fill_(0)\r\n\r\ndef val(net, val_dataset, criterion):\r\n    val_loader = DataLoader(val_dataset, batch_size=params.val_batchSize, shuffle=True, num_workers=params.workers)\r\n    print('Start val')\r\n    for p in crnn.parameters():\r\n        p.requires_grad = False\r\n    net.eval()\r\n    i = 0\r\n    n_correct = 0\r\n    loss_avg = utils.averager()\r\n\r\n    for i_batch, (image, index) in enumerate(val_loader):\r\n        image = image.to(device)\r\n        label = utils.get_batch_label(val_dataset, index)\r\n        preds = crnn(image)\r\n        batch_size = image.size(0)\r\n        index = np.array(index.data.numpy())\r\n        text, length = converter.encode(label)\r\n        preds_size = Variable(torch.IntTensor([preds.size(0)] * batch_size))\r\n        cost = criterion(preds, text, preds_size, length) / batch_size\r\n        loss_avg.add(cost)\r\n        _, preds = preds.max(2)\r\n        preds = preds.transpose(1, 0).contiguous().view(-1)\r\n        sim_preds = converter.decode(preds.data, preds_size.data, raw=False)\r\n        \r\n        for pred, target in zip(sim_preds, label):\r\n            if pred == target:\r\n                n_correct += 1\r\n        if (i_batch+1)%1000 == 0:\r\n            print('[%d/%d]' %\r\n                      (i_batch, len(val_loader)))\r\n        # if i_batch == max_i:\r\n        #    break\r\n    raw_preds = converter.decode(preds.data, preds_size.data, raw=True)[:params.n_test_disp]\r\n    for raw_pred, pred, gt in zip(raw_preds, sim_preds, label):\r\n        print('%-20s => %-20s, gt: %-20s' % (raw_pred, pred, gt))\r\n    print(n_correct)\r\n    print(len(val_dataset))\r\n    accuracy = n_correct / float(len(val_dataset))\r\n    print('Test loss: %f, accuray: %f' % (loss_avg.val(), accuracy))\r\n    return accuracy\r\n\r\ndef main(crnn, val_dataset, criterion):\r\n    crnn = crnn.to(device)\r\n    criterion = criterion.to(device)\r\n    accuracy = val(crnn, val_dataset, criterion)\r\n    return accuracy\r\n\r\nif __name__ == '__main__':\r\n    manualSeed=10\r\n    random.seed(manualSeed)\r\n    np.random.seed(manualSeed)\r\n    torch.manual_seed(manualSeed)\r\n    cudnn.benchmark = True\r\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n    # alphabet = alphabet = utils.to_alphabet(\"H:/DL-DATASET/BaiduTextR/train.list\")\r\n\r\n    # store model path\r\n    if not os.path.exists('./expr'):\r\n        os.mkdir('./expr')\r\n    # read train set\r\n    val_dataset = baiduDataset(\"./rar/images\", \"./crnn_labels/test.txt.bk\", params.alphabet, False, (params.imgW, params.imgH))\r\n    # val_dataset = baiduDataset(\"/mnt/data/ocr/car_plates/val/CCPD2\", \"./CCPD2_label.txt\", params.alphabet, False, (params.imgW, params.imgH))\r\n    # val_dataset = baiduDataset(\"/mnt/data/ocr/car_plates/val/white\", \"./white_label.txt\", params.alphabet, False, (params.imgW, params.imgH))\r\n    converter = utils.strLabelConverter(val_dataset.alphabet)\r\n    nclass = len(params.alphabet) + 1\r\n    nc = 1\r\n    # TODO why not mean\r\n    criterion = torch.nn.CTCLoss(reduction='sum')\r\n    # cnn and rnn\r\n    crnn = crnn.CRNN(32, nc, nclass, params.nh)\r\n    crnn.apply(weights_init)\r\n    crnn.load_state_dict(torch.load(\"/home/keke/crnn_chinese_characters_rec/trained_models/mixed_second_finetune_acc97p7.pth\"))\r\n    # crnn.load_state_dict(torch.load(\"/home/keke/Lets_OCR/recognizer/crnn/w160_bs64_model/netCRNN_4_48000.pth\"))\r\n    if params.crnn != '':\r\n        print('loading pretrained model from %s' % params.crnn)\r\n        crnn.load_state_dict(torch.load(params.crnn))\r\n    \r\n    main(crnn, val_dataset, criterion)\r\n```\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/Sierkinhane/CRNN_Chinese_Characters_Rec/issues/136/timeline","performed_via_github_app":null,"state_reason":null}