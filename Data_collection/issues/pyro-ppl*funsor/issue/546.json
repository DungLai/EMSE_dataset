{"url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546","repository_url":"https://api.github.com/repos/pyro-ppl/funsor","labels_url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546/labels{/name}","comments_url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546/comments","events_url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546/events","html_url":"https://github.com/pyro-ppl/funsor/issues/546","id":959203591,"node_id":"MDU6SXNzdWU5NTkyMDM1OTE=","number":546,"title":"Add array protocol dispatch methods to top-level Funsor class","user":{"login":"eb8680","id":2032320,"node_id":"MDQ6VXNlcjIwMzIzMjA=","avatar_url":"https://avatars.githubusercontent.com/u/2032320?v=4","gravatar_id":"","url":"https://api.github.com/users/eb8680","html_url":"https://github.com/eb8680","followers_url":"https://api.github.com/users/eb8680/followers","following_url":"https://api.github.com/users/eb8680/following{/other_user}","gists_url":"https://api.github.com/users/eb8680/gists{/gist_id}","starred_url":"https://api.github.com/users/eb8680/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eb8680/subscriptions","organizations_url":"https://api.github.com/users/eb8680/orgs","repos_url":"https://api.github.com/users/eb8680/repos","events_url":"https://api.github.com/users/eb8680/events{/privacy}","received_events_url":"https://api.github.com/users/eb8680/received_events","type":"User","site_admin":false},"labels":[{"id":1262045842,"node_id":"MDU6TGFiZWwxMjYyMDQ1ODQy","url":"https://api.github.com/repos/pyro-ppl/funsor/labels/discussion","name":"discussion","color":"fef2c0","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-03T14:55:27Z","updated_at":"2021-08-04T00:55:02Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Now that PyTorch supports tensor subtyping and function overloading with `__torch_function__`, should we add `__array_function__` and `__torch_function__` methods to `funsor.terms.Funsor` to allow evaluation of (some) PyTorch/Numpy code on Funsors?\r\n\r\nHere is the meat of a `Funsor.__torch_function__` implementation, modulo handling of edge cases; `__array_function__` for the Numpy backend would be very similar:\r\n```py\r\nclass Funsor:\r\n    ...\r\n    def __torch_function__(self, func, types, args=(), kwargs=None):\r\n        # exploit our op registry: ops should know how to handle and convert their arguments\r\n        try:\r\n            op = getattr(funsor.ops, func.__name__)\r\n        except AttributeError:\r\n            op = funsor.ops.make_op(func). # handle e.g. nn.Module or dist.Transform instances\r\n        return op(*args, **kwargs)\r\n```\r\nThe motivating application is as a much simpler and more general alternative to the dimension tracking via effectful `to_data`/`to_funsor` primitives in `pyro.contrib.funsor`, which is somewhat confusing. This would also simplify @ordabayevy's work in #543 and elsewhere by removing the need for special `torch.Tensor` subclasses that duplicate Funsor broadcasting semantics.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/pyro-ppl/funsor/issues/546/timeline","performed_via_github_app":null,"state_reason":null}