{"url":"https://api.github.com/repos/netrack/keras-metrics/issues/45","repository_url":"https://api.github.com/repos/netrack/keras-metrics","labels_url":"https://api.github.com/repos/netrack/keras-metrics/issues/45/labels{/name}","comments_url":"https://api.github.com/repos/netrack/keras-metrics/issues/45/comments","events_url":"https://api.github.com/repos/netrack/keras-metrics/issues/45/events","html_url":"https://github.com/netrack/keras-metrics/issues/45","id":487514199,"node_id":"MDU6SXNzdWU0ODc1MTQxOTk=","number":45,"title":"Discrepancy between keras-metrics and scikit-learn","user":{"login":"david-b-6","id":54715380,"node_id":"MDQ6VXNlcjU0NzE1Mzgw","avatar_url":"https://avatars.githubusercontent.com/u/54715380?v=4","gravatar_id":"","url":"https://api.github.com/users/david-b-6","html_url":"https://github.com/david-b-6","followers_url":"https://api.github.com/users/david-b-6/followers","following_url":"https://api.github.com/users/david-b-6/following{/other_user}","gists_url":"https://api.github.com/users/david-b-6/gists{/gist_id}","starred_url":"https://api.github.com/users/david-b-6/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/david-b-6/subscriptions","organizations_url":"https://api.github.com/users/david-b-6/orgs","repos_url":"https://api.github.com/users/david-b-6/repos","events_url":"https://api.github.com/users/david-b-6/events{/privacy}","received_events_url":"https://api.github.com/users/david-b-6/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-08-30T14:29:36Z","updated_at":"2019-09-02T09:08:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi all,\r\n\r\nWondering if you might be able to shed some light on what's going on here. Is this a bug? Thanks.\r\n\r\nI'm using:\r\ntensorflow gpu 1.13.1\r\nkeras 2.2.4 (very latest pip installed form github repo)\r\nkeras-metrics 1.1.0\r\nnumpy 1.16.4\r\nscikit-learn 0.21.2\r\n\r\n\r\nHere's the situation...\r\n\r\nI'm training a ResNet on a multiclass problem (seven classes total). I'm trying to track the precision, recall and F1 for each class at each epoch. If I compare the validation set output from the last epoch with the values in that scikit learn calculates in its classification report after calling predict, they are vastly different.\r\n\r\nFor example, after 3 epochs the precision, recall and F1 of each class in the validation set is:\r\n\r\nval_precision: 0.5000\r\nval_precision_1: 0.3333\r\nval_precision_2: 0.6000\r\nval_precision_3: 0.3333\r\nval_precision_4: 0.5641\r\nval_precision_5: 0.8972\r\nval_precision_6: 0.3500\r\n\r\nval_recall: 0.0312\r\nval_recall_1: 0.0196\r\nval_recall_2: 0.0275\r\nval_recall_3: 0.0909\r\nval_recall_4: 0.1982\r\nval_recall_5: 0.8075\r\nval_recall_6: 0.5000\r\n\r\nval_f1_score: 0.0588\r\nval_f1_score_1: 0.0370\r\nval_f1_score_2: 0.0526\r\nval_f1_score_3: 0.1429\r\nval_f1_score_4: 0.2933\r\nval_f1_score_5: 0.8500\r\nval_f1_score_6: 0.4118\r\n\r\nBut the scikit-learns confusion matrix and classification report shows:\r\n\r\nConfusion matrix\r\n[[  0   0  28   0   4   0   0]\r\n [  0   0  44   0   7   0   0]\r\n [  0   0 102   0   7   0   0]\r\n [  0   0  11   0   0   0   0]\r\n [  0   0  99   0  12   0   0]\r\n [  0   0 657   0  13   0   0]\r\n [  0   0  14   0   0   0   0]]\r\n\r\nClassification Report\r\n           precision    recall  f1-score   support\r\n\r\n           0       0.00      0.00      0.00        32\r\n           1       0.00      0.00      0.00        51\r\n           2       0.11      0.94      0.19       109\r\n           3       0.00      0.00      0.00        11\r\n           4       0.28      0.11      0.16       111\r\n           5       0.00      0.00      0.00       670\r\n           6       0.00      0.00      0.00        14\r\n\r\n        accuracy                                0.11       998\r\n     macro avg       0.06      0.15      0.05       998\r\nweighted avg       0.04      0.11      0.04       998\r\n\r\n\r\nHere's my code:\r\n\r\n```py\r\nimport numpy as np\r\nnp.random.seed(1)\r\n\r\nimport tensorflow as tf\r\ntf.set_random_seed(1)\r\n\r\nimport random as rn\r\nrn.seed(1)\r\n\r\nimport keras\r\nfrom keras import layers, models, optimizers\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\nfrom sklearn.metrics import confusion_matrix, classification_report\r\nfrom keras_applications.resnet import ResNet50\r\nfrom math import ceil\r\nimport keras_metrics as km\r\n\r\n\r\n\r\ntrain_images = np.load('path to tensor')\r\ntrain_labels = np.load('path to tensor')\r\n\r\nvalidation_images = np.load('path to tensor')\r\nvalidation_labels = np.load('path to tensor')\r\n\r\ninput_height = 150\r\ninput_width = 150\r\ninput_depth = 3\r\n\r\nnum_train_images = len(train_images)\r\nnum_validation_images = len(validation_images)\r\n\r\n\r\nsteps_per_epoch = ceil(num_train_images / 32)\r\nvalidation_steps = ceil(num_validation_images / 32)\r\n\r\n\r\ntrain_labels = keras.utils.to_categorical(train_labels, 7)\r\nvalidation_labels = keras.utils.to_categorical(validation_labels, 7)\r\n\r\n\r\ntrain_datagen = ImageDataGenerator(rescale=1./255,\r\n                                   dtype='float32')\r\n\r\n\r\nval_datagen = ImageDataGenerator(rescale=1./255,\r\n                                 dtype='float32')\r\n\r\ntrain_datagen.fit(train_images)\r\nval_datagen.fit(validation_images)\r\n\r\n\r\ntrain_generator = train_datagen.flow(train_images,\r\n                                     train_labels,\r\n                                     batch_size=32)\r\n\r\nvalidation_generator = val_datagen.flow(validation_images,\r\n                                               validation_labels,\r\n                                               batch_size=32)\r\n\r\npretrained = ResNet50(weights='imagenet',\r\n                     backend=keras.backend,\r\n                     layers=keras.layers,\r\n                     models=keras.models,\r\n                     utils=keras.utils,\r\n                     include_top=False,\r\n                     input_shape=(input_height, input_width, input_depth))\r\n\r\n\r\nmodel = models.Sequential()\r\nmodel.add(pretrained)\r\nmodel.add(layers.Flatten())\r\nmodel.add(layers.Dense(128, activation='relu'))\r\nmodel.add(layers.Dense(7, activation='softmax'))\r\n\r\n\r\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.00001),\r\n                         loss='categorical_crossentropy',\r\n                        metrics=['categorical_accuracy',\r\n                       km.categorical_precision(label=0),\r\n                       km.categorical_precision(label=1),\r\n                       km.categorical_precision(label=2),\r\n                       km.categorical_precision(label=3),\r\n                       km.categorical_precision(label=4),\r\n                       km.categorical_precision(label=5),\r\n                       km.categorical_precision(label=6),\r\n                       km.categorical_recall(label=0),\r\n                       km.categorical_recall(label=1),\r\n                       km.categorical_recall(label=2),\r\n                       km.categorical_recall(label=3),\r\n                       km.categorical_recall(label=4),\r\n                       km.categorical_recall(label=5),\r\n                       km.categorical_recall(label=6),\r\n                       km.categorical_f1_score(label=0),\r\n                       km.categorical_f1_score(label=1),\r\n                       km.categorical_f1_score(label=2),\r\n                       km.categorical_f1_score(label=3),\r\n                       km.categorical_f1_score(label=4),\r\n                       km.categorical_f1_score(label=5),\r\n                       km.categorical_f1_score(label=6)\r\n                       ])\r\n\r\nwith tf.Session() as s:\r\n    s.run(tf.global_variables_initializer())\r\n    history = model.fit_generator(train_generator,\r\n                              steps_per_epoch=steps_per_epoch,\r\n                              epochs=3,\r\n                              validation_data=validation_generator,\r\n                              validation_steps=validation_steps,\r\n                              shuffle=True,\r\n                              verbose=1)\r\n\r\n    predictions = model.predict(validation_images)\r\n\r\n    predicted_classes = np.argmax(predictions, axis=1)\r\n\r\n    validation_labels = np.argmax(validation_labels, axis=1)\r\n\r\n    c_matrix = confusion_matrix(validation_labels, predicted_classes)\r\n    print(c_matrix)\r\n\r\n    report = classification_report(validation_labels, predicted_classes)\r\n    print(report)\r\n```","closed_by":null,"reactions":{"url":"https://api.github.com/repos/netrack/keras-metrics/issues/45/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/netrack/keras-metrics/issues/45/timeline","performed_via_github_app":null,"state_reason":null}