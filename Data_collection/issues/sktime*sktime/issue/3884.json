{"url":"https://api.github.com/repos/sktime/sktime/issues/3884","repository_url":"https://api.github.com/repos/sktime/sktime","labels_url":"https://api.github.com/repos/sktime/sktime/issues/3884/labels{/name}","comments_url":"https://api.github.com/repos/sktime/sktime/issues/3884/comments","events_url":"https://api.github.com/repos/sktime/sktime/issues/3884/events","html_url":"https://github.com/sktime/sktime/issues/3884","id":1475190006,"node_id":"I_kwDOCVKAsc5X7Zz2","number":3884,"title":"[ENH] evaluate should accept ensembler separate from forecasters","user":{"login":"RNKuhns","id":26907244,"node_id":"MDQ6VXNlcjI2OTA3MjQ0","avatar_url":"https://avatars.githubusercontent.com/u/26907244?v=4","gravatar_id":"","url":"https://api.github.com/users/RNKuhns","html_url":"https://github.com/RNKuhns","followers_url":"https://api.github.com/users/RNKuhns/followers","following_url":"https://api.github.com/users/RNKuhns/following{/other_user}","gists_url":"https://api.github.com/users/RNKuhns/gists{/gist_id}","starred_url":"https://api.github.com/users/RNKuhns/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RNKuhns/subscriptions","organizations_url":"https://api.github.com/users/RNKuhns/orgs","repos_url":"https://api.github.com/users/RNKuhns/repos","events_url":"https://api.github.com/users/RNKuhns/events{/privacy}","received_events_url":"https://api.github.com/users/RNKuhns/received_events","type":"User","site_admin":false},"labels":[{"id":3105906374,"node_id":"MDU6TGFiZWwzMTA1OTA2Mzc0","url":"https://api.github.com/repos/sktime/sktime/labels/module:forecasting","name":"module:forecasting","color":"35FCCE","default":false,"description":""},{"id":3796180314,"node_id":"LA_kwDOCVKAsc7iRR1a","url":"https://api.github.com/repos/sktime/sktime/labels/enhancement","name":"enhancement","color":"fef2c0","default":true,"description":"Adding new functionality"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2022-12-04T21:13:45Z","updated_at":"2022-12-05T00:21:58Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\n\nIn the forecasting module, the evaluate function currently lets a user use timeseries cross-validation to compare different forecasters.\n\nThis can be individual forecasters or an ensemble forecaster. It is common place in forecasting that appropriately defined ensembles of forecasters produce better forecasts than individual forecasters. The evaluate function currently allows this comparison to be made by passing an ensemble forecaster alongside the individual forecasters to be compared against.\n\nHowever, this is inefficient in common place forecasting evaluation use cases. For example,  fitting several forecasters and then comparing the results amongst the forecasters and different ensembles of the forecasters. In a simple case, where the forecasters are compared to a single ensemble, this would currently result in the forecasters each being fit and predicted twice for each evaluation window (once for each forecaster individually and once in the ensemble forecaster). If there are multiple ensembles being compared the level of duplication grows. This can also be \"slow\" in the simple case if any of the individual forecasters are \"slow\" to fit/predict or there are many evaluation windows for refitting. \n\n\n\n**Describe the solution you'd like**\n\nsktime should provide the ability to ensemble forecasts without fitting the underlying models (e.g. given some forecasts, apply the ensembler to the forecasts rather than accepting the training data, fitting the models, making the forecasts and then ensembling them).\n\nThe evaluate function should make use of this functionality by including a parameter that accepts an ensembler or list of ensemblers. The evaluate function should use this to provide results for individual forecasters and the specified ensembles (in a given evaluation window the results of the individual forecasters could be temporarily stored for use by provided ensemblers) to let users more efficiently perform commonplace forecast evaluation (in common practice, users should almost always be comparing their forecasts to a challenger model or models, and also comparing if combinations of the model forecasts are better than the individual models).\n\n**Describe alternatives you've considered**\nThe alternative is the status quo, but as currently constituted, comparing ensembles of forecasters involves inefficiency in performing a standard forecast evaluation.\n\n**Additional context**\nI'd propose adding a method to each ensemble forecasters that lets the ensembling be applied to external forecasts. For current ensemblers there would need to be a refactor  to separate the application of the ensembling approach from the fitting/predicting of underlying models (into a method also used by the ensemble if a user calls fit/predict on the ensembler). But this seems like an easier way to re-use code than something like separating this out into a separate class/function.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/sktime/sktime/issues/3884/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/sktime/sktime/issues/3884/timeline","performed_via_github_app":null,"state_reason":null}