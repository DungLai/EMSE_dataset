{"url":"https://api.github.com/repos/sktime/sktime/issues/1175","repository_url":"https://api.github.com/repos/sktime/sktime","labels_url":"https://api.github.com/repos/sktime/sktime/issues/1175/labels{/name}","comments_url":"https://api.github.com/repos/sktime/sktime/issues/1175/comments","events_url":"https://api.github.com/repos/sktime/sktime/issues/1175/events","html_url":"https://github.com/sktime/sktime/issues/1175","id":948880797,"node_id":"MDU6SXNzdWU5NDg4ODA3OTc=","number":1175,"title":"Add Module for Statistical Tests","user":{"login":"RNKuhns","id":26907244,"node_id":"MDQ6VXNlcjI2OTA3MjQ0","avatar_url":"https://avatars.githubusercontent.com/u/26907244?v=4","gravatar_id":"","url":"https://api.github.com/users/RNKuhns","html_url":"https://github.com/RNKuhns","followers_url":"https://api.github.com/users/RNKuhns/followers","following_url":"https://api.github.com/users/RNKuhns/following{/other_user}","gists_url":"https://api.github.com/users/RNKuhns/gists{/gist_id}","starred_url":"https://api.github.com/users/RNKuhns/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RNKuhns/subscriptions","organizations_url":"https://api.github.com/users/RNKuhns/orgs","repos_url":"https://api.github.com/users/RNKuhns/repos","events_url":"https://api.github.com/users/RNKuhns/events{/privacy}","received_events_url":"https://api.github.com/users/RNKuhns/received_events","type":"User","site_admin":false},"labels":[{"id":1180363817,"node_id":"MDU6TGFiZWwxMTgwMzYzODE3","url":"https://api.github.com/repos/sktime/sktime/labels/API%20design","name":"API design","color":"874db7","default":false,"description":"API design & software architecture"},{"id":1180366006,"node_id":"MDU6TGFiZWwxMTgwMzY2MDA2","url":"https://api.github.com/repos/sktime/sktime/labels/implementing%20framework","name":"implementing framework","color":"61c7f9","default":false,"description":"Implementing frameworks for new learning tasks"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-07-20T17:29:55Z","updated_at":"2021-07-22T13:43:44Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\nStatistical tests have common use-cases in timeseries analysis, including inspect the properties of a timeseries (e.g. stationarity testing, checking normality) to guide modeling decisions and also to evaluate model output (including evaluating quality of forecasts). \r\n\r\nAdding a interface for statistical tests will allow Sktime to add relevant functionality in this area. In addition to the tests themselves, this will help enable conditional transformers (think differencing a series if it is non-stationary or using a BoxCox transform based on results of normality or conditional variance test) and post-hoc forecast evaluation/benchmarking (Diebold-Mariano and other tests of one set of forecasts against another).\r\n\r\n**Describe the solution you'd like**\r\nA interface and module for statistical tests in Sktime. The module's `base` would include a class that will be the basis for all tests. \r\n\r\nMy thoughts on the interface are generally:\r\n\r\n1. Tests will be estimators (they need to be fitted)\r\n2. Instead of `transform` or `predict` they should have a `report` method that returns test results\r\n    - Reported results should be standardized regardless of test and I think they should be p-value, test statistic, and whether null was rejected (all tests would have hyper-parameter `report_detail` that defaults to True and reports all three items. But if it is set to False only whether the null was rejected would be reported). Note if a test doesn't have a p-value or test statistic that part of return will be None for that test.\r\n3. Plan to follow logic of Forecaster refactor and keep the code in non-public methods standardized across sub-classes\r\n\r\nProposed `BaseStatisticalTest` is presented below. \r\n\r\n```python\r\nclass BaseStatisticalTest(BaseEstimator):\r\n   \r\n    def __init__(\r\n        test_hyper_parameteres..., \r\n        hypothesis=\"two-sided\",\r\n        report_detail=True\r\n    ):\r\n        ...\r\n        self.p_value = None\r\n        self.test_statisic = None\r\n        self.reject_null = None\r\n        \r\n    def _fit(Y, X=None):\r\n        \"\"\" Logic to fit each test.\"\"\"\r\n       ...\r\n       #assume things below are calculated in _fit above\r\n       self.p_value = p_value\r\n       self.test_statistic = test_statistic \r\n       self.reject_null = reject_null\r\n       return self\r\n        \r\n    def fit(Y, X=None):\r\n        \"\"\"Would remain same in each Test's class.\"\"\"\r\n        ...\r\n        # Input checks, etc happen above\r\n        return self._fit(Y, X=X)\r\n        \r\n    def _report()\r\n        \"\"\"Logic to return the information to from report.\r\n        \r\n        Returns\r\n        --------\r\n        Plan is to return just boolean reject_null if hyper-parameter `report_detail=False`\r\n        Otherwise will report the following:\r\n        p_value : float or None\r\n            P-value associated with statistical test. If no p-value is \r\n            available for a test then will return None.\r\n        test_statistic : float or None\r\n            Test statistic from the statistical test. If no test \r\n            statistic is available for a test then will return None.\r\n        reject_null : bool\r\n            Whether the Test's Null Hypothesis was rejected.\r\n        \"\"\"\r\n        ...\r\n        if report_detail:\r\n            return self.p_value, self.test_statistic, self.reject_null\r\n        else:\r\n            return self.reject_null\r\n            \r\n    def report():\r\n        \"\"\"Would be same for every test.\"\"\"\r\n        self.check_is_fitted()\r\n        ...\r\n        return self._report()\r\n\r\n    def fit_report(Y, X=None):\r\n        \"\"\"Would be same for every test.\"\"\"\r\n        return self.fit(Y, X=X).report()\r\n        \r\n    def print_results():\r\n        \"\"\"Pretty printing the test hyperparameters, timeseries being tested and results.\"\"\"\r\n        ...\r\n        return None\r\n        \r\n    def results_to_pandas():\r\n        \"\"\"Output results to a pd.DataFrame.\r\n        \r\n        Useful when you want to apply a test to many series and capture \r\n        the results.\r\n        \r\n        Returns\r\n        -------\r\n        results_df : pd.DataFrame\r\n            DataFrame containing results in standardized format.\r\n        \"\"\"\r\n        \r\n        return results_df\r\n        \r\n    def results_to_excel(...):\r\n        \"\"\"Output results to an excel file.\r\n        \r\n        Useful when you want to apply a test to many series and capture \r\n        the results and store the results on disk incrementally (potentially\r\n        if applying to many series and you want to ensure results are saved even\r\n        if workflow gets stopped (so you don't have to start at beginning).\r\n        \r\n        Returns\r\n        -------\r\n        None\r\n        \"\"\"\r\n        self.results_to_pandas().to_excel(...)\r\n        return None\r\n        \r\n    def results_to_csv(...):\r\n        \"\"\"Output results to an csv file.\r\n        \r\n        Useful when you want to apply a test to many series and capture \r\n        the results and store the results on disk incrementally (potentially\r\n        if applying to many series and you want to ensure results are saved even\r\n        if workflow gets stopped (so you don't have to start at beginning).\r\n        \r\n        Returns\r\n        -------\r\n        None\r\n        \"\"\"\r\n        self.results_to_pandas().to_csv(...)\r\n        return None\r\n```\r\n\r\nNote that I'm open on design details, particularly the naming conventions (I don't have strong feelings about use of report, results, or something else) and likewise if we want to call this something other than statistical tests that is fine too). \r\n\r\nThe main outstanding questions (other then general feedback) involve around the interface for accepting different types of input that works across a range of tests. \r\n\r\nThis needs to cover:\r\n1. Univariate diagnostic tests of timeseries \"properties\" (e.g. normality, stationarity, auto-correlation, etc)\r\n2. Multivariate diagnostic tests (e.g. Granger causality or cointegration)\r\n3. Panel diagnostic tests (panel extension of stationarity tests, etc)\r\n4.  Post-hoc tests of one set of forecasts (whether they be univariate, multivariate or potentially panel) against another (\"Y_Other\")\r\n\r\nInitial thoughts to solve this would be for `fit` to accept either a pd.Series, pd.DataFrame or NumPy array \"Y\" and optionally accept exogenous data \"X\" (some tests will use this others won't) and determine how to proceed based on the type of tests.\r\n\r\n- For \"univariate\" diagnostic tests, if a pd.Series is received the test would be done on the series (optionally using X if it is needed). If a pd.DataFrame is received the test would be applied to each series in the DataFrame. \r\n    - Should we return an error if panel data is received? I'm leaning this way for now but later we could run test for each series for each panel member.\r\n- For a \"multivariate\" diagnostic test if a pd.Series (or single series DataFrame or NumPy array) then an appropriate message is raised; otherwise tests uses multivariate data from the DataFrame or NumPy array. \r\n- Panel tests would likewise raise error if appropriate panel data format not received and otherwise run as expected.\r\n- In all cases, we'd have error checking to make sure the X data is of the correct dimension to be used with the Y data\r\n\r\nThis leaves the last piece, which is how to accept the data for post-hoc tests. Note that these tests often can be applied to univariate data, while an extension allows them to be applied to multivariate data. I'd propose we don't want separate classes based on that distinction. Instead, I propose the following logic:\r\n1. Optionally pass  \"Y_Other\" in `fit` (kind of like how we handle y_train in performance metrics). If Y_other is received we check its dimension against Y and assume we are doing a post-hoc comparison of Y against \"Y_Other\"\r\n2. If \"Y_Other\" is not passed and a pd.Series is received then raise an error (you'd have nothing to compare series against)\r\n3. If \"Y_Other\" is not passed and Y is a pd.DataFrame then make assumptions about its structure and proceed with test (e.g. if it has 2 columns test column 1 against column 2).\r\n\r\nNote that I will edit this comment later to add a list of tests that can be interfaced (primarily from statsmodels) and a set of tests we'd need to write ourselves. \r\n\r\nPlan would be to chunk this out in phases: \r\n1. Decide on framework and implement BaseStatisticalTest and unit tests\r\n2. Have issue with checklist of good first issues for interfacing tests in Statsmodels (and possibly elsewhere if we can avoid adding un-needed additional dependencies)\r\n3. Create issue with checklist of tests we need to code ourselves (as of now these are mostly post-hoc tests and some boutique extensions of diagnostic tests)\r\n\r\n**Describe alternatives you've considered**\r\nAn alternative I've considered is to import and use tests from other packages (Statsmodels) when available. But there are tests not in Statsmodels that we should add (post-hoc forecast evaluation ones in particular). Having a common interface that can be used to adapt Statsmodels tests to our format and also be used for our own statistical tests seems like the way to go to me for uniformity.\r\n\r\nNote that in terms of the interface, one consideration I've had is whether to have a separate base class for post-hoc tests and diagnostic tests. Main difference is interface for `fit` as the diagnostic tests don't need to worry about \"Y_Other\".","closed_by":null,"reactions":{"url":"https://api.github.com/repos/sktime/sktime/issues/1175/reactions","total_count":3,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/sktime/sktime/issues/1175/timeline","performed_via_github_app":null,"state_reason":null}