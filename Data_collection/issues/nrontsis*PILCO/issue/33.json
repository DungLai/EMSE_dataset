{"url":"https://api.github.com/repos/nrontsis/PILCO/issues/33","repository_url":"https://api.github.com/repos/nrontsis/PILCO","labels_url":"https://api.github.com/repos/nrontsis/PILCO/issues/33/labels{/name}","comments_url":"https://api.github.com/repos/nrontsis/PILCO/issues/33/comments","events_url":"https://api.github.com/repos/nrontsis/PILCO/issues/33/events","html_url":"https://github.com/nrontsis/PILCO/issues/33","id":487783634,"node_id":"MDU6SXNzdWU0ODc3ODM2MzQ=","number":33,"title":"Implement `predict_on_noisy_input` with MCMC in gpytorch","user":{"login":"jaztsong","id":7253796,"node_id":"MDQ6VXNlcjcyNTM3OTY=","avatar_url":"https://avatars.githubusercontent.com/u/7253796?v=4","gravatar_id":"","url":"https://api.github.com/users/jaztsong","html_url":"https://github.com/jaztsong","followers_url":"https://api.github.com/users/jaztsong/followers","following_url":"https://api.github.com/users/jaztsong/following{/other_user}","gists_url":"https://api.github.com/users/jaztsong/gists{/gist_id}","starred_url":"https://api.github.com/users/jaztsong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jaztsong/subscriptions","organizations_url":"https://api.github.com/users/jaztsong/orgs","repos_url":"https://api.github.com/users/jaztsong/repos","events_url":"https://api.github.com/users/jaztsong/events{/privacy}","received_events_url":"https://api.github.com/users/jaztsong/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-08-31T15:54:49Z","updated_at":"2019-09-07T15:04:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm recently starting to re-implement PILCO in pytorch for better intergration with my other works. To leverage the fast prediction (KISS-GP) in gpytorch, I decided to use MCMC sampling approach to implement the core function in `mgpr.py` - `predict_on_noisy_input` which use _moment matching_ based on the original paper. \r\n\r\nHowever, the result I got from sampling is dramatically different from _moment matching_. I wonder if anyone can help me identify the problem. The following code shows both `optimize` and `predict_on_noisy_input`.\r\n\r\n```python\r\n    def optimize(self,restarts=1, training_iter = 200):\r\n        self.likelihood.train()\r\n        self.model.train()\r\n\r\n        # Use the adam optimizer\r\n        optimizer = torch.optim.Adam([\r\n            {'params': self.model.parameters()},  # Includes GaussianLikelihood parameters\r\n            ], lr=self.lr)\r\n        # \"Loss\" for GPs - the marginal log likelihood\r\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\r\n        for i in range(training_iter):\r\n            # Zero gradients from previous iteration\r\n            optimizer.zero_grad()\r\n            # Output from model\r\n            output = self.model(self.X)\r\n             # Calc loss and backprop gradients\r\n            loss = -mll(output, self.Y).sum()\r\n            loss.backward()\r\n            print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\r\n            optimizer.step()\r\n\r\n\r\n\r\n\r\n    def predict_on_noisy_inputs(self, m, s, num_samps=500):\r\n        \"\"\"\r\n        Approximate GP regression at noisy inputs via moment matching\r\n        IN: mean (m) (row vector) and (s) variance of the state\r\n        OUT: mean (M) (row vector), variance (S) of the action\r\n             and inv(s)*input-ouputcovariance\r\n\r\n        We adopt the sampling approach by leveraging the power of GPU\r\n        \"\"\"\r\n        assert(m.shape[1] == self.num_dims and s.shape == (self.num_dims,self.num_dims))\r\n        self.likelihood.eval()\r\n        self.model.eval()\r\n\r\n        if self.cuda == True:\r\n            m = torch.tensor(m).float().cuda()\r\n            s = torch.tensor(s).float().cuda()\r\n            inv_s = torch.inverse(s)\r\n\r\n        sample_model = torch.distributions.MultivariateNormal(m,s)\r\n        pred_inputs = sample_model.sample((num_samps,)).float()\r\n        pred_inputs[pred_inputs != pred_inputs] = 0\r\n        pred_inputs,_ = torch.sort(pred_inputs,dim=0)\r\n        pred_inputs = pred_inputs.reshape(num_samps,self.num_dims).repeat(self.num_outputs,1,1)\r\n\r\n        #centralize X ?\r\n        # self.model.set_train_data(self.centralized_input(m),self.Y)\r\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\r\n            pred_outputs = self.model(pred_inputs)\r\n\r\n\r\n\r\n\r\n        #Calculate mean, variance and inv(s)* input-output covariance\r\n        M = torch.mean(pred_outputs.mean,1)[None,:]\r\n        V_ = torch.cat((pred_inputs[0].t(),pred_outputs.mean),0)\r\n        fact = 1.0 / (V_.size(1) - 1)\r\n        V_ -= torch.mean(V_, dim=1, keepdim=True)\r\n        V_t = V_.t()  # if complex: mt = m.t().conj()\r\n        covs =  fact * V_.matmul(V_t).squeeze()\r\n        V = covs[0:self.num_dims,self.num_dims:]\r\n        V = inv_s @ V\r\n        S = covs[self.num_dims:,self.num_dims:]\r\n\r\n\r\n        return M, S, V\r\n``` ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/nrontsis/PILCO/issues/33/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nrontsis/PILCO/issues/33/timeline","performed_via_github_app":null,"state_reason":null}