{"url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356","repository_url":"https://api.github.com/repos/catalyst-team/catalyst","labels_url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356/labels{/name}","comments_url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356/comments","events_url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356/events","html_url":"https://github.com/catalyst-team/catalyst/issues/356","id":490838398,"node_id":"MDU6SXNzdWU0OTA4MzgzOTg=","number":356,"title":"How to run catalyst with distributed training?","user":{"login":"ngxbac","id":11550123,"node_id":"MDQ6VXNlcjExNTUwMTIz","avatar_url":"https://avatars.githubusercontent.com/u/11550123?v=4","gravatar_id":"","url":"https://api.github.com/users/ngxbac","html_url":"https://github.com/ngxbac","followers_url":"https://api.github.com/users/ngxbac/followers","following_url":"https://api.github.com/users/ngxbac/following{/other_user}","gists_url":"https://api.github.com/users/ngxbac/gists{/gist_id}","starred_url":"https://api.github.com/users/ngxbac/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ngxbac/subscriptions","organizations_url":"https://api.github.com/users/ngxbac/orgs","repos_url":"https://api.github.com/users/ngxbac/repos","events_url":"https://api.github.com/users/ngxbac/events{/privacy}","received_events_url":"https://api.github.com/users/ngxbac/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-09-09T01:37:19Z","updated_at":"2019-12-29T10:54:55Z","closed_at":"2019-12-29T10:54:55Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi, \r\nI am trying to run distributed training, but I havent had success yet. \r\nIt seems that we need to run with command like: `python -m torch.distributed.launch` to start distributed training. However, I havent seen any documents related to this feature.  \r\n\r\nI played around with some settings as follow:\r\nMy environment: 4 GPUs, I want to use 2 GPUs for distributed training. \r\n- config.yml: \r\n```\r\ndistributed_params:\r\n  opt_level: O1\r\n  rank: 0\r\n```\r\n\r\n- Bash file: \r\n```bash\r\n#!/usr/bin/env bash\r\n\r\nexport CUDA_VISIBLE_DEVICES=2,3\r\n\r\nexport MASTER_PORT=1235\r\nexport MASTER_ADDR=0.0.0.0\r\nexport WORLD_SIZE=1\r\nexport RANK=0\r\n\r\ncatalyst-dl run \\\r\n    --config=<config> \\\r\n    --logdir=$LOGDIR \\\r\n    --out_dir=$LOGDIR:str \\\r\n    --verbose\r\n```\r\n\r\n- Results \r\nI can manage the program running by the setting above. However, there is only one GPU running. \r\nMay be the reason is [here](https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/utils/torch.py#L39). I wonder how can I make 2GPUs running in this situation. ","closed_by":{"login":"Scitator","id":7606451,"node_id":"MDQ6VXNlcjc2MDY0NTE=","avatar_url":"https://avatars.githubusercontent.com/u/7606451?v=4","gravatar_id":"","url":"https://api.github.com/users/Scitator","html_url":"https://github.com/Scitator","followers_url":"https://api.github.com/users/Scitator/followers","following_url":"https://api.github.com/users/Scitator/following{/other_user}","gists_url":"https://api.github.com/users/Scitator/gists{/gist_id}","starred_url":"https://api.github.com/users/Scitator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Scitator/subscriptions","organizations_url":"https://api.github.com/users/Scitator/orgs","repos_url":"https://api.github.com/users/Scitator/repos","events_url":"https://api.github.com/users/Scitator/events{/privacy}","received_events_url":"https://api.github.com/users/Scitator/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/catalyst-team/catalyst/issues/356/timeline","performed_via_github_app":null,"state_reason":"completed"}