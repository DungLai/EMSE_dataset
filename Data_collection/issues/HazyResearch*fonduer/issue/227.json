{"url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227","repository_url":"https://api.github.com/repos/HazyResearch/fonduer","labels_url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227/labels{/name}","comments_url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227/comments","events_url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227/events","html_url":"https://github.com/HazyResearch/fonduer/issues/227","id":424400822,"node_id":"MDU6SXNzdWU0MjQ0MDA4MjI=","number":227,"title":"Not compatible with spaCy v2.1.0","user":{"login":"HiromuHota","id":20668349,"node_id":"MDQ6VXNlcjIwNjY4MzQ5","avatar_url":"https://avatars.githubusercontent.com/u/20668349?v=4","gravatar_id":"","url":"https://api.github.com/users/HiromuHota","html_url":"https://github.com/HiromuHota","followers_url":"https://api.github.com/users/HiromuHota/followers","following_url":"https://api.github.com/users/HiromuHota/following{/other_user}","gists_url":"https://api.github.com/users/HiromuHota/gists{/gist_id}","starred_url":"https://api.github.com/users/HiromuHota/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HiromuHota/subscriptions","organizations_url":"https://api.github.com/users/HiromuHota/orgs","repos_url":"https://api.github.com/users/HiromuHota/repos","events_url":"https://api.github.com/users/HiromuHota/events{/privacy}","received_events_url":"https://api.github.com/users/HiromuHota/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-03-22T21:09:47Z","updated_at":"2019-03-26T08:06:22Z","closed_at":"2019-03-26T08:06:22Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\nFonduer is not compatible with spaCy v2.1.0\r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n1. Install spaCy v2.1.0\r\n2. Run tests\r\n\r\n**Expected behavior**\r\n\r\nBe compatible with spaCy v2.1.0 and higher\r\n\r\n**Error Logs/Screenshots**\r\n\r\n```\r\njovyan@1a70f98cfba1:~/fonduer$ pip list | grep spacy\r\nspacy                    2.1.2       \r\njovyan@1a70f98cfba1:~/fonduer$ pytest tests/parser/test_parser.py::test_parse_md_details\r\n====================================================================================== test session starts ======================================================================================\r\nplatform linux -- Python 3.7.1, pytest-4.3.1, py-1.8.0, pluggy-0.9.0 -- /opt/conda/bin/python\r\ncachedir: .pytest_cache\r\nrootdir: /home/jovyan/fonduer/tests, inifile: pytest.ini\r\ncollected 1 item                                                                                                                                                                                \r\n\r\ntests/parser/test_parser.py::test_parse_md_details FAILED                                                                                                                                 [100%]\r\n\r\n=========================================================================================== FAILURES ============================================================================================\r\n_____________________________________________________________________________________ test_parse_md_details _____________________________________________________________________________________\r\n\r\ncaplog = <_pytest.logging.LogCaptureFixture object at 0x7f4728ad9e48>\r\n\r\n    def test_parse_md_details(caplog):\r\n        \"\"\"Test the parser with the md document.\"\"\"\r\n        caplog.set_level(logging.INFO)\r\n        logger = logging.getLogger(__name__)\r\n    \r\n        docs_path = \"tests/data/html_simple/md.html\"\r\n        pdf_path = \"tests/data/pdf_simple/md.pdf\"\r\n    \r\n        # Preprocessor for the Docs\r\n        preprocessor = HTMLDocPreprocessor(docs_path)\r\n        doc = next(preprocessor._parse_file(docs_path, \"md\"))\r\n    \r\n        # Check that doc has a name\r\n        assert doc.name == \"md\"\r\n    \r\n        # Check that doc does not have any of these\r\n        assert len(doc.figures) == 0\r\n        assert len(doc.tables) == 0\r\n        assert len(doc.cells) == 0\r\n        assert len(doc.sentences) == 0\r\n    \r\n        # Create an Parser and parse the md document\r\n        parser_udf = get_parser_udf(\r\n            structural=True,\r\n            tabular=True,\r\n            lingual=True,\r\n            visual=True,\r\n            pdf_path=pdf_path,\r\n            language=\"en\",\r\n        )\r\n>       for _ in parser_udf.apply(doc):\r\n\r\ntests/parser/test_parser.py:80: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\nsrc/fonduer/parser/parser.py:241: in apply\r\n    for _ in self.parse(document, text):\r\nsrc/fonduer/parser/parser.py:810: in parse\r\n    tokenized_sentences += [y for y in self._parse_node(node, state)]\r\nsrc/fonduer/parser/parser.py:810: in <listcomp>\r\n    tokenized_sentences += [y for y in self._parse_node(node, state)]\r\nsrc/fonduer/parser/parser.py:755: in _parse_node\r\n    yield from self._parse_paragraph(node, state)\r\nsrc/fonduer/parser/parser.py:653: in _parse_paragraph\r\n    yield from self._parse_sentence(paragraph, node, state)\r\nsrc/fonduer/parser/parser.py:493: in _parse_sentence\r\n    for parts in self.tokenize_and_split_sentences(document, text):\r\nsrc/fonduer/parser/spacy_parser.py:275: in split_sentences\r\n    sbd = self.model.create_pipe(\"sbd\")  # add sentencizer\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nself = <spacy.lang.en.English object at 0x7f472899ef60>, name = 'sbd', config = {}\r\n\r\n    def create_pipe(self, name, config=dict()):\r\n        \"\"\"Create a pipeline component from a factory.\r\n    \r\n        name (unicode): Factory name to look up in `Language.factories`.\r\n        config (dict): Configuration parameters to initialise component.\r\n        RETURNS (callable): Pipeline component.\r\n    \r\n        DOCS: https://spacy.io/api/language#create_pipe\r\n        \"\"\"\r\n        if name not in self.factories:\r\n            if name == \"sbd\":\r\n>               raise KeyError(Errors.E108.format(name=name))\r\nE               KeyError: \"[E108] As of spaCy v2.1, the pipe name `sbd` has been deprecated in favor of the pipe name `sentencizer`, which does the same thing. For example, use `nlp.create_pipeline('sentencizer')`\"\r\n\r\n/opt/conda/lib/python3.7/site-packages/spacy/language.py:251: KeyError\r\n======================================================================================= warnings summary ========================================================================================\r\n/opt/conda/lib/python3.7/site-packages/thinc/neural/train.py:7\r\n  /opt/conda/lib/python3.7/site-packages/thinc/neural/train.py:7: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n    from .optimizers import Adam, linear_decay\r\n\r\n/opt/conda/lib/python3.7/site-packages/thinc/check.py:4\r\n/opt/conda/lib/python3.7/site-packages/thinc/check.py:4\r\n/opt/conda/lib/python3.7/site-packages/thinc/check.py:4\r\n  /opt/conda/lib/python3.7/site-packages/thinc/check.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n    from collections import Sequence, Sized, Iterable, Callable\r\n\r\n/opt/conda/lib/python3.7/site-packages/jsonschema/compat.py:6\r\n  /opt/conda/lib/python3.7/site-packages/jsonschema/compat.py:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\r\n    from collections import MutableMapping, Sequence  # noqa\r\n\r\nparser/test_parser.py::test_parse_md_details\r\n  /opt/conda/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 176 from C header, got 216 from PyObject\r\n    return f(*args, **kwds)\r\n\r\n-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n============================================================================= 1 failed, 6 warnings in 2.75 seconds ==============================================================================\r\n```\r\n\r\n**Environment (please complete the following information):**\r\n - OS: Ubuntu 18.04\r\n - PostgreSQL Version: 11.1-1.pgdg90+1\r\n - Poppler Utils Version: 0.62.0\r\n - Fonduer Version: 138e4be9a26ce7cd7086664dd9db22bcfb17b6b0\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n","closed_by":{"login":"senwu","id":5580008,"node_id":"MDQ6VXNlcjU1ODAwMDg=","avatar_url":"https://avatars.githubusercontent.com/u/5580008?v=4","gravatar_id":"","url":"https://api.github.com/users/senwu","html_url":"https://github.com/senwu","followers_url":"https://api.github.com/users/senwu/followers","following_url":"https://api.github.com/users/senwu/following{/other_user}","gists_url":"https://api.github.com/users/senwu/gists{/gist_id}","starred_url":"https://api.github.com/users/senwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/senwu/subscriptions","organizations_url":"https://api.github.com/users/senwu/orgs","repos_url":"https://api.github.com/users/senwu/repos","events_url":"https://api.github.com/users/senwu/events{/privacy}","received_events_url":"https://api.github.com/users/senwu/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/HazyResearch/fonduer/issues/227/timeline","performed_via_github_app":null,"state_reason":"completed"}