{"url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19","repository_url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion","labels_url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19/labels{/name}","comments_url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19/comments","events_url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19/events","html_url":"https://github.com/fangchangma/self-supervised-depth-completion/issues/19","id":430626562,"node_id":"MDU6SXNzdWU0MzA2MjY1NjI=","number":19,"title":"training with vlp-16 dataset","user":{"login":"tzuchiehTT","id":17619096,"node_id":"MDQ6VXNlcjE3NjE5MDk2","avatar_url":"https://avatars.githubusercontent.com/u/17619096?v=4","gravatar_id":"","url":"https://api.github.com/users/tzuchiehTT","html_url":"https://github.com/tzuchiehTT","followers_url":"https://api.github.com/users/tzuchiehTT/followers","following_url":"https://api.github.com/users/tzuchiehTT/following{/other_user}","gists_url":"https://api.github.com/users/tzuchiehTT/gists{/gist_id}","starred_url":"https://api.github.com/users/tzuchiehTT/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tzuchiehTT/subscriptions","organizations_url":"https://api.github.com/users/tzuchiehTT/orgs","repos_url":"https://api.github.com/users/tzuchiehTT/repos","events_url":"https://api.github.com/users/tzuchiehTT/events{/privacy}","received_events_url":"https://api.github.com/users/tzuchiehTT/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2019-04-08T19:47:17Z","updated_at":"2020-04-11T02:24:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi Fangchang,\r\n\r\nOur lab is currently working on a project which requires generating depth maps from our vlp-16 lidar and camera setting. Your work looks great as the depth map solution. Since we got different size images as input, I think what we need to do to use this network is (1) read in our own calibration information (K) and (2) crop input images as (width, high) both multiples of 16 (since we got errors when going through decode layers with some other sizes), is that right?\r\n\r\nWe've tested with a rather small dataset (only ~700 frames) and got results like the figure showing below.\r\nWe are wondering if the dataset is too small or the depth info from vlp-16 is too sparse since the results remain clear projected lines. It would be great if you have any suggestions, thanks!\r\n\r\n![comparison_best](https://user-images.githubusercontent.com/17619096/55751769-7e84f800-5a14-11e9-8840-575da8af0cc4.png)\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/fangchangma/self-supervised-depth-completion/issues/19/timeline","performed_via_github_app":null,"state_reason":null}