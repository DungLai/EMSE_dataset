{"url":"https://api.github.com/repos/alvations/sacremoses/issues/125","repository_url":"https://api.github.com/repos/alvations/sacremoses","labels_url":"https://api.github.com/repos/alvations/sacremoses/issues/125/labels{/name}","comments_url":"https://api.github.com/repos/alvations/sacremoses/issues/125/comments","events_url":"https://api.github.com/repos/alvations/sacremoses/issues/125/events","html_url":"https://github.com/alvations/sacremoses/issues/125","id":1085791967,"node_id":"I_kwDOB8RSPc5At97f","number":125,"title":"Which of the 100 languages used in mBERT are not supported by this tokenizer?","user":{"login":"fake-warrior8","id":40395156,"node_id":"MDQ6VXNlcjQwMzk1MTU2","avatar_url":"https://avatars.githubusercontent.com/u/40395156?v=4","gravatar_id":"","url":"https://api.github.com/users/fake-warrior8","html_url":"https://github.com/fake-warrior8","followers_url":"https://api.github.com/users/fake-warrior8/followers","following_url":"https://api.github.com/users/fake-warrior8/following{/other_user}","gists_url":"https://api.github.com/users/fake-warrior8/gists{/gist_id}","starred_url":"https://api.github.com/users/fake-warrior8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fake-warrior8/subscriptions","organizations_url":"https://api.github.com/users/fake-warrior8/orgs","repos_url":"https://api.github.com/users/fake-warrior8/repos","events_url":"https://api.github.com/users/fake-warrior8/events{/privacy}","received_events_url":"https://api.github.com/users/fake-warrior8/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-12-21T12:40:20Z","updated_at":"2022-05-02T11:19:33Z","closed_at":"2022-05-02T11:19:33Z","author_association":"NONE","active_lock_reason":null,"body":"Hi, I'd like to know what are the general languages supported by this tokenizer tool.  Is this tokenizer tool  compatible with the 100 languages used by mBERT or XLM-R? I know that some languages such as Chinese and Thai require some additional word segmentation preprocessing for raw texts. So, are there any other languages out of those 100 languages that require additional preporcessing beyond this tokenizer tool?","closed_by":{"login":"alvations","id":1050316,"node_id":"MDQ6VXNlcjEwNTAzMTY=","avatar_url":"https://avatars.githubusercontent.com/u/1050316?v=4","gravatar_id":"","url":"https://api.github.com/users/alvations","html_url":"https://github.com/alvations","followers_url":"https://api.github.com/users/alvations/followers","following_url":"https://api.github.com/users/alvations/following{/other_user}","gists_url":"https://api.github.com/users/alvations/gists{/gist_id}","starred_url":"https://api.github.com/users/alvations/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alvations/subscriptions","organizations_url":"https://api.github.com/users/alvations/orgs","repos_url":"https://api.github.com/users/alvations/repos","events_url":"https://api.github.com/users/alvations/events{/privacy}","received_events_url":"https://api.github.com/users/alvations/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/alvations/sacremoses/issues/125/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/alvations/sacremoses/issues/125/timeline","performed_via_github_app":null,"state_reason":"completed"}