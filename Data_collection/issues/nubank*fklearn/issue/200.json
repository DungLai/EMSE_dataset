{"url":"https://api.github.com/repos/nubank/fklearn/issues/200","repository_url":"https://api.github.com/repos/nubank/fklearn","labels_url":"https://api.github.com/repos/nubank/fklearn/issues/200/labels{/name}","comments_url":"https://api.github.com/repos/nubank/fklearn/issues/200/comments","events_url":"https://api.github.com/repos/nubank/fklearn/issues/200/events","html_url":"https://github.com/nubank/fklearn/issues/200","id":1287751147,"node_id":"I_kwDOCk64ls5MwYXr","number":200,"title":"Include kwargs in the evaluator's wrappers","user":{"login":"boriero","id":28151821,"node_id":"MDQ6VXNlcjI4MTUxODIx","avatar_url":"https://avatars.githubusercontent.com/u/28151821?v=4","gravatar_id":"","url":"https://api.github.com/users/boriero","html_url":"https://github.com/boriero","followers_url":"https://api.github.com/users/boriero/followers","following_url":"https://api.github.com/users/boriero/following{/other_user}","gists_url":"https://api.github.com/users/boriero/gists{/gist_id}","starred_url":"https://api.github.com/users/boriero/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/boriero/subscriptions","organizations_url":"https://api.github.com/users/boriero/orgs","repos_url":"https://api.github.com/users/boriero/repos","events_url":"https://api.github.com/users/boriero/events{/privacy}","received_events_url":"https://api.github.com/users/boriero/received_events","type":"User","site_admin":false},"labels":[{"id":1251232928,"node_id":"MDU6TGFiZWwxMjUxMjMyOTI4","url":"https://api.github.com/repos/nubank/fklearn/labels/bug","name":"bug","color":"d73a4a","default":true,"description":"Something isn't working"},{"id":1251232930,"node_id":"MDU6TGFiZWwxMjUxMjMyOTMw","url":"https://api.github.com/repos/nubank/fklearn/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-06-28T18:53:35Z","updated_at":"2022-06-28T18:53:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"### Instructions\r\n- Include kwargs in the evaluator's functions\r\n\r\n**from**\r\n\r\n```\r\ndef precision_evaluator(test_data: pd.DataFrame,\r\n                        threshold: float = 0.5,\r\n                        prediction_column: str = \"prediction\",\r\n                        target_column: str = \"target\",\r\n                        eval_name: str = None) -> EvalReturnType:\r\n\r\n    eval_fn = generic_sklearn_evaluator(\"precision_evaluator__\", precision_score)\r\n    eval_data = test_data.assign(**{prediction_column: (test_data[prediction_column] > threshold).astype(int)})\r\n    return eval_fn(eval_data, prediction_column, target_column, eval_name)\r\n```\r\n\r\n**to**\r\n\r\n```\r\ndef precision_evaluator(\r\n    test_data: pd.DataFrame,\r\n    threshold: float = 0.5,\r\n    prediction_column: str = \"prediction\",\r\n    target_column: str = \"target\",\r\n    eval_name: str = None,\r\n    **kwargs,\r\n) -> EvalReturnType:   \r\n\r\n    eval_fn = generic_sklearn_evaluator(\"precision_evaluator__\", precision_score)\r\n    eval_data = test_data.assign(**{prediction_column: (tet_data[prediction_column] > threshold).astype(int)})\r\n    return eval_fn(eval_data, prediction_column, target_column, eval_name, **kwargs)\r\n```\r\n\r\n### Describe the feature and the current state.\r\n- Evaluators are parsed through a function that does not have **kwargs, so one cannot use other parametrizations than the default.\r\n\r\n### Will this change a current behavior? How?\r\n- One will be able, as required by my project, to have the precision and recall by label and not an average of labels, which can only be changed by setting the proper parameter, as done below. Furthermore, with only this change, any king of extra parametrization for the evaluators will be possible\r\n\r\n`precision_evaluator(target_column=target, average=None, labels=[0, 1])` \r\n\r\n### Extra information\r\n- Given the structure of the function of the generic evaluator _generic_sklearn_evaluator_, it seems to me that to have **kwargs was the intention since the beginning, but they missed the kwarg in the individual evaluator's wrappers, as it can be read in its definition:\r\n\r\n```\r\ndef generic_sklearn_evaluator(name_prefix: str, sklearn_metric: Callable[..., float]) -> UncurriedEvalFnType:\r\n    \"\"\"\r\n    Returns an evaluator build from a metric from sklearn.metrics\r\n    Parameters\r\n    ----------\r\n    name_prefix: str\r\n        The default name of the evaluator will be name_prefix + target_column.\r\n    sklearn_metric: Callable\r\n        Metric function from sklearn.metrics. It should take as parameters y_true, y_score, kwargs.\r\n\r\n```\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/nubank/fklearn/issues/200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/nubank/fklearn/issues/200/timeline","performed_via_github_app":null,"state_reason":null}