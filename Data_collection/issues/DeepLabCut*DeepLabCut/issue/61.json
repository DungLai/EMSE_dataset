{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61","repository_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut","labels_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61/labels{/name}","comments_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61/comments","events_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61/events","html_url":"https://github.com/DeepLabCut/DeepLabCut/issues/61","id":360374889,"node_id":"MDU6SXNzdWUzNjAzNzQ4ODk=","number":61,"title":"Enqueue operation was cancelled","user":{"login":"omaghsoudi","id":8738931,"node_id":"MDQ6VXNlcjg3Mzg5MzE=","avatar_url":"https://avatars.githubusercontent.com/u/8738931?v=4","gravatar_id":"","url":"https://api.github.com/users/omaghsoudi","html_url":"https://github.com/omaghsoudi","followers_url":"https://api.github.com/users/omaghsoudi/followers","following_url":"https://api.github.com/users/omaghsoudi/following{/other_user}","gists_url":"https://api.github.com/users/omaghsoudi/gists{/gist_id}","starred_url":"https://api.github.com/users/omaghsoudi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/omaghsoudi/subscriptions","organizations_url":"https://api.github.com/users/omaghsoudi/orgs","repos_url":"https://api.github.com/users/omaghsoudi/repos","events_url":"https://api.github.com/users/omaghsoudi/events{/privacy}","received_events_url":"https://api.github.com/users/omaghsoudi/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2018-09-14T16:35:07Z","updated_at":"2018-09-15T11:54:10Z","closed_at":"2018-09-15T11:54:10Z","author_association":"NONE","active_lock_reason":null,"body":"Hi guys,\r\n\r\nI ran to an issue with training my own dataset (even your dataset gives the same error).\r\nOn step 6, when training using the following code:\r\n``` python\r\nTF_CUDNN_USE_AUTOTUNE=0 CUDA_VISIBLE_DEVICES=0 python3 ../../../train.py \r\n```\r\nAt  iteration 1,030,000 I get the following error:\r\n```\r\niteration: 1000000 loss: 0.0013 lr: 0.001\r\niteration: 1001000 loss: 0.0013 lr: 0.001\r\niteration: 1002000 loss: 0.0013 lr: 0.001\r\niteration: 1003000 loss: 0.0013 lr: 0.001\r\niteration: 1004000 loss: 0.0013 lr: 0.001\r\niteration: 1005000 loss: 0.0013 lr: 0.001\r\niteration: 1006000 loss: 0.0013 lr: 0.001\r\niteration: 1007000 loss: 0.0013 lr: 0.001\r\niteration: 1008000 loss: 0.0013 lr: 0.001\r\niteration: 1009000 loss: 0.0013 lr: 0.001\r\niteration: 1010000 loss: 0.0014 lr: 0.001\r\niteration: 1011000 loss: 0.0013 lr: 0.001\r\niteration: 1012000 loss: 0.0013 lr: 0.001\r\niteration: 1013000 loss: 0.0013 lr: 0.001\r\niteration: 1014000 loss: 0.0013 lr: 0.001\r\niteration: 1015000 loss: 0.0013 lr: 0.001\r\niteration: 1016000 loss: 0.0013 lr: 0.001\r\niteration: 1017000 loss: 0.0013 lr: 0.001\r\niteration: 1018000 loss: 0.0013 lr: 0.001\r\niteration: 1019000 loss: 0.0013 lr: 0.001\r\niteration: 1020000 loss: 0.0013 lr: 0.001\r\niteration: 1021000 loss: 0.0014 lr: 0.001\r\niteration: 1022000 loss: 0.0013 lr: 0.001\r\niteration: 1023000 loss: 0.0013 lr: 0.001\r\niteration: 1024000 loss: 0.0013 lr: 0.001\r\niteration: 1025000 loss: 0.0013 lr: 0.001\r\niteration: 1026000 loss: 0.0013 lr: 0.001\r\niteration: 1027000 loss: 0.0013 lr: 0.001\r\niteration: 1028000 loss: 0.0013 lr: 0.001\r\niteration: 1029000 loss: 0.0012 lr: 0.001\r\niteration: 1030000 loss: 0.0013 lr: 0.001\r\n2018-09-12 21:34:07.680932: W tensorflow/core/kernels/queue_base.cc:277] _0_fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1278, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1263, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\r\n\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"../../../train.py\", line 49, in load_and_enqueue\r\n    sess.run(enqueue_op, feed_dict=food)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 877, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1100, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1272, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1291, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.CancelledError: Enqueue operation was cancelled\r\n\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\r\n\r\nCaused by op 'fifo_queue_enqueue', defined at:\r\n  File \"../../../train.py\", line 140, in <module>\r\n    train()\r\n  File \"../../../train.py\", line 83, in train\r\n    batch, enqueue_op, placeholders = setup_preloading(batch_spec)\r\n  File \"../../../train.py\", line 35, in setup_preloading\r\n    enqueue_op = q.enqueue(placeholders_list)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/data_flow_ops.py\", line 339, in enqueue\r\n    self._queue_ref, vals, name=scope)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3977, in queue_enqueue_v2\r\n    timeout_ms=timeout_ms, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nCancelledError (see above for traceback): Enqueue operation was cancelled\r\n\t [[Node: fifo_queue_enqueue = QueueEnqueueV2[Tcomponents=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](fifo_queue, _arg_Placeholder_0_0, _arg_Placeholder_1_0_1, _arg_Placeholder_2_0_2, _arg_Placeholder_3_0_3, _arg_Placeholder_4_0_4)]]\r\n\r\n``` \r\n\r\nAny idea what the issue can be?\r\n\r\n\r\n\r\nI am checking the \"pose_cfg.yaml\" file; it sounds that you limit it in multi step by 1,030,000. Is it right?\r\nIs there any reason that it should be 1,030,000?\r\n\r\n\r\nI should say that everything is working after that for testing and evaluating the trained netrow; but this error sounds weird.\r\n\r\nThank you!\r\n","closed_by":{"login":"AlexEMG","id":20850270,"node_id":"MDQ6VXNlcjIwODUwMjcw","avatar_url":"https://avatars.githubusercontent.com/u/20850270?v=4","gravatar_id":"","url":"https://api.github.com/users/AlexEMG","html_url":"https://github.com/AlexEMG","followers_url":"https://api.github.com/users/AlexEMG/followers","following_url":"https://api.github.com/users/AlexEMG/following{/other_user}","gists_url":"https://api.github.com/users/AlexEMG/gists{/gist_id}","starred_url":"https://api.github.com/users/AlexEMG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AlexEMG/subscriptions","organizations_url":"https://api.github.com/users/AlexEMG/orgs","repos_url":"https://api.github.com/users/AlexEMG/repos","events_url":"https://api.github.com/users/AlexEMG/events{/privacy}","received_events_url":"https://api.github.com/users/AlexEMG/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/61/timeline","performed_via_github_app":null,"state_reason":"completed"}