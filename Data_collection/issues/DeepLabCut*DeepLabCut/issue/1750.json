{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750","repository_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut","labels_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750/labels{/name}","comments_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750/comments","events_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750/events","html_url":"https://github.com/DeepLabCut/DeepLabCut/issues/1750","id":1190452902,"node_id":"I_kwDOB5BM6c5G9N6m","number":1750,"title":"Concurrency in multi-animal estimation","user":{"login":"sneakers-the-rat","id":12961499,"node_id":"MDQ6VXNlcjEyOTYxNDk5","avatar_url":"https://avatars.githubusercontent.com/u/12961499?v=4","gravatar_id":"","url":"https://api.github.com/users/sneakers-the-rat","html_url":"https://github.com/sneakers-the-rat","followers_url":"https://api.github.com/users/sneakers-the-rat/followers","following_url":"https://api.github.com/users/sneakers-the-rat/following{/other_user}","gists_url":"https://api.github.com/users/sneakers-the-rat/gists{/gist_id}","starred_url":"https://api.github.com/users/sneakers-the-rat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sneakers-the-rat/subscriptions","organizations_url":"https://api.github.com/users/sneakers-the-rat/orgs","repos_url":"https://api.github.com/users/sneakers-the-rat/repos","events_url":"https://api.github.com/users/sneakers-the-rat/events{/privacy}","received_events_url":"https://api.github.com/users/sneakers-the-rat/received_events","type":"User","site_admin":false},"labels":[{"id":880550036,"node_id":"MDU6TGFiZWw4ODA1NTAwMzY=","url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/labels/enhancement","name":"enhancement","color":"a2eeef","default":true,"description":"New feature or request"},{"id":1019981644,"node_id":"MDU6TGFiZWwxMDE5OTgxNjQ0","url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/labels/contributions-welcome","name":"contributions-welcome","color":"0de065","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"AlexEMG","id":20850270,"node_id":"MDQ6VXNlcjIwODUwMjcw","avatar_url":"https://avatars.githubusercontent.com/u/20850270?v=4","gravatar_id":"","url":"https://api.github.com/users/AlexEMG","html_url":"https://github.com/AlexEMG","followers_url":"https://api.github.com/users/AlexEMG/followers","following_url":"https://api.github.com/users/AlexEMG/following{/other_user}","gists_url":"https://api.github.com/users/AlexEMG/gists{/gist_id}","starred_url":"https://api.github.com/users/AlexEMG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AlexEMG/subscriptions","organizations_url":"https://api.github.com/users/AlexEMG/orgs","repos_url":"https://api.github.com/users/AlexEMG/repos","events_url":"https://api.github.com/users/AlexEMG/events{/privacy}","received_events_url":"https://api.github.com/users/AlexEMG/received_events","type":"User","site_admin":false},"assignees":[{"login":"AlexEMG","id":20850270,"node_id":"MDQ6VXNlcjIwODUwMjcw","avatar_url":"https://avatars.githubusercontent.com/u/20850270?v=4","gravatar_id":"","url":"https://api.github.com/users/AlexEMG","html_url":"https://github.com/AlexEMG","followers_url":"https://api.github.com/users/AlexEMG/followers","following_url":"https://api.github.com/users/AlexEMG/following{/other_user}","gists_url":"https://api.github.com/users/AlexEMG/gists{/gist_id}","starred_url":"https://api.github.com/users/AlexEMG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AlexEMG/subscriptions","organizations_url":"https://api.github.com/users/AlexEMG/orgs","repos_url":"https://api.github.com/users/AlexEMG/repos","events_url":"https://api.github.com/users/AlexEMG/events{/privacy}","received_events_url":"https://api.github.com/users/AlexEMG/received_events","type":"User","site_admin":false},{"login":"jeylau","id":30733203,"node_id":"MDQ6VXNlcjMwNzMzMjAz","avatar_url":"https://avatars.githubusercontent.com/u/30733203?v=4","gravatar_id":"","url":"https://api.github.com/users/jeylau","html_url":"https://github.com/jeylau","followers_url":"https://api.github.com/users/jeylau/followers","following_url":"https://api.github.com/users/jeylau/following{/other_user}","gists_url":"https://api.github.com/users/jeylau/gists{/gist_id}","starred_url":"https://api.github.com/users/jeylau/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jeylau/subscriptions","organizations_url":"https://api.github.com/users/jeylau/orgs","repos_url":"https://api.github.com/users/jeylau/repos","events_url":"https://api.github.com/users/jeylau/events{/privacy}","received_events_url":"https://api.github.com/users/jeylau/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2022-04-02T01:23:45Z","updated_at":"2022-05-13T14:38:01Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Is your feature request related to a problem? Please describe.**\r\n\r\nSince the prediction loop runs synchronously, it only uses the GPU ~half of the time-ish. circled part is the CPU-based pose extraction routine (and the green column to its right is the time spent loading frames into memory)\r\n\r\n<img width=\"1427\" alt=\"Screen Shot 2022-04-01 at 5 27 02 PM\" src=\"https://user-images.githubusercontent.com/12961499/161358260-5267ad89-9108-4327-b32d-ec2aff105ced.png\">\r\n\r\nthat's sorta slower than it needs to be.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nRun the three parts of the inference loop (loading frames, prediction, and the cpu-based stuff) concurrently.\r\n\r\nTo do that: \r\n* `predict_batched_peaks_and_costs` needs to be split into two functions, one to run the session, and one to do the CPU part: https://github.com/DeepLabCut/DeepLabCut/blob/686cf5e86f155e95bb494d6243b1e66059266a4e/deeplabcut/pose_estimation_tensorflow/core/predict_multianimal.py#L205\r\n* frame loading and cpu analysis need to be put into separate processes. Here's one totally naive and hacky way of doing that with Processs and Queue. Note that I haven't tested this at all because of tensorflow problems that i absolutely don't have the patience for right now:\r\n\r\n(see full example here: https://github.com/sneakers-the-rat/DeepLabCut/commit/9d0cf498ea1ff81e8739748507152dd7af844999 )\r\n\r\n```python\r\n\r\ndef batch_reader(cap, cfg, batch_size:int, queue:mp.Queue):\r\n    counter = 0\r\n    strwidth=64\r\n    while cap.video.isOpened():\r\n        # avoid loading the whole movie into memory without explicit locking\r\n        if queue.qsize() > 2:\r\n            continue\r\n        # get another batch and put it in the queue!\r\n        batch = []\r\n        inds = []\r\n        for i in range(batch_size):\r\n            frame = cap.read_frame(crop=cfg[\"cropping\"])\r\n            if frame is None:\r\n                break\r\n            frame = img_as_ubyte(frame)\r\n            if frame.shape[-1] == 4:\r\n                frame = rgba2rgb(frame)\r\n            batch.append(frame)\r\n            inds.append(counter)\r\n            counter += 1\r\n        batch = np.stack(batch)\r\n        queue.put((batch, inds))\r\n\r\ndef batch_extractor(in_q, out_q):\r\n    for data in iter(in_q, 'END'):\r\n        pose_cfg, scmaps, locrefs, pafs, peaks = data\r\n        # the CPU part\r\n        out_q.put(predict.extract_peaks_and_costs(pose_cfg, scmaps, locrefs, pafs, peaks))\r\n```\r\n\r\nSo then `GetPoseandCostsF` can look like this:\r\n\r\n```python\r\ndef GetPoseandCostsF():\r\n    # ... the stuff before the loop...\r\n    # make queues for concurrent processing\r\n    frame_q = mp.Queue()\r\n    extract_q_put = mp.Queue()\r\n    extract_q_get = mp.Queue()\r\n    # process to extract frames in batches\r\n    frame_proc = mp.Process(target=batch_reader, args=(cap, cfg, batchsize, frame_q))\r\n    frame_proc.start()\r\n    extractor_proc = mp.Process(target=batch_extractor, args=(extract_q_put, extract_q_get))\r\n    extractor_proc.start()\r\n\r\n    while cap.video.isOpened():\r\n\r\n        frames, inds = frame_q.get()\r\n        scmaps, locrefs, pafs, peaks = predict.predict_only(frames, sess, inputs, outputs)\r\n        extract_q_put.put((dlc_cfg, scmaps, locrefs, pafs, peaks))\r\n        try:\r\n            D = extract_q_get.get_nowait()\r\n            for ind, data in zip(inds, D):\r\n                db[\"frame\" + str(ind).zfill(strwidth)] = data\r\n\r\n                pbar.update(1)\r\n        except Empty:\r\n            pass\r\n        # the rest of the function\r\n```\r\n\r\nso that while the GPU is processing the frames, the video reader can get loading the next round of frames and the CPU part can be analyzing the last batch. It seems like tensorflow would probably have some means of feeding data to a continuously running session as well, but i don't know enough about TF to sketch that.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nwell i mean this is just a sketch, there are definitely cleaner ways of doing this. this is made sorta needlessly tricky because it looks like there's a lot of duplicated code (eg `GetPoseandCostsF` vs. `GetPoseandCostsS` which could be simplified by just separating the batch iteration logic from the main loop ), so if y'all are interested in parallel inference i would probably do some cleanup first to make it easier to write and test (like i get it, i'm dealing with some of my own code like this atm, much easier to clean up other ppls code than your own lol)\r\n\r\n**Additional context**\r\n\r\n~ gotta go fast ~\r\n","closed_by":null,"reactions":{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":1,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1750/timeline","performed_via_github_app":null,"state_reason":null}