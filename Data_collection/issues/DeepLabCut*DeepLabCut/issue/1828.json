{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828","repository_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut","labels_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828/labels{/name}","comments_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828/comments","events_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828/events","html_url":"https://github.com/DeepLabCut/DeepLabCut/issues/1828","id":1233318198,"node_id":"I_kwDOB5BM6c5JgvE2","number":1828,"title":"create_training_dataset read-only file system error with docker on cluster","user":{"login":"nmtimme","id":38138179,"node_id":"MDQ6VXNlcjM4MTM4MTc5","avatar_url":"https://avatars.githubusercontent.com/u/38138179?v=4","gravatar_id":"","url":"https://api.github.com/users/nmtimme","html_url":"https://github.com/nmtimme","followers_url":"https://api.github.com/users/nmtimme/followers","following_url":"https://api.github.com/users/nmtimme/following{/other_user}","gists_url":"https://api.github.com/users/nmtimme/gists{/gist_id}","starred_url":"https://api.github.com/users/nmtimme/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nmtimme/subscriptions","organizations_url":"https://api.github.com/users/nmtimme/orgs","repos_url":"https://api.github.com/users/nmtimme/repos","events_url":"https://api.github.com/users/nmtimme/events{/privacy}","received_events_url":"https://api.github.com/users/nmtimme/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"jeylau","id":30733203,"node_id":"MDQ6VXNlcjMwNzMzMjAz","avatar_url":"https://avatars.githubusercontent.com/u/30733203?v=4","gravatar_id":"","url":"https://api.github.com/users/jeylau","html_url":"https://github.com/jeylau","followers_url":"https://api.github.com/users/jeylau/followers","following_url":"https://api.github.com/users/jeylau/following{/other_user}","gists_url":"https://api.github.com/users/jeylau/gists{/gist_id}","starred_url":"https://api.github.com/users/jeylau/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jeylau/subscriptions","organizations_url":"https://api.github.com/users/jeylau/orgs","repos_url":"https://api.github.com/users/jeylau/repos","events_url":"https://api.github.com/users/jeylau/events{/privacy}","received_events_url":"https://api.github.com/users/jeylau/received_events","type":"User","site_admin":false},"assignees":[{"login":"jeylau","id":30733203,"node_id":"MDQ6VXNlcjMwNzMzMjAz","avatar_url":"https://avatars.githubusercontent.com/u/30733203?v=4","gravatar_id":"","url":"https://api.github.com/users/jeylau","html_url":"https://github.com/jeylau","followers_url":"https://api.github.com/users/jeylau/followers","following_url":"https://api.github.com/users/jeylau/following{/other_user}","gists_url":"https://api.github.com/users/jeylau/gists{/gist_id}","starred_url":"https://api.github.com/users/jeylau/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jeylau/subscriptions","organizations_url":"https://api.github.com/users/jeylau/orgs","repos_url":"https://api.github.com/users/jeylau/repos","events_url":"https://api.github.com/users/jeylau/events{/privacy}","received_events_url":"https://api.github.com/users/jeylau/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2022-05-12T01:00:06Z","updated_at":"2023-01-09T14:55:13Z","closed_at":"2022-05-12T11:17:40Z","author_association":"NONE","active_lock_reason":null,"body":"### Is there an existing issue for this?\n\n- [X] I have searched the existing issues\n\n### Bug description\n\nI'm attempting to run deeplabcut on a university cluster (Indiana University: Carbonate) using the deeplabcut docker (lastest-gui tag) via singularity (our university's preferred way to use dockers). I have been able to get to the create_training_dataset step (import deeplabcut, create new project, extract frames, label frames, all fine). However, when I run create_training_dataset, I get the following error:\r\n\r\nOSError: [Errno 30] Read-only file system: '/usr/local/lib/python3.8/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt'\r\n\r\nI spoke with my university IT staff and they agree that it seems like deeplabcut is trying to write to this python3.8 directory for which I do not have write permission. They suggested finding a way to bindmount this requested directory to another directory where I do have write permissions. However, we are unsure how to do this and I thought someone in the deeplabcut community might have an idea of how to address this problem. I previously had to bindmount a directory with write permissions when I open the deeplabcut docker in singularity to resolve some other write permission errors, but I'm unsure how to do this in this case. Perhaps this is a problem with singularity or how singularity handles the docker? I'd greatly appreciate any advice you can provide. Thanks for your time!\n\n### Operating System\n\nRed Hat Enterprise Linux 7\n\n### DeepLabCut version\n\n2.2.0.2\n\n### DeepLabCut mode\n\nsingle animal\n\n### Device type\n\ngpu\n\n### Steps To Reproduce\n\nUsing singularity, in the latest-gui docker, run...\r\nipython\r\nimport deeplabcut\r\ndeeplabcut.create_training_dataset(ConfigPath, augmenter_type='imgaug')\n\n### Relevant log output\n\n```shell\nDownloading a ImageNet-pretrained model from http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz....\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\n<ipython-input-4-128412801f55> in <module>\r\n----> 1 deeplabcut.create_training_dataset(ConfigPath, augmenter_type='imgaug')\r\n\r\n/usr/local/lib/python3.8/dist-packages/deeplabcut/generate_training_dataset/trainingsetmanipulation.py in create_training_dataset(config, num_shuffles, Shuffles, windows2linux, userfeedback, trainIndices, testIndices, net_type, augmenter_type)\r\n    798         dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()\r\n    799         defaultconfigfile = os.path.join(dlcparent_path, \"pose_cfg.yaml\")\r\n--> 800         model_path, num_shuffles = auxfun_models.Check4weights(\r\n    801             net_type, Path(dlcparent_path), num_shuffles\r\n    802         )\r\n\r\n/usr/local/lib/python3.8/dist-packages/deeplabcut/utils/auxfun_models.py in Check4weights(modeltype, parent_path, num_shuffles)\r\n     63         else:\r\n     64             if not model_path.is_file():\r\n---> 65                 Downloadweights(modeltype, model_path)\r\n     66 \r\n     67     return str(model_path), num_shuffles\r\n\r\n/usr/local/lib/python3.8/dist-packages/deeplabcut/utils/auxfun_models.py in Downloadweights(modeltype, model_path)\r\n     89         response = urllib.request.urlopen(url)\r\n     90         with tarfile.open(fileobj=BytesIO(response.read()), mode=\"r:gz\") as tar:\r\n---> 91             tar.extractall(path=target_dir)\r\n     92     except KeyError:\r\n     93         print(\"Model does not exist: \", modeltype)\r\n\r\n/usr/lib/python3.8/tarfile.py in extractall(self, path, members, numeric_owner)\r\n   2026                 tarinfo.mode = 0o700\r\n   2027             # Do not set_attrs directories, as we will do that further down\r\n-> 2028             self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\r\n   2029                          numeric_owner=numeric_owner)\r\n   2030 \r\n\r\n/usr/lib/python3.8/tarfile.py in extract(self, member, path, set_attrs, numeric_owner)\r\n   2067 \r\n   2068         try:\r\n-> 2069             self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\r\n   2070                                  set_attrs=set_attrs,\r\n   2071                                  numeric_owner=numeric_owner)\r\n\r\n/usr/lib/python3.8/tarfile.py in _extract_member(self, tarinfo, targetpath, set_attrs, numeric_owner)\r\n   2139 \r\n   2140         if tarinfo.isreg():\r\n-> 2141             self.makefile(tarinfo, targetpath)\r\n   2142         elif tarinfo.isdir():\r\n   2143             self.makedir(tarinfo, targetpath)\r\n\r\n/usr/lib/python3.8/tarfile.py in makefile(self, tarinfo, targetpath)\r\n   2180         source.seek(tarinfo.offset_data)\r\n   2181         bufsize = self.copybufsize\r\n-> 2182         with bltn_open(targetpath, \"wb\") as target:\r\n   2183             if tarinfo.sparse is not None:\r\n   2184                 for offset, size in tarinfo.sparse:\r\n\r\nOSError: [Errno 30] Read-only file system: '/usr/local/lib/python3.8/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt'\n```\n\n\n### Anything else?\n\n_No response_\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/DeepLabCut/DeepLabCut/blob/master/CODE_OF_CONDUCT.md)","closed_by":{"login":"MMathisLab","id":28102185,"node_id":"MDQ6VXNlcjI4MTAyMTg1","avatar_url":"https://avatars.githubusercontent.com/u/28102185?v=4","gravatar_id":"","url":"https://api.github.com/users/MMathisLab","html_url":"https://github.com/MMathisLab","followers_url":"https://api.github.com/users/MMathisLab/followers","following_url":"https://api.github.com/users/MMathisLab/following{/other_user}","gists_url":"https://api.github.com/users/MMathisLab/gists{/gist_id}","starred_url":"https://api.github.com/users/MMathisLab/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MMathisLab/subscriptions","organizations_url":"https://api.github.com/users/MMathisLab/orgs","repos_url":"https://api.github.com/users/MMathisLab/repos","events_url":"https://api.github.com/users/MMathisLab/events{/privacy}","received_events_url":"https://api.github.com/users/MMathisLab/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/1828/timeline","performed_via_github_app":null,"state_reason":"completed"}