{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837","repository_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut","labels_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837/labels{/name}","comments_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837/comments","events_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837/events","html_url":"https://github.com/DeepLabCut/DeepLabCut/issues/837","id":673432329,"node_id":"MDU6SXNzdWU2NzM0MzIzMjk=","number":837,"title":"2070 RTX: LossTensor is inf or nan (redux)","user":{"login":"tbenst","id":863327,"node_id":"MDQ6VXNlcjg2MzMyNw==","avatar_url":"https://avatars.githubusercontent.com/u/863327?v=4","gravatar_id":"","url":"https://api.github.com/users/tbenst","html_url":"https://github.com/tbenst","followers_url":"https://api.github.com/users/tbenst/followers","following_url":"https://api.github.com/users/tbenst/following{/other_user}","gists_url":"https://api.github.com/users/tbenst/gists{/gist_id}","starred_url":"https://api.github.com/users/tbenst/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbenst/subscriptions","organizations_url":"https://api.github.com/users/tbenst/orgs","repos_url":"https://api.github.com/users/tbenst/repos","events_url":"https://api.github.com/users/tbenst/events{/privacy}","received_events_url":"https://api.github.com/users/tbenst/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2020-08-05T10:29:08Z","updated_at":"2020-11-03T13:24:39Z","closed_at":"2020-08-07T00:48:40Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\nOn one of my datasets, I kept hitting https://github.com/DeepLabCut/DeepLabCut/issues/62. By displaying each iteration, I got a sense for how editing my .csv files could delay this issue. I discovered that DLC really does not like frames with no targets, i.e. [this csv file](https://gist.github.com/tbenst/dbe9616091354bc0a53ad46e51716183) reliably blows up the loss and crashes training. In this case it's fine to remove the lines with `,,,,,`, but sometimes there are frames where no targets are visible and DLC should be able to handle these cases, perhaps via gradient clipping.\r\n\r\n**To Fix (for stray search engine refugees)**\r\n1) open all csv in `labeled_data` folder in editor of choice, eg `code */*NAME.csv` for vscode\r\n2) remove all labeled frames with all blanks or mostly blank\r\n3) `dlc.convertcsv2h5(config_yaml, userfeedback=False)`\r\n3) check labels\r\n4) create training dataset\r\n\r\nedit: did not fully resolve issue :/. Everything looks normal and then suddenly....loss explodes!\r\n\r\n```\r\niteration: 190 loss: 0.0150 lr: 0.005\r\niteration: 191 loss: 0.0388 lr: 0.005\r\niteration: 192 loss: 0.0145 lr: 0.005\r\niteration: 193 loss: 0.0358 lr: 0.005\r\niteration: 194 loss: 0.0156 lr: 0.005\r\niteration: 195 loss: 0.0271 lr: 0.005\r\niteration: 196 loss: 0.0243 lr: 0.005\r\niteration: 197 loss: 0.0267 lr: 0.005\r\niteration: 198 loss: 0.0184 lr: 0.005\r\niteration: 199 loss: 0.0218 lr: 0.005\r\niteration: 200 loss: 0.0253 lr: 0.005\r\niteration: 201 loss: 0.0262 lr: 0.005\r\niteration: 202 loss: 0.0228 lr: 0.005\r\niteration: 203 loss: 0.0174 lr: 0.005\r\niteration: 204 loss: 0.0160 lr: 0.005\r\niteration: 205 loss: 0.0232 lr: 0.005\r\niteration: 206 loss: 0.0233 lr: 0.005\r\niteration: 207 loss: 0.0168 lr: 0.005\r\niteration: 208 loss: 0.0419 lr: 0.005\r\niteration: 209 loss: 0.0168 lr: 0.005\r\niteration: 210 loss: 0.0221 lr: 0.005\r\niteration: 211 loss: 0.0254 lr: 0.005\r\niteration: 212 loss: 0.0427 lr: 0.005\r\niteration: 213 loss: 0.0269 lr: 0.005\r\niteration: 214 loss: 0.0377 lr: 0.005\r\niteration: 215 loss: 0.0174 lr: 0.005\r\niteration: 216 loss: 0.0324 lr: 0.005\r\niteration: 217 loss: 0.0294 lr: 0.005\r\niteration: 218 loss: 0.0302 lr: 0.005\r\niteration: 219 loss: 0.0383 lr: 0.005\r\niteration: 220 loss: 0.0433 lr: 0.005\r\niteration: 221 loss: 0.0237 lr: 0.005\r\niteration: 222 loss: 0.0150 lr: 0.005\r\niteration: 223 loss: 0.0210 lr: 0.005\r\niteration: 224 loss: 0.0358 lr: 0.005\r\niteration: 225 loss: 0.0256 lr: 0.005\r\niteration: 226 loss: 0.0221 lr: 0.005\r\niteration: 227 loss: 0.0205 lr: 0.005\r\niteration: 228 loss: 0.0204 lr: 0.005\r\niteration: 229 loss: 0.0358 lr: 0.005\r\niteration: 230 loss: 2696.8628 lr: 0.005\r\n2020-08-05 10:45:41.525535: E tensorflow/core/kernels/check_numerics_op.cc:185] abnormal_detected_host @0x7ff448a0cc00 = {1, 0} LossTensor is inf or nan\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\r\n\t [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=[\"loc:@train_op/control_dependency\"], message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](add)]]\r\n\t [[Node: train_op/control_dependency/_1651 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10156_train_op/control_dependency\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 134, in train_network\r\n    raise e\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 132, in train_network\r\n    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep,keepdeconvweights=keepdeconvweights,allow_growth=allow_growth) #pass on path and file name for pose_cfg.yaml!\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\", line 190, in train\r\n    feed_dict={learning_rate: current_lr})\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: LossTensor is inf or nan : Tensor had NaN values\r\n\t [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=[\"loc:@train_op/control_dependency\"], message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](add)]]\r\n\t [[Node: train_op/control_dependency/_1651 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10156_train_op/control_dependency\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n\r\nCaused by op 'train_op/CheckNumerics', defined at:\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/training.py\", line 132, in train_network\r\n    train(str(poseconfigfile),displayiters,saveiters,maxiters,max_to_keep=max_snapshots_to_keep,keepdeconvweights=keepdeconvweights,allow_growth=allow_growth) #pass on path and file name for pose_cfg.yaml!\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\", line 151, in train\r\n    learning_rate, train_op = get_optimizer(total_loss, cfg)\r\n  File \"/usr/local/lib/python3.6/dist-packages/deeplabcut/pose_estimation_tensorflow/train.py\", line 102, in get_optimizer\r\n    train_op = slim.learning.create_train_op(loss_op, optimizer)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 439, in create_train_op\r\n    check_numerics=check_numerics)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/training/python/training/training.py\", line 464, in create_train_op\r\n    'LossTensor is inf or nan')\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 737, in check_numerics\r\n    \"CheckNumerics\", tensor=tensor, message=message, name=name)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nInvalidArgumentError (see above for traceback): LossTensor is inf or nan : Tensor had NaN values\r\n\t [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, _class=[\"loc:@train_op/control_dependency\"], message=\"LossTensor is inf or nan\", _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](add)]]\r\n\t [[Node: train_op/control_dependency/_1651 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_10156_train_op/control_dependency\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\n```\r\n\r\nedit2: I am running using https://github.com/DeepLabCut/Docker4DeepLabCut2.0/blob/testing/Dockerfile","closed_by":{"login":"tbenst","id":863327,"node_id":"MDQ6VXNlcjg2MzMyNw==","avatar_url":"https://avatars.githubusercontent.com/u/863327?v=4","gravatar_id":"","url":"https://api.github.com/users/tbenst","html_url":"https://github.com/tbenst","followers_url":"https://api.github.com/users/tbenst/followers","following_url":"https://api.github.com/users/tbenst/following{/other_user}","gists_url":"https://api.github.com/users/tbenst/gists{/gist_id}","starred_url":"https://api.github.com/users/tbenst/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tbenst/subscriptions","organizations_url":"https://api.github.com/users/tbenst/orgs","repos_url":"https://api.github.com/users/tbenst/repos","events_url":"https://api.github.com/users/tbenst/events{/privacy}","received_events_url":"https://api.github.com/users/tbenst/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/837/timeline","performed_via_github_app":null,"state_reason":"completed"}