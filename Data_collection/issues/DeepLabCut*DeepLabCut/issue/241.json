{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241","repository_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut","labels_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241/labels{/name}","comments_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241/comments","events_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241/events","html_url":"https://github.com/DeepLabCut/DeepLabCut/issues/241","id":420148247,"node_id":"MDU6SXNzdWU0MjAxNDgyNDc=","number":241,"title":"What are necessary files to share trained DLC models?","user":{"login":"etterguillaume","id":35970969,"node_id":"MDQ6VXNlcjM1OTcwOTY5","avatar_url":"https://avatars.githubusercontent.com/u/35970969?v=4","gravatar_id":"","url":"https://api.github.com/users/etterguillaume","html_url":"https://github.com/etterguillaume","followers_url":"https://api.github.com/users/etterguillaume/followers","following_url":"https://api.github.com/users/etterguillaume/following{/other_user}","gists_url":"https://api.github.com/users/etterguillaume/gists{/gist_id}","starred_url":"https://api.github.com/users/etterguillaume/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/etterguillaume/subscriptions","organizations_url":"https://api.github.com/users/etterguillaume/orgs","repos_url":"https://api.github.com/users/etterguillaume/repos","events_url":"https://api.github.com/users/etterguillaume/events{/privacy}","received_events_url":"https://api.github.com/users/etterguillaume/received_events","type":"User","site_admin":false},"labels":[{"id":880550040,"node_id":"MDU6TGFiZWw4ODA1NTAwNDA=","url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/labels/question","name":"question","color":"d876e3","default":true,"description":"user question (not great for github ;)"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2019-03-12T18:43:46Z","updated_at":"2019-03-13T19:31:38Z","closed_at":"2019-03-13T13:32:41Z","author_association":"NONE","active_lock_reason":null,"body":"**Your Operating system and DeepLabCut version**\r\nGoogle Colab\r\n\r\n**Describe the problem**\r\nI have trained successfully DLC models on Google Colab, and was able to perform some inference on new videos in the same environment. Now, I would like to 'export' this trained model in a small compressed file that I could keep on the cloud and download on Google Colab whenever I would like to process new videos (Google Colab being very convenient, but I guess the general question concerns sharing models in general)\r\nSince I would like to make the model as light as possible, I tried to avoid including the original training videos and the checkpoint files that are not used. However, when I try to run deeplabcut.analyze_videos(path_config_file,VideoFilePath) it seems to go through the analysis procedure seamlessly...\r\n\r\n_Using snapshot-41000 for model /content/DLC_trained_model/dlc-models/iteration-0/Open fieldFeb1-trainset95shuffle1\r\nINFO:tensorflow:Restoring parameters from /content/DLC_trained_model/dlc-models/iteration-0/Open fieldFeb1-trainset95shuffle1/train/snapshot-41000\r\nINFO:tensorflow:Restoring parameters from /content/DLC_trained_model/dlc-models/iteration-0/Open fieldFeb1-trainset95shuffle1/train/snapshot-41000\r\nThe videos are analyzed. Now your research can truly start! \r\n You can create labeled videos with 'create_labeled_video'.\r\nIf the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!_\r\n\r\n...however there is not progress bar and no videos are created. \r\nNow I know this is a lot of fiddling around so I did not expect it to work right away, but I was wondering:\r\n- What files are still necessary when running inference?\r\n- Namely, would I still need the training videos?\r\n- Do I need all checkpoints to be present or only the one being used?\r\n- Why is there no error message being issued?\r\n\r\nIn my previous training experiments that were successful, my trained models were inside the DeepLabCut git cloned folder: should my trained datasets always be inside the DeepLabCut folder?\r\n\r\nI am sure many are interested in sharing trained models easily (after all at the end of the day the training phase should be only a small portion of the real work...)\r\n\r\nThanks again for all your support and take care!","closed_by":{"login":"AlexEMG","id":20850270,"node_id":"MDQ6VXNlcjIwODUwMjcw","avatar_url":"https://avatars.githubusercontent.com/u/20850270?v=4","gravatar_id":"","url":"https://api.github.com/users/AlexEMG","html_url":"https://github.com/AlexEMG","followers_url":"https://api.github.com/users/AlexEMG/followers","following_url":"https://api.github.com/users/AlexEMG/following{/other_user}","gists_url":"https://api.github.com/users/AlexEMG/gists{/gist_id}","starred_url":"https://api.github.com/users/AlexEMG/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AlexEMG/subscriptions","organizations_url":"https://api.github.com/users/AlexEMG/orgs","repos_url":"https://api.github.com/users/AlexEMG/repos","events_url":"https://api.github.com/users/AlexEMG/events{/privacy}","received_events_url":"https://api.github.com/users/AlexEMG/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/DeepLabCut/DeepLabCut/issues/241/timeline","performed_via_github_app":null,"state_reason":"completed"}