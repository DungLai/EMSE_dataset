{"url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134","repository_url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch","labels_url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134/labels{/name}","comments_url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134/comments","events_url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134/events","html_url":"https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134","id":818534529,"node_id":"MDU6SXNzdWU4MTg1MzQ1Mjk=","number":134,"title":"Converting COCO to OpenPose error?","user":{"login":"yfedberts","id":19324916,"node_id":"MDQ6VXNlcjE5MzI0OTE2","avatar_url":"https://avatars.githubusercontent.com/u/19324916?v=4","gravatar_id":"","url":"https://api.github.com/users/yfedberts","html_url":"https://github.com/yfedberts","followers_url":"https://api.github.com/users/yfedberts/followers","following_url":"https://api.github.com/users/yfedberts/following{/other_user}","gists_url":"https://api.github.com/users/yfedberts/gists{/gist_id}","starred_url":"https://api.github.com/users/yfedberts/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yfedberts/subscriptions","organizations_url":"https://api.github.com/users/yfedberts/orgs","repos_url":"https://api.github.com/users/yfedberts/repos","events_url":"https://api.github.com/users/yfedberts/events{/privacy}","received_events_url":"https://api.github.com/users/yfedberts/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-01T06:43:32Z","updated_at":"2021-03-02T03:47:23Z","closed_at":"2021-03-02T03:47:23Z","author_association":"NONE","active_lock_reason":null,"body":"Hello there, first of all thanks for the work it's very helpful!\r\n\r\nHowever I'm confused as to how I can generate a proper output using the demo.py file and converting that to the OpenPose COCO-18 output?\r\n\r\nI'm using the `convert_to_coco()` function from val.py to write the keypoints and use the following code to convert it to OpenPose but the result is still far from what the OpenPose model produces, was wondering if you might have any insight regarding this. Any help is appreciated, thanks in advance!\r\n\r\n**CODE:**\r\n\r\n```\r\ndef convert_coco_to_openpose_cords(coco_keypoints_list):\r\n    # coco keypoints: [x1,y1,v1,...,xk,yk,vk]       (k=17)\r\n    #     ['Nose', Leye', 'Reye', 'Lear', 'Rear', 'Lsho', 'Rsho', 'Lelb',\r\n    #      'Relb', 'Lwri', 'Rwri', 'Lhip', 'Rhip', 'Lkne', 'Rkne', 'Lank', 'Rank']\r\n    # openpose keypoints: [y1,...,yk], [x1,...xk]   (k=18, with Neck)\r\n    #     ['Nose', *'Neck'*, 'Rsho', 'Relb', 'Rwri', 'Lsho', 'Lelb', 'Lwri','Rhip',\r\n    #      'Rkne', 'Rank', 'Lhip', 'Lkne', 'Lank', 'Leye', 'Reye', 'Lear', 'Rear']\r\n    indices = [0, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 1, 2, 3, 4]\r\n    y_cords = []\r\n    x_cords = []\r\n    print(coco_keypoints_list[0 * 3 + 2])\r\n    for i in indices:\r\n        xi = coco_keypoints_list[i * 3 + 0]\r\n        yi = coco_keypoints_list[i * 3 + 1]\r\n        vi = coco_keypoints_list[i * 3 + 2]\r\n        print(vi)\r\n        if vi == 0: # not labeled\r\n            y_cords.append(0)\r\n            x_cords.append(0)\r\n        elif vi == 1:   # labeled but not visible\r\n            y_cords.append(yi)\r\n            x_cords.append(xi)\r\n        elif vi == 2:   # labeled and visible\r\n            y_cords.append(yi)\r\n            x_cords.append(xi)\r\n        else:\r\n            raise ValueError(\"vi value: {}\".format(vi))\r\n    # Get 'Neck' keypoint by interpolating between 'Lsho' and 'Rsho' keypoints\r\n    l_shoulder_index = 5\r\n    r_shoulder_index = 6\r\n    l_shoulder_keypoint = coco_keypoints_list[l_shoulder_index*3:(l_shoulder_index+1)*3]\r\n    r_shoulder_keypoint = coco_keypoints_list[r_shoulder_index*3:(r_shoulder_index+1)*3]\r\n    if l_shoulder_keypoint[2] > 0 and r_shoulder_keypoint[2] > 0:\r\n        neck_keypoint_y = int((l_shoulder_keypoint[1]+r_shoulder_keypoint[1])/2.)\r\n        neck_keypoint_x = int((l_shoulder_keypoint[0]+r_shoulder_keypoint[0])/2.)\r\n    else:\r\n        neck_keypoint_y = neck_keypoint_x = MISSING_VALUE\r\n    open_pose_neck_index = 1\r\n    y_cords.insert(open_pose_neck_index, neck_keypoint_y)\r\n    x_cords.insert(open_pose_neck_index, neck_keypoint_x)\r\n    res = np.concatenate([np.expand_dims(y_cords, -1),\r\n                        np.expand_dims(x_cords, -1)], axis=1)\r\n    print(res.flatten())\r\n    return res.flatten()\r\n\r\n```\r\n\r\nFor reference I'm trying to produce the same output as the following code for OpenPose:\r\n```\r\nimport cv2\r\nimport time\r\nimport numpy as np\r\nfrom random import randint\r\nimport argparse\r\nimport json \r\n\r\nimage1 = cv2.imread(\"B:/tryonproj/viton_app/custom_images/test/image/model1.jpg\")\r\nprotoFile = \"B:/tryonproj/viton_app/openpose/models/pose/coco/pose_deploy_linevec.prototxt\"\r\nweightsFile = \"B:/tryonproj/viton_app/openpose/models/pose/coco/pose_iter_440000.caffemodel\"\r\nnPoints = 18\r\n# COCO Output Format\r\nkeypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank', 'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']\r\n\r\nPOSE_PAIRS = [[1,2], [1,5], [2,3], [3,4], [5,6], [6,7],\r\n              [1,8], [8,9], [9,10], [1,11], [11,12], [12,13],\r\n              [1,0], [0,14], [14,16], [0,15], [15,17],\r\n              [2,17], [5,16] ]\r\n\r\n# index of pafs correspoding to the POSE_PAIRS\r\n# e.g for POSE_PAIR(1,2), the PAFs are located at indices (31,32) of output, Similarly, (1,5) -> (39,40) and so on.\r\nmapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44],\r\n          [19,20], [21,22], [23,24], [25,26], [27,28], [29,30],\r\n          [47,48], [49,50], [53,54], [51,52], [55,56],\r\n          [37,38], [45,46]]\r\n\r\ncolors = [ [0,100,255], [0,100,255], [0,255,255], [0,100,255], [0,255,255], [0,100,255],\r\n         [0,255,0], [255,200,100], [255,0,255], [0,255,0], [255,200,100], [255,0,255],\r\n         [0,0,255], [255,0,0], [200,200,0], [255,0,0], [200,200,0], [0,0,0]]\r\n\r\ndef getKeypoints(probMap, threshold=0.1):\r\n\r\n    mapSmooth = cv2.GaussianBlur(probMap,(3,3),0,0)\r\n\r\n    mapMask = np.uint8(mapSmooth>threshold)\r\n    keypoints = []\r\n\r\n    #find the blobs\r\n    contours, _ = cv2.findContours(mapMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\r\n\r\n    #for each blob find the maxima\r\n    for cnt in contours:\r\n        blobMask = np.zeros(mapMask.shape)\r\n        blobMask = cv2.fillConvexPoly(blobMask, cnt, 1)\r\n        maskedProbMap = mapSmooth * blobMask\r\n        _, maxVal, _, maxLoc = cv2.minMaxLoc(maskedProbMap)\r\n        keypoints.append(maxLoc + (probMap[maxLoc[1], maxLoc[0]],))\r\n\r\n    return keypoints\r\n\r\n\r\n# Find valid connections between the different joints of a all persons present\r\ndef getValidPairs(output):\r\n    valid_pairs = []\r\n    invalid_pairs = []\r\n    n_interp_samples = 10\r\n    paf_score_th = 0.1\r\n    conf_th = 0.7\r\n    # loop for every POSE_PAIR\r\n    for k in range(len(mapIdx)):\r\n        # A->B constitute a limb\r\n        pafA = output[0, mapIdx[k][0], :, :]\r\n        pafB = output[0, mapIdx[k][1], :, :]\r\n        pafA = cv2.resize(pafA, (frameWidth, frameHeight))\r\n        pafB = cv2.resize(pafB, (frameWidth, frameHeight))\r\n\r\n        # Find the keypoints for the first and second limb\r\n        candA = detected_keypoints[POSE_PAIRS[k][0]]\r\n        candB = detected_keypoints[POSE_PAIRS[k][1]]\r\n        nA = len(candA)\r\n        nB = len(candB)\r\n\r\n        # If keypoints for the joint-pair is detected\r\n        # check every joint in candA with every joint in candB\r\n        # Calculate the distance vector between the two joints\r\n        # Find the PAF values at a set of interpolated points between the joints\r\n        # Use the above formula to compute a score to mark the connection valid\r\n\r\n        if( nA != 0 and nB != 0):\r\n            valid_pair = np.zeros((0,3))\r\n            for i in range(nA):\r\n                max_j=-1\r\n                maxScore = -1\r\n                found = 0\r\n                for j in range(nB):\r\n                    # Find d_ij\r\n                    d_ij = np.subtract(candB[j][:2], candA[i][:2])\r\n                    norm = np.linalg.norm(d_ij)\r\n                    if norm:\r\n                        d_ij = d_ij / norm\r\n                    else:\r\n                        continue\r\n                    # Find p(u)\r\n                    interp_coord = list(zip(np.linspace(candA[i][0], candB[j][0], num=n_interp_samples),\r\n                                            np.linspace(candA[i][1], candB[j][1], num=n_interp_samples)))\r\n                    # Find L(p(u))\r\n                    paf_interp = []\r\n                    for k in range(len(interp_coord)):\r\n                        paf_interp.append([pafA[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))],\r\n                                           pafB[int(round(interp_coord[k][1])), int(round(interp_coord[k][0]))] ])\r\n                    # Find E\r\n                    paf_scores = np.dot(paf_interp, d_ij)\r\n                    avg_paf_score = sum(paf_scores)/len(paf_scores)\r\n\r\n                    # Check if the connection is valid\r\n                    # If the fraction of interpolated vectors aligned with PAF is higher then threshold -> Valid Pair\r\n                    if ( len(np.where(paf_scores > paf_score_th)[0]) / n_interp_samples ) > conf_th :\r\n                        if avg_paf_score > maxScore:\r\n                            max_j = j\r\n                            maxScore = avg_paf_score\r\n                            found = 1\r\n                # Append the connection to the list\r\n                if found:\r\n                    valid_pair = np.append(valid_pair, [[candA[i][3], candB[max_j][3], maxScore]], axis=0)\r\n\r\n            # Append the detected connections to the global list\r\n            valid_pairs.append(valid_pair)\r\n        else: # If no keypoints are detected\r\n            print(\"No Connection : k = {}\".format(k))\r\n            invalid_pairs.append(k)\r\n            valid_pairs.append([])\r\n    return valid_pairs, invalid_pairs\r\n\r\n\r\n# This function creates a list of keypoints belonging to each person\r\n# For each detected valid pair, it assigns the joint(s) to a person\r\ndef getPersonwiseKeypoints(valid_pairs, invalid_pairs):\r\n    # the last number in each row is the overall score\r\n    personwiseKeypoints = -1 * np.ones((0, 19))\r\n\r\n    for k in range(len(mapIdx)):\r\n        if k not in invalid_pairs:\r\n            partAs = valid_pairs[k][:,0]\r\n            partBs = valid_pairs[k][:,1]\r\n            indexA, indexB = np.array(POSE_PAIRS[k])\r\n\r\n            for i in range(len(valid_pairs[k])):\r\n                found = 0\r\n                person_idx = -1\r\n                for j in range(len(personwiseKeypoints)):\r\n                    if personwiseKeypoints[j][indexA] == partAs[i]:\r\n                        person_idx = j\r\n                        found = 1\r\n                        break\r\n\r\n                if found:\r\n                    personwiseKeypoints[person_idx][indexB] = partBs[i]\r\n                    personwiseKeypoints[person_idx][-1] += keypoints_list[partBs[i].astype(int), 2] + valid_pairs[k][i][2]\r\n\r\n                # if find no partA in the subset, create a new subset\r\n                elif not found and k < 17:\r\n                    row = -1 * np.ones(19)\r\n                    row[indexA] = partAs[i]\r\n                    row[indexB] = partBs[i]\r\n                    # add the keypoint_scores for the two keypoints and the paf_score\r\n                    row[-1] = sum(keypoints_list[valid_pairs[k][i,:2].astype(int), 2]) + valid_pairs[k][i][2]\r\n                    personwiseKeypoints = np.vstack([personwiseKeypoints, row])\r\n    return personwiseKeypoints\r\n\r\n\r\nframeWidth = image1.shape[1]\r\nframeHeight = image1.shape[0]\r\n\r\nt = time.time()\r\nnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\r\n#if args.device == \"cpu\":\r\nnet.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)\r\nprint(\"Using CPU device\")\r\n# elif args.device == \"gpu\":\r\n#     net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\r\n#     net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\r\n#     print(\"Using GPU device\")\r\n\r\n# Fix the input Height and get the width according to the Aspect Ratio\r\ninHeight = 368\r\ninWidth = int((inHeight/frameHeight)*frameWidth)\r\n\r\ninpBlob = cv2.dnn.blobFromImage(image1, 1.0 / 255, (inWidth, inHeight),\r\n                          (0, 0, 0), swapRB=False, crop=False)\r\n\r\nnet.setInput(inpBlob)\r\noutput = net.forward()\r\nprint(\"Time Taken in forward pass = {}\".format(time.time() - t))\r\n\r\ndetected_keypoints = []\r\nkeypoints_list = np.zeros((0,3))\r\nkeypoint_id = 0\r\nthreshold = 0.1\r\n\r\nfor part in range(nPoints):\r\n    probMap = output[0,part,:,:]\r\n    probMap = cv2.resize(probMap, (image1.shape[1], image1.shape[0]))\r\n    keypoints = getKeypoints(probMap, threshold)\r\n    print(\"Keypoints - {} : {}\".format(keypointsMapping[part], keypoints))\r\n    keypoints_with_id = []\r\n    for i in range(len(keypoints)):\r\n        keypoints_with_id.append(keypoints[i] + (keypoint_id,))\r\n        keypoints_list = np.vstack([keypoints_list, keypoints[i]])\r\n        keypoint_id += 1\r\n\r\n    detected_keypoints.append(keypoints_with_id)\r\n\r\n\r\nframeClone = image1.copy()\r\npose_keypoints = []\r\nfor i in range(nPoints):\r\n    if detected_keypoints[i] ==[]:\r\n        pose_keypoints.append(0)\r\n        pose_keypoints.append(0)\r\n        pose_keypoints.append(0)       \r\n\r\n    for j in range(len(detected_keypoints[i])):\r\n\r\n        pose_keypoints.append(detected_keypoints[i][j][0])\r\n        pose_keypoints.append(detected_keypoints[i][j][1])\r\n        pose_keypoints.append(detected_keypoints[i][j][2].astype(float))\r\n        cv2.circle(frameClone, detected_keypoints[i][j][0:2], 5, colors[i], -1, cv2.LINE_AA)\r\n\r\njson_data = {\"version\": 1.0, \"people\": [\r\n            {\"face_keypoints\": [],\r\n            \"pose_keypoints\":pose_keypoints,\r\n            \"hand_right_keypoints\": [], \r\n            \"hand_left_keypoints\": []\r\n            }]}\r\n\r\nwith open('img1.json', 'w') as outfile:\r\n    json.dump(json_data, outfile)\r\n\r\ncv2.imwrite(\"Detected_Pose.jpg\" , frameClone)\r\n```\r\n\r\n**Expected Output**\r\n`\r\n[111, 30, 0.7532092332839966, 102, 80, 0.6586357951164246, 73, 75, 0.6044015884399414, 61, 141, 0.38997340202331543, 51, 197, 0.6426763534545898, 134, 85, 0.5974704027175903, 144, 146, 0.4016340970993042, 135, 198, 0.18667912483215332, 73, 201, 0.41162723302841187, 53, 254, 0.2564643919467926, 0, 0, 0, 111, 202, 0.3822706639766693, 100, 254, 0.1309833526611328, 0, 0, 0, 101, 23, 0.7547257542610168, 118, 24, 0.7571808695793152, 91, 30, 0.6050272583961487, 133, 34, 0.6260395646095276]\r\n`\r\n\r\n**Produced Output**\r\n`[112.5, 30.5, 1.0, 105.0, 81.0, 1.0, 74.5, 76.5, 1.0, 64.5, 136.5, 1.0, 50.5, 196.5, 1.0, 136.5, 86.5, 1.0, 140.5, 156.5, 1.0, 132.5, 218.5, 1.0, 72.5, 198.5, 1.0, 32.5, 254.5, 1.0, 0.0, 0.0, 0.0, 112.5, 204.5, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 122.5, 26.5, 1.0, 104.5, 24.5, 1.0, 132.5, 34.5, 1.0, 94.5, 30.5, 1.0]`\r\n\r\nI'm a bit new at this so sorry if it's something obvious that I missed but any help would be very much appreciated, thanks again!","closed_by":{"login":"yfedberts","id":19324916,"node_id":"MDQ6VXNlcjE5MzI0OTE2","avatar_url":"https://avatars.githubusercontent.com/u/19324916?v=4","gravatar_id":"","url":"https://api.github.com/users/yfedberts","html_url":"https://github.com/yfedberts","followers_url":"https://api.github.com/users/yfedberts/followers","following_url":"https://api.github.com/users/yfedberts/following{/other_user}","gists_url":"https://api.github.com/users/yfedberts/gists{/gist_id}","starred_url":"https://api.github.com/users/yfedberts/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yfedberts/subscriptions","organizations_url":"https://api.github.com/users/yfedberts/orgs","repos_url":"https://api.github.com/users/yfedberts/repos","events_url":"https://api.github.com/users/yfedberts/events{/privacy}","received_events_url":"https://api.github.com/users/yfedberts/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/Daniil-Osokin/lightweight-human-pose-estimation.pytorch/issues/134/timeline","performed_via_github_app":null,"state_reason":"completed"}