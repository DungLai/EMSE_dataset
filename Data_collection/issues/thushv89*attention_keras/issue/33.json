{"url":"https://api.github.com/repos/thushv89/attention_keras/issues/33","repository_url":"https://api.github.com/repos/thushv89/attention_keras","labels_url":"https://api.github.com/repos/thushv89/attention_keras/issues/33/labels{/name}","comments_url":"https://api.github.com/repos/thushv89/attention_keras/issues/33/comments","events_url":"https://api.github.com/repos/thushv89/attention_keras/issues/33/events","html_url":"https://github.com/thushv89/attention_keras/issues/33","id":577440624,"node_id":"MDU6SXNzdWU1Nzc0NDA2MjQ=","number":33,"title":"Bahdanau attention","user":{"login":"nilavghosh","id":657155,"node_id":"MDQ6VXNlcjY1NzE1NQ==","avatar_url":"https://avatars.githubusercontent.com/u/657155?v=4","gravatar_id":"","url":"https://api.github.com/users/nilavghosh","html_url":"https://github.com/nilavghosh","followers_url":"https://api.github.com/users/nilavghosh/followers","following_url":"https://api.github.com/users/nilavghosh/following{/other_user}","gists_url":"https://api.github.com/users/nilavghosh/gists{/gist_id}","starred_url":"https://api.github.com/users/nilavghosh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nilavghosh/subscriptions","organizations_url":"https://api.github.com/users/nilavghosh/orgs","repos_url":"https://api.github.com/users/nilavghosh/repos","events_url":"https://api.github.com/users/nilavghosh/events{/privacy}","received_events_url":"https://api.github.com/users/nilavghosh/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-03-08T03:55:09Z","updated_at":"2020-06-06T23:28:21Z","closed_at":"2020-06-06T23:27:44Z","author_association":"NONE","active_lock_reason":null,"body":"https://github.com/thushv89/attention_keras/blob/f7c6f40cb207431d0229c38992eb93ad17d38e20/examples/nmt/model.py#L30\r\n\r\nIs the implementation here a variation of the Bahdanau attention paper?. As per the paper during training the alignment vector is concatenated with the embedded target of the previous timestep then this vector is supplied to the decoder. \r\n\r\nhttps://github.com/thushv89/attention_keras/blob/f7c6f40cb207431d0229c38992eb93ad17d38e20/examples/nmt/model.py#L35\r\nIn the code base here, this concatenated vector is directly 'softmaxed' to get the predicted output.\r\n\r\nAre these implementation fundamentally the same?","closed_by":{"login":"thushv89","id":1381369,"node_id":"MDQ6VXNlcjEzODEzNjk=","avatar_url":"https://avatars.githubusercontent.com/u/1381369?v=4","gravatar_id":"","url":"https://api.github.com/users/thushv89","html_url":"https://github.com/thushv89","followers_url":"https://api.github.com/users/thushv89/followers","following_url":"https://api.github.com/users/thushv89/following{/other_user}","gists_url":"https://api.github.com/users/thushv89/gists{/gist_id}","starred_url":"https://api.github.com/users/thushv89/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thushv89/subscriptions","organizations_url":"https://api.github.com/users/thushv89/orgs","repos_url":"https://api.github.com/users/thushv89/repos","events_url":"https://api.github.com/users/thushv89/events{/privacy}","received_events_url":"https://api.github.com/users/thushv89/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/thushv89/attention_keras/issues/33/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/thushv89/attention_keras/issues/33/timeline","performed_via_github_app":null,"state_reason":"completed"}