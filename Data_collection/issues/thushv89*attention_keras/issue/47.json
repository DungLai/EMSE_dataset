{"url":"https://api.github.com/repos/thushv89/attention_keras/issues/47","repository_url":"https://api.github.com/repos/thushv89/attention_keras","labels_url":"https://api.github.com/repos/thushv89/attention_keras/issues/47/labels{/name}","comments_url":"https://api.github.com/repos/thushv89/attention_keras/issues/47/comments","events_url":"https://api.github.com/repos/thushv89/attention_keras/issues/47/events","html_url":"https://github.com/thushv89/attention_keras/issues/47","id":822054092,"node_id":"MDU6SXNzdWU4MjIwNTQwOTI=","number":47,"title":"ValueError: The first argument to `Layer.call` must always be passed.","user":{"login":"SuryaVikram","id":24975126,"node_id":"MDQ6VXNlcjI0OTc1MTI2","avatar_url":"https://avatars.githubusercontent.com/u/24975126?v=4","gravatar_id":"","url":"https://api.github.com/users/SuryaVikram","html_url":"https://github.com/SuryaVikram","followers_url":"https://api.github.com/users/SuryaVikram/followers","following_url":"https://api.github.com/users/SuryaVikram/following{/other_user}","gists_url":"https://api.github.com/users/SuryaVikram/gists{/gist_id}","starred_url":"https://api.github.com/users/SuryaVikram/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SuryaVikram/subscriptions","organizations_url":"https://api.github.com/users/SuryaVikram/orgs","repos_url":"https://api.github.com/users/SuryaVikram/repos","events_url":"https://api.github.com/users/SuryaVikram/events{/privacy}","received_events_url":"https://api.github.com/users/SuryaVikram/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-04T11:43:47Z","updated_at":"2022-03-18T03:51:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello, thank you for sharing this.\r\n\r\nI am getting this error when am trying to run this in Colab\r\n\"ValueError: The first argument to `Layer.call` must always be passed.\"\r\n\r\n\r\nThis is my model code:\r\nfrom attention import AttentionLayer\r\n\r\nfrom keras import backend as K \r\nK.clear_session() \r\nlatent_dim = 100 \r\nembedding_dim=100\r\n\r\n# Encoder \r\nencoder_inputs = Input(shape=(max_len_text,)) \r\nenc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \r\n\r\n#LSTM 1 \r\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \r\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \r\n\r\n#LSTM 2 \r\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \r\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \r\n\r\n#LSTM 3 \r\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \r\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \r\n\r\n# Set up the decoder. \r\ndecoder_inputs = Input(shape=(None,)) \r\ndec_emb_layer = Embedding(y_voc_size, latent_dim,trainable=True) \r\ndec_emb = dec_emb_layer(decoder_inputs) \r\n\r\n#LSTM using encoder_states as initial state\r\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \r\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \r\n\r\n#Attention Layer\r\nattn_layer = AttentionLayer(name='attention_layer') \r\nattn_out, attn_states = attn_layer()([encoder_outputs, decoder_outputs]) \r\n\r\n# Concat attention output and decoder LSTM output \r\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\r\n\r\n#Dense layer\r\ndecoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \r\ndecoder_outputs = decoder_dense(decoder_outputs) \r\n\r\n# Define the model\r\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \r\nmodel.summary()\r\n\r\n----------------------------------------------------------\r\nPlease advice if I am missing something, thank you","closed_by":null,"reactions":{"url":"https://api.github.com/repos/thushv89/attention_keras/issues/47/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/thushv89/attention_keras/issues/47/timeline","performed_via_github_app":null,"state_reason":null}