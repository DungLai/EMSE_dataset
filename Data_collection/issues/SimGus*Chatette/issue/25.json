{"url":"https://api.github.com/repos/SimGus/Chatette/issues/25","repository_url":"https://api.github.com/repos/SimGus/Chatette","labels_url":"https://api.github.com/repos/SimGus/Chatette/issues/25/labels{/name}","comments_url":"https://api.github.com/repos/SimGus/Chatette/issues/25/comments","events_url":"https://api.github.com/repos/SimGus/Chatette/issues/25/events","html_url":"https://github.com/SimGus/Chatette/issues/25","id":518099737,"node_id":"MDU6SXNzdWU1MTgwOTk3Mzc=","number":25,"title":"Theory question: using word vectors for similarity generation","user":{"login":"Zylatis","id":1456631,"node_id":"MDQ6VXNlcjE0NTY2MzE=","avatar_url":"https://avatars.githubusercontent.com/u/1456631?v=4","gravatar_id":"","url":"https://api.github.com/users/Zylatis","html_url":"https://github.com/Zylatis","followers_url":"https://api.github.com/users/Zylatis/followers","following_url":"https://api.github.com/users/Zylatis/following{/other_user}","gists_url":"https://api.github.com/users/Zylatis/gists{/gist_id}","starred_url":"https://api.github.com/users/Zylatis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Zylatis/subscriptions","organizations_url":"https://api.github.com/users/Zylatis/orgs","repos_url":"https://api.github.com/users/Zylatis/repos","events_url":"https://api.github.com/users/Zylatis/events{/privacy}","received_events_url":"https://api.github.com/users/Zylatis/received_events","type":"User","site_admin":false},"labels":[{"id":1026174248,"node_id":"MDU6TGFiZWwxMDI2MTc0MjQ4","url":"https://api.github.com/repos/SimGus/Chatette/labels/question","name":"question","color":"AB47BC","default":true,"description":"Interrogation on the functioning of the program"},{"id":1138683350,"node_id":"MDU6TGFiZWwxMTM4NjgzMzUw","url":"https://api.github.com/repos/SimGus/Chatette/labels/improvement","name":"improvement","color":"4DD0E1","default":false,"description":"Improvement of an existing feature"},{"id":1190793207,"node_id":"MDU6TGFiZWwxMTkwNzkzMjA3","url":"https://api.github.com/repos/SimGus/Chatette/labels/feedback","name":"feedback","color":"AB47BC","default":false,"description":"User comment"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2019-11-05T22:44:56Z","updated_at":"2019-11-10T20:39:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey,\r\n\r\nI'm a big fan of Rasa and these NLU-set generation platforms, but in my experience (as noted) they can quickly lead to overfitting as it can be hard to generate the true range of data you might expect from real labeled data (perhaps an unrealistic expectation).\r\n\r\nI think, in part, the reason for this is the inability of rule based substitutions/synonyms to really capture this variety. \r\n\r\nThus, I wonder if it might be useful to explore substitutions based on some unsupervised embeddings. For example, rather than specifying synonyms we use a word2vec model to choose words based on similarity. One could even go further and use something like BERT to utilise context.\r\n\r\nThis might seem a bit circular but in my mind is akin to semi-supervised learning. The assumption would be of course that the word vectors are appropriate to the domain or application. That being said, the w2v process is unsupervised and so people who do have domain specific data, even if unlabelled, could benefit from it.\r\n\r\nA couple of motivating examples. I have been looking at building an NLU system to extract intent and entities for a chatbot to give quotes for a freight company. Part of the issue here is that some of the entities needed are address components (cities, suburbs, postcodes) which can be a bit tricky, even with a gazetter. The paragraphs we would like to process are also sometimes quite long. I have found that intent classification was quite straight forward, but the NER was harder (also needs to extract dimensions like length, width, height and weight). The variety in the observed data is significant. For example\r\n\r\n```\r\n- Just wanted to check if you pick up a fridge from [sydney](suburb) and deliver to the [Hunter valley](suburb)\r\n- Customer is missing [1,000](quantity) of item XXXXX from order. Weight is approx [500 lbs](weight)\r\n- I have made a booking for a package to be delivered from [Ballarat](suburb) to [Port Macquarie](suburb) starting Monday.\r\n- Hi I have a client in [Moranbah](suburb) [QLD](state) [4744](postcode) which wishes to pick up from the freight depo. were is the freight depo at [Moranbah](suburb) and the address please?\r\n``` \r\n\r\nThese are all quite similar but I think, maybe naively and if so please do prove me wrong, quite hard to extract good DSL rules to generate things like this.\r\n\r\nSimilarly, it would be very interesting if one could actually 'train' the DSL rules based on an input dataset, again using word vectors.\r\n\r\nApologies for the long post and if this is the wrong forum for this. I think these tools are crucial for NLU and I'm just looking for ways to extend their applicability. ","closed_by":null,"reactions":{"url":"https://api.github.com/repos/SimGus/Chatette/issues/25/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/SimGus/Chatette/issues/25/timeline","performed_via_github_app":null,"state_reason":null}