{"url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69","repository_url":"https://api.github.com/repos/NRCan/geo-deep-learning","labels_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69/labels{/name}","comments_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69/comments","events_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69/events","html_url":"https://github.com/NRCan/geo-deep-learning/issues/69","id":420579578,"node_id":"MDU6SXNzdWU0MjA1Nzk1Nzg=","number":69,"title":"Lidar point cloud classification","user":{"login":"ymoisan","id":7468595,"node_id":"MDQ6VXNlcjc0Njg1OTU=","avatar_url":"https://avatars.githubusercontent.com/u/7468595?v=4","gravatar_id":"","url":"https://api.github.com/users/ymoisan","html_url":"https://github.com/ymoisan","followers_url":"https://api.github.com/users/ymoisan/followers","following_url":"https://api.github.com/users/ymoisan/following{/other_user}","gists_url":"https://api.github.com/users/ymoisan/gists{/gist_id}","starred_url":"https://api.github.com/users/ymoisan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ymoisan/subscriptions","organizations_url":"https://api.github.com/users/ymoisan/orgs","repos_url":"https://api.github.com/users/ymoisan/repos","events_url":"https://api.github.com/users/ymoisan/events{/privacy}","received_events_url":"https://api.github.com/users/ymoisan/received_events","type":"User","site_admin":false},"labels":[{"id":1049334785,"node_id":"MDU6TGFiZWwxMDQ5MzM0Nzg1","url":"https://api.github.com/repos/NRCan/geo-deep-learning/labels/duplicate","name":"duplicate","color":"cfd3d7","default":true,"description":"This issue or pull request already exists"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2019-03-13T15:55:04Z","updated_at":"2019-09-04T13:25:46Z","closed_at":"2019-09-04T13:25:46Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"> IMPORTANT : This is a duplicate of #2\r\n\r\n# Lidar point cloud classification\r\n\r\n## Context\r\n\r\nAirborne [lidar systems](https://en.wikipedia.org/wiki/Lidar) generate data in the form of [point clouds](https://en.wikipedia.org/wiki/Point_cloud).  The American Society for Photogrammetry and Remote Sensing (ASPRS) -- which now tags itself as *The Imaging and Geospatial Information Society* -- has a [Lidar division](https://www.asprs.org/divisions-committees/lidar-division) the mission of which is \"to provide a forum for collection development and dissemination of information related to the best practices in developing maintaining and operating kinematic laser scanners and associated sensors\".  \r\n\r\nThe *LAS Working Group* maintains the [LAS file format](https://www.asprs.org/divisions-committees/lidar-division/laser-las-file-format-exchange-activities) standard, which is the default format for storing lidar data points.  The latest version of the standard, [LAS 1.4](https://www.asprs.org/wp-content/uploads/2010/12/LAS_1_4_r13.pdf), provides the structure of an individual point data record:\r\n\r\n**Table 1: Point Data Record Format 0**\r\n\r\n| Item                            | Format            | Size    | Required |\r\n|---------------------------------|-------------------|---------|----------|\r\n| X                               | long              | 4 bytes | *        |\r\n| Y                               | long              | 4 bytes | *        |\r\n| Z                               | long              | 4 bytes | *        |\r\n| Intensity                       | unsigned short    | 2 bytes |          |\r\n| Return Number                   | 3 bits (bits 0–2) | 3 bits  | *        |\r\n| Number of Returns (given pulse) | 3 bits (bits 3–5) | 3 bits  | *        |\r\n| Scan Direction Flag             | 1 bit (bit 6)     | 1 bit   | *        |\r\n| Edge of Flight Line             | 1 bit (bit 7)     | 1 bit   | *        |\r\n| **Classification**              | **unsigned char**     | **1 byte**  | *        |\r\n| Scan Angle Rank                 | char              | 1 byte  | *        |\r\n| User Data                       | unsigned char     | 1 byte  |          |\r\n| Point Source ID                 | unsigned short    | 2 bytes | *        |\r\n\r\n<br>\r\n\r\nThe **classification** item in the table above is further specified in the following two tables:\r\n<br>\r\n\r\n**Table 2: Classification Bit definition (field encoding)**\r\n\r\n| Bit | Field Name | Description |\r\n|---------|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------|\r\n| **0:4** | **Classification** | **Standard ASPRS classification from 0 - 31 as defined in the classification table for legacy point formats (see table 3 below)** |\r\n| 5 | Synthetic | If set then this point was created by a technique other than LIDAR collection such as digitized from a photogrammetric stereo model or by traversing a waveform. |\r\n| 6 | Key-point | If set, this point is considered to be a model key-point and thus generally should not be withheld in a thinning algorithm. |\r\n| 7 | Withheld | f set, this point should not be included in processing (synonymous with Deleted). |                                                                               |\r\n\r\n<br>\r\n**Table 3: ASPRS Standard LIDAR Point Class Values**\r\n\r\n\r\n| Classification Value **(bits 0:4)** | Meaning |\r\n|---------------------------------|-------------------------------|\r\n| 0 | Created, never classified |\r\n| 1 | Unclassified |\r\n| 2 | Ground |\r\n| 3 | Low Vegetation |\r\n| 4 | Medium Vegetation |\r\n| 5 | High Vegetation |\r\n| 6 | Building |\r\n| 7 | Low Point (noise) |\r\n| 8 | Model Key-point (mass point) |\r\n| 9 | Water |\r\n| 10 | Reserved for ASPRS Definition |\r\n| 11 | Reserved for ASPRS Definition |\r\n| 12 | Overlap Points2 |\r\n| 13-31 | Reserved for ASPRS Definition |\r\n\r\n<br>\r\nTherefore, the format for the classification field is a bit encoding with the lower five bits used for the class value (as shown in table 3 above) and the three high bits used for flags.\r\n\r\n## Problem\r\n\r\nThe issue here is that the classification of lidar points is both expensive (typically 30 % of total data acquisition cost) and rather unreliable.  We would like to devise a deep learning approach to help us classify data points independently, that is irrespective of class values assigned (or not) by the data provider.\r\n\r\n\r\n# Literature review\r\n\r\nThere are roughly two types of methods for lidar point cloud classification using DL:\r\n\r\n- rasterization of certain point atttributes so that CNNs may be used\r\n- direct manipulation of the point cloud, for example using voxels\r\n\r\nThe methods we are most interested in are those that have a PyTorch implementation.\r\n\r\n## CNN\r\n\r\n### [Classifying airborne LiDAR point clouds via deep features learned by a multi-scale convolutional neural network](https://www.tandfonline.com/doi/abs/10.1080/13658816.2018.1431840?journalCode=tgis20)\r\n\r\n\"With several selected attributes of LiDAR point clouds, our method first creates a group of multi-scale contextual images for each point in the data using interpolation. Taking the contextual images as inputs, a multi-scale convolutional neural network (MCNN) is then designed and trained to learn the deep features of LiDAR points across various scales. A softmax regression classifier (SRC) is finally employed to generate classification results of the data with a combination of the deep features learned from various scales. \"\r\n\r\n### [3D Point Cloud Classification and Segmentation using 3D Modified Fisher Vector Representation for Convolutional Neural Networks](https://arxiv.org/abs/1711.08241)\r\n\r\n\"The point cloud ... the common solution [for classification] of transforming the data into a 3D voxel grid introduces its own challenges, mainly large memory size ... we propose a novel 3D point cloud representation called 3D Modified Fisher Vectors (3DmFV) ... it combines the discrete structure of a grid with continuous generalization of Fisher vectors ... Using the grid enables us to design a new CNN architecture for point cloud classification and part segmentation.\"\r\n\r\n### [Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs](https://github.com/loicland/superpoint_graph)\r\n\r\n## Point clouds\r\n\r\nUsing CNNs implies rasterization of point attribute values, which in turn implies degradation of the level of information contained in the point cloud.  Ideally, we would like our algorithms to work directly in the point cloud.  The main problem here is the data volume.  We should strive to find approaches that minimize data I/O.  Things like [Entwine](https://entwine.io/) may allow us to request point data for training or inference via web services and therefore avoid  data duplication.  \r\n\r\n### [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593)\r\n\r\n> Point cloud ... due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. \r\n\r\n\"This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective.\"\r\n\r\n### [Spherical Convolutional Neural Network for 3D Point Clouds](https://arxiv.org/abs/1805.07872)\r\n\r\n\"We propose a neural network for 3D point cloud processing that exploits `spherical' convolution kernels and octree partitioning of space. The proposed metric-based spherical kernels systematically quantize point neighborhoods to identify local geometric structures in data ... The network architecture itself is guided by octree data structuring that takes full advantage of the sparse nature of irregular point clouds. We specify spherical kernels with the help of neurons in each layer that in turn are associated with spatial locations. We exploit this association to avert dynamic kernel generation during network training, that enables efficient learning with high resolution point clouds. We demonstrate the utility of the spherical convolutional neural network for 3D object classification on standard benchmark datasets.\"\r\n\r\n### [Interactive Visualization of 10M+ 3D Points with New Open-Source Python Package PPTK](https://developer.here.com/blog/interactive-visualization-of-10m-3d-points-with-new-open-source-python-package-pptk)\r\n\r\n\"The PPTK viewer is part of a larger effort of developing a Python toolkit for not only visualizing but also processing point data.\"\r\n","closed_by":{"login":"mpelchat04","id":38693210,"node_id":"MDQ6VXNlcjM4NjkzMjEw","avatar_url":"https://avatars.githubusercontent.com/u/38693210?v=4","gravatar_id":"","url":"https://api.github.com/users/mpelchat04","html_url":"https://github.com/mpelchat04","followers_url":"https://api.github.com/users/mpelchat04/followers","following_url":"https://api.github.com/users/mpelchat04/following{/other_user}","gists_url":"https://api.github.com/users/mpelchat04/gists{/gist_id}","starred_url":"https://api.github.com/users/mpelchat04/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mpelchat04/subscriptions","organizations_url":"https://api.github.com/users/mpelchat04/orgs","repos_url":"https://api.github.com/users/mpelchat04/repos","events_url":"https://api.github.com/users/mpelchat04/events{/privacy}","received_events_url":"https://api.github.com/users/mpelchat04/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/69/timeline","performed_via_github_app":null,"state_reason":"completed"}