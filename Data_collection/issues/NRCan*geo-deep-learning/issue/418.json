{"url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418","repository_url":"https://api.github.com/repos/NRCan/geo-deep-learning","labels_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418/labels{/name}","comments_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418/comments","events_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418/events","html_url":"https://github.com/NRCan/geo-deep-learning/issues/418","id":1482702205,"node_id":"I_kwDOCNASdM5YYD19","number":418,"title":"SegmentationDataset: read patches csv in __init__ and store content in memory for later use","user":{"login":"remtav","id":34774759,"node_id":"MDQ6VXNlcjM0Nzc0NzU5","avatar_url":"https://avatars.githubusercontent.com/u/34774759?v=4","gravatar_id":"","url":"https://api.github.com/users/remtav","html_url":"https://github.com/remtav","followers_url":"https://api.github.com/users/remtav/followers","following_url":"https://api.github.com/users/remtav/following{/other_user}","gists_url":"https://api.github.com/users/remtav/gists{/gist_id}","starred_url":"https://api.github.com/users/remtav/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/remtav/subscriptions","organizations_url":"https://api.github.com/users/remtav/orgs","repos_url":"https://api.github.com/users/remtav/repos","events_url":"https://api.github.com/users/remtav/events{/privacy}","received_events_url":"https://api.github.com/users/remtav/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2022-12-07T19:43:09Z","updated_at":"2022-12-07T19:43:49Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Not a priority: SegmentationDataset is going to get a huge refactoring in the course of the next weeks while addressing #152, which already started with PR #406\r\n\r\n***\r\n\r\nCsvs created during tiling list all paths to created patches. With large datasets, these csvs contain thousands of rows.\r\n\r\nWe should store the content of csv in memory to start with rather that [open it every time the \\_\\_getitem\\_\\_ method is used](https://github.com/NRCan/geo-deep-learning/blob/develop/dataset/create_dataset.py#L76) in SegmentationDataset, then select the row with specific index we want. Also, using pandas to read csv at init would probably be slightly faster. \r\n\r\nThe current implementation, with the csv being constantly read into memory is probably creating a fair amount of overhead during training (although maybe not if num_workers > 0).\r\n\r\nalready implemented in this draft branch on torchgeo:\r\nhttps://github.com/remtav/torchgeo/blob/ccmeo-dataset/torchgeo/datasets/ccmeo.py#L170","closed_by":null,"reactions":{"url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/NRCan/geo-deep-learning/issues/418/timeline","performed_via_github_app":null,"state_reason":null}