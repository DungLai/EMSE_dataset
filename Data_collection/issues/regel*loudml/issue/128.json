{"url":"https://api.github.com/repos/regel/loudml/issues/128","repository_url":"https://api.github.com/repos/regel/loudml","labels_url":"https://api.github.com/repos/regel/loudml/issues/128/labels{/name}","comments_url":"https://api.github.com/repos/regel/loudml/issues/128/comments","events_url":"https://api.github.com/repos/regel/loudml/issues/128/events","html_url":"https://github.com/regel/loudml/issues/128","id":472196207,"node_id":"MDU6SXNzdWU0NzIxOTYyMDc=","number":128,"title":"Latent/reconstruction loss function might have an error","user":{"login":"EpsDel","id":53219730,"node_id":"MDQ6VXNlcjUzMjE5NzMw","avatar_url":"https://avatars.githubusercontent.com/u/53219730?v=4","gravatar_id":"","url":"https://api.github.com/users/EpsDel","html_url":"https://github.com/EpsDel","followers_url":"https://api.github.com/users/EpsDel/followers","following_url":"https://api.github.com/users/EpsDel/following{/other_user}","gists_url":"https://api.github.com/users/EpsDel/gists{/gist_id}","starred_url":"https://api.github.com/users/EpsDel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/EpsDel/subscriptions","organizations_url":"https://api.github.com/users/EpsDel/orgs","repos_url":"https://api.github.com/users/EpsDel/repos","events_url":"https://api.github.com/users/EpsDel/events{/privacy}","received_events_url":"https://api.github.com/users/EpsDel/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2019-07-24T10:09:55Z","updated_at":"2019-10-10T05:34:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have recalculated the latent loss function from the M-ELBO under the assumptions made in the original DONUT paper (https://arxiv.org/abs/1802.03903) and got different results, from what you used in `donut.py`. Perfomance might improve when using the correct latent loss. In the pdf you can find my derivations and detailed explanations.\r\n\r\n[Latent_Loss_Issue.pdf](https://github.com/regel/loudml/files/3425922/Latent_Loss_Git_Issue.pdf)\r\n\r\nFurthermore, you don't parametrize the variance in the decoder, as is done in the original DONUT model, which leads to a different reconstruction loss. You can see this by computing the maximum likelihood function of the multivariate Normal distribution, which will then additionally depend on the variance. This is fine in general, but the advantages of modeling the variance is a slightly more expressive model, and the ability to say that the model is more uncertain about some dimensions than other dimensions. The disadvantage of parametrizing the variance is a higher vulnerability to overfitting a small dataset by memorizing the mean perfectly and letting the variance converge to zero, leading to negative infinity loss.\r\n\r\nAlthough there are many more things to say concerning the architecture and type of the autoencoder, I want to bring the following to your attention: Using the logarithmic variance as output of the encoder allows it to capture variances of different scale more efficiently and is common practise. However, notice that the logarithm is (negatively) unbounded for values approaching zero. This causes the ReLU activation functions to saturate and the weights input to these neurons will not be updated anymore during the gradient descent (or possibly a modified version thereof). This problem can be mitigated by using the approach in the original DONUT paper (section 3.1) where an epsilon trick is used.","closed_by":null,"reactions":{"url":"https://api.github.com/repos/regel/loudml/issues/128/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/regel/loudml/issues/128/timeline","performed_via_github_app":null,"state_reason":null}