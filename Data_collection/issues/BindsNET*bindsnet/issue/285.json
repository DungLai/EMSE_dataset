{"url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285","repository_url":"https://api.github.com/repos/BindsNET/bindsnet","labels_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285/labels{/name}","comments_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285/comments","events_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285/events","html_url":"https://github.com/BindsNET/bindsnet/issues/285","id":458824152,"node_id":"MDU6SXNzdWU0NTg4MjQxNTI=","number":285,"title":"Order of things in EnvironmentPipeline","user":{"login":"Huizerd","id":15855769,"node_id":"MDQ6VXNlcjE1ODU1NzY5","avatar_url":"https://avatars.githubusercontent.com/u/15855769?v=4","gravatar_id":"","url":"https://api.github.com/users/Huizerd","html_url":"https://github.com/Huizerd","followers_url":"https://api.github.com/users/Huizerd/followers","following_url":"https://api.github.com/users/Huizerd/following{/other_user}","gists_url":"https://api.github.com/users/Huizerd/gists{/gist_id}","starred_url":"https://api.github.com/users/Huizerd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Huizerd/subscriptions","organizations_url":"https://api.github.com/users/Huizerd/orgs","repos_url":"https://api.github.com/users/Huizerd/repos","events_url":"https://api.github.com/users/Huizerd/events{/privacy}","received_events_url":"https://api.github.com/users/Huizerd/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":{"login":"dee0512","id":7398932,"node_id":"MDQ6VXNlcjczOTg5MzI=","avatar_url":"https://avatars.githubusercontent.com/u/7398932?v=4","gravatar_id":"","url":"https://api.github.com/users/dee0512","html_url":"https://github.com/dee0512","followers_url":"https://api.github.com/users/dee0512/followers","following_url":"https://api.github.com/users/dee0512/following{/other_user}","gists_url":"https://api.github.com/users/dee0512/gists{/gist_id}","starred_url":"https://api.github.com/users/dee0512/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dee0512/subscriptions","organizations_url":"https://api.github.com/users/dee0512/orgs","repos_url":"https://api.github.com/users/dee0512/repos","events_url":"https://api.github.com/users/dee0512/events{/privacy}","received_events_url":"https://api.github.com/users/dee0512/received_events","type":"User","site_admin":false},"assignees":[{"login":"dee0512","id":7398932,"node_id":"MDQ6VXNlcjczOTg5MzI=","avatar_url":"https://avatars.githubusercontent.com/u/7398932?v=4","gravatar_id":"","url":"https://api.github.com/users/dee0512","html_url":"https://github.com/dee0512","followers_url":"https://api.github.com/users/dee0512/followers","following_url":"https://api.github.com/users/dee0512/following{/other_user}","gists_url":"https://api.github.com/users/dee0512/gists{/gist_id}","starred_url":"https://api.github.com/users/dee0512/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dee0512/subscriptions","organizations_url":"https://api.github.com/users/dee0512/orgs","repos_url":"https://api.github.com/users/dee0512/repos","events_url":"https://api.github.com/users/dee0512/events{/privacy}","received_events_url":"https://api.github.com/users/dee0512/received_events","type":"User","site_admin":false}],"milestone":null,"comments":10,"created_at":"2019-06-20T19:09:16Z","updated_at":"2020-05-23T01:11:33Z","closed_at":"2020-05-23T01:11:33Z","author_association":"COLLABORATOR","active_lock_reason":null,"body":"So I'm currently wondering whether there is something inherently wrong with the order of things happening in the `EnvironmentPipeline`, which is meant for RL problems.\r\n\r\nIn RL, one step usually looks as follows: after the agent has taken an action `a`, we get back a reward `R` and a new state `s'`, and we update our action-value function `Q(s, a)` based on the reward. In case of function approximation (neural networks representing the action-value function), we update these based on the reward `R` they got for perceiving state `s` and choosing action `a`.\r\n\r\nCurrently, it seems that the `EnvironmentPipeline` implements things in a different order. First, we take an action based on how the spiking net perceived the environment's state:\r\nhttps://github.com/Hananel-Hazan/bindsnet/blob/fb1403d2b2c36ec816d4aa4322293e89ba68e61c/bindsnet/pipeline/environment_pipeline.py#L123\r\nand get back a new state and the reward for taking this action. So far so good.\r\n\r\nThen, this new state is fed into the spiking net, along with the reward we got for the past action:\r\nhttps://github.com/Hananel-Hazan/bindsnet/blob/fb1403d2b2c36ec816d4aa4322293e89ba68e61c/bindsnet/pipeline/environment_pipeline.py#L153\r\nand this is where I think it goes wrong. Because inside the `network.run()`, the new state is first pushed through the network:\r\nhttps://github.com/Hananel-Hazan/bindsnet/blob/fb1403d2b2c36ec816d4aa4322293e89ba68e61c/bindsnet/network/network.py#L312\r\n\r\nand only after are the weights updated according to the reward:\r\nhttps://github.com/Hananel-Hazan/bindsnet/blob/fb1403d2b2c36ec816d4aa4322293e89ba68e61c/bindsnet/network/network.py#L337\r\n\r\nwhich actually belonged to the old state. In other words, the state of the network when adjusting the weights is not representative of the network that actually achieved the reward.\r\n\r\nA more correct approach would be to either update the weights before the new state gets fed through, or take a new action based on output spikes generated by the new state (which gives a new reward) before the weight update.\r\n\r\nPlease tell me whether any of this makes sense, or whether I am forgetting/missing something! Thanks :)","closed_by":{"login":"Hananel-Hazan","id":3954715,"node_id":"MDQ6VXNlcjM5NTQ3MTU=","avatar_url":"https://avatars.githubusercontent.com/u/3954715?v=4","gravatar_id":"","url":"https://api.github.com/users/Hananel-Hazan","html_url":"https://github.com/Hananel-Hazan","followers_url":"https://api.github.com/users/Hananel-Hazan/followers","following_url":"https://api.github.com/users/Hananel-Hazan/following{/other_user}","gists_url":"https://api.github.com/users/Hananel-Hazan/gists{/gist_id}","starred_url":"https://api.github.com/users/Hananel-Hazan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Hananel-Hazan/subscriptions","organizations_url":"https://api.github.com/users/Hananel-Hazan/orgs","repos_url":"https://api.github.com/users/Hananel-Hazan/repos","events_url":"https://api.github.com/users/Hananel-Hazan/events{/privacy}","received_events_url":"https://api.github.com/users/Hananel-Hazan/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/285/timeline","performed_via_github_app":null,"state_reason":"completed"}