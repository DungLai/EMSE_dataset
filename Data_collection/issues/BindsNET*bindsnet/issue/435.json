{"url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435","repository_url":"https://api.github.com/repos/BindsNET/bindsnet","labels_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435/labels{/name}","comments_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435/comments","events_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435/events","html_url":"https://github.com/BindsNET/bindsnet/issues/435","id":757046213,"node_id":"MDU6SXNzdWU3NTcwNDYyMTM=","number":435,"title":"Wrong `if` in `bindsnet.network.topology.LocalConnection.compute`","user":{"login":"danielgafni","id":49863538,"node_id":"MDQ6VXNlcjQ5ODYzNTM4","avatar_url":"https://avatars.githubusercontent.com/u/49863538?v=4","gravatar_id":"","url":"https://api.github.com/users/danielgafni","html_url":"https://github.com/danielgafni","followers_url":"https://api.github.com/users/danielgafni/followers","following_url":"https://api.github.com/users/danielgafni/following{/other_user}","gists_url":"https://api.github.com/users/danielgafni/gists{/gist_id}","starred_url":"https://api.github.com/users/danielgafni/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielgafni/subscriptions","organizations_url":"https://api.github.com/users/danielgafni/orgs","repos_url":"https://api.github.com/users/danielgafni/repos","events_url":"https://api.github.com/users/danielgafni/events{/privacy}","received_events_url":"https://api.github.com/users/danielgafni/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2020-12-04T11:56:24Z","updated_at":"2020-12-14T00:59:04Z","closed_at":"2020-12-14T00:59:04Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hello!\r\n\r\nI'm working with a LocalConnection.\r\nPlease, check its `compute` method... As I understand, the first part of the `if` is wrong. The tensor it returns has a wrong shape for target layers of shapes with more than 2 dimensions. I'm providing a minimal code (I'm running it from a Jupyter Notebook) example that breaks for me:\r\n\r\n```python\r\nfrom torchvision.datasets import FashionMNIST\r\nfashion_mnist = FashionMNIST(root=\"fasion-mnist\", download=True)\r\n\r\nfrom bindsnet.encoding import PoissonEncoder\r\nimport torch\r\nfrom torchvision import transforms\r\n\r\ntime_max = 250\r\ndt = 1\r\nintensity = 127.5\r\ncrop = 20\r\n\r\n# DataLoader\r\nclass EncodedFashionMnist(torch.utils.data.Dataset):\r\n    def __init__(self, data, crop=20, time=250, intensity=127.5, dt=1):\r\n        self.data = data\r\n        self.crop = crop\r\n        self.time = time\r\n        self.intensity = intensity\r\n        self.dt = dt\r\n        \r\n        self.center_crop = transforms.CenterCrop(20)\r\n        self.poisson_encoder = PoissonEncoder(time=time_max, intensity=intensity, dt=dt)\r\n        self.transform = transforms.Compose([\r\n            self.center_crop, transforms.ToTensor(), self.poisson_encoder\r\n        ])\r\n        \r\n        \r\n    def __getitem__(self, i):\r\n        return {\"encoded_image\": self.transform(self.data[i][0]), \"label\": self.data[i][1]}\r\n    \r\n    def __len__(self):\r\n        return len(self.data)\r\n\r\nencoded_fashion_mnist = EncodedFashionMnist(fashion_mnist)\r\n\r\ntrain_dataloader = torch.utils.data.DataLoader(\r\n    encoded_fashion_mnist,\r\n    batch_size=1,\r\n)\r\n\r\n# Network\r\nfrom bindsnet.network import Network\r\nfrom bindsnet.learning import PostPre\r\nfrom bindsnet.network.monitors import Monitor\r\nfrom bindsnet.network.nodes import AdaptiveLIFNodes, Input\r\nfrom bindsnet.network.topology import LocalConnection, Connection\r\nfrom bindsnet.analysis.plotting import (\r\n    plot_input,\r\n    plot_spikes,\r\n    plot_conv2d_weights,\r\n    plot_voltages,\r\n)\r\nimport numpy as np\r\n\r\n# Hyperparameters\r\npadding = 0\r\nkernel_size = 12\r\nstride = 4\r\nconv_size = int((crop - kernel_size + 2 * padding) / stride) + 1\r\ntc_decay = 20.0\r\nthresh = -52\r\nrefrac = 2\r\nwmin = 0\r\nwmax = 1\r\nmean_weight = 0.4\r\nn_filters = 25\r\ntau_pos = 12.\r\ntau_neg = 5.\r\n\r\nnetwork = Network(learning=True)\r\nn_input = crop ** 2\r\ninput_layer = Input(\r\n    n=n_input, shape=(1, crop, crop), traces=True, refrac=refrac\r\n)\r\nn_output = n_filters * conv_size * conv_size\r\noutput_shape = int(np.sqrt(n_output))\r\noutput_layer = AdaptiveLIFNodes(\r\n    n=n_output,\r\n    shape=(n_filters, conv_size, conv_size),\r\n    traces=True,\r\n    thres=thresh,\r\n    tc_trace_pre=tau_pos,\r\n    tc_trace_post=tau_neg,\r\n    tc_decay=tc_decay,\r\n    theta_plus=0.05,\r\n    tc_theta_decay=1e6,\r\n    refrac=refrac,\r\n)\r\n\r\nkernel_prod = kernel_size ** 2\r\n\r\nnorm = mean_weight * kernel_prod\r\n\r\nconnection_XY = LocalConnection(\r\n    input_layer,\r\n    output_layer,\r\n    n_filters=n_filters,\r\n    kernel_size=kernel_size,\r\n    stride=stride,\r\n    update_rule=PostPre,\r\n    norm=norm,\r\n    nu=[1e-4, 1e-2],\r\n    wmin=wmin,\r\n    wmax=wmax,\r\n)\r\n\r\nnetwork.add_layer(input_layer, name=\"X\")\r\nnetwork.add_layer(output_layer, name=\"Y\")\r\nnetwork.add_connection(connection_XY, source=\"X\", target=\"Y\")\r\n\r\nfrom tqdm.notebook import tqdm\r\n\r\nfor batch in tqdm(train_dataloader):\r\n    network.run(inputs={\"X\": batch[\"encoded_image\"].squeeze(0)}, time=time_max, input_time_dim=0)\r\n```\r\n\r\nSo here is the method I'm talking about:\r\n\r\n```python\r\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\r\n        # language=rst\r\n        \"\"\"\r\n        Compute pre-activations given spikes using layer weights.\r\n        :param s: Incoming spikes.\r\n        :return: Incoming spikes multiplied by synaptic weights (with or without\r\n            decaying spike activation).\r\n        \"\"\"\r\n        # Compute multiplication of pre-activations by connection weights.\r\n        if self.w.shape[0] == self.source.n and self.w.shape[1] == self.target.n:\r\n            return s.float().view(s.size(0), -1) @ self.w + self.b\r\n        else:\r\n            a_post = (\r\n                s.float().view(s.size(0), -1)\r\n                @ self.w.view(self.source.n, self.target.n)\r\n                + self.b\r\n            )\r\n            return a_post.view(*self.target.shape)\r\n```\r\n\r\nMy code example above works if I change it to:\r\n\r\n```python\r\n    def compute(self, s: torch.Tensor) -> torch.Tensor:\r\n        # language=rst\r\n        \"\"\"\r\n        Compute pre-activations given spikes using layer weights.\r\n\r\n        :param s: Incoming spikes.\r\n        :return: Incoming spikes multiplied by synaptic weights (with or without\r\n            decaying spike activation).\r\n        \"\"\"\r\n        # Compute multiplication of pre-activations by connection weights.\r\n        a_post = (\r\n            s.float().view(s.size(0), -1)\r\n            @ self.w.view(self.source.n, self.target.n)\r\n            + self.b\r\n        )\r\n        return a_post.view(*self.target.shape)\r\n```\r\n\r\nSo basically the first part of the `if` is wrong. This check is not working for layer shapes of more than two dimensions, probably. In my case:\r\n\r\nthe output must have shape: (25, 3, 3)\r\nbut it has shape: (1, 225)\r\n\r\nI have:\r\nsource.n: 400\r\ntarget.n: 225\r\nself.w shape: (400, 225)\r\n\r\nSo the check in this `if` is taking this case wrong, because actually my Y layer shape is complex, not just (1, 225). \r\n\r\nThe second part of the if takes care of it by doing `.view(*self.target.shape)`. \r\nMaybe, this should be the default behavior? I can submit a pull request.\r\nPlease tell me if I need to clarify something or provide any files or logs.","closed_by":{"login":"Hananel-Hazan","id":3954715,"node_id":"MDQ6VXNlcjM5NTQ3MTU=","avatar_url":"https://avatars.githubusercontent.com/u/3954715?v=4","gravatar_id":"","url":"https://api.github.com/users/Hananel-Hazan","html_url":"https://github.com/Hananel-Hazan","followers_url":"https://api.github.com/users/Hananel-Hazan/followers","following_url":"https://api.github.com/users/Hananel-Hazan/following{/other_user}","gists_url":"https://api.github.com/users/Hananel-Hazan/gists{/gist_id}","starred_url":"https://api.github.com/users/Hananel-Hazan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Hananel-Hazan/subscriptions","organizations_url":"https://api.github.com/users/Hananel-Hazan/orgs","repos_url":"https://api.github.com/users/Hananel-Hazan/repos","events_url":"https://api.github.com/users/Hananel-Hazan/events{/privacy}","received_events_url":"https://api.github.com/users/Hananel-Hazan/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/BindsNET/bindsnet/issues/435/timeline","performed_via_github_app":null,"state_reason":"completed"}