{"url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80","repository_url":"https://api.github.com/repos/pavlin-policar/openTSNE","labels_url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80/labels{/name}","comments_url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80/comments","events_url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80/events","html_url":"https://github.com/pavlin-policar/openTSNE/issues/80","id":469784754,"node_id":"MDU6SXNzdWU0Njk3ODQ3NTQ=","number":80,"title":"Why is KL decreasing faster with default parameters compared to the FIt-SNE implementation?","user":{"login":"dkobak","id":8970231,"node_id":"MDQ6VXNlcjg5NzAyMzE=","avatar_url":"https://avatars.githubusercontent.com/u/8970231?v=4","gravatar_id":"","url":"https://api.github.com/users/dkobak","html_url":"https://github.com/dkobak","followers_url":"https://api.github.com/users/dkobak/followers","following_url":"https://api.github.com/users/dkobak/following{/other_user}","gists_url":"https://api.github.com/users/dkobak/gists{/gist_id}","starred_url":"https://api.github.com/users/dkobak/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dkobak/subscriptions","organizations_url":"https://api.github.com/users/dkobak/orgs","repos_url":"https://api.github.com/users/dkobak/repos","events_url":"https://api.github.com/users/dkobak/events{/privacy}","received_events_url":"https://api.github.com/users/dkobak/received_events","type":"User","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2019-07-18T13:56:05Z","updated_at":"2019-08-12T11:02:47Z","closed_at":"2019-08-12T11:01:28Z","author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I was under the impression that all important optimization parameters used in openTSNE are the same as in FIt-SNE:\r\n\r\n```\r\nclass openTSNE.TSNE(n_components=2, perplexity=30, learning_rate=200, \r\nearly_exaggeration_iter=250, early_exaggeration=12, n_iter=750, exaggeration=None, theta=0.5, \r\nn_interpolation_points=3, min_num_intervals=50, ints_in_interval=1, initialization='pca', \r\nmetric='euclidean', metric_params=None, initial_momentum=0.5, final_momentum=0.8, \r\nmin_grad_norm=1e-08, max_grad_norm=None, n_jobs=1, neighbors='approx', \r\nnegative_gradient_method='fft', callbacks=None, callbacks_every_iters=50, random_state=None)\r\n```\r\n\r\nand\r\n\r\n```\r\ndef fast_tsne(X, theta=.5, perplexity=30, map_dims=2, max_iter=1000, \r\n              stop_early_exag_iter=250, K=-1, sigma=-1, nbody_algo='FFT', knn_algo='annoy',\r\n              mom_switch_iter=250, momentum=.5, final_momentum=.8, learning_rate=200,\r\n              early_exag_coeff=12, no_momentum_during_exag=False, n_trees=50, \r\n              search_k=None, start_late_exag_iter=-1, late_exag_coeff=-1,\r\n              nterms=3, intervals_per_integer=1, min_num_intervals=50,            \r\n              seed=-1, initialization=None, load_affinities=None,\r\n              perplexity_list=None, df=1, return_loss=False, nthreads=None)\r\n```\r\n\r\nHowever, I have just noticed when using one particular dataset that KL decreases faster when I use openTSNE. When I run this:\r\n\r\n```\r\nZ = fast_tsne(X, perplexity=30, seed=42)\r\n```\r\n\r\nI get \r\n\r\n```\r\nIteration 50 (50 iterations in 0.91 seconds), cost 6.444297\r\nIteration 100 (50 iterations in 0.87 seconds), cost 6.270150\r\nIteration 150 (50 iterations in 0.90 seconds), cost 4.691937\r\nIteration 200 (50 iterations in 0.88 seconds), cost 4.266439\r\nIteration 250 (50 iterations in 0.86 seconds), cost 4.019699\r\nUnexaggerating Ps by 12.000000\r\nIteration 300 (50 iterations in 0.89 seconds), cost 3.404685\r\nIteration 350 (50 iterations in 0.91 seconds), cost 3.043569\r\nIteration 400 (50 iterations in 0.87 seconds), cost 2.794367\r\nIteration 450 (50 iterations in 0.92 seconds), cost 2.613639\r\nIteration 500 (50 iterations in 0.93 seconds), cost 2.471313\r\nIteration 550 (50 iterations in 1.14 seconds), cost 2.356686\r\nIteration 600 (50 iterations in 1.28 seconds), cost 2.271862\r\nIteration 650 (50 iterations in 1.47 seconds), cost 2.190770\r\nIteration 700 (50 iterations in 1.68 seconds), cost 2.127095\r\nIteration 750 (50 iterations in 1.98 seconds), cost 2.074458\r\nIteration 800 (50 iterations in 2.50 seconds), cost 2.027436\r\nIteration 850 (50 iterations in 2.42 seconds), cost 1.981508\r\nIteration 900 (50 iterations in 2.63 seconds), cost 1.940266\r\nIteration 950 (50 iterations in 3.39 seconds), cost 1.915046\r\nIteration 1000 (50 iterations in 3.72 seconds), cost 1.875520\r\nWrote the 23822 x 2 data matrix successfully.\r\n```\r\n\r\nwhereas when I run\r\n\r\n```\r\nZo = TSNE(initialization='random', random_state=42, callbacks=ErrorLogger(), n_jobs=-1).fit(X)\r\n```\r\n\r\nI get\r\n\r\n```\r\nIteration   50, KL divergence  6.0437, 50 iterations in 1.6216 sec\r\nIteration  100, KL divergence  4.5617, 50 iterations in 1.6222 sec\r\nIteration  150, KL divergence  4.2162, 50 iterations in 1.6578 sec\r\nIteration  200, KL divergence  4.0487, 50 iterations in 1.7035 sec\r\nIteration  250, KL divergence  3.9388, 50 iterations in 1.6815 sec\r\nIteration   50, KL divergence  3.1818, 50 iterations in 1.6840 sec\r\nIteration  100, KL divergence  2.7344, 50 iterations in 1.8720 sec\r\nIteration  150, KL divergence  2.4542, 50 iterations in 1.9890 sec\r\nIteration  200, KL divergence  2.2686, 50 iterations in 2.1730 sec\r\nIteration  250, KL divergence  2.1360, 50 iterations in 2.3945 sec\r\nIteration  300, KL divergence  2.0377, 50 iterations in 2.6713 sec\r\nIteration  350, KL divergence  1.9628, 50 iterations in 2.9563 sec\r\nIteration  400, KL divergence  1.9047, 50 iterations in 3.4022 sec\r\nIteration  450, KL divergence  1.8599, 50 iterations in 3.6570 sec\r\nIteration  500, KL divergence  1.8257, 50 iterations in 3.9738 sec\r\nIteration  550, KL divergence  1.7997, 50 iterations in 4.6501 sec\r\nIteration  600, KL divergence  1.7799, 50 iterations in 5.0196 sec\r\nIteration  650, KL divergence  1.7647, 50 iterations in 5.3889 sec\r\nIteration  700, KL divergence  1.7527, 50 iterations in 5.7517 sec\r\nIteration  750, KL divergence  1.7420, 50 iterations in 5.6339 sec\r\n```\r\n\r\nand indeed with FIt-SNE I get the embedding that spans roughly from -50 to 50, whereas with openTSNE it spans roughly from -80 to 80. I would expect this to happen if openTNSE used higher learning rate, but it's set to 200 in both implementations.","closed_by":{"login":"pavlin-policar","id":5758119,"node_id":"MDQ6VXNlcjU3NTgxMTk=","avatar_url":"https://avatars.githubusercontent.com/u/5758119?v=4","gravatar_id":"","url":"https://api.github.com/users/pavlin-policar","html_url":"https://github.com/pavlin-policar","followers_url":"https://api.github.com/users/pavlin-policar/followers","following_url":"https://api.github.com/users/pavlin-policar/following{/other_user}","gists_url":"https://api.github.com/users/pavlin-policar/gists{/gist_id}","starred_url":"https://api.github.com/users/pavlin-policar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pavlin-policar/subscriptions","organizations_url":"https://api.github.com/users/pavlin-policar/orgs","repos_url":"https://api.github.com/users/pavlin-policar/repos","events_url":"https://api.github.com/users/pavlin-policar/events{/privacy}","received_events_url":"https://api.github.com/users/pavlin-policar/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/pavlin-policar/openTSNE/issues/80/timeline","performed_via_github_app":null,"state_reason":"completed"}