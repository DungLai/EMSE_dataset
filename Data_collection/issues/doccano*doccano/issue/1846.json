{"url":"https://api.github.com/repos/doccano/doccano/issues/1846","repository_url":"https://api.github.com/repos/doccano/doccano","labels_url":"https://api.github.com/repos/doccano/doccano/issues/1846/labels{/name}","comments_url":"https://api.github.com/repos/doccano/doccano/issues/1846/comments","events_url":"https://api.github.com/repos/doccano/doccano/issues/1846/events","html_url":"https://github.com/doccano/doccano/issues/1846","id":1254191994,"node_id":"I_kwDOB-j9wM5KwXN6","number":1846,"title":"Add inter-rater agreement to statistics and raw output","user":{"login":"LizMcQuillan","id":11297053,"node_id":"MDQ6VXNlcjExMjk3MDUz","avatar_url":"https://avatars.githubusercontent.com/u/11297053?v=4","gravatar_id":"","url":"https://api.github.com/users/LizMcQuillan","html_url":"https://github.com/LizMcQuillan","followers_url":"https://api.github.com/users/LizMcQuillan/followers","following_url":"https://api.github.com/users/LizMcQuillan/following{/other_user}","gists_url":"https://api.github.com/users/LizMcQuillan/gists{/gist_id}","starred_url":"https://api.github.com/users/LizMcQuillan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/LizMcQuillan/subscriptions","organizations_url":"https://api.github.com/users/LizMcQuillan/orgs","repos_url":"https://api.github.com/users/LizMcQuillan/repos","events_url":"https://api.github.com/users/LizMcQuillan/events{/privacy}","received_events_url":"https://api.github.com/users/LizMcQuillan/received_events","type":"User","site_admin":false},"labels":[{"id":1203581726,"node_id":"MDU6TGFiZWwxMjAzNTgxNzI2","url":"https://api.github.com/repos/doccano/doccano/labels/feature%20request","name":"feature request","color":"FBC904","default":false,"description":"feature request for doccano"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2022-05-31T17:46:12Z","updated_at":"2022-05-31T23:59:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Feature description\r\n---------\r\nAdd inter-rater agreement metrics both at the label level and in aggregate.\r\n\r\nUse Case\r\n----------\r\nFor every project where I have more than 1 annotator, I compute inter-rater agreement at multiple levels. Having this in Doccano would save a bit of repetitive hassle. How I envision this working is that at the label level, we can see whether two or more annotators agree on a given label both for a single datapoint and a percentage aggregation of agreement between all annotators.\r\n\r\nCurrently, I need to export data from Doccano and reformat all of the data to even try to compute this. For example, with sequence labeling, Doccano records annotators in the order an annotator clicks the sequence on the screen and so in situations with multiple labels they are out of order making it so that I can't merely look for a match or no match in the list of labels. I need to parse every label and compute the agreement from there.\r\n\r\nEven a basic percentage match (e.g. annotators agree for 64% of labels) would be a big improvement from nothing.\r\n\r\n\r\n`Env: MacOS`","closed_by":null,"reactions":{"url":"https://api.github.com/repos/doccano/doccano/issues/1846/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/doccano/doccano/issues/1846/timeline","performed_via_github_app":null,"state_reason":null}