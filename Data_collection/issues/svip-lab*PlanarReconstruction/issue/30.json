{"url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30","repository_url":"https://api.github.com/repos/svip-lab/PlanarReconstruction","labels_url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30/labels{/name}","comments_url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30/comments","events_url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30/events","html_url":"https://github.com/svip-lab/PlanarReconstruction/issues/30","id":602461952,"node_id":"MDU6SXNzdWU2MDI0NjE5NTI=","number":30,"title":"3D point cloud model with RGB value (texture attached)","user":{"login":"JanineCHEN","id":37527041,"node_id":"MDQ6VXNlcjM3NTI3MDQx","avatar_url":"https://avatars.githubusercontent.com/u/37527041?v=4","gravatar_id":"","url":"https://api.github.com/users/JanineCHEN","html_url":"https://github.com/JanineCHEN","followers_url":"https://api.github.com/users/JanineCHEN/followers","following_url":"https://api.github.com/users/JanineCHEN/following{/other_user}","gists_url":"https://api.github.com/users/JanineCHEN/gists{/gist_id}","starred_url":"https://api.github.com/users/JanineCHEN/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JanineCHEN/subscriptions","organizations_url":"https://api.github.com/users/JanineCHEN/orgs","repos_url":"https://api.github.com/users/JanineCHEN/repos","events_url":"https://api.github.com/users/JanineCHEN/events{/privacy}","received_events_url":"https://api.github.com/users/JanineCHEN/received_events","type":"User","site_admin":false},"labels":[{"id":1247663089,"node_id":"MDU6TGFiZWwxMjQ3NjYzMDg5","url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/labels/question","name":"question","color":"d876e3","default":true,"description":"Further information is requested"}],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2020-04-18T13:35:26Z","updated_at":"2020-04-18T15:14:57Z","closed_at":"2020-04-18T15:14:57Z","author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\nIt seems like the write_ply.py can only generate 3D point cloud model with a single channel, and the RGB values of each pixel are missing after the 3D remapping. I guess I might still missing something when conducting the implementation. Highly appreciate if you could help clarify the step with texturized 3D remapping (keeping the original pixel values). Thank you!\r\n\r\nI have blended predict.py with write_ply.py as following:\r\n\r\n```\r\nimport os\r\nimport cv2\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom distutils.version import LooseVersion\r\n\r\nfrom sacred import Experiment\r\nfrom easydict import EasyDict as edict\r\n\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport torchvision.transforms as tf\r\n\r\nfrom models.baseline_same import Baseline as UNet\r\nfrom utils.disp import tensor_to_image\r\nfrom utils.disp import colors_256 as colors\r\nfrom bin_mean_shift import Bin_Mean_Shift\r\nfrom modules import get_coordinate_map\r\nfrom utils.loss import Q_loss\r\nfrom instance_parameter_loss import InstanceParameterLoss\r\n\r\nex = Experiment()\r\n\r\nfolder = './outputs'\r\nindex = 0\r\n\r\n@ex.main\r\ndef predict(_run, _log):\r\n    cfg = edict(_run.config)\r\n\r\n    torch.manual_seed(cfg.seed)\r\n    np.random.seed(cfg.seed)\r\n    random.seed(cfg.seed)\r\n\r\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n\r\n    # build network\r\n    network = UNet(cfg.model)\r\n\r\n    if not (cfg.resume_dir == 'None'):\r\n        model_dict = torch.load(cfg.resume_dir, map_location=lambda storage, loc: storage)\r\n        network.load_state_dict(model_dict)\r\n\r\n    # load nets into gpu\r\n    if cfg.num_gpus > 1 and torch.cuda.is_available():\r\n        network = torch.nn.DataParallel(network)\r\n    network.to(device)\r\n    network.eval()\r\n\r\n    transforms = tf.Compose([\r\n        tf.ToTensor(),\r\n        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n    ])\r\n\r\n    bin_mean_shift = Bin_Mean_Shift(device=device)\r\n    k_inv_dot_xy1 = get_coordinate_map(device)\r\n    instance_parameter_loss = InstanceParameterLoss(k_inv_dot_xy1)\r\n\r\n    h, w = 192, 256\r\n\r\n    focal_length = 517.97\r\n    offset_x = 320\r\n    offset_y = 240\r\n\r\n    K = [[focal_length, 0, offset_x],\r\n         [0, focal_length, offset_y],\r\n         [0, 0, 1]]\r\n\r\n    K_inv = np.linalg.inv(np.array(K))\r\n\r\n    K_inv_dot_xy_1 = np.zeros((3, h, w))\r\n\r\n    for y in range(h):\r\n        for x in range(w):\r\n            yy = float(y) / h * 480\r\n            xx = float(x) / w * 640\r\n                \r\n            ray = np.dot(K_inv,\r\n                         np.array([xx, yy, 1]).reshape(3, 1))\r\n            K_inv_dot_xy_1[:, y, x] = ray[:, 0]\r\n\r\n\r\n    with torch.no_grad():\r\n        image = cv2.imread(cfg.image_path)\r\n        # the network is trained with 192*256 and the intrinsic parameter is set as ScanNet\r\n        image = cv2.resize(image, (w, h))\r\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\r\n        image = Image.fromarray(image)\r\n        image = transforms(image)\r\n        image = image.to(device).unsqueeze(0)\r\n        # forward pass\r\n        logit, embedding, _, _, param = network(image)\r\n\r\n        prob = torch.sigmoid(logit[0])\r\n        \r\n        # infer per pixel depth using per pixel plane parameter, currently Q_loss need a dummy gt_depth as input\r\n        _, _, per_pixel_depth = Q_loss(param, k_inv_dot_xy1, torch.ones_like(logit))\r\n\r\n        # fast mean shift\r\n        segmentation, sampled_segmentation, sample_param = bin_mean_shift.test_forward(\r\n            prob, embedding[0], param, mask_threshold=0.1)\r\n\r\n        # since GT plane segmentation is somewhat noise, the boundary of plane in GT is not well aligned, \r\n        # we thus use avg_pool_2d to smooth the segmentation results\r\n        b = segmentation.t().view(1, -1, h, w)\r\n        pooling_b = torch.nn.functional.avg_pool2d(b, (7, 7), stride=1, padding=(3, 3))\r\n        b = pooling_b.view(-1, h*w).t()\r\n        segmentation = b\r\n\r\n        # infer instance depth\r\n        instance_loss, instance_depth, instance_abs_disntace, instance_parameter = instance_parameter_loss(\r\n            segmentation, sampled_segmentation, sample_param, torch.ones_like(logit), torch.ones_like(logit), False)\r\n\r\n        # return cluster results\r\n        segmentation = segmentation.cpu().numpy().argmax(axis=1)\r\n\r\n        # mask out non planar region\r\n        segmentation[prob.cpu().numpy().reshape(-1) <= 0.1] = 20\r\n        segmentation = segmentation.reshape(h, w)\r\n\r\n        # visualization and evaluation\r\n        image = tensor_to_image(image.cpu()[0])\r\n        mask = (prob > 0.1).float().cpu().numpy().reshape(h, w)\r\n        depth = instance_depth.cpu().numpy()[0, 0].reshape(h, w)\r\n        per_pixel_depth = per_pixel_depth.cpu().numpy()[0, 0].reshape(h, w)\r\n\r\n        # use per pixel depth for non planar region\r\n        depth = depth * (segmentation != 20) + per_pixel_depth * (segmentation == 20)\r\n\r\n        # change non planar to zero, so non planar region use the black color\r\n        segmentation += 1\r\n        segmentation[segmentation == 21] = 0\r\n\r\n        pred_seg = cv2.resize(np.stack([colors[segmentation, 0],\r\n                                        colors[segmentation, 1],\r\n                                        colors[segmentation, 2]], axis=2), (w, h))\r\n\r\n        # blend image\r\n        blend_pred = (pred_seg * 0.4 + image * 0.6).astype(np.uint8)\r\n\r\n        mask = cv2.resize((mask * 255).astype(np.uint8), (w, h))\r\n        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\r\n\r\n        # visualize depth map as PlaneNet\r\n        depth = 255 - np.clip(depth / 5 * 255, 0, 255).astype(np.uint8)\r\n        depth = cv2.cvtColor(cv2.resize(depth, (w, h)), cv2.COLOR_GRAY2BGR)\r\n\r\n        image_c = np.concatenate((image, pred_seg, blend_pred, mask, depth), axis=1)\r\n\r\n    imageFilename = str(index) + '_model_texture.png'\r\n    cv2.imwrite(folder + '/' + imageFilename, image_c)\r\n\r\n    # create face from segmentation\r\n    faces = []\r\n    for y in range(h-1):\r\n        for x in range(w-1):\r\n            segmentIndex = segmentation[y, x]\r\n            # ignore non planar region\r\n            if segmentIndex == 0:\r\n                continue\r\n\r\n            # add face if three pixel has same segmentatioin\r\n            depths = [depth[y][x], depth[y + 1][x], depth[y + 1][x + 1]]\r\n            if segmentation[y + 1, x] == segmentIndex and segmentation[y + 1, x + 1] == segmentIndex and np.array(depths).min() > 0 and np.array(depths).max() < 10:\r\n                faces.append((x, y, x, y + 1, x + 1, y + 1))\r\n\r\n            depths = [depth[y][x], depth[y][x + 1], depth[y + 1][x + 1]]\r\n            if segmentation[y][x + 1] == segmentIndex and segmentation[y + 1][x + 1] == segmentIndex and np.array(depths).min() > 0 and np.array(depths).max() < 10:\r\n                faces.append((x, y, x + 1, y + 1, x + 1, y))\r\n\r\n\r\n    with open(folder + '/' + str(index) + '_model.ply', 'w') as f:\r\n        header = \"\"\"ply\r\nformat ascii 1.0\r\ncomment VCGLIB generated\r\ncomment TextureFile \"\"\"\r\n        header += imageFilename\r\n        header += \"\"\"\r\nelement vertex \"\"\"\r\n        header += str(h * w)\r\n        header += \"\"\"\r\nproperty float x\r\nproperty float y\r\nproperty float z\r\nelement face \"\"\"\r\n        header += str(len(faces))\r\n        header += \"\"\"\r\nproperty list uchar int vertex_indices\r\nproperty list uchar float texcoord\r\nend_header\r\n\"\"\"\r\n        f.write(header)\r\n        for y in range(h):\r\n            for x in range(w):\r\n                segmentIndex = segmentation[y][x]\r\n                if segmentIndex == 20:\r\n                    f.write(\"0.0 0.0 0.0\\n\")\r\n                    continue\r\n                ray = K_inv_dot_xy_1[:, y, x]\r\n                X, Y, Z = ray * depth[y, x]\r\n                f.write(str(X) + ' ' + str(Y) + ' ' + str(Z) + '\\n')\r\n\r\n        for face in faces:\r\n            f.write('3 ')\r\n            for c in range(3):\r\n                f.write(str(face[c * 2 + 1] * w + face[c * 2]) + ' ')\r\n            f.write('6 ')\r\n            for c in range(3):\r\n                f.write(str(float(face[c * 2]) / w) + ' ' + str(1 - float(face[c * 2 + 1]) / h) + ' ')\r\n            f.write('\\n')\r\n        f.close()\r\n        pass\r\n    return\r\n\r\n\r\nif __name__ == '__main__':\r\n    assert LooseVersion(torch.__version__) >= LooseVersion('0.4.0'), \\\r\n        'PyTorch>=0.4.0 is required'\r\n\r\n    ex.add_config('./configs/predict.yaml')\r\n    ex.run_commandline()\r\n```\r\n\r\nAnd run the code using the same way as indicated in readme:\r\n`python predict.py with resume_dir=pretrained.pt image_path=images/test.png`\r\n\r\nAnd what I was able to get:\r\n![test](https://user-images.githubusercontent.com/37527041/79639327-cf016a80-81bd-11ea-90aa-47c6adbd9712.png)\r\n\r\nI am keen to reproduce the texturized 3D model, any help would be great!","closed_by":{"login":"bertjiazheng","id":8075304,"node_id":"MDQ6VXNlcjgwNzUzMDQ=","avatar_url":"https://avatars.githubusercontent.com/u/8075304?v=4","gravatar_id":"","url":"https://api.github.com/users/bertjiazheng","html_url":"https://github.com/bertjiazheng","followers_url":"https://api.github.com/users/bertjiazheng/followers","following_url":"https://api.github.com/users/bertjiazheng/following{/other_user}","gists_url":"https://api.github.com/users/bertjiazheng/gists{/gist_id}","starred_url":"https://api.github.com/users/bertjiazheng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bertjiazheng/subscriptions","organizations_url":"https://api.github.com/users/bertjiazheng/orgs","repos_url":"https://api.github.com/users/bertjiazheng/repos","events_url":"https://api.github.com/users/bertjiazheng/events{/privacy}","received_events_url":"https://api.github.com/users/bertjiazheng/received_events","type":"User","site_admin":false},"reactions":{"url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/svip-lab/PlanarReconstruction/issues/30/timeline","performed_via_github_app":null,"state_reason":"completed"}